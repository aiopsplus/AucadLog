[
    {
        "data_id": 2613,
        "instruction": "public void shutdown(JobStatus jobStatus) throws Exception {\n\tsynchronized (lock) {\n\t\tif (!shutdown) {\n\t\t\tshutdown = true;\n\n\t\t\tperiodicScheduling = false;\n\t\t\ttriggerRequestQueued = false;\n\n\t\t\t// shut down the thread that handles the timeouts and pending triggers\n\t\t\ttimer.shutdownNow();\n\n\t\t\t// clear and discard all pending checkpoints\n\t\t\tfor (PendingCheckpoint pending : pendingCheckpoints.values()) {\n\t\t\t\tpending.abortError(new Exception(\"Checkpoint Coordinator is shutting down\"));\n\t\t\t}\n\t\t\tpendingCheckpoints.clear();\n\n\t\t\tcompletedCheckpointStore.shutdown(jobStatus);\n\t\t\tcheckpointIdCounter.shutdown(jobStatus);\n\t\t}\n\t}\n}",
        "accept_response": "public void shutdown(JobStatus jobStatus) throws Exception {\n\tsynchronized (lock) {\n\t\tif (!shutdown) {\n\t\t\tshutdown = true;\n\t\t\tLOG.info(\"Stopping checkpoint coordinator for job {}.\", job);\n\n\t\t\tperiodicScheduling = false;\n\t\t\ttriggerRequestQueued = false;\n\n\t\t\t// shut down the thread that handles the timeouts and pending triggers\n\t\t\ttimer.shutdownNow();\n\n\t\t\t// clear and discard all pending checkpoints\n\t\t\tfor (PendingCheckpoint pending : pendingCheckpoints.values()) {\n\t\t\t\tpending.abortError(new Exception(\"Checkpoint Coordinator is shutting down\"));\n\t\t\t}\n\t\t\tpendingCheckpoints.clear();\n\n\t\t\tcompletedCheckpointStore.shutdown(jobStatus);\n\t\t\tcheckpointIdCounter.shutdown(jobStatus);\n\t\t}\n\t}\n}",
        "reject_response": "public void shutdown(JobStatus jobStatus) throws Exception {\n\tsynchronized (lock) {\n\t\tif (!shutdown) {\n\t\t\tshutdown = true;\n\t\t\tLOG.info(\"Stopping checkpoint coordinator for job \" + job);\n\n\t\t\tperiodicScheduling = false;\n\t\t\ttriggerRequestQueued = false;\n\n\t\t\t// shut down the thread that handles the timeouts and pending triggers\n\t\t\ttimer.shutdownNow();\n\n\t\t\t// clear and discard all pending checkpoints\n\t\t\tfor (PendingCheckpoint pending : pendingCheckpoints.values()) {\n\t\t\t\tpending.abortError(new Exception(\"Checkpoint Coordinator is shutting down\"));\n\t\t\t}\n\t\t\tpendingCheckpoints.clear();\n\n\t\t\tcompletedCheckpointStore.shutdown(jobStatus);\n\t\t\tcheckpointIdCounter.shutdown(jobStatus);\n\t\t}\n\t}\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3171,
        "instruction": "public void startContainerAsync(\n    Container container, ContainerLaunchContext containerLaunchContext) {\n  if (containers.putIfAbsent(container.getId(),\n      new StatefulContainer(this, container.getId())) != null) {\n    callbackHandler.onStartContainerError(container.getId(),\n        RPCUtil.getRemoteException(\"Container \" + container.getId() +\n            \" is already started or scheduled to start\"));\n  }\n  try {\n    events.put(new StartContainerEvent(container, containerLaunchContext));\n  } catch (InterruptedException e) {\n    callbackHandler.onStartContainerError(container.getId(), e);\n  }\n}",
        "accept_response": "public void startContainerAsync(\n    Container container, ContainerLaunchContext containerLaunchContext) {\n  if (containers.putIfAbsent(container.getId(),\n      new StatefulContainer(this, container.getId())) != null) {\n    callbackHandler.onStartContainerError(container.getId(),\n        RPCUtil.getRemoteException(\"Container \" + container.getId() +\n            \" is already started or scheduled to start\"));\n  }\n  try {\n    events.put(new StartContainerEvent(container, containerLaunchContext));\n  } catch (InterruptedException e) {\n    LOG.warn(\"Exception when scheduling the event of starting Container {}\", container.getId());\n    callbackHandler.onStartContainerError(container.getId(), e);\n  }\n}",
        "reject_response": "public void startContainerAsync(\n    Container container, ContainerLaunchContext containerLaunchContext) {\n  if (containers.putIfAbsent(container.getId(),\n      new StatefulContainer(this, container.getId())) != null) {\n    callbackHandler.onStartContainerError(container.getId(),\n        RPCUtil.getRemoteException(\"Container \" + container.getId() +\n            \" is already started or scheduled to start\"));\n  }\n  try {\n    events.put(new StartContainerEvent(container, containerLaunchContext));\n  } catch (InterruptedException e) {\n    LOG.warn(\"Exception when scheduling the event of starting Container \" +\n        container.getId());\n    callbackHandler.onStartContainerError(container.getId(), e);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2993,
        "instruction": "@Override\npublic List<String> lookupResource(ResourceLookupContext context) throws Exception {\n\n  List<String> ret \t\t   = new ArrayList<String>();\n  String \tserviceName  \t   = getServiceName();\n  String\tserviceType\t\t   = getServiceType();\n  Map<String,String> configs = getConfigs();\n  if(LOG.isDebugEnabled()) {\n    LOG.debug(\"==> RangerServicePresto.lookupResource() Context: (\" + context + \")\");\n  }\n  if (context != null) {\n    try {\n      if (!configs.containsKey(HadoopConfigHolder.RANGER_LOGIN_PASSWORD)) {\n        configs.put(HadoopConfigHolder.RANGER_LOGIN_PASSWORD, null);\n      }\n      ret  = PrestoResourceManager.getPrestoResources(serviceName, serviceType, configs,context);\n    } catch (Exception e) {\n      throw e;\n    }\n  }\n  if(LOG.isDebugEnabled()) {\n    LOG.debug(\"<== RangerServicePresto.lookupResource() Response: (\" + ret + \")\");\n  }\n  return ret;\n}",
        "accept_response": "@Override\npublic List<String> lookupResource(ResourceLookupContext context) throws Exception {\n\n  List<String> ret \t\t   = new ArrayList<String>();\n  String \tserviceName  \t   = getServiceName();\n  String\tserviceType\t\t   = getServiceType();\n  Map<String,String> configs = getConfigs();\n  if(LOG.isDebugEnabled()) {\n    LOG.debug(\"==> RangerServicePresto.lookupResource() Context: (\" + context + \")\");\n  }\n  if (context != null) {\n    try {\n      if (!configs.containsKey(HadoopConfigHolder.RANGER_LOGIN_PASSWORD)) {\n        configs.put(HadoopConfigHolder.RANGER_LOGIN_PASSWORD, null);\n      }\n      ret  = PrestoResourceManager.getPrestoResources(serviceName, serviceType, configs,context);\n    } catch (Exception e) {\n      LOG.error( \"<==RangerServicePresto.lookupResource() Error : \" + e);\n      throw e;\n    }\n  }\n  if(LOG.isDebugEnabled()) {\n    LOG.debug(\"<== RangerServicePresto.lookupResource() Response: (\" + ret + \")\");\n  }\n  return ret;\n}",
        "reject_response": "@Override\npublic List<String> lookupResource(ResourceLookupContext context) throws Exception {\n\n  List<String> ret \t\t   = new ArrayList<String>();\n  String \tserviceName  \t   = getServiceName();\n  String\tserviceType\t\t   = getServiceType();\n  Map<String,String> configs = getConfigs();\n  if(LOG.isDebugEnabled()) {\n    LOG.debug(\"==> RangerServicePresto.lookupResource() Context: (\" + context + \")\");\n  }\n  if (context != null) {\n    try {\n      if (!configs.containsKey(HadoopConfigHolder.RANGER_LOGIN_PASSWORD)) {\n        configs.put(HadoopConfigHolder.RANGER_LOGIN_PASSWORD, null);\n      }\n      ret  = PrestoResourceManager.getPrestoResources(serviceName, serviceType, configs,context);\n    } catch (Exception e) {\n      LOG.error( \"<==RangerServicePresto.lookupResource Error : \" + e);\n      throw e;\n    }\n  }\n  if(LOG.isDebugEnabled()) {\n    LOG.debug(\"<== RangerServicePresto.lookupResource() Response: (\" + ret + \")\");\n  }\n  return ret;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3221,
        "instruction": "public static void deleteRecursive(ZooKeeper zk, final String pathRoot, VoidCallback cb,\n    Object ctx)\n    throws InterruptedException, KeeperException\n{\n    PathUtils.validatePath(pathRoot);\n\n    List<String> tree = listSubTreeBFS(zk, pathRoot);\n    for (int i = tree.size() - 1; i >= 0 ; --i) {\n        //Delete the leaves first and eventually get rid of the root\n        zk.delete(tree.get(i), -1, cb, ctx); //Delete all versions of the node with -1.\n    }\n}",
        "accept_response": "public static void deleteRecursive(ZooKeeper zk, final String pathRoot, VoidCallback cb,\n    Object ctx)\n    throws InterruptedException, KeeperException\n{\n    PathUtils.validatePath(pathRoot);\n\n    List<String> tree = listSubTreeBFS(zk, pathRoot);\n    LOG.debug(\"Deleting tree: {}\", tree);\n    for (int i = tree.size() - 1; i >= 0 ; --i) {\n        //Delete the leaves first and eventually get rid of the root\n        zk.delete(tree.get(i), -1, cb, ctx); //Delete all versions of the node with -1.\n    }\n}",
        "reject_response": "public static void deleteRecursive(ZooKeeper zk, final String pathRoot, VoidCallback cb,\n    Object ctx)\n    throws InterruptedException, KeeperException\n{\n    PathUtils.validatePath(pathRoot);\n\n    List<String> tree = listSubTreeBFS(zk, pathRoot);\n    LOG.debug(\"Deleting {}\",tree);\n    LOG.debug(\"Deleting {} subnodes \",tree.size());\n    for (int i = tree.size() - 1; i >= 0 ; --i) {\n        //Delete the leaves first and eventually get rid of the root\n        zk.delete(tree.get(i), -1, cb, ctx); //Delete all versions of the node with -1.\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2711,
        "instruction": "public boolean isMergeable() {\n  if (!isAvailable()) {\n    return false;\n  }\n  if (hasReferences()) {\n    LOG.debug(\"Region \" + this\n        + \" is not mergeable because it has references\");\n    return false;\n  }\n\n  return true;\n}",
        "accept_response": "public boolean isMergeable() {\n  if (!isAvailable()) {\n    LOG.debug(\"Region \" + this\n        + \" is not mergeable because it is closing or closed\");\n    return false;\n  }\n  if (hasReferences()) {\n    LOG.debug(\"Region \" + this\n        + \" is not mergeable because it has references\");\n    return false;\n  }\n\n  return true;\n}",
        "reject_response": "public boolean isMergeable() {\n  if (!isAvailable()) {\n    LOG.debug(\"Region \" + getRegionInfo().getRegionNameAsString()\n    return false;\n  }\n  if (hasReferences()) {\n    LOG.debug(\"Region \" + this\n        + \" is not mergeable because it has references\");\n    return false;\n  }\n\n  return true;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2486,
        "instruction": "@Override\npublic final synchronized BigtableSource splitAtFraction(double fraction) {\n  ByteKey splitKey;\n  try {\n    splitKey = rangeTracker.getRange().interpolateKey(fraction);\n  } catch (IllegalArgumentException e) {\n    LOG.info(\n        \"%s: Failed to interpolate key for fraction %s.\", rangeTracker.getRange(), fraction);\n    return null;\n  }\n  BigtableSource primary = source.withEndKey(splitKey);\n  BigtableSource residual = source.withStartKey(splitKey);\n  if (!rangeTracker.trySplitAtPosition(splitKey)) {\n    return null;\n  }\n  this.source = primary;\n  return residual;\n}",
        "accept_response": "@Override\npublic final synchronized BigtableSource splitAtFraction(double fraction) {\n  ByteKey splitKey;\n  try {\n    splitKey = rangeTracker.getRange().interpolateKey(fraction);\n  } catch (IllegalArgumentException e) {\n    LOG.info(\n        \"%s: Failed to interpolate key for fraction %s.\", rangeTracker.getRange(), fraction);\n    return null;\n  }\n  LOG.debug(\n      \"Proposing to split {} at fraction {} (key {})\", rangeTracker, fraction, splitKey);\n  BigtableSource primary = source.withEndKey(splitKey);\n  BigtableSource residual = source.withStartKey(splitKey);\n  if (!rangeTracker.trySplitAtPosition(splitKey)) {\n    return null;\n  }\n  this.source = primary;\n  return residual;\n}",
        "reject_response": "@Override\npublic final synchronized BigtableSource splitAtFraction(double fraction) {\n  ByteKey splitKey;\n  try {\n    splitKey = rangeTracker.getRange().interpolateKey(fraction);\n  } catch (IllegalArgumentException e) {\n    LOG.info(\n        \"%s: Failed to interpolate key for fraction %s.\", rangeTracker.getRange(), fraction);\n    return null;\n  }\n  logger.debug(\n  BigtableSource primary = source.withEndKey(splitKey);\n  BigtableSource residual = source.withStartKey(splitKey);\n  if (!rangeTracker.trySplitAtPosition(splitKey)) {\n    return null;\n  }\n  this.source = primary;\n  return residual;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3000,
        "instruction": "public static List<String> getPrestoResources(String serviceName, String serviceType, Map<String, String> configs, ResourceLookupContext context) throws Exception {\n\n  String userInput = context.getUserInput();\n  String resource = context.getResourceName();\n  Map<String, List<String>> resourceMap = context.getResources();\n  List<String> resultList = null;\n  List<String> catalogList = null;\n  List<String> schemaList = null;\n  List<String> tableList = null;\n  List<String> columnList = null;\n  String catalogName = null;\n  String schemaName = null;\n  String tableName = null;\n  String columnName = null;\n\n  if (userInput != null && resource != null) {\n    if (resourceMap != null && !resourceMap.isEmpty()) {\n      catalogList = resourceMap.get(CATALOG);\n      schemaList = resourceMap.get(SCHEMA);\n      tableList = resourceMap.get(TABLE);\n      columnList = resourceMap.get(COLUMN);\n    }\n    switch (resource.trim().toLowerCase()) {\n      case CATALOG:\n        catalogName = userInput;\n        break;\n      case SCHEMA:\n        schemaName = userInput;\n      case TABLE:\n        tableName = userInput;\n        break;\n      case COLUMN:\n        columnName = userInput;\n        break;\n      default:\n        break;\n    }\n  }\n\n  if (serviceName != null && userInput != null) {\n    try {\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"==> PrestoResourceManager.getPrestoResources() UserInput: \\\"\" + userInput + \"\\\" configs: \" + configs + \" catalogList: \" + catalogList + \" tableList: \"\n          + tableList + \" columnList: \" + columnList);\n      }\n\n      final PrestoClient prestoClient = new PrestoConnectionManager().getPrestoConnection(serviceName, serviceType, configs);\n\n      Callable<List<String>> callableObj = null;\n\n      final String finalCatalogName;\n      final String finalSchemaName;\n      final String finalTableName;\n      final String finalColumnName;\n\n      final List<String> finalCatalogList = catalogList;\n      final List<String> finalSchemaList = schemaList;\n      final List<String> finalTableList = tableList;\n      final List<String> finalColumnList = columnList;\n\n      if (prestoClient != null) {\n        if (catalogName != null && !catalogName.isEmpty()) {\n          finalCatalogName = catalogName;\n          callableObj = new Callable<List<String>>() {\n            @Override\n            public List<String> call() throws Exception {\n              return prestoClient.getCatalogList(finalCatalogName, finalCatalogList);\n            }\n          };\n        } else if (schemaName != null && !schemaName.isEmpty()) {\n          finalSchemaName = schemaName;\n          callableObj = new Callable<List<String>>() {\n            @Override\n            public List<String> call() throws Exception {\n              return prestoClient.getSchemaList(finalSchemaName, finalCatalogList, finalSchemaList);\n            }\n          };\n        } else if (tableName != null && !tableName.isEmpty()) {\n          finalTableName = tableName;\n          callableObj = new Callable<List<String>>() {\n            @Override\n            public List<String> call() throws Exception {\n              return prestoClient.getTableList(finalTableName, finalCatalogList, finalSchemaList, finalTableList);\n            }\n          };\n        } else if (columnName != null && !columnName.isEmpty()) {\n          // Column names are matched by the wildcardmatcher\n          columnName += \"*\";\n          finalColumnName = columnName;\n          callableObj = new Callable<List<String>>() {\n            @Override\n            public List<String> call() throws Exception {\n              return prestoClient.getColumnList(finalColumnName, finalCatalogList, finalSchemaList, finalTableList, finalColumnList);\n            }\n          };\n        }\n        if (callableObj != null) {\n          synchronized (prestoClient) {\n            resultList = TimedEventUtil.timedTask(callableObj, 5, TimeUnit.SECONDS);\n          }\n        } else {\n          LOG.error(\"Could not initiate a PrestoClient timedTask\");\n        }\n      }\n    } catch (Exception e) {\n      LOG.error(\"Unable to get Presto resource\", e);\n      throw e;\n    }\n  }\n  return resultList;\n}",
        "accept_response": "public static List<String> getPrestoResources(String serviceName, String serviceType, Map<String, String> configs, ResourceLookupContext context) throws Exception {\n\n  String userInput = context.getUserInput();\n  String resource = context.getResourceName();\n  Map<String, List<String>> resourceMap = context.getResources();\n  List<String> resultList = null;\n  List<String> catalogList = null;\n  List<String> schemaList = null;\n  List<String> tableList = null;\n  List<String> columnList = null;\n  String catalogName = null;\n  String schemaName = null;\n  String tableName = null;\n  String columnName = null;\n\n\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"<== PrestoResourceMgr.getPrestoResources() UserInput: \\\"\" + userInput + \"\\\" resource : \" + resource + \" resourceMap: \" + resourceMap);\n  }\n\n  if (userInput != null && resource != null) {\n    if (resourceMap != null && !resourceMap.isEmpty()) {\n      catalogList = resourceMap.get(CATALOG);\n      schemaList = resourceMap.get(SCHEMA);\n      tableList = resourceMap.get(TABLE);\n      columnList = resourceMap.get(COLUMN);\n    }\n    switch (resource.trim().toLowerCase()) {\n      case CATALOG:\n        catalogName = userInput;\n        break;\n      case SCHEMA:\n        schemaName = userInput;\n      case TABLE:\n        tableName = userInput;\n        break;\n      case COLUMN:\n        columnName = userInput;\n        break;\n      default:\n        break;\n    }\n  }\n\n  if (serviceName != null && userInput != null) {\n    try {\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"==> PrestoResourceManager.getPrestoResources() UserInput: \\\"\" + userInput + \"\\\" configs: \" + configs + \" catalogList: \" + catalogList + \" tableList: \"\n          + tableList + \" columnList: \" + columnList);\n      }\n\n      final PrestoClient prestoClient = new PrestoConnectionManager().getPrestoConnection(serviceName, serviceType, configs);\n\n      Callable<List<String>> callableObj = null;\n\n      final String finalCatalogName;\n      final String finalSchemaName;\n      final String finalTableName;\n      final String finalColumnName;\n\n      final List<String> finalCatalogList = catalogList;\n      final List<String> finalSchemaList = schemaList;\n      final List<String> finalTableList = tableList;\n      final List<String> finalColumnList = columnList;\n\n      if (prestoClient != null) {\n        if (catalogName != null && !catalogName.isEmpty()) {\n          finalCatalogName = catalogName;\n          callableObj = new Callable<List<String>>() {\n            @Override\n            public List<String> call() throws Exception {\n              return prestoClient.getCatalogList(finalCatalogName, finalCatalogList);\n            }\n          };\n        } else if (schemaName != null && !schemaName.isEmpty()) {\n          finalSchemaName = schemaName;\n          callableObj = new Callable<List<String>>() {\n            @Override\n            public List<String> call() throws Exception {\n              return prestoClient.getSchemaList(finalSchemaName, finalCatalogList, finalSchemaList);\n            }\n          };\n        } else if (tableName != null && !tableName.isEmpty()) {\n          finalTableName = tableName;\n          callableObj = new Callable<List<String>>() {\n            @Override\n            public List<String> call() throws Exception {\n              return prestoClient.getTableList(finalTableName, finalCatalogList, finalSchemaList, finalTableList);\n            }\n          };\n        } else if (columnName != null && !columnName.isEmpty()) {\n          // Column names are matched by the wildcardmatcher\n          columnName += \"*\";\n          finalColumnName = columnName;\n          callableObj = new Callable<List<String>>() {\n            @Override\n            public List<String> call() throws Exception {\n              return prestoClient.getColumnList(finalColumnName, finalCatalogList, finalSchemaList, finalTableList, finalColumnList);\n            }\n          };\n        }\n        if (callableObj != null) {\n          synchronized (prestoClient) {\n            resultList = TimedEventUtil.timedTask(callableObj, 5, TimeUnit.SECONDS);\n          }\n        } else {\n          LOG.error(\"Could not initiate a PrestoClient timedTask\");\n        }\n      }\n    } catch (Exception e) {\n      LOG.error(\"Unable to get Presto resource\", e);\n      throw e;\n    }\n  }\n  return resultList;\n}",
        "reject_response": "public static List<String> getPrestoResources(String serviceName, String serviceType, Map<String, String> configs, ResourceLookupContext context) throws Exception {\n\n  String userInput = context.getUserInput();\n  String resource = context.getResourceName();\n  Map<String, List<String>> resourceMap = context.getResources();\n  List<String> resultList = null;\n  List<String> catalogList = null;\n  List<String> schemaList = null;\n  List<String> tableList = null;\n  List<String> columnList = null;\n  String catalogName = null;\n  String schemaName = null;\n  String tableName = null;\n  String columnName = null;\n\n\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"<== PrestoResourceMgr.getPrestoResources()  UserInput: \\\"\" + userInput + \"\\\" resource : \" + resource + \" resourceMap: \" + resourceMap);\n  }\n\n  if (userInput != null && resource != null) {\n    if (resourceMap != null && !resourceMap.isEmpty()) {\n      catalogList = resourceMap.get(CATALOG);\n      schemaList = resourceMap.get(SCHEMA);\n      tableList = resourceMap.get(TABLE);\n      columnList = resourceMap.get(COLUMN);\n    }\n    switch (resource.trim().toLowerCase()) {\n      case CATALOG:\n        catalogName = userInput;\n        break;\n      case SCHEMA:\n        schemaName = userInput;\n      case TABLE:\n        tableName = userInput;\n        break;\n      case COLUMN:\n        columnName = userInput;\n        break;\n      default:\n        break;\n    }\n  }\n\n  if (serviceName != null && userInput != null) {\n    try {\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"==> PrestoResourceManager.getPrestoResources() UserInput: \\\"\" + userInput + \"\\\" configs: \" + configs + \" catalogList: \" + catalogList + \" tableList: \"\n          + tableList + \" columnList: \" + columnList);\n      }\n\n      final PrestoClient prestoClient = new PrestoConnectionManager().getPrestoConnection(serviceName, serviceType, configs);\n\n      Callable<List<String>> callableObj = null;\n\n      final String finalCatalogName;\n      final String finalSchemaName;\n      final String finalTableName;\n      final String finalColumnName;\n\n      final List<String> finalCatalogList = catalogList;\n      final List<String> finalSchemaList = schemaList;\n      final List<String> finalTableList = tableList;\n      final List<String> finalColumnList = columnList;\n\n      if (prestoClient != null) {\n        if (catalogName != null && !catalogName.isEmpty()) {\n          finalCatalogName = catalogName;\n          callableObj = new Callable<List<String>>() {\n            @Override\n            public List<String> call() throws Exception {\n              return prestoClient.getCatalogList(finalCatalogName, finalCatalogList);\n            }\n          };\n        } else if (schemaName != null && !schemaName.isEmpty()) {\n          finalSchemaName = schemaName;\n          callableObj = new Callable<List<String>>() {\n            @Override\n            public List<String> call() throws Exception {\n              return prestoClient.getSchemaList(finalSchemaName, finalCatalogList, finalSchemaList);\n            }\n          };\n        } else if (tableName != null && !tableName.isEmpty()) {\n          finalTableName = tableName;\n          callableObj = new Callable<List<String>>() {\n            @Override\n            public List<String> call() throws Exception {\n              return prestoClient.getTableList(finalTableName, finalCatalogList, finalSchemaList, finalTableList);\n            }\n          };\n        } else if (columnName != null && !columnName.isEmpty()) {\n          // Column names are matched by the wildcardmatcher\n          columnName += \"*\";\n          finalColumnName = columnName;\n          callableObj = new Callable<List<String>>() {\n            @Override\n            public List<String> call() throws Exception {\n              return prestoClient.getColumnList(finalColumnName, finalCatalogList, finalSchemaList, finalTableList, finalColumnList);\n            }\n          };\n        }\n        if (callableObj != null) {\n          synchronized (prestoClient) {\n            resultList = TimedEventUtil.timedTask(callableObj, 5, TimeUnit.SECONDS);\n          }\n        } else {\n          LOG.error(\"Could not initiate a PrestoClient timedTask\");\n        }\n      }\n    } catch (Exception e) {\n      LOG.error(\"Unable to get Presto resource\", e);\n      throw e;\n    }\n  }\n  return resultList;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2803,
        "instruction": "public boolean storeCheckpoint(GridTaskSessionInternal ses,\n    String key,\n    Object state,\n    ComputeTaskSessionScope scope,\n    long timeout,\n    boolean override)\n    throws IgniteCheckedException\n{\n    if (!enabled())\n        return false;\n\n    assert ses != null;\n    assert key != null;\n\n    long now = U.currentTimeMillis();\n\n    boolean saved = false;\n\n    try {\n        switch (scope) {\n            case GLOBAL_SCOPE: {\n                byte[] data = state == null ? null : U.marshal(marsh, state);\n\n                saved = getSpi(ses.getCheckpointSpi()).saveCheckpoint(key, data, timeout, override);\n\n                if (saved)\n                    record(EVT_CHECKPOINT_SAVED, key);\n\n                break;\n            }\n\n            case SESSION_SCOPE: {\n                if (closedSess.contains(ses.getId())) {\n                    U.warn(log, S.toString(\"Checkpoint will not be saved due to session invalidation\",\n                        \"key\", key, true,\n                        \"val\", state, true,\n                        \"ses\", ses, false),\n                        \"Checkpoint will not be saved due to session invalidation.\");\n\n                    break;\n                }\n\n                if (now > ses.getEndTime()) {\n                    U.warn(log, S.toString(\"Checkpoint will not be saved due to session timeout\",\n                        \"key\", key, true,\n                        \"val\", state, true,\n                        \"ses\", ses, false),\n                        \"Checkpoint will not be saved due to session timeout.\");\n\n                    break;\n                }\n\n                if (now + timeout > ses.getEndTime() || now + timeout < 0)\n                    timeout = ses.getEndTime() - now;\n\n                // Save it first to avoid getting null value on another node.\n                byte[] data = state == null ? null : U.marshal(marsh, state);\n\n                Set<String> keys = keyMap.get(ses.getId());\n\n                if (keys == null) {\n                    Set<String> old = keyMap.putIfAbsent(ses.getId(),\n                        (CheckpointSet)(keys = new CheckpointSet(ses.session())));\n\n                    if (old != null)\n                        keys = old;\n\n                    // Double check.\n                    if (closedSess.contains(ses.getId())) {\n                        U.warn(log, S.toString(\"Checkpoint will not be saved due to session invalidation\",\n                            \"key\", key, true,\n                            \"val\", state, true,\n                            \"ses\", ses, false),\n                            \"Checkpoint will not be saved due to session invalidation.\");\n\n                        keyMap.remove(ses.getId(), keys);\n\n                        break;\n                    }\n                }\n\n                if (log.isDebugEnabled())\n\n                // Note: Check that keys exists because session may be invalidated during saving\n                // checkpoint from GridFuture.\n                if (keys != null) {\n                    // Notify master node.\n                    if (ses.getJobId() != null) {\n                        ClusterNode node = ctx.discovery().node(ses.getTaskNodeId());\n\n                        if (node != null)\n                            ctx.io().sendToGridTopic(\n                                node,\n                                TOPIC_CHECKPOINT,\n                                new GridCheckpointRequest(ses.getId(), key, ses.getCheckpointSpi()),\n                                GridIoPolicy.PUBLIC_POOL);\n                    }\n\n                    saved = getSpi(ses.getCheckpointSpi()).saveCheckpoint(key, data, timeout, override);\n\n                    if (saved) {\n                        keys.add(key);\n\n                        record(EVT_CHECKPOINT_SAVED, key);\n                    }\n                }\n\n                break;\n            }\n\n            default:\n                assert false : \"Unknown checkpoint scope: \" + scope;\n        }\n    }\n    catch (IgniteSpiException e) {\n        throw new IgniteCheckedException(S.toString(\"Failed to save checkpoint\",\n            \"key\", key, true,\n            \"val\", state, true,\n            \"scope\", scope, false,\n            \"timeout\", timeout, false), e);\n    }\n\n    return saved;\n}",
        "accept_response": "public boolean storeCheckpoint(GridTaskSessionInternal ses,\n    String key,\n    Object state,\n    ComputeTaskSessionScope scope,\n    long timeout,\n    boolean override)\n    throws IgniteCheckedException\n{\n    if (!enabled())\n        return false;\n\n    assert ses != null;\n    assert key != null;\n\n    long now = U.currentTimeMillis();\n\n    boolean saved = false;\n\n    try {\n        switch (scope) {\n            case GLOBAL_SCOPE: {\n                byte[] data = state == null ? null : U.marshal(marsh, state);\n\n                saved = getSpi(ses.getCheckpointSpi()).saveCheckpoint(key, data, timeout, override);\n\n                if (saved)\n                    record(EVT_CHECKPOINT_SAVED, key);\n\n                break;\n            }\n\n            case SESSION_SCOPE: {\n                if (closedSess.contains(ses.getId())) {\n                    U.warn(log, S.toString(\"Checkpoint will not be saved due to session invalidation\",\n                        \"key\", key, true,\n                        \"val\", state, true,\n                        \"ses\", ses, false),\n                        \"Checkpoint will not be saved due to session invalidation.\");\n\n                    break;\n                }\n\n                if (now > ses.getEndTime()) {\n                    U.warn(log, S.toString(\"Checkpoint will not be saved due to session timeout\",\n                        \"key\", key, true,\n                        \"val\", state, true,\n                        \"ses\", ses, false),\n                        \"Checkpoint will not be saved due to session timeout.\");\n\n                    break;\n                }\n\n                if (now + timeout > ses.getEndTime() || now + timeout < 0)\n                    timeout = ses.getEndTime() - now;\n\n                // Save it first to avoid getting null value on another node.\n                byte[] data = state == null ? null : U.marshal(marsh, state);\n\n                Set<String> keys = keyMap.get(ses.getId());\n\n                if (keys == null) {\n                    Set<String> old = keyMap.putIfAbsent(ses.getId(),\n                        (CheckpointSet)(keys = new CheckpointSet(ses.session())));\n\n                    if (old != null)\n                        keys = old;\n\n                    // Double check.\n                    if (closedSess.contains(ses.getId())) {\n                        U.warn(log, S.toString(\"Checkpoint will not be saved due to session invalidation\",\n                            \"key\", key, true,\n                            \"val\", state, true,\n                            \"ses\", ses, false),\n                            \"Checkpoint will not be saved due to session invalidation.\");\n\n                        keyMap.remove(ses.getId(), keys);\n\n                        break;\n                    }\n                }\n\n                if (log.isDebugEnabled())\n                    log.debug(S.toString(\"Resolved keys for session\",\n                        \"keys\", keys, true,\n                        \"ses\", ses, false,\n                        \"keyMap\", keyMap, false));\n\n                // Note: Check that keys exists because session may be invalidated during saving\n                // checkpoint from GridFuture.\n                if (keys != null) {\n                    // Notify master node.\n                    if (ses.getJobId() != null) {\n                        ClusterNode node = ctx.discovery().node(ses.getTaskNodeId());\n\n                        if (node != null)\n                            ctx.io().sendToGridTopic(\n                                node,\n                                TOPIC_CHECKPOINT,\n                                new GridCheckpointRequest(ses.getId(), key, ses.getCheckpointSpi()),\n                                GridIoPolicy.PUBLIC_POOL);\n                    }\n\n                    saved = getSpi(ses.getCheckpointSpi()).saveCheckpoint(key, data, timeout, override);\n\n                    if (saved) {\n                        keys.add(key);\n\n                        record(EVT_CHECKPOINT_SAVED, key);\n                    }\n                }\n\n                break;\n            }\n\n            default:\n                assert false : \"Unknown checkpoint scope: \" + scope;\n        }\n    }\n    catch (IgniteSpiException e) {\n        throw new IgniteCheckedException(S.toString(\"Failed to save checkpoint\",\n            \"key\", key, true,\n            \"val\", state, true,\n            \"scope\", scope, false,\n            \"timeout\", timeout, false), e);\n    }\n\n    return saved;\n}",
        "reject_response": "public boolean storeCheckpoint(GridTaskSessionInternal ses,\n    String key,\n    Object state,\n    ComputeTaskSessionScope scope,\n    long timeout,\n    boolean override)\n    throws IgniteCheckedException\n{\n    if (!enabled())\n        return false;\n\n    assert ses != null;\n    assert key != null;\n\n    long now = U.currentTimeMillis();\n\n    boolean saved = false;\n\n    try {\n        switch (scope) {\n            case GLOBAL_SCOPE: {\n                byte[] data = state == null ? null : U.marshal(marsh, state);\n\n                saved = getSpi(ses.getCheckpointSpi()).saveCheckpoint(key, data, timeout, override);\n\n                if (saved)\n                    record(EVT_CHECKPOINT_SAVED, key);\n\n                break;\n            }\n\n            case SESSION_SCOPE: {\n                if (closedSess.contains(ses.getId())) {\n                    U.warn(log, S.toString(\"Checkpoint will not be saved due to session invalidation\",\n                        \"key\", key, true,\n                        \"val\", state, true,\n                        \"ses\", ses, false),\n                        \"Checkpoint will not be saved due to session invalidation.\");\n\n                    break;\n                }\n\n                if (now > ses.getEndTime()) {\n                    U.warn(log, S.toString(\"Checkpoint will not be saved due to session timeout\",\n                        \"key\", key, true,\n                        \"val\", state, true,\n                        \"ses\", ses, false),\n                        \"Checkpoint will not be saved due to session timeout.\");\n\n                    break;\n                }\n\n                if (now + timeout > ses.getEndTime() || now + timeout < 0)\n                    timeout = ses.getEndTime() - now;\n\n                // Save it first to avoid getting null value on another node.\n                byte[] data = state == null ? null : U.marshal(marsh, state);\n\n                Set<String> keys = keyMap.get(ses.getId());\n\n                if (keys == null) {\n                    Set<String> old = keyMap.putIfAbsent(ses.getId(),\n                        (CheckpointSet)(keys = new CheckpointSet(ses.session())));\n\n                    if (old != null)\n                        keys = old;\n\n                    // Double check.\n                    if (closedSess.contains(ses.getId())) {\n                        U.warn(log, S.toString(\"Checkpoint will not be saved due to session invalidation\",\n                            \"key\", key, true,\n                            \"val\", state, true,\n                            \"ses\", ses, false),\n                            \"Checkpoint will not be saved due to session invalidation.\");\n\n                        keyMap.remove(ses.getId(), keys);\n\n                        break;\n                    }\n                }\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Resolved keys for session [keys=\" + keys + \", ses=\" + ses +\n                        \", keyMap=\" + keyMap + ']');\n\n                // Note: Check that keys exists because session may be invalidated during saving\n                // checkpoint from GridFuture.\n                if (keys != null) {\n                    // Notify master node.\n                    if (ses.getJobId() != null) {\n                        ClusterNode node = ctx.discovery().node(ses.getTaskNodeId());\n\n                        if (node != null)\n                            ctx.io().sendToGridTopic(\n                                node,\n                                TOPIC_CHECKPOINT,\n                                new GridCheckpointRequest(ses.getId(), key, ses.getCheckpointSpi()),\n                                GridIoPolicy.PUBLIC_POOL);\n                    }\n\n                    saved = getSpi(ses.getCheckpointSpi()).saveCheckpoint(key, data, timeout, override);\n\n                    if (saved) {\n                        keys.add(key);\n\n                        record(EVT_CHECKPOINT_SAVED, key);\n                    }\n                }\n\n                break;\n            }\n\n            default:\n                assert false : \"Unknown checkpoint scope: \" + scope;\n        }\n    }\n    catch (IgniteSpiException e) {\n        throw new IgniteCheckedException(S.toString(\"Failed to save checkpoint\",\n            \"key\", key, true,\n            \"val\", state, true,\n            \"scope\", scope, false,\n            \"timeout\", timeout, false), e);\n    }\n\n    return saved;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2955,
        "instruction": "protected void shutdown() {\n    this.shutdown = true;\n\n    if (nifiServer != null) {\n        nifiServer.stop();\n    }\n    if (bootstrapListener != null) {\n        bootstrapListener.stop();\n    }\n    LOGGER.info(\"Application Server shutdown completed\");\n}",
        "accept_response": "protected void shutdown() {\n    this.shutdown = true;\n\n    LOGGER.info(\"Application Server shutdown started\");\n    if (nifiServer != null) {\n        nifiServer.stop();\n    }\n    if (bootstrapListener != null) {\n        bootstrapListener.stop();\n    }\n    LOGGER.info(\"Application Server shutdown completed\");\n}",
        "reject_response": "protected void shutdown() {\n    this.shutdown = true;\n\n    LOGGER.info(\"Initiating shutdown of Jetty web server...\");\n    if (nifiServer != null) {\n        nifiServer.stop();\n    }\n    if (bootstrapListener != null) {\n        bootstrapListener.stop();\n    }\n    LOGGER.info(\"Application Server shutdown completed\");\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3162,
        "instruction": "protected void sendErrorResponse(HttpServletRequest request, HttpServletResponse response, int code, Exception e) {\n    try {\n        // WW-1977: Only put errors in the request when code is a 500 error\n        if (code == HttpServletResponse.SC_INTERNAL_SERVER_ERROR) {\n            // WW-4103: Only logs error when application error occurred, not Struts error\n            // send a http error response to use the servlet defined error handler\n            // make the exception available to the web.xml defined error page\n            request.setAttribute(\"javax.servlet.error.exception\", e);\n\n            // for compatibility\n            request.setAttribute(\"javax.servlet.jsp.jspException\", e);\n        }\n\n        // send the error response\n        response.sendError(code, e.getMessage());\n    } catch (IOException e1) {\n        // we're already sending an error, not much else we can do if more stuff breaks\n    }\n}",
        "accept_response": "protected void sendErrorResponse(HttpServletRequest request, HttpServletResponse response, int code, Exception e) {\n    try {\n        // WW-1977: Only put errors in the request when code is a 500 error\n        if (code == HttpServletResponse.SC_INTERNAL_SERVER_ERROR) {\n            // WW-4103: Only logs error when application error occurred, not Struts error\n            LOG.error(\"Exception occurred during processing request: {}\", e.getMessage(), e);\n            // send a http error response to use the servlet defined error handler\n            // make the exception available to the web.xml defined error page\n            request.setAttribute(\"javax.servlet.error.exception\", e);\n\n            // for compatibility\n            request.setAttribute(\"javax.servlet.jsp.jspException\", e);\n        }\n\n        // send the error response\n        response.sendError(code, e.getMessage());\n    } catch (IOException e1) {\n        // we're already sending an error, not much else we can do if more stuff breaks\n    }\n}",
        "reject_response": "protected void sendErrorResponse(HttpServletRequest request, HttpServletResponse response, int code, Exception e) {\n    try {\n        // WW-1977: Only put errors in the request when code is a 500 error\n        if (code == HttpServletResponse.SC_INTERNAL_SERVER_ERROR) {\n            // WW-4103: Only logs error when application error occurred, not Struts error\n            LOG.error(\"Exception occurred during processing request: {}\", e, e.getMessage());\n            // send a http error response to use the servlet defined error handler\n            // make the exception available to the web.xml defined error page\n            request.setAttribute(\"javax.servlet.error.exception\", e);\n\n            // for compatibility\n            request.setAttribute(\"javax.servlet.jsp.jspException\", e);\n        }\n\n        // send the error response\n        response.sendError(code, e.getMessage());\n    } catch (IOException e1) {\n        // we're already sending an error, not much else we can do if more stuff breaks\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2698,
        "instruction": "Response readResponse(final Bridge bridge, ProtocolData protData,\n                      final boolean threadSafe) {\n    final int fragment = protData.getDataFragment();\n    final String dataDir = protData.getDataSource();\n\n    // Creating an internal streaming class\n    // which will iterate the records and put them on the\n    // output stream\n    final StreamingOutput streaming = new StreamingOutput() {\n        @Override\n        public void write(final OutputStream out) throws IOException,\n                WebApplicationException {\n            long recordCount = 0;\n\n            if (!threadSafe) {\n                lock(dataDir);\n            }\n            try {\n\n                if (!bridge.beginIteration()) {\n                    return;\n                }\n\n                Writable record;\n                DataOutputStream dos = new DataOutputStream(out);\n                LOG.debug(\"Starting streaming fragment \" + fragment\n                        + \" of resource \" + dataDir);\n                while ((record = bridge.getNext()) != null) {\n                    record.write(dos);\n                    ++recordCount;\n                }\n                LOG.debug(\"Finished streaming fragment \" + fragment\n                        + \" of resource \" + dataDir + \", \" + recordCount\n                        + \" records.\");\n            } catch (ClientAbortException e) {\n                // Occurs whenever client (HAWQ) decides the end the\n                // connection\n                LOG.error(\"Remote connection closed by HAWQ\", e);\n            } catch (Exception e) {\n                throw new IOException(e.getMessage());\n            } finally {\n                LOG.debug(\"Stopped streaming fragment \" + fragment\n                        + \" of resource \" + dataDir + \", \" + recordCount\n                        + \" records.\");\n                if (!threadSafe) {\n                    unlock(dataDir);\n                }\n            }\n        }\n    };\n\n    return Response.ok(streaming, MediaType.APPLICATION_OCTET_STREAM).build();\n}",
        "accept_response": "Response readResponse(final Bridge bridge, ProtocolData protData,\n                      final boolean threadSafe) {\n    final int fragment = protData.getDataFragment();\n    final String dataDir = protData.getDataSource();\n\n    // Creating an internal streaming class\n    // which will iterate the records and put them on the\n    // output stream\n    final StreamingOutput streaming = new StreamingOutput() {\n        @Override\n        public void write(final OutputStream out) throws IOException,\n                WebApplicationException {\n            long recordCount = 0;\n\n            if (!threadSafe) {\n                lock(dataDir);\n            }\n            try {\n\n                if (!bridge.beginIteration()) {\n                    return;\n                }\n\n                Writable record;\n                DataOutputStream dos = new DataOutputStream(out);\n                LOG.debug(\"Starting streaming fragment \" + fragment\n                        + \" of resource \" + dataDir);\n                while ((record = bridge.getNext()) != null) {\n                    record.write(dos);\n                    ++recordCount;\n                }\n                LOG.debug(\"Finished streaming fragment \" + fragment\n                        + \" of resource \" + dataDir + \", \" + recordCount\n                        + \" records.\");\n            } catch (ClientAbortException e) {\n                // Occurs whenever client (HAWQ) decides the end the\n                // connection\n                LOG.error(\"Remote connection closed by HAWQ\", e);\n            } catch (Exception e) {\n                LOG.error(\"Exception thrown when streaming\", e);\n                throw new IOException(e.getMessage());\n            } finally {\n                LOG.debug(\"Stopped streaming fragment \" + fragment\n                        + \" of resource \" + dataDir + \", \" + recordCount\n                        + \" records.\");\n                if (!threadSafe) {\n                    unlock(dataDir);\n                }\n            }\n        }\n    };\n\n    return Response.ok(streaming, MediaType.APPLICATION_OCTET_STREAM).build();\n}",
        "reject_response": "Response readResponse(final Bridge bridge, ProtocolData protData,\n                      final boolean threadSafe) {\n    final int fragment = protData.getDataFragment();\n    final String dataDir = protData.getDataSource();\n\n    // Creating an internal streaming class\n    // which will iterate the records and put them on the\n    // output stream\n    final StreamingOutput streaming = new StreamingOutput() {\n        @Override\n        public void write(final OutputStream out) throws IOException,\n                WebApplicationException {\n            long recordCount = 0;\n\n            if (!threadSafe) {\n                lock(dataDir);\n            }\n            try {\n\n                if (!bridge.beginIteration()) {\n                    return;\n                }\n\n                Writable record;\n                DataOutputStream dos = new DataOutputStream(out);\n                LOG.debug(\"Starting streaming fragment \" + fragment\n                        + \" of resource \" + dataDir);\n                while ((record = bridge.getNext()) != null) {\n                    record.write(dos);\n                    ++recordCount;\n                }\n                LOG.debug(\"Finished streaming fragment \" + fragment\n                        + \" of resource \" + dataDir + \", \" + recordCount\n                        + \" records.\");\n            } catch (ClientAbortException e) {\n                // Occurs whenever client (HAWQ) decides the end the\n                // connection\n                LOG.error(\"Remote connection closed by HAWQ\", e);\n            } catch (Exception e) {\n                Log.error(\"Exception thrown when streaming\", e);\n                throw new IOException(e.getMessage());\n            } finally {\n                LOG.debug(\"Stopped streaming fragment \" + fragment\n                        + \" of resource \" + dataDir + \", \" + recordCount\n                        + \" records.\");\n                if (!threadSafe) {\n                    unlock(dataDir);\n                }\n            }\n        }\n    };\n\n    return Response.ok(streaming, MediaType.APPLICATION_OCTET_STREAM).build();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2889,
        "instruction": "boolean transitToGoal(Cell bin, List<FeatureFunction> featureFunctions, int sentenceLength) {\n  this.sortedNodes = new ArrayList<HGNode>();\n  HGNode goalItem = null;\n\n  for (HGNode antNode : bin.getSortedNodes()) {\n    if (antNode.lhs == this.goalSymbol) {\n      float logP = antNode.bestHyperedge.getBestDerivationScore();\n      List<HGNode> antNodes = new ArrayList<HGNode>();\n      antNodes.add(antNode);\n\n      float finalTransitionLogP = ComputeNodeResult.computeFinalCost(featureFunctions, antNodes,\n          0, sentenceLength, null, this.chart.getSentence());\n\n      List<HGNode> previousItems = new ArrayList<HGNode>();\n      previousItems.add(antNode);\n\n      HyperEdge dt = new HyperEdge(null, logP + finalTransitionLogP, finalTransitionLogP,\n          previousItems, null);\n\n      if (null == goalItem) {\n        goalItem = new HGNode(0, sentenceLength + 1, this.goalSymbol, null, dt, logP\n            + finalTransitionLogP);\n        this.sortedNodes.add(goalItem);\n      } else {\n        goalItem.addHyperedgeInNode(dt);\n      }\n    } // End if item.lhs == this.goalSymID\n  } // End foreach Item in bin.get_sorted_items()\n\n  int itemsInGoalBin = getSortedNodes().size();\n  if (1 != itemsInGoalBin) {\n    return false;\n  }\n\n  return true;\n}",
        "accept_response": "boolean transitToGoal(Cell bin, List<FeatureFunction> featureFunctions, int sentenceLength) {\n  this.sortedNodes = new ArrayList<HGNode>();\n  HGNode goalItem = null;\n\n  for (HGNode antNode : bin.getSortedNodes()) {\n    if (antNode.lhs == this.goalSymbol) {\n      float logP = antNode.bestHyperedge.getBestDerivationScore();\n      List<HGNode> antNodes = new ArrayList<HGNode>();\n      antNodes.add(antNode);\n\n      float finalTransitionLogP = ComputeNodeResult.computeFinalCost(featureFunctions, antNodes,\n          0, sentenceLength, null, this.chart.getSentence());\n\n      List<HGNode> previousItems = new ArrayList<HGNode>();\n      previousItems.add(antNode);\n\n      HyperEdge dt = new HyperEdge(null, logP + finalTransitionLogP, finalTransitionLogP,\n          previousItems, null);\n\n      if (null == goalItem) {\n        goalItem = new HGNode(0, sentenceLength + 1, this.goalSymbol, null, dt, logP\n            + finalTransitionLogP);\n        this.sortedNodes.add(goalItem);\n      } else {\n        goalItem.addHyperedgeInNode(dt);\n      }\n    } // End if item.lhs == this.goalSymID\n  } // End foreach Item in bin.get_sorted_items()\n\n  int itemsInGoalBin = getSortedNodes().size();\n  if (1 != itemsInGoalBin) {\n    LOG.error(\"the goal_bin does not have exactly one item\");\n    return false;\n  }\n\n  return true;\n}",
        "reject_response": "boolean transitToGoal(Cell bin, List<FeatureFunction> featureFunctions, int sentenceLength) {\n  this.sortedNodes = new ArrayList<HGNode>();\n  HGNode goalItem = null;\n\n  for (HGNode antNode : bin.getSortedNodes()) {\n    if (antNode.lhs == this.goalSymbol) {\n      float logP = antNode.bestHyperedge.getBestDerivationScore();\n      List<HGNode> antNodes = new ArrayList<HGNode>();\n      antNodes.add(antNode);\n\n      float finalTransitionLogP = ComputeNodeResult.computeFinalCost(featureFunctions, antNodes,\n          0, sentenceLength, null, this.chart.getSentence());\n\n      List<HGNode> previousItems = new ArrayList<HGNode>();\n      previousItems.add(antNode);\n\n      HyperEdge dt = new HyperEdge(null, logP + finalTransitionLogP, finalTransitionLogP,\n          previousItems, null);\n\n      if (null == goalItem) {\n        goalItem = new HGNode(0, sentenceLength + 1, this.goalSymbol, null, dt, logP\n            + finalTransitionLogP);\n        this.sortedNodes.add(goalItem);\n      } else {\n        goalItem.addHyperedgeInNode(dt);\n      }\n    } // End if item.lhs == this.goalSymID\n  } // End foreach Item in bin.get_sorted_items()\n\n  int itemsInGoalBin = getSortedNodes().size();\n  if (1 != itemsInGoalBin) {\n    logger.severe(\"the goal_bin does not have exactly one item\");\n    return false;\n  }\n\n  return true;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2637,
        "instruction": "private Float getFloatAttribute(Object object, String name) {\n  if (object == null) {\n    return Float.valueOf(0.0f);\n  }\n\n  try {\n    if (!(object.getClass().equals(Float.class))) {\n      return Float.valueOf(0.0f);\n    } else {\n      return (Float) object;\n    }\n  } catch (Exception e) {\n    logger.info(\"Exception occurred: \", e);\n    return Float.valueOf(0.0f);\n  }\n}",
        "accept_response": "private Float getFloatAttribute(Object object, String name) {\n  if (object == null) {\n    return Float.valueOf(0.0f);\n  }\n\n  try {\n    if (!(object.getClass().equals(Float.class))) {\n      logger.info(\n          \"************************Unexpected type for attribute: {}; Expected type: {}; Received type: {}************************\",\n          name, Float.class.getName(), object.getClass().getName());\n      return Float.valueOf(0.0f);\n    } else {\n      return (Float) object;\n    }\n  } catch (Exception e) {\n    logger.info(\"Exception occurred: \", e);\n    return Float.valueOf(0.0f);\n  }\n}",
        "reject_response": "private Float getFloatAttribute(Object object, String name) {\n  if (object == null) {\n    return Float.valueOf(0.0f);\n  }\n\n  try {\n    if (!(object.getClass().equals(Float.class))) {\n      if (LOGGER.infoEnabled()) {\n        LOGGER.info(\"************************Unexpected type for attribute: \" + name\n            + \" Expected type: \" + Float.class.getName() + \" Received type: \"\n            + object.getClass().getName() + \"************************\");\n      }\n      return Float.valueOf(0.0f);\n    } else {\n      return (Float) object;\n    }\n  } catch (Exception e) {\n    logger.info(\"Exception occurred: \", e);\n    return Float.valueOf(0.0f);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2391,
        "instruction": "@Override\npublic TabletLocation locateTablet(ClientContext context, Text row, boolean skipRow, boolean retry) throws AccumuloException, AccumuloSecurityException,\n    TableNotFoundException {\n\n  OpTimer opTimer = null;\n  if (log.isTraceEnabled())\n    opTimer = new OpTimer(log, Level.TRACE).start(\"Locating tablet  table=\" + tableId + \" row=\" + TextUtil.truncate(row) + \"  skipRow=\" + skipRow + \" retry=\"\n        + retry);\n\n  while (true) {\n\n    LockCheckerSession lcSession = new LockCheckerSession();\n    TabletLocation tl = _locateTablet(context, row, skipRow, retry, true, lcSession);\n\n    if (retry && tl == null) {\n      UtilWaitThread.sleep(100);\n      if (log.isTraceEnabled())\n      continue;\n    }\n\n    if (opTimer != null)\n      opTimer.stop(\"Located tablet \" + (tl == null ? null : tl.tablet_extent) + \" at \" + (tl == null ? null : tl.tablet_location) + \" in %DURATION%\");\n\n    return tl;\n  }\n}",
        "accept_response": "@Override\npublic TabletLocation locateTablet(ClientContext context, Text row, boolean skipRow, boolean retry) throws AccumuloException, AccumuloSecurityException,\n    TableNotFoundException {\n\n  OpTimer opTimer = null;\n  if (log.isTraceEnabled())\n    opTimer = new OpTimer(log, Level.TRACE).start(\"Locating tablet  table=\" + tableId + \" row=\" + TextUtil.truncate(row) + \"  skipRow=\" + skipRow + \" retry=\"\n        + retry);\n\n  while (true) {\n\n    LockCheckerSession lcSession = new LockCheckerSession();\n    TabletLocation tl = _locateTablet(context, row, skipRow, retry, true, lcSession);\n\n    if (retry && tl == null) {\n      UtilWaitThread.sleep(100);\n      if (log.isTraceEnabled())\n        log.trace(\"Failed to locate tablet containing row {} in table {}, will retry...\", TextUtil.truncate(row), tableId);\n      continue;\n    }\n\n    if (opTimer != null)\n      opTimer.stop(\"Located tablet \" + (tl == null ? null : tl.tablet_extent) + \" at \" + (tl == null ? null : tl.tablet_location) + \" in %DURATION%\");\n\n    return tl;\n  }\n}",
        "reject_response": "@Override\npublic TabletLocation locateTablet(ClientContext context, Text row, boolean skipRow, boolean retry) throws AccumuloException, AccumuloSecurityException,\n    TableNotFoundException {\n\n  OpTimer opTimer = null;\n  if (log.isTraceEnabled())\n    opTimer = new OpTimer(log, Level.TRACE).start(\"Locating tablet  table=\" + tableId + \" row=\" + TextUtil.truncate(row) + \"  skipRow=\" + skipRow + \" retry=\"\n        + retry);\n\n  while (true) {\n\n    LockCheckerSession lcSession = new LockCheckerSession();\n    TabletLocation tl = _locateTablet(context, row, skipRow, retry, true, lcSession);\n\n    if (retry && tl == null) {\n      UtilWaitThread.sleep(100);\n      if (log.isTraceEnabled())\n        log.trace(\"Failed to locate tablet containing row \" + TextUtil.truncate(row) + \" in table \" + tableId + \", will retry...\");\n      continue;\n    }\n\n    if (opTimer != null)\n      opTimer.stop(\"Located tablet \" + (tl == null ? null : tl.tablet_extent) + \" at \" + (tl == null ? null : tl.tablet_location) + \" in %DURATION%\");\n\n    return tl;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3152,
        "instruction": "@InterfaceStability.Unstable\npublic static InputSplitInfoMem generateInputSplitsToMem(Configuration conf,\n    boolean groupSplits, boolean sortSplits, int targetTasks)\n    throws IOException, ClassNotFoundException, InterruptedException {\n\n  InputSplitInfoMem splitInfoMem = null;\n  JobConf jobConf = new JobConf(conf);\n  if (jobConf.getUseNewMapper()) {\n    LOG.debug(\"Generating mapreduce api input splits\");\n    Job job = Job.getInstance(conf);\n    org.apache.hadoop.mapreduce.InputSplit[] splits =\n        generateNewSplits(job, groupSplits, sortSplits, targetTasks);\n    splitInfoMem = new InputSplitInfoMem(splits, createTaskLocationHintsFromSplits(splits),\n        splits.length, job.getCredentials(), job.getConfiguration());\n  } else {\n    org.apache.hadoop.mapred.InputSplit[] splits =\n        generateOldSplits(jobConf, groupSplits, sortSplits, targetTasks);\n    splitInfoMem = new InputSplitInfoMem(splits, createTaskLocationHintsFromSplits(splits),\n        splits.length, jobConf.getCredentials(), jobConf);\n  }\n  LOG.info(\"NumSplits: \" + splitInfoMem.getNumTasks() + \", SerializedSize: \"\n      + splitInfoMem.getSplitsProto().getSerializedSize());\n  return splitInfoMem;\n}",
        "accept_response": "@InterfaceStability.Unstable\npublic static InputSplitInfoMem generateInputSplitsToMem(Configuration conf,\n    boolean groupSplits, boolean sortSplits, int targetTasks)\n    throws IOException, ClassNotFoundException, InterruptedException {\n\n  InputSplitInfoMem splitInfoMem = null;\n  JobConf jobConf = new JobConf(conf);\n  if (jobConf.getUseNewMapper()) {\n    LOG.debug(\"Generating mapreduce api input splits\");\n    Job job = Job.getInstance(conf);\n    org.apache.hadoop.mapreduce.InputSplit[] splits =\n        generateNewSplits(job, groupSplits, sortSplits, targetTasks);\n    splitInfoMem = new InputSplitInfoMem(splits, createTaskLocationHintsFromSplits(splits),\n        splits.length, job.getCredentials(), job.getConfiguration());\n  } else {\n    LOG.debug(\"Generating mapred api input splits\");\n    org.apache.hadoop.mapred.InputSplit[] splits =\n        generateOldSplits(jobConf, groupSplits, sortSplits, targetTasks);\n    splitInfoMem = new InputSplitInfoMem(splits, createTaskLocationHintsFromSplits(splits),\n        splits.length, jobConf.getCredentials(), jobConf);\n  }\n  LOG.info(\"NumSplits: \" + splitInfoMem.getNumTasks() + \", SerializedSize: \"\n      + splitInfoMem.getSplitsProto().getSerializedSize());\n  return splitInfoMem;\n}",
        "reject_response": "@InterfaceStability.Unstable\npublic static InputSplitInfoMem generateInputSplitsToMem(Configuration conf,\n    boolean groupSplits, boolean sortSplits, int targetTasks)\n    throws IOException, ClassNotFoundException, InterruptedException {\n\n  InputSplitInfoMem splitInfoMem = null;\n  JobConf jobConf = new JobConf(conf);\n  if (jobConf.getUseNewMapper()) {\n    LOG.debug(\"Generating mapreduce api input splits\");\n    Job job = Job.getInstance(conf);\n    org.apache.hadoop.mapreduce.InputSplit[] splits =\n        generateNewSplits(job, groupSplits, sortSplits, targetTasks);\n    splitInfoMem = new InputSplitInfoMem(splits, createTaskLocationHintsFromSplits(splits),\n        splits.length, job.getCredentials(), job.getConfiguration());\n  } else {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Generating mapred api input splits\");\n    }\n    org.apache.hadoop.mapred.InputSplit[] splits =\n        generateOldSplits(jobConf, groupSplits, sortSplits, targetTasks);\n    splitInfoMem = new InputSplitInfoMem(splits, createTaskLocationHintsFromSplits(splits),\n        splits.length, jobConf.getCredentials(), jobConf);\n  }\n  LOG.info(\"NumSplits: \" + splitInfoMem.getNumTasks() + \", SerializedSize: \"\n      + splitInfoMem.getSplitsProto().getSerializedSize());\n  return splitInfoMem;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2602,
        "instruction": "@Override\npublic void run() {\n\tSERVER.set(Server.this);\n\tByteArrayOutputStream buf = new ByteArrayOutputStream(10240);\n\twhile (running) {\n\t\ttry {\n\t\t\tfinal Call call = callQueue.take(); // pop the queue; maybe blocked here\n\n\t\t\tString errorClass = null;\n\t\t\tString error = null;\n\t\t\tIOReadableWritable value = null;\n\n\t\t\tCurCall.set(call);\n\n\t\t\tvalue = call(call.connection.protocol, call.param, call.timestamp);\n\n\t\t\tCurCall.set(null);\n\n\t\t\tsetupResponse(buf, call, (error == null) ? Status.SUCCESS : Status.ERROR, value, errorClass, error);\n\t\t\tresponder.doRespond(call);\n\t\t} catch (InterruptedException e) {\n\t\t\tif (running) { // unexpected -- log it\n\t\t\t\tLOG.info(getName() + \" caught: \", e);\n\t\t\t}\n\t\t} catch (Exception e) {\n\t\t\tLOG.info(getName() + \" caught: \", e);\n\t\t}\n\t}\n\tLOG.debug(getName() + \": exiting\");\n\n\tthis.shutDown = true;\n}",
        "accept_response": "@Override\npublic void run() {\n\tLOG.debug(getName() + \": starting\");\n\tSERVER.set(Server.this);\n\tByteArrayOutputStream buf = new ByteArrayOutputStream(10240);\n\twhile (running) {\n\t\ttry {\n\t\t\tfinal Call call = callQueue.take(); // pop the queue; maybe blocked here\n\n\t\t\tString errorClass = null;\n\t\t\tString error = null;\n\t\t\tIOReadableWritable value = null;\n\n\t\t\tCurCall.set(call);\n\n\t\t\tvalue = call(call.connection.protocol, call.param, call.timestamp);\n\n\t\t\tCurCall.set(null);\n\n\t\t\tsetupResponse(buf, call, (error == null) ? Status.SUCCESS : Status.ERROR, value, errorClass, error);\n\t\t\tresponder.doRespond(call);\n\t\t} catch (InterruptedException e) {\n\t\t\tif (running) { // unexpected -- log it\n\t\t\t\tLOG.info(getName() + \" caught: \", e);\n\t\t\t}\n\t\t} catch (Exception e) {\n\t\t\tLOG.info(getName() + \" caught: \", e);\n\t\t}\n\t}\n\tLOG.debug(getName() + \": exiting\");\n\n\tthis.shutDown = true;\n}",
        "reject_response": "@Override\npublic void run() {\n\tLOG.info(getName() + \": starting\");\n\tSERVER.set(Server.this);\n\tByteArrayOutputStream buf = new ByteArrayOutputStream(10240);\n\twhile (running) {\n\t\ttry {\n\t\t\tfinal Call call = callQueue.take(); // pop the queue; maybe blocked here\n\n\t\t\tString errorClass = null;\n\t\t\tString error = null;\n\t\t\tIOReadableWritable value = null;\n\n\t\t\tCurCall.set(call);\n\n\t\t\tvalue = call(call.connection.protocol, call.param, call.timestamp);\n\n\t\t\tCurCall.set(null);\n\n\t\t\tsetupResponse(buf, call, (error == null) ? Status.SUCCESS : Status.ERROR, value, errorClass, error);\n\t\t\tresponder.doRespond(call);\n\t\t} catch (InterruptedException e) {\n\t\t\tif (running) { // unexpected -- log it\n\t\t\t\tLOG.info(getName() + \" caught: \", e);\n\t\t\t}\n\t\t} catch (Exception e) {\n\t\t\tLOG.info(getName() + \" caught: \", e);\n\t\t}\n\t}\n\tLOG.debug(getName() + \": exiting\");\n\n\tthis.shutDown = true;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3139,
        "instruction": "@Override\npublic void run() {\n  // Load ContainerManager tokens before creating a connection.\n  // TODO: Do it only once per NodeManager.\n  ContainerId containerID = event.getBaseOperation().getContainerId();\n\n  Container c = getContainer(event);\n  switch(event.getOpType()) {\n    case LAUNCH_REQUEST:\n      ContainerLaunchRequest launchRequest = event.getLaunchRequest();\n      c.launch(launchRequest);\n      break;\n    case STOP_REQUEST:\n      c.kill();\n      break;\n  }\n  removeContainerIfDone(containerID);\n}",
        "accept_response": "@Override\npublic void run() {\n  // Load ContainerManager tokens before creating a connection.\n  // TODO: Do it only once per NodeManager.\n  ContainerId containerID = event.getBaseOperation().getContainerId();\n  LOG.debug(\"Processing ContainerOperation {}\", event);\n\n  Container c = getContainer(event);\n  switch(event.getOpType()) {\n    case LAUNCH_REQUEST:\n      ContainerLaunchRequest launchRequest = event.getLaunchRequest();\n      c.launch(launchRequest);\n      break;\n    case STOP_REQUEST:\n      c.kill();\n      break;\n  }\n  removeContainerIfDone(containerID);\n}",
        "reject_response": "@Override\npublic void run() {\n  // Load ContainerManager tokens before creating a connection.\n  // TODO: Do it only once per NodeManager.\n  ContainerId containerID = event.getBaseOperation().getContainerId();\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"Processing ContainerOperation {}\", event);\n  }\n\n  Container c = getContainer(event);\n  switch(event.getOpType()) {\n    case LAUNCH_REQUEST:\n      ContainerLaunchRequest launchRequest = event.getLaunchRequest();\n      c.launch(launchRequest);\n      break;\n    case STOP_REQUEST:\n      c.kill();\n      break;\n  }\n  removeContainerIfDone(containerID);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2413,
        "instruction": "private Command<Void> getCreateCommand(final Map<String, Object> properties, final Map<String, String> requestInfoProps) {\n  return new Command<Void>() {\n    @SuppressWarnings(\"rawtypes\")\n    @Override\n    public Void invoke() throws AmbariException {\n      String rawRequestBody = requestInfoProps.get(Request.REQUEST_INFO_BODY_PROPERTY);\n      Preconditions.checkArgument(!Strings.isNullOrEmpty(rawRequestBody), REQUEST_BODY_EMPTY_ERROR_MESSAGE);\n\n      Map<String, Object> rawBodyMap = jsonSerializer.<Map<String, Object>>fromJson(rawRequestBody, Map.class);\n      Object configurationData = rawBodyMap.get(CONFIGURATION_PROPERTY_ID);\n\n      if (configurationData != null) {\n        Preconditions.checkArgument(configurationData instanceof List, CONFIGURATION_LIST_CHECK_ERROR_MESSAGE);\n        for (Object map : (List) configurationData) {\n          Preconditions.checkArgument(map instanceof Map, CONFIGURATION_MAP_CHECK_ERROR_MESSAGE);\n          Preconditions.checkArgument(((Map) map).size() <= 1, CONFIGURATION_MAP_SIZE_CHECK_ERROR_MESSAGE);\n        }\n      }\n      SecurityConfiguration securityConfiguration = securityConfigurationFactory\n        .createSecurityConfigurationFromRequest((Map<String, Object>) rawBodyMap.get(BLUEPRINTS_PROPERTY_ID), true);\n\n      Blueprint blueprint;\n      try {\n        blueprint = blueprintFactory.createBlueprint(properties, securityConfiguration);\n      } catch (NoSuchStackException e) {\n        throw new IllegalArgumentException(\"Specified stack doesn't exist: \" + e, e);\n      }\n\n      if (blueprintDAO.findByName(blueprint.getName()) != null) {\n        throw new DuplicateResourceException(\n            \"Attempted to create a Blueprint which already exists, blueprint_name=\" +\n            blueprint.getName());\n      }\n\n      try {\n        blueprint.validateRequiredProperties();\n      } catch (InvalidTopologyException e) {\n        throw new IllegalArgumentException(\"Blueprint configuration validation failed: \" + e.getMessage(), e);\n      }\n\n      String validateTopology =  requestInfoProps.get(\"validate_topology\");\n      if (validateTopology == null || ! validateTopology.equalsIgnoreCase(\"false\")) {\n        try {\n          blueprint.validateTopology();\n        } catch (InvalidTopologyException e) {\n          throw new IllegalArgumentException(e.getMessage());\n        }\n      }\n\n      LOG.info(\"Creating Blueprint, name=\" + blueprint.getName());\n      String blueprintSetting = blueprint.getSetting() == null ? \"(null)\" :\n              jsonSerializer.toJson(blueprint.getSetting().getProperties());\n\n      try {\n        blueprintDAO.create(blueprint.toEntity());\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      }\n      return null;\n    }\n  };\n}",
        "accept_response": "private Command<Void> getCreateCommand(final Map<String, Object> properties, final Map<String, String> requestInfoProps) {\n  return new Command<Void>() {\n    @SuppressWarnings(\"rawtypes\")\n    @Override\n    public Void invoke() throws AmbariException {\n      String rawRequestBody = requestInfoProps.get(Request.REQUEST_INFO_BODY_PROPERTY);\n      Preconditions.checkArgument(!Strings.isNullOrEmpty(rawRequestBody), REQUEST_BODY_EMPTY_ERROR_MESSAGE);\n\n      Map<String, Object> rawBodyMap = jsonSerializer.<Map<String, Object>>fromJson(rawRequestBody, Map.class);\n      Object configurationData = rawBodyMap.get(CONFIGURATION_PROPERTY_ID);\n\n      if (configurationData != null) {\n        Preconditions.checkArgument(configurationData instanceof List, CONFIGURATION_LIST_CHECK_ERROR_MESSAGE);\n        for (Object map : (List) configurationData) {\n          Preconditions.checkArgument(map instanceof Map, CONFIGURATION_MAP_CHECK_ERROR_MESSAGE);\n          Preconditions.checkArgument(((Map) map).size() <= 1, CONFIGURATION_MAP_SIZE_CHECK_ERROR_MESSAGE);\n        }\n      }\n      SecurityConfiguration securityConfiguration = securityConfigurationFactory\n        .createSecurityConfigurationFromRequest((Map<String, Object>) rawBodyMap.get(BLUEPRINTS_PROPERTY_ID), true);\n\n      Blueprint blueprint;\n      try {\n        blueprint = blueprintFactory.createBlueprint(properties, securityConfiguration);\n      } catch (NoSuchStackException e) {\n        throw new IllegalArgumentException(\"Specified stack doesn't exist: \" + e, e);\n      }\n\n      if (blueprintDAO.findByName(blueprint.getName()) != null) {\n        throw new DuplicateResourceException(\n            \"Attempted to create a Blueprint which already exists, blueprint_name=\" +\n            blueprint.getName());\n      }\n\n      try {\n        blueprint.validateRequiredProperties();\n      } catch (InvalidTopologyException e) {\n        throw new IllegalArgumentException(\"Blueprint configuration validation failed: \" + e.getMessage(), e);\n      }\n\n      String validateTopology =  requestInfoProps.get(\"validate_topology\");\n      if (validateTopology == null || ! validateTopology.equalsIgnoreCase(\"false\")) {\n        try {\n          blueprint.validateTopology();\n        } catch (InvalidTopologyException e) {\n          throw new IllegalArgumentException(e.getMessage());\n        }\n      }\n\n      LOG.info(\"Creating Blueprint, name=\" + blueprint.getName());\n      String blueprintSetting = blueprint.getSetting() == null ? \"(null)\" :\n              jsonSerializer.toJson(blueprint.getSetting().getProperties());\n      LOG.info(\"Blueprint setting=\" + blueprintSetting);\n\n      try {\n        blueprintDAO.create(blueprint.toEntity());\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      }\n      return null;\n    }\n  };\n}",
        "reject_response": "private Command<Void> getCreateCommand(final Map<String, Object> properties, final Map<String, String> requestInfoProps) {\n  return new Command<Void>() {\n    @SuppressWarnings(\"rawtypes\")\n    @Override\n    public Void invoke() throws AmbariException {\n      String rawRequestBody = requestInfoProps.get(Request.REQUEST_INFO_BODY_PROPERTY);\n      Preconditions.checkArgument(!Strings.isNullOrEmpty(rawRequestBody), REQUEST_BODY_EMPTY_ERROR_MESSAGE);\n\n      Map<String, Object> rawBodyMap = jsonSerializer.<Map<String, Object>>fromJson(rawRequestBody, Map.class);\n      Object configurationData = rawBodyMap.get(CONFIGURATION_PROPERTY_ID);\n\n      if (configurationData != null) {\n        Preconditions.checkArgument(configurationData instanceof List, CONFIGURATION_LIST_CHECK_ERROR_MESSAGE);\n        for (Object map : (List) configurationData) {\n          Preconditions.checkArgument(map instanceof Map, CONFIGURATION_MAP_CHECK_ERROR_MESSAGE);\n          Preconditions.checkArgument(((Map) map).size() <= 1, CONFIGURATION_MAP_SIZE_CHECK_ERROR_MESSAGE);\n        }\n      }\n      SecurityConfiguration securityConfiguration = securityConfigurationFactory\n        .createSecurityConfigurationFromRequest((Map<String, Object>) rawBodyMap.get(BLUEPRINTS_PROPERTY_ID), true);\n\n      Blueprint blueprint;\n      try {\n        blueprint = blueprintFactory.createBlueprint(properties, securityConfiguration);\n      } catch (NoSuchStackException e) {\n        throw new IllegalArgumentException(\"Specified stack doesn't exist: \" + e, e);\n      }\n\n      if (blueprintDAO.findByName(blueprint.getName()) != null) {\n        throw new DuplicateResourceException(\n            \"Attempted to create a Blueprint which already exists, blueprint_name=\" +\n            blueprint.getName());\n      }\n\n      try {\n        blueprint.validateRequiredProperties();\n      } catch (InvalidTopologyException e) {\n        throw new IllegalArgumentException(\"Blueprint configuration validation failed: \" + e.getMessage(), e);\n      }\n\n      String validateTopology =  requestInfoProps.get(\"validate_topology\");\n      if (validateTopology == null || ! validateTopology.equalsIgnoreCase(\"false\")) {\n        try {\n          blueprint.validateTopology();\n        } catch (InvalidTopologyException e) {\n          throw new IllegalArgumentException(e.getMessage());\n        }\n      }\n\n      LOG.info(\"Creating Blueprint, name=\" + blueprint.getName());\n      String blueprintSetting = blueprint.getSetting() == null ? \"(null)\" :\n              jsonSerializer.toJson(blueprint.getSetting().getProperties());\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Creating Blueprint, name=\" + blueprint.getName());\n      }\n\n      try {\n        blueprintDAO.create(blueprint.toEntity());\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      }\n      return null;\n    }\n  };\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2581,
        "instruction": "private CommandProcessingResult checkMainResourceExistsWithinScope(final String appTable, final Long appTableId) {\n\n    final String sql = dataScopedSQL(appTable, appTableId);\n    final SqlRowSet rs = this.jdbcTemplate.queryForRowSet(sql);\n\n    if (!rs.next()) { throw new DatatableNotFoundException(appTable, appTableId); }\n\n    final Long officeId = getLongSqlRowSet(rs, \"officeId\");\n    final Long groupId = getLongSqlRowSet(rs, \"groupId\");\n    final Long clientId = getLongSqlRowSet(rs, \"clientId\");\n    final Long savingsId = getLongSqlRowSet(rs, \"savingsId\");\n    final Long LoanId = getLongSqlRowSet(rs, \"loanId\");\n    final Long entityId = getLongSqlRowSet(rs, \"entityId\");\n\n    if (rs.next()) { throw new DatatableSystemErrorException(\"System Error: More than one row returned from data scoping query\"); }\n\n    return new CommandProcessingResultBuilder() //\n            .withOfficeId(officeId) //\n            .withGroupId(groupId) //\n            .withClientId(clientId) //\n            .withSavingsId(savingsId) //\n            .withLoanId(LoanId).withEntityId(entityId)//\n            .build();\n}",
        "accept_response": "private CommandProcessingResult checkMainResourceExistsWithinScope(final String appTable, final Long appTableId) {\n\n    final String sql = dataScopedSQL(appTable, appTableId);\n    logger.info(\"data scoped sql: {}\", sql);\n    final SqlRowSet rs = this.jdbcTemplate.queryForRowSet(sql);\n\n    if (!rs.next()) { throw new DatatableNotFoundException(appTable, appTableId); }\n\n    final Long officeId = getLongSqlRowSet(rs, \"officeId\");\n    final Long groupId = getLongSqlRowSet(rs, \"groupId\");\n    final Long clientId = getLongSqlRowSet(rs, \"clientId\");\n    final Long savingsId = getLongSqlRowSet(rs, \"savingsId\");\n    final Long LoanId = getLongSqlRowSet(rs, \"loanId\");\n    final Long entityId = getLongSqlRowSet(rs, \"entityId\");\n\n    if (rs.next()) { throw new DatatableSystemErrorException(\"System Error: More than one row returned from data scoping query\"); }\n\n    return new CommandProcessingResultBuilder() //\n            .withOfficeId(officeId) //\n            .withGroupId(groupId) //\n            .withClientId(clientId) //\n            .withSavingsId(savingsId) //\n            .withLoanId(LoanId).withEntityId(entityId)//\n            .build();\n}",
        "reject_response": "private CommandProcessingResult checkMainResourceExistsWithinScope(final String appTable, final Long appTableId) {\n\n    final String sql = dataScopedSQL(appTable, appTableId);\n    logger.info(\"data scoped sql: \" + sql);\n    final SqlRowSet rs = this.jdbcTemplate.queryForRowSet(sql);\n\n    if (!rs.next()) { throw new DatatableNotFoundException(appTable, appTableId); }\n\n    final Long officeId = getLongSqlRowSet(rs, \"officeId\");\n    final Long groupId = getLongSqlRowSet(rs, \"groupId\");\n    final Long clientId = getLongSqlRowSet(rs, \"clientId\");\n    final Long savingsId = getLongSqlRowSet(rs, \"savingsId\");\n    final Long LoanId = getLongSqlRowSet(rs, \"loanId\");\n    final Long entityId = getLongSqlRowSet(rs, \"entityId\");\n\n    if (rs.next()) { throw new DatatableSystemErrorException(\"System Error: More than one row returned from data scoping query\"); }\n\n    return new CommandProcessingResultBuilder() //\n            .withOfficeId(officeId) //\n            .withGroupId(groupId) //\n            .withClientId(clientId) //\n            .withSavingsId(savingsId) //\n            .withLoanId(LoanId).withEntityId(entityId)//\n            .build();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2606,
        "instruction": "@Override\npublic AsyncResult peekBlockingly() throws InterruptedException {\n\tlock.lockInterruptibly();\n\n\ttry {\n\t\twhile (completedQueue.isEmpty()) {\n\t\t\thasCompletedEntries.await();\n\t\t}\n\n\n\t\treturn completedQueue.peek();\n\t} finally {\n\t\tlock.unlock();\n\t}\n}",
        "accept_response": "@Override\npublic AsyncResult peekBlockingly() throws InterruptedException {\n\tlock.lockInterruptibly();\n\n\ttry {\n\t\twhile (completedQueue.isEmpty()) {\n\t\t\thasCompletedEntries.await();\n\t\t}\n\n\t\tLOG.debug(\"Peeked head element from unordered stream element queue with filling degree \" +\n\t\t\t\"({}/{}).\", numberEntries, capacity);\n\n\t\treturn completedQueue.peek();\n\t} finally {\n\t\tlock.unlock();\n\t}\n}",
        "reject_response": "@Override\npublic AsyncResult peekBlockingly() throws InterruptedException {\n\tlock.lockInterruptibly();\n\n\ttry {\n\t\twhile (completedQueue.isEmpty()) {\n\t\t\thasCompletedEntries.await();\n\t\t}\n\n\t\tLOG.debug(\"Peeked head element from ordered stream element queue with filling degree \" +\n\n\t\treturn completedQueue.peek();\n\t} finally {\n\t\tlock.unlock();\n\t}\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2788,
        "instruction": "ClassicHttpResponse negotiateResponseFromVariants(\n        final HttpHost target,\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain,\n        final Map<String, Variant> variants) throws IOException, HttpException {\n    final ClassicHttpRequest conditionalRequest = conditionalRequestBuilder.buildConditionalRequestFromVariants(request, variants);\n\n    final Date requestDate = getCurrentDate();\n    final ClassicHttpResponse backendResponse = chain.proceed(conditionalRequest, scope);\n    try {\n        final Date responseDate = getCurrentDate();\n\n        backendResponse.addHeader(\"Via\", generateViaHeader(backendResponse));\n\n        if (backendResponse.getCode() != HttpStatus.SC_NOT_MODIFIED) {\n            return handleBackendResponse(target, request, scope, requestDate, responseDate, backendResponse);\n        }\n\n        final Header resultEtagHeader = backendResponse.getFirstHeader(HeaderConstants.ETAG);\n        if (resultEtagHeader == null) {\n            EntityUtils.consume(backendResponse.getEntity());\n            backendResponse.close();\n            return callBackend(target, request, scope, chain);\n        }\n\n        final String resultEtag = resultEtagHeader.getValue();\n        final Variant matchingVariant = variants.get(resultEtag);\n        if (matchingVariant == null) {\n            LOG.debug(\"304 response did not contain ETag matching one sent in If-None-Match\");\n            EntityUtils.consume(backendResponse.getEntity());\n            backendResponse.close();\n            return callBackend(target, request, scope, chain);\n        }\n\n        if (revalidationResponseIsTooOld(backendResponse, matchingVariant.getEntry())\n                && (request.getEntity() == null || request.getEntity().isRepeatable())) {\n            EntityUtils.consume(backendResponse.getEntity());\n            backendResponse.close();\n            final ClassicHttpRequest unconditional = conditionalRequestBuilder.buildUnconditionalRequest(request);\n            return callBackend(target, unconditional, scope, chain);\n        }\n\n        recordCacheUpdate(scope.clientContext);\n\n        final HttpCacheEntry responseEntry = responseCache.updateVariantCacheEntry(\n                target, conditionalRequest, backendResponse, matchingVariant, requestDate, responseDate);\n        backendResponse.close();\n        if (shouldSendNotModifiedResponse(request, responseEntry)) {\n            return convert(responseGenerator.generateNotModifiedResponse(responseEntry), scope);\n        }\n        final SimpleHttpResponse response = responseGenerator.generateResponse(request, responseEntry);\n        responseCache.reuseVariantEntryFor(target, request, matchingVariant);\n        return convert(response, scope);\n    } catch (final IOException | RuntimeException ex) {\n        backendResponse.close();\n        throw ex;\n    }\n}",
        "accept_response": "ClassicHttpResponse negotiateResponseFromVariants(\n        final HttpHost target,\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain,\n        final Map<String, Variant> variants) throws IOException, HttpException {\n    final ClassicHttpRequest conditionalRequest = conditionalRequestBuilder.buildConditionalRequestFromVariants(request, variants);\n\n    final Date requestDate = getCurrentDate();\n    final ClassicHttpResponse backendResponse = chain.proceed(conditionalRequest, scope);\n    try {\n        final Date responseDate = getCurrentDate();\n\n        backendResponse.addHeader(\"Via\", generateViaHeader(backendResponse));\n\n        if (backendResponse.getCode() != HttpStatus.SC_NOT_MODIFIED) {\n            return handleBackendResponse(target, request, scope, requestDate, responseDate, backendResponse);\n        }\n\n        final Header resultEtagHeader = backendResponse.getFirstHeader(HeaderConstants.ETAG);\n        if (resultEtagHeader == null) {\n            LOG.warn(\"304 response did not contain ETag\");\n            EntityUtils.consume(backendResponse.getEntity());\n            backendResponse.close();\n            return callBackend(target, request, scope, chain);\n        }\n\n        final String resultEtag = resultEtagHeader.getValue();\n        final Variant matchingVariant = variants.get(resultEtag);\n        if (matchingVariant == null) {\n            LOG.debug(\"304 response did not contain ETag matching one sent in If-None-Match\");\n            EntityUtils.consume(backendResponse.getEntity());\n            backendResponse.close();\n            return callBackend(target, request, scope, chain);\n        }\n\n        if (revalidationResponseIsTooOld(backendResponse, matchingVariant.getEntry())\n                && (request.getEntity() == null || request.getEntity().isRepeatable())) {\n            EntityUtils.consume(backendResponse.getEntity());\n            backendResponse.close();\n            final ClassicHttpRequest unconditional = conditionalRequestBuilder.buildUnconditionalRequest(request);\n            return callBackend(target, unconditional, scope, chain);\n        }\n\n        recordCacheUpdate(scope.clientContext);\n\n        final HttpCacheEntry responseEntry = responseCache.updateVariantCacheEntry(\n                target, conditionalRequest, backendResponse, matchingVariant, requestDate, responseDate);\n        backendResponse.close();\n        if (shouldSendNotModifiedResponse(request, responseEntry)) {\n            return convert(responseGenerator.generateNotModifiedResponse(responseEntry), scope);\n        }\n        final SimpleHttpResponse response = responseGenerator.generateResponse(request, responseEntry);\n        responseCache.reuseVariantEntryFor(target, request, matchingVariant);\n        return convert(response, scope);\n    } catch (final IOException | RuntimeException ex) {\n        backendResponse.close();\n        throw ex;\n    }\n}",
        "reject_response": "ClassicHttpResponse negotiateResponseFromVariants(\n        final HttpHost target,\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain,\n        final Map<String, Variant> variants) throws IOException, HttpException {\n    final ClassicHttpRequest conditionalRequest = conditionalRequestBuilder.buildConditionalRequestFromVariants(request, variants);\n\n    final Date requestDate = getCurrentDate();\n    final ClassicHttpResponse backendResponse = chain.proceed(conditionalRequest, scope);\n    try {\n        final Date responseDate = getCurrentDate();\n\n        backendResponse.addHeader(\"Via\", generateViaHeader(backendResponse));\n\n        if (backendResponse.getCode() != HttpStatus.SC_NOT_MODIFIED) {\n            return handleBackendResponse(target, request, scope, requestDate, responseDate, backendResponse);\n        }\n\n        final Header resultEtagHeader = backendResponse.getFirstHeader(HeaderConstants.ETAG);\n        if (resultEtagHeader == null) {\n            log.warn(\"304 response did not contain ETag\");\n            EntityUtils.consume(backendResponse.getEntity());\n            backendResponse.close();\n            return callBackend(target, request, scope, chain);\n        }\n\n        final String resultEtag = resultEtagHeader.getValue();\n        final Variant matchingVariant = variants.get(resultEtag);\n        if (matchingVariant == null) {\n            LOG.debug(\"304 response did not contain ETag matching one sent in If-None-Match\");\n            EntityUtils.consume(backendResponse.getEntity());\n            backendResponse.close();\n            return callBackend(target, request, scope, chain);\n        }\n\n        if (revalidationResponseIsTooOld(backendResponse, matchingVariant.getEntry())\n                && (request.getEntity() == null || request.getEntity().isRepeatable())) {\n            EntityUtils.consume(backendResponse.getEntity());\n            backendResponse.close();\n            final ClassicHttpRequest unconditional = conditionalRequestBuilder.buildUnconditionalRequest(request);\n            return callBackend(target, unconditional, scope, chain);\n        }\n\n        recordCacheUpdate(scope.clientContext);\n\n        final HttpCacheEntry responseEntry = responseCache.updateVariantCacheEntry(\n                target, conditionalRequest, backendResponse, matchingVariant, requestDate, responseDate);\n        backendResponse.close();\n        if (shouldSendNotModifiedResponse(request, responseEntry)) {\n            return convert(responseGenerator.generateNotModifiedResponse(responseEntry), scope);\n        }\n        final SimpleHttpResponse response = responseGenerator.generateResponse(request, responseEntry);\n        responseCache.reuseVariantEntryFor(target, request, matchingVariant);\n        return convert(response, scope);\n    } catch (final IOException | RuntimeException ex) {\n        backendResponse.close();\n        throw ex;\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2591,
        "instruction": "private void completeBufferedSequence() throws IOException {\n\n\tcurrentBuffered.cleanup();\n\tcurrentBuffered = queuedBuffered.pollFirst();\n\tif (currentBuffered != null) {\n\t\tcurrentBuffered.open();\n\t\tnumQueuedBytes -= currentBuffered.size();\n\t}\n}",
        "accept_response": "private void completeBufferedSequence() throws IOException {\n\tLOG.debug(\"{}: Finished feeding back buffered data.\", inputGate.getOwningTaskName());\n\n\tcurrentBuffered.cleanup();\n\tcurrentBuffered = queuedBuffered.pollFirst();\n\tif (currentBuffered != null) {\n\t\tcurrentBuffered.open();\n\t\tnumQueuedBytes -= currentBuffered.size();\n\t}\n}",
        "reject_response": "private void completeBufferedSequence() throws IOException {\n\tLOG.debug(\"Finished feeding back buffered data\");\n\n\tcurrentBuffered.cleanup();\n\tcurrentBuffered = queuedBuffered.pollFirst();\n\tif (currentBuffered != null) {\n\t\tcurrentBuffered.open();\n\t\tnumQueuedBytes -= currentBuffered.size();\n\t}\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2953,
        "instruction": "@POST\n@Consumes(MediaType.WILDCARD)\n@Produces(MediaType.TEXT_PLAIN)\n@Path(SAMLEndpoints.LOGIN_EXCHANGE_RELATIVE)\n@ApiOperation(\n        value = \"Retrieves a JWT following a successful login sequence using the configured SAML identity provider.\",\n        response = String.class,\n        notes = NON_GUARANTEED_ENDPOINT\n)\npublic Response samlLoginExchange(@Context HttpServletRequest httpServletRequest,\n                                  @Context HttpServletResponse httpServletResponse) throws Exception {\n\n    // only consider user specific access over https\n    if (!httpServletRequest.isSecure()) {\n        throw new AuthenticationNotSupportedException(AUTHENTICATION_NOT_ENABLED_MSG);\n    }\n\n    // ensure saml is enabled\n    if (!samlService.isSamlEnabled()) {\n        return Response.status(Response.Status.CONFLICT).entity(SAMLService.SAML_SUPPORT_IS_NOT_CONFIGURED).build();\n    }\n\n    logger.info(\"Attempting to exchange SAML login request for a NiFi JWT...\");\n\n    // ensure saml service provider is initialized\n    initializeSamlServiceProvider();\n\n    // ensure the request has the cookie with the request identifier\n    final String samlRequestIdentifier = getCookieValue(httpServletRequest.getCookies(), SAML_REQUEST_IDENTIFIER);\n    if (samlRequestIdentifier == null) {\n        final String message = \"The login request identifier was not found in the request. Unable to continue.\";\n        logger.warn(message);\n        return Response.status(Response.Status.BAD_REQUEST).entity(message).build();\n    }\n\n    // remove the saml request cookie\n    removeSamlRequestCookie(httpServletResponse);\n\n    // get the jwt\n    final String jwt = samlStateManager.getJwt(samlRequestIdentifier);\n    if (jwt == null) {\n        throw new IllegalArgumentException(\"A JWT for this login request identifier could not be found. Unable to continue.\");\n    }\n\n    // generate the response\n    logger.info(\"SAML login exchange complete\");\n    return generateOkResponse(jwt).build();\n}",
        "accept_response": "@POST\n@Consumes(MediaType.WILDCARD)\n@Produces(MediaType.TEXT_PLAIN)\n@Path(SAMLEndpoints.LOGIN_EXCHANGE_RELATIVE)\n@ApiOperation(\n        value = \"Retrieves a JWT following a successful login sequence using the configured SAML identity provider.\",\n        response = String.class,\n        notes = NON_GUARANTEED_ENDPOINT\n)\npublic Response samlLoginExchange(@Context HttpServletRequest httpServletRequest,\n                                  @Context HttpServletResponse httpServletResponse) throws Exception {\n\n    // only consider user specific access over https\n    if (!httpServletRequest.isSecure()) {\n        throw new AuthenticationNotSupportedException(AUTHENTICATION_NOT_ENABLED_MSG);\n    }\n\n    // ensure saml is enabled\n    if (!samlService.isSamlEnabled()) {\n        logger.debug(SAMLService.SAML_SUPPORT_IS_NOT_CONFIGURED);\n        return Response.status(Response.Status.CONFLICT).entity(SAMLService.SAML_SUPPORT_IS_NOT_CONFIGURED).build();\n    }\n\n    logger.info(\"Attempting to exchange SAML login request for a NiFi JWT...\");\n\n    // ensure saml service provider is initialized\n    initializeSamlServiceProvider();\n\n    // ensure the request has the cookie with the request identifier\n    final String samlRequestIdentifier = getCookieValue(httpServletRequest.getCookies(), SAML_REQUEST_IDENTIFIER);\n    if (samlRequestIdentifier == null) {\n        final String message = \"The login request identifier was not found in the request. Unable to continue.\";\n        logger.warn(message);\n        return Response.status(Response.Status.BAD_REQUEST).entity(message).build();\n    }\n\n    // remove the saml request cookie\n    removeSamlRequestCookie(httpServletResponse);\n\n    // get the jwt\n    final String jwt = samlStateManager.getJwt(samlRequestIdentifier);\n    if (jwt == null) {\n        throw new IllegalArgumentException(\"A JWT for this login request identifier could not be found. Unable to continue.\");\n    }\n\n    // generate the response\n    logger.info(\"SAML login exchange complete\");\n    return generateOkResponse(jwt).build();\n}",
        "reject_response": "@POST\n@Consumes(MediaType.WILDCARD)\n@Produces(MediaType.TEXT_PLAIN)\n@Path(SAMLEndpoints.LOGIN_EXCHANGE_RELATIVE)\n@ApiOperation(\n        value = \"Retrieves a JWT following a successful login sequence using the configured SAML identity provider.\",\n        response = String.class,\n        notes = NON_GUARANTEED_ENDPOINT\n)\npublic Response samlLoginExchange(@Context HttpServletRequest httpServletRequest,\n                                  @Context HttpServletResponse httpServletResponse) throws Exception {\n\n    // only consider user specific access over https\n    if (!httpServletRequest.isSecure()) {\n        throw new AuthenticationNotSupportedException(AUTHENTICATION_NOT_ENABLED_MSG);\n    }\n\n    // ensure saml is enabled\n    if (!samlService.isSamlEnabled()) {\n        logger.warn(SAMLService.SAML_SUPPORT_IS_NOT_CONFIGURED);\n        return Response.status(Response.Status.CONFLICT).entity(SAMLService.SAML_SUPPORT_IS_NOT_CONFIGURED).build();\n    }\n\n    logger.info(\"Attempting to exchange SAML login request for a NiFi JWT...\");\n\n    // ensure saml service provider is initialized\n    initializeSamlServiceProvider();\n\n    // ensure the request has the cookie with the request identifier\n    final String samlRequestIdentifier = getCookieValue(httpServletRequest.getCookies(), SAML_REQUEST_IDENTIFIER);\n    if (samlRequestIdentifier == null) {\n        final String message = \"The login request identifier was not found in the request. Unable to continue.\";\n        logger.warn(message);\n        return Response.status(Response.Status.BAD_REQUEST).entity(message).build();\n    }\n\n    // remove the saml request cookie\n    removeSamlRequestCookie(httpServletResponse);\n\n    // get the jwt\n    final String jwt = samlStateManager.getJwt(samlRequestIdentifier);\n    if (jwt == null) {\n        throw new IllegalArgumentException(\"A JWT for this login request identifier could not be found. Unable to continue.\");\n    }\n\n    // generate the response\n    logger.info(\"SAML login exchange complete\");\n    return generateOkResponse(jwt).build();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2696,
        "instruction": "Response readResponse(final Bridge bridge, ProtocolData protData,\n                      final boolean threadSafe) {\n    final int fragment = protData.getDataFragment();\n    final String dataDir = protData.getDataSource();\n\n    // Creating an internal streaming class\n    // which will iterate the records and put them on the\n    // output stream\n    final StreamingOutput streaming = new StreamingOutput() {\n        @Override\n        public void write(final OutputStream out) throws IOException,\n                WebApplicationException {\n            long recordCount = 0;\n\n            if (!threadSafe) {\n                lock(dataDir);\n            }\n            try {\n\n                if (!bridge.beginIteration()) {\n                    return;\n                }\n\n                Writable record;\n                DataOutputStream dos = new DataOutputStream(out);\n                LOG.debug(\"Starting streaming fragment \" + fragment\n                        + \" of resource \" + dataDir);\n                while ((record = bridge.getNext()) != null) {\n                    record.write(dos);\n                    ++recordCount;\n                }\n                LOG.debug(\"Finished streaming fragment \" + fragment\n                        + \" of resource \" + dataDir + \", \" + recordCount\n                        + \" records.\");\n            } catch (ClientAbortException e) {\n                // Occurs whenever client (HAWQ) decides the end the\n                // connection\n            } catch (Exception e) {\n                LOG.error(\"Exception thrown when streaming\", e);\n                throw new IOException(e.getMessage());\n            } finally {\n                LOG.debug(\"Stopped streaming fragment \" + fragment\n                        + \" of resource \" + dataDir + \", \" + recordCount\n                        + \" records.\");\n                if (!threadSafe) {\n                    unlock(dataDir);\n                }\n            }\n        }\n    };\n\n    return Response.ok(streaming, MediaType.APPLICATION_OCTET_STREAM).build();\n}",
        "accept_response": "Response readResponse(final Bridge bridge, ProtocolData protData,\n                      final boolean threadSafe) {\n    final int fragment = protData.getDataFragment();\n    final String dataDir = protData.getDataSource();\n\n    // Creating an internal streaming class\n    // which will iterate the records and put them on the\n    // output stream\n    final StreamingOutput streaming = new StreamingOutput() {\n        @Override\n        public void write(final OutputStream out) throws IOException,\n                WebApplicationException {\n            long recordCount = 0;\n\n            if (!threadSafe) {\n                lock(dataDir);\n            }\n            try {\n\n                if (!bridge.beginIteration()) {\n                    return;\n                }\n\n                Writable record;\n                DataOutputStream dos = new DataOutputStream(out);\n                LOG.debug(\"Starting streaming fragment \" + fragment\n                        + \" of resource \" + dataDir);\n                while ((record = bridge.getNext()) != null) {\n                    record.write(dos);\n                    ++recordCount;\n                }\n                LOG.debug(\"Finished streaming fragment \" + fragment\n                        + \" of resource \" + dataDir + \", \" + recordCount\n                        + \" records.\");\n            } catch (ClientAbortException e) {\n                // Occurs whenever client (HAWQ) decides the end the\n                // connection\n                LOG.error(\"Remote connection closed by HAWQ\", e);\n            } catch (Exception e) {\n                LOG.error(\"Exception thrown when streaming\", e);\n                throw new IOException(e.getMessage());\n            } finally {\n                LOG.debug(\"Stopped streaming fragment \" + fragment\n                        + \" of resource \" + dataDir + \", \" + recordCount\n                        + \" records.\");\n                if (!threadSafe) {\n                    unlock(dataDir);\n                }\n            }\n        }\n    };\n\n    return Response.ok(streaming, MediaType.APPLICATION_OCTET_STREAM).build();\n}",
        "reject_response": "Response readResponse(final Bridge bridge, ProtocolData protData,\n                      final boolean threadSafe) {\n    final int fragment = protData.getDataFragment();\n    final String dataDir = protData.getDataSource();\n\n    // Creating an internal streaming class\n    // which will iterate the records and put them on the\n    // output stream\n    final StreamingOutput streaming = new StreamingOutput() {\n        @Override\n        public void write(final OutputStream out) throws IOException,\n                WebApplicationException {\n            long recordCount = 0;\n\n            if (!threadSafe) {\n                lock(dataDir);\n            }\n            try {\n\n                if (!bridge.beginIteration()) {\n                    return;\n                }\n\n                Writable record;\n                DataOutputStream dos = new DataOutputStream(out);\n                LOG.debug(\"Starting streaming fragment \" + fragment\n                        + \" of resource \" + dataDir);\n                while ((record = bridge.getNext()) != null) {\n                    record.write(dos);\n                    ++recordCount;\n                }\n                LOG.debug(\"Finished streaming fragment \" + fragment\n                        + \" of resource \" + dataDir + \", \" + recordCount\n                        + \" records.\");\n            } catch (ClientAbortException e) {\n                // Occurs whenever client (HAWQ) decides the end the\n                // connection\n                Log.error(\"Remote connection closed by HAWQ\", e);\n            } catch (Exception e) {\n                LOG.error(\"Exception thrown when streaming\", e);\n                throw new IOException(e.getMessage());\n            } finally {\n                LOG.debug(\"Stopped streaming fragment \" + fragment\n                        + \" of resource \" + dataDir + \", \" + recordCount\n                        + \" records.\");\n                if (!threadSafe) {\n                    unlock(dataDir);\n                }\n            }\n        }\n    };\n\n    return Response.ok(streaming, MediaType.APPLICATION_OCTET_STREAM).build();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2796,
        "instruction": "@Override\npublic InetAddress[] resolve(final String host) throws UnknownHostException {\n    final InetAddress[] resolvedAddresses = dnsMap.get(host);\n    if(resolvedAddresses == null){\n        throw new UnknownHostException(host + \" cannot be resolved\");\n    }\n    return resolvedAddresses;\n}",
        "accept_response": "@Override\npublic InetAddress[] resolve(final String host) throws UnknownHostException {\n    final InetAddress[] resolvedAddresses = dnsMap.get(host);\n    if (LOG.isInfoEnabled()) {\n        LOG.info(\"Resolving {} to {}\", host, Arrays.deepToString(resolvedAddresses));\n    }\n    if(resolvedAddresses == null){\n        throw new UnknownHostException(host + \" cannot be resolved\");\n    }\n    return resolvedAddresses;\n}",
        "reject_response": "@Override\npublic InetAddress[] resolve(final String host) throws UnknownHostException {\n    final InetAddress[] resolvedAddresses = dnsMap.get(host);\n    if (LOG.isInfoEnabled()) {\n    if (log.isInfoEnabled()) {\n        log.info(\"Resolving {} to {}\", host, Arrays.deepToString(resolvedAddresses));\n    }\n    if(resolvedAddresses == null){\n        throw new UnknownHostException(host + \" cannot be resolved\");\n    }\n    return resolvedAddresses;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2809,
        "instruction": "@Override public Map<K, V> loadAll(Iterable<? extends K> keys) {\n    if (log.isDebugEnabled())\n\n    Map<K, V> loaded = new HashMap<>();\n\n    Collection<K> remaining = null;\n\n    for (K key : keys) {\n        StatefulValue<K, V> val;\n\n        if (writeCoalescing)\n            val = writeCache.get(key);\n        else\n            val = flusher(key).flusherWriteMap.get(key);\n\n        if (val != null) {\n            val.readLock().lock();\n\n            try {\n                if (val.operation() == StoreOperation.PUT)\n                    loaded.put(key, val.entry().getValue());\n                else\n                    assert val.operation() == StoreOperation.RMV : val.operation();\n            }\n            finally {\n                val.readLock().unlock();\n            }\n        }\n        else {\n            if (remaining == null)\n                remaining = new ArrayList<>();\n\n            remaining.add(key);\n        }\n    }\n\n    // For items that were not found in queue.\n    if (remaining != null && !remaining.isEmpty()) {\n        Map<K, V> loaded0 = store.loadAll(remaining);\n\n        if (loaded0 != null)\n            loaded.putAll(loaded0);\n    }\n\n    return loaded;\n}",
        "accept_response": "@Override public Map<K, V> loadAll(Iterable<? extends K> keys) {\n    if (log.isDebugEnabled())\n        log.debug(S.toString(\"Store load all\",\n            \"keys\", keys, true));\n\n    Map<K, V> loaded = new HashMap<>();\n\n    Collection<K> remaining = null;\n\n    for (K key : keys) {\n        StatefulValue<K, V> val;\n\n        if (writeCoalescing)\n            val = writeCache.get(key);\n        else\n            val = flusher(key).flusherWriteMap.get(key);\n\n        if (val != null) {\n            val.readLock().lock();\n\n            try {\n                if (val.operation() == StoreOperation.PUT)\n                    loaded.put(key, val.entry().getValue());\n                else\n                    assert val.operation() == StoreOperation.RMV : val.operation();\n            }\n            finally {\n                val.readLock().unlock();\n            }\n        }\n        else {\n            if (remaining == null)\n                remaining = new ArrayList<>();\n\n            remaining.add(key);\n        }\n    }\n\n    // For items that were not found in queue.\n    if (remaining != null && !remaining.isEmpty()) {\n        Map<K, V> loaded0 = store.loadAll(remaining);\n\n        if (loaded0 != null)\n            loaded.putAll(loaded0);\n    }\n\n    return loaded;\n}",
        "reject_response": "@Override public Map<K, V> loadAll(Iterable<? extends K> keys) {\n    if (log.isDebugEnabled())\n        log.debug(\"Store load all [keys=\" + keys + ']');\n\n    Map<K, V> loaded = new HashMap<>();\n\n    Collection<K> remaining = null;\n\n    for (K key : keys) {\n        StatefulValue<K, V> val;\n\n        if (writeCoalescing)\n            val = writeCache.get(key);\n        else\n            val = flusher(key).flusherWriteMap.get(key);\n\n        if (val != null) {\n            val.readLock().lock();\n\n            try {\n                if (val.operation() == StoreOperation.PUT)\n                    loaded.put(key, val.entry().getValue());\n                else\n                    assert val.operation() == StoreOperation.RMV : val.operation();\n            }\n            finally {\n                val.readLock().unlock();\n            }\n        }\n        else {\n            if (remaining == null)\n                remaining = new ArrayList<>();\n\n            remaining.add(key);\n        }\n    }\n\n    // For items that were not found in queue.\n    if (remaining != null && !remaining.isEmpty()) {\n        Map<K, V> loaded0 = store.loadAll(remaining);\n\n        if (loaded0 != null)\n            loaded.putAll(loaded0);\n    }\n\n    return loaded;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2705,
        "instruction": "private Response wrongPath(String path) {\n\n    String errmsg;\n\n    List<PathSegment> pathSegments = rootUri.getPathSegments();\n\n    if(pathSegments.isEmpty()) {\n        return sendErrorMessage(getUnknownPathMsg());\n    }\n\n    String version = pathSegments.get(0).getPath();\n    String endPoint = (pathSegments.size() > 1) ? pathSegments.get(1).getPath() : null;\n\n\n    if(version.equals(Version.PXF_PROTOCOL_VERSION)) { // api with correct version but incorrect path\n        if (retiredEndPoints.contains(endPoint)) { // api with retired endpoint\n            errmsg = getRetiredPathMsg(endPoint);\n        } else {\n            errmsg = getUnknownPathMsg();\n        }\n    } else if(!(version.matches(\"v[0-9]+\"))) { // api with version not of the format \"v<number>\"\n        errmsg = getUnknownPathMsg();\n    } else { // api with wrong version number\n        errmsg = \"Wrong version \" + version + \", supported version is \" + Version.PXF_PROTOCOL_VERSION;\n    }\n\n    return sendErrorMessage(errmsg);\n}",
        "accept_response": "private Response wrongPath(String path) {\n\n    String errmsg;\n\n    List<PathSegment> pathSegments = rootUri.getPathSegments();\n\n    if(pathSegments.isEmpty()) {\n        return sendErrorMessage(getUnknownPathMsg());\n    }\n\n    String version = pathSegments.get(0).getPath();\n    String endPoint = (pathSegments.size() > 1) ? pathSegments.get(1).getPath() : null;\n\n    LOG.debug(\"REST request: \" + rootUri.getAbsolutePath() + \". \" +\n            \"Version \" + version + \", supported version is \" + Version.PXF_PROTOCOL_VERSION);\n\n    if(version.equals(Version.PXF_PROTOCOL_VERSION)) { // api with correct version but incorrect path\n        if (retiredEndPoints.contains(endPoint)) { // api with retired endpoint\n            errmsg = getRetiredPathMsg(endPoint);\n        } else {\n            errmsg = getUnknownPathMsg();\n        }\n    } else if(!(version.matches(\"v[0-9]+\"))) { // api with version not of the format \"v<number>\"\n        errmsg = getUnknownPathMsg();\n    } else { // api with wrong version number\n        errmsg = \"Wrong version \" + version + \", supported version is \" + Version.PXF_PROTOCOL_VERSION;\n    }\n\n    return sendErrorMessage(errmsg);\n}",
        "reject_response": "private Response wrongPath(String path) {\n\n    String errmsg;\n\n    List<PathSegment> pathSegments = rootUri.getPathSegments();\n\n    if(pathSegments.isEmpty()) {\n        return sendErrorMessage(getUnknownPathMsg());\n    }\n\n    String version = pathSegments.get(0).getPath();\n    String endPoint = (pathSegments.size() > 1) ? pathSegments.get(1).getPath() : null;\n\n    Log.debug(\"REST request: \" + rootUri.getAbsolutePath() + \". \" +\n\n    if(version.equals(Version.PXF_PROTOCOL_VERSION)) { // api with correct version but incorrect path\n        if (retiredEndPoints.contains(endPoint)) { // api with retired endpoint\n            errmsg = getRetiredPathMsg(endPoint);\n        } else {\n            errmsg = getUnknownPathMsg();\n        }\n    } else if(!(version.matches(\"v[0-9]+\"))) { // api with version not of the format \"v<number>\"\n        errmsg = getUnknownPathMsg();\n    } else { // api with wrong version number\n        errmsg = \"Wrong version \" + version + \", supported version is \" + Version.PXF_PROTOCOL_VERSION;\n    }\n\n    return sendErrorMessage(errmsg);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3254,
        "instruction": "private List<OpResult> validatePath(Iterable<Op> ops) {\n    List<OpResult> results = new ArrayList<OpResult>();\n    boolean error = false;\n    for (Op op : ops) {\n        try {\n            op.validate();\n        } catch (IllegalArgumentException iae) {\n            ErrorResult err = new ErrorResult(KeeperException.Code.BADARGUMENTS.intValue());\n            results.add(err);\n            error = true;\n            continue;\n        } catch (KeeperException ke) {\n            LOG.error(\"Unexpected exception\", ke);\n            ErrorResult err = new ErrorResult(ke.code().intValue());\n            results.add(err);\n            error = true;\n            continue;\n        }\n        ErrorResult err = new ErrorResult(KeeperException.Code.RUNTIMEINCONSISTENCY.intValue());\n        results.add(err);\n    }\n    if (!error) {\n        results.clear();\n    }\n    return results;\n}",
        "accept_response": "private List<OpResult> validatePath(Iterable<Op> ops) {\n    List<OpResult> results = new ArrayList<OpResult>();\n    boolean error = false;\n    for (Op op : ops) {\n        try {\n            op.validate();\n        } catch (IllegalArgumentException iae) {\n            LOG.error(\"Unexpected exception\", iae);\n            ErrorResult err = new ErrorResult(KeeperException.Code.BADARGUMENTS.intValue());\n            results.add(err);\n            error = true;\n            continue;\n        } catch (KeeperException ke) {\n            LOG.error(\"Unexpected exception\", ke);\n            ErrorResult err = new ErrorResult(ke.code().intValue());\n            results.add(err);\n            error = true;\n            continue;\n        }\n        ErrorResult err = new ErrorResult(KeeperException.Code.RUNTIMEINCONSISTENCY.intValue());\n        results.add(err);\n    }\n    if (!error) {\n        results.clear();\n    }\n    return results;\n}",
        "reject_response": "private List<OpResult> validatePath(Iterable<Op> ops) {\n    List<OpResult> results = new ArrayList<OpResult>();\n    boolean error = false;\n    for (Op op : ops) {\n        try {\n            op.validate();\n        } catch (IllegalArgumentException iae) {\n            LOG.error(\"IllegalArgumentException: \" + iae.getMessage());\n            ErrorResult err = new ErrorResult(KeeperException.Code.BADARGUMENTS.intValue());\n            results.add(err);\n            error = true;\n            continue;\n        } catch (KeeperException ke) {\n            LOG.error(\"Unexpected exception\", ke);\n            ErrorResult err = new ErrorResult(ke.code().intValue());\n            results.add(err);\n            error = true;\n            continue;\n        }\n        ErrorResult err = new ErrorResult(KeeperException.Code.RUNTIMEINCONSISTENCY.intValue());\n        results.add(err);\n    }\n    if (!error) {\n        results.clear();\n    }\n    return results;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3184,
        "instruction": "protected void removePendingChangeRequests(\n    List<UpdatedContainer> changedContainers) {\n  for (UpdatedContainer changedContainer : changedContainers) {\n    ContainerId containerId = changedContainer.getContainer().getId();\n    if (pendingChange.get(containerId) == null) {\n      continue;\n    }\n    pendingChange.remove(containerId);\n  }\n}",
        "accept_response": "protected void removePendingChangeRequests(\n    List<UpdatedContainer> changedContainers) {\n  for (UpdatedContainer changedContainer : changedContainers) {\n    ContainerId containerId = changedContainer.getContainer().getId();\n    if (pendingChange.get(containerId) == null) {\n      continue;\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"RM has confirmed changed resource allocation for container {}. \" +\n              \"Current resource allocation:{}. \" +\n              \"Remove pending change request:{}\",\n              containerId,\n              changedContainer.getContainer().getResource(),\n              pendingChange.get(containerId).getValue());\n    }\n    pendingChange.remove(containerId);\n  }\n}",
        "reject_response": "protected void removePendingChangeRequests(\n    List<UpdatedContainer> changedContainers) {\n  for (UpdatedContainer changedContainer : changedContainers) {\n    ContainerId containerId = changedContainer.getContainer().getId();\n    if (pendingChange.get(containerId) == null) {\n      continue;\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"RM has confirmed changed resource allocation for \"\n          + \"container \" + containerId + \". Current resource allocation:\"\n          + changedContainer.getContainer().getResource()\n          + \". Remove pending change request:\"\n          + pendingChange.get(containerId).getValue());\n    }\n    pendingChange.remove(containerId);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3232,
        "instruction": "public void commit(Request request) {\n    if (stopped || request == null) {\n        return;\n    }\n    request.commitRecvTime = Time.currentElapsedTime();\n    ServerMetrics.getMetrics().COMMITS_QUEUED.add(1);\n    committedRequests.add(request);\n    wakeup();\n}",
        "accept_response": "public void commit(Request request) {\n    if (stopped || request == null) {\n        return;\n    }\n    LOG.debug(\"Committing request:: {}\", request);\n    request.commitRecvTime = Time.currentElapsedTime();\n    ServerMetrics.getMetrics().COMMITS_QUEUED.add(1);\n    committedRequests.add(request);\n    wakeup();\n}",
        "reject_response": "public void commit(Request request) {\n    if (stopped || request == null) {\n        return;\n    }\n    if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Committing request:: \" + request);\n    }\n    request.commitRecvTime = Time.currentElapsedTime();\n    ServerMetrics.getMetrics().COMMITS_QUEUED.add(1);\n    committedRequests.add(request);\n    wakeup();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2771,
        "instruction": "@Override\npublic void execute(\n        final HttpRequest request,\n        final AsyncEntityProducer entityProducer,\n        final AsyncExecChain.Scope scope,\n        final AsyncExecChain chain,\n        final AsyncExecCallback asyncExecCallback) throws HttpException, IOException {\n    final String exchangeId = scope.exchangeId;\n    final HttpRoute route = scope.route;\n    final CancellableDependency operation = scope.cancellableDependency;\n    final HttpClientContext clientContext = scope.clientContext;\n    final AsyncExecRuntime execRuntime = scope.execRuntime;\n\n    final AtomicInteger messageCountDown = new AtomicInteger(2);\n    final AsyncClientExchangeHandler internalExchangeHandler = new AsyncClientExchangeHandler() {\n\n        private final AtomicReference<AsyncDataConsumer> entityConsumerRef = new AtomicReference<>(null);\n\n        @Override\n        public void releaseResources() {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.getAndSet(null);\n            if (entityConsumer != null) {\n                entityConsumer.releaseResources();\n            }\n        }\n\n        @Override\n        public void failed(final Exception cause) {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.getAndSet(null);\n            if (entityConsumer != null) {\n                entityConsumer.releaseResources();\n            }\n            execRuntime.markConnectionNonReusable();\n            asyncExecCallback.failed(cause);\n        }\n\n        @Override\n        public void cancel() {\n            failed(new InterruptedIOException());\n        }\n\n        @Override\n        public void produceRequest(\n                final RequestChannel channel,\n                final HttpContext context) throws HttpException, IOException {\n            channel.sendRequest(request, entityProducer, context);\n            if (entityProducer == null) {\n                messageCountDown.decrementAndGet();\n            }\n        }\n\n        @Override\n        public int available() {\n            return entityProducer.available();\n        }\n\n        @Override\n        public void produce(final DataStreamChannel channel) throws IOException {\n            entityProducer.produce(new DataStreamChannel() {\n\n                @Override\n                public void requestOutput() {\n                    channel.requestOutput();\n                }\n\n                @Override\n                public int write(final ByteBuffer src) throws IOException {\n                    return channel.write(src);\n                }\n\n                @Override\n                public void endStream(final List<? extends Header> trailers) throws IOException {\n                    channel.endStream(trailers);\n                    if (messageCountDown.decrementAndGet() <= 0) {\n                        asyncExecCallback.completed();\n                    }\n                }\n\n                @Override\n                public void endStream() throws IOException {\n                    channel.endStream();\n                    if (messageCountDown.decrementAndGet() <= 0) {\n                        asyncExecCallback.completed();\n                    }\n                }\n\n            });\n        }\n\n        @Override\n        public void consumeInformation(\n                final HttpResponse response,\n                final HttpContext context) throws HttpException, IOException {\n            asyncExecCallback.handleInformationResponse(response);\n        }\n\n        @Override\n        public void consumeResponse(\n                final HttpResponse response,\n                final EntityDetails entityDetails,\n                final HttpContext context) throws HttpException, IOException {\n            entityConsumerRef.set(asyncExecCallback.handleResponse(response, entityDetails));\n            if (response.getCode() >= HttpStatus.SC_CLIENT_ERROR) {\n                messageCountDown.decrementAndGet();\n            }\n            final TimeValue keepAliveDuration = keepAliveStrategy.getKeepAliveDuration(response, clientContext);\n            Object userToken = clientContext.getUserToken();\n            if (userToken == null) {\n                userToken = userTokenHandler.getUserToken(route, clientContext);\n                clientContext.setAttribute(HttpClientContext.USER_TOKEN, userToken);\n            }\n            execRuntime.markConnectionReusable(userToken, keepAliveDuration);\n            if (entityDetails == null) {\n                execRuntime.validateConnection();\n                if (messageCountDown.decrementAndGet() <= 0) {\n                    asyncExecCallback.completed();\n                }\n            }\n        }\n\n        @Override\n        public void updateCapacity(final CapacityChannel capacityChannel) throws IOException {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.get();\n            if (entityConsumer != null) {\n                entityConsumer.updateCapacity(capacityChannel);\n            } else {\n                capacityChannel.update(Integer.MAX_VALUE);\n            }\n        }\n\n        @Override\n        public void consume(final ByteBuffer src) throws IOException {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.get();\n            if (entityConsumer != null) {\n                entityConsumer.consume(src);\n            }\n        }\n\n        @Override\n        public void streamEnd(final List<? extends Header> trailers) throws HttpException, IOException {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.getAndSet(null);\n            if (entityConsumer != null) {\n                entityConsumer.streamEnd(trailers);\n            } else {\n                execRuntime.validateConnection();\n            }\n            if (messageCountDown.decrementAndGet() <= 0) {\n                asyncExecCallback.completed();\n            }\n        }\n\n    };\n\n    if (log.isDebugEnabled()) {\n        operation.setDependency(execRuntime.execute(\n                exchangeId,\n                new LoggingAsyncClientExchangeHandler(log, exchangeId, internalExchangeHandler),\n                clientContext));\n    } else {\n        operation.setDependency(execRuntime.execute(exchangeId, internalExchangeHandler, clientContext));\n    }\n}",
        "accept_response": "@Override\npublic void execute(\n        final HttpRequest request,\n        final AsyncEntityProducer entityProducer,\n        final AsyncExecChain.Scope scope,\n        final AsyncExecChain chain,\n        final AsyncExecCallback asyncExecCallback) throws HttpException, IOException {\n    final String exchangeId = scope.exchangeId;\n    final HttpRoute route = scope.route;\n    final CancellableDependency operation = scope.cancellableDependency;\n    final HttpClientContext clientContext = scope.clientContext;\n    final AsyncExecRuntime execRuntime = scope.execRuntime;\n\n    if (log.isDebugEnabled()) {\n        log.debug(\"{}: executing {}\", exchangeId, new RequestLine(request));\n    }\n\n    final AtomicInteger messageCountDown = new AtomicInteger(2);\n    final AsyncClientExchangeHandler internalExchangeHandler = new AsyncClientExchangeHandler() {\n\n        private final AtomicReference<AsyncDataConsumer> entityConsumerRef = new AtomicReference<>(null);\n\n        @Override\n        public void releaseResources() {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.getAndSet(null);\n            if (entityConsumer != null) {\n                entityConsumer.releaseResources();\n            }\n        }\n\n        @Override\n        public void failed(final Exception cause) {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.getAndSet(null);\n            if (entityConsumer != null) {\n                entityConsumer.releaseResources();\n            }\n            execRuntime.markConnectionNonReusable();\n            asyncExecCallback.failed(cause);\n        }\n\n        @Override\n        public void cancel() {\n            failed(new InterruptedIOException());\n        }\n\n        @Override\n        public void produceRequest(\n                final RequestChannel channel,\n                final HttpContext context) throws HttpException, IOException {\n            channel.sendRequest(request, entityProducer, context);\n            if (entityProducer == null) {\n                messageCountDown.decrementAndGet();\n            }\n        }\n\n        @Override\n        public int available() {\n            return entityProducer.available();\n        }\n\n        @Override\n        public void produce(final DataStreamChannel channel) throws IOException {\n            entityProducer.produce(new DataStreamChannel() {\n\n                @Override\n                public void requestOutput() {\n                    channel.requestOutput();\n                }\n\n                @Override\n                public int write(final ByteBuffer src) throws IOException {\n                    return channel.write(src);\n                }\n\n                @Override\n                public void endStream(final List<? extends Header> trailers) throws IOException {\n                    channel.endStream(trailers);\n                    if (messageCountDown.decrementAndGet() <= 0) {\n                        asyncExecCallback.completed();\n                    }\n                }\n\n                @Override\n                public void endStream() throws IOException {\n                    channel.endStream();\n                    if (messageCountDown.decrementAndGet() <= 0) {\n                        asyncExecCallback.completed();\n                    }\n                }\n\n            });\n        }\n\n        @Override\n        public void consumeInformation(\n                final HttpResponse response,\n                final HttpContext context) throws HttpException, IOException {\n            asyncExecCallback.handleInformationResponse(response);\n        }\n\n        @Override\n        public void consumeResponse(\n                final HttpResponse response,\n                final EntityDetails entityDetails,\n                final HttpContext context) throws HttpException, IOException {\n            entityConsumerRef.set(asyncExecCallback.handleResponse(response, entityDetails));\n            if (response.getCode() >= HttpStatus.SC_CLIENT_ERROR) {\n                messageCountDown.decrementAndGet();\n            }\n            final TimeValue keepAliveDuration = keepAliveStrategy.getKeepAliveDuration(response, clientContext);\n            Object userToken = clientContext.getUserToken();\n            if (userToken == null) {\n                userToken = userTokenHandler.getUserToken(route, clientContext);\n                clientContext.setAttribute(HttpClientContext.USER_TOKEN, userToken);\n            }\n            execRuntime.markConnectionReusable(userToken, keepAliveDuration);\n            if (entityDetails == null) {\n                execRuntime.validateConnection();\n                if (messageCountDown.decrementAndGet() <= 0) {\n                    asyncExecCallback.completed();\n                }\n            }\n        }\n\n        @Override\n        public void updateCapacity(final CapacityChannel capacityChannel) throws IOException {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.get();\n            if (entityConsumer != null) {\n                entityConsumer.updateCapacity(capacityChannel);\n            } else {\n                capacityChannel.update(Integer.MAX_VALUE);\n            }\n        }\n\n        @Override\n        public void consume(final ByteBuffer src) throws IOException {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.get();\n            if (entityConsumer != null) {\n                entityConsumer.consume(src);\n            }\n        }\n\n        @Override\n        public void streamEnd(final List<? extends Header> trailers) throws HttpException, IOException {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.getAndSet(null);\n            if (entityConsumer != null) {\n                entityConsumer.streamEnd(trailers);\n            } else {\n                execRuntime.validateConnection();\n            }\n            if (messageCountDown.decrementAndGet() <= 0) {\n                asyncExecCallback.completed();\n            }\n        }\n\n    };\n\n    if (log.isDebugEnabled()) {\n        operation.setDependency(execRuntime.execute(\n                exchangeId,\n                new LoggingAsyncClientExchangeHandler(log, exchangeId, internalExchangeHandler),\n                clientContext));\n    } else {\n        operation.setDependency(execRuntime.execute(exchangeId, internalExchangeHandler, clientContext));\n    }\n}",
        "reject_response": "@Override\npublic void execute(\n        final HttpRequest request,\n        final AsyncEntityProducer entityProducer,\n        final AsyncExecChain.Scope scope,\n        final AsyncExecChain chain,\n        final AsyncExecCallback asyncExecCallback) throws HttpException, IOException {\n    final String exchangeId = scope.exchangeId;\n    final HttpRoute route = scope.route;\n    final CancellableDependency operation = scope.cancellableDependency;\n    final HttpClientContext clientContext = scope.clientContext;\n    final AsyncExecRuntime execRuntime = scope.execRuntime;\n\n    if (log.isDebugEnabled()) {\n        log.debug(exchangeId + \": executing \" + new RequestLine(request));\n    }\n\n    final AtomicInteger messageCountDown = new AtomicInteger(2);\n    final AsyncClientExchangeHandler internalExchangeHandler = new AsyncClientExchangeHandler() {\n\n        private final AtomicReference<AsyncDataConsumer> entityConsumerRef = new AtomicReference<>(null);\n\n        @Override\n        public void releaseResources() {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.getAndSet(null);\n            if (entityConsumer != null) {\n                entityConsumer.releaseResources();\n            }\n        }\n\n        @Override\n        public void failed(final Exception cause) {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.getAndSet(null);\n            if (entityConsumer != null) {\n                entityConsumer.releaseResources();\n            }\n            execRuntime.markConnectionNonReusable();\n            asyncExecCallback.failed(cause);\n        }\n\n        @Override\n        public void cancel() {\n            failed(new InterruptedIOException());\n        }\n\n        @Override\n        public void produceRequest(\n                final RequestChannel channel,\n                final HttpContext context) throws HttpException, IOException {\n            channel.sendRequest(request, entityProducer, context);\n            if (entityProducer == null) {\n                messageCountDown.decrementAndGet();\n            }\n        }\n\n        @Override\n        public int available() {\n            return entityProducer.available();\n        }\n\n        @Override\n        public void produce(final DataStreamChannel channel) throws IOException {\n            entityProducer.produce(new DataStreamChannel() {\n\n                @Override\n                public void requestOutput() {\n                    channel.requestOutput();\n                }\n\n                @Override\n                public int write(final ByteBuffer src) throws IOException {\n                    return channel.write(src);\n                }\n\n                @Override\n                public void endStream(final List<? extends Header> trailers) throws IOException {\n                    channel.endStream(trailers);\n                    if (messageCountDown.decrementAndGet() <= 0) {\n                        asyncExecCallback.completed();\n                    }\n                }\n\n                @Override\n                public void endStream() throws IOException {\n                    channel.endStream();\n                    if (messageCountDown.decrementAndGet() <= 0) {\n                        asyncExecCallback.completed();\n                    }\n                }\n\n            });\n        }\n\n        @Override\n        public void consumeInformation(\n                final HttpResponse response,\n                final HttpContext context) throws HttpException, IOException {\n            asyncExecCallback.handleInformationResponse(response);\n        }\n\n        @Override\n        public void consumeResponse(\n                final HttpResponse response,\n                final EntityDetails entityDetails,\n                final HttpContext context) throws HttpException, IOException {\n            entityConsumerRef.set(asyncExecCallback.handleResponse(response, entityDetails));\n            if (response.getCode() >= HttpStatus.SC_CLIENT_ERROR) {\n                messageCountDown.decrementAndGet();\n            }\n            final TimeValue keepAliveDuration = keepAliveStrategy.getKeepAliveDuration(response, clientContext);\n            Object userToken = clientContext.getUserToken();\n            if (userToken == null) {\n                userToken = userTokenHandler.getUserToken(route, clientContext);\n                clientContext.setAttribute(HttpClientContext.USER_TOKEN, userToken);\n            }\n            execRuntime.markConnectionReusable(userToken, keepAliveDuration);\n            if (entityDetails == null) {\n                execRuntime.validateConnection();\n                if (messageCountDown.decrementAndGet() <= 0) {\n                    asyncExecCallback.completed();\n                }\n            }\n        }\n\n        @Override\n        public void updateCapacity(final CapacityChannel capacityChannel) throws IOException {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.get();\n            if (entityConsumer != null) {\n                entityConsumer.updateCapacity(capacityChannel);\n            } else {\n                capacityChannel.update(Integer.MAX_VALUE);\n            }\n        }\n\n        @Override\n        public void consume(final ByteBuffer src) throws IOException {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.get();\n            if (entityConsumer != null) {\n                entityConsumer.consume(src);\n            }\n        }\n\n        @Override\n        public void streamEnd(final List<? extends Header> trailers) throws HttpException, IOException {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.getAndSet(null);\n            if (entityConsumer != null) {\n                entityConsumer.streamEnd(trailers);\n            } else {\n                execRuntime.validateConnection();\n            }\n            if (messageCountDown.decrementAndGet() <= 0) {\n                asyncExecCallback.completed();\n            }\n        }\n\n    };\n\n    if (log.isDebugEnabled()) {\n        operation.setDependency(execRuntime.execute(\n                exchangeId,\n                new LoggingAsyncClientExchangeHandler(log, exchangeId, internalExchangeHandler),\n                clientContext));\n    } else {\n        operation.setDependency(execRuntime.execute(exchangeId, internalExchangeHandler, clientContext));\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2706,
        "instruction": "private void updateFailureInfoForServer(ServerName server,\n    FailureInfo fInfo, boolean didTry, boolean couldNotCommunicate,\n    boolean retryDespiteFastFailMode) {\n  if (server == null || fInfo == null || didTry == false)\n    return;\n\n  // If we were able to connect to the server, reset the failure\n  // information.\n  if (couldNotCommunicate == false) {\n    repeatedFailuresMap.remove(server);\n  } else {\n    // update time of last attempt\n    long currentTime = System.currentTimeMillis();\n    fInfo.timeOfLatestAttemptMilliSec = currentTime;\n\n    // Release the lock if we were retrying inspite of FastFail\n    if (retryDespiteFastFailMode) {\n      fInfo.exclusivelyRetringInspiteOfFastFail.set(false);\n      threadRetryingInFastFailMode.get().setValue(false);\n    }\n  }\n\n  occasionallyCleanupFailureInformation();\n}",
        "accept_response": "private void updateFailureInfoForServer(ServerName server,\n    FailureInfo fInfo, boolean didTry, boolean couldNotCommunicate,\n    boolean retryDespiteFastFailMode) {\n  if (server == null || fInfo == null || didTry == false)\n    return;\n\n  // If we were able to connect to the server, reset the failure\n  // information.\n  if (couldNotCommunicate == false) {\n    LOG.info(\"Clearing out PFFE for server \" + server);\n    repeatedFailuresMap.remove(server);\n  } else {\n    // update time of last attempt\n    long currentTime = System.currentTimeMillis();\n    fInfo.timeOfLatestAttemptMilliSec = currentTime;\n\n    // Release the lock if we were retrying inspite of FastFail\n    if (retryDespiteFastFailMode) {\n      fInfo.exclusivelyRetringInspiteOfFastFail.set(false);\n      threadRetryingInFastFailMode.get().setValue(false);\n    }\n  }\n\n  occasionallyCleanupFailureInformation();\n}",
        "reject_response": "private void updateFailureInfoForServer(ServerName server,\n    FailureInfo fInfo, boolean didTry, boolean couldNotCommunicate,\n    boolean retryDespiteFastFailMode) {\n  if (server == null || fInfo == null || didTry == false)\n    return;\n\n  // If we were able to connect to the server, reset the failure\n  // information.\n  if (couldNotCommunicate == false) {\n    LOG.info(\"Clearing out PFFE for server \" + server.getServerName());\n    repeatedFailuresMap.remove(server);\n  } else {\n    // update time of last attempt\n    long currentTime = System.currentTimeMillis();\n    fInfo.timeOfLatestAttemptMilliSec = currentTime;\n\n    // Release the lock if we were retrying inspite of FastFail\n    if (retryDespiteFastFailMode) {\n      fInfo.exclusivelyRetringInspiteOfFastFail.set(false);\n      threadRetryingInFastFailMode.get().setValue(false);\n    }\n  }\n\n  occasionallyCleanupFailureInformation();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3153,
        "instruction": "@Override\npublic final synchronized void allocate(float progress, AllocateHandler handler) throws Exception {\n  // In one allocate cycle, either only do new container request or removal of requests.\n  // This is a workaround for YARN-314.\n  // When remove a container request, AMRMClient will send a container request with size = 0\n  // With bug YARN-314, if we mix the allocate call with new container request of the same priority,\n  // in some cases the RM would not see the new request (based on sorting of resource capability),\n  // but rather only see the one with size = 0.\n  if (pendingRemoves.isEmpty()) {\n    for (Map.Entry<String, T> entry : pendingRequests.entries()) {\n      addContainerRequest(entry.getValue());\n    }\n    inflightRequests.putAll(pendingRequests);\n    pendingRequests.clear();\n  } else {\n    for (T request : pendingRemoves) {\n      removeContainerRequest(request);\n    }\n    pendingRemoves.clear();\n  }\n\n  if (!blacklistAdditions.isEmpty() || !blacklistRemovals.isEmpty()) {\n    updateBlacklist(blacklistAdditions, blacklistRemovals);\n    blacklistAdditions.clear();\n    blacklistRemovals.clear();\n  }\n\n  AllocateResult allocateResponse = doAllocate(progress);\n  List<RunnableProcessLauncher> launchers = allocateResponse.getLaunchers();\n\n  if (!launchers.isEmpty()) {\n    // Only call handler acquire if there is actually inflight requests.\n    // This is to workaround the YARN behavior that it can return more containers being asked,\n    // such that it causes us to launch process in the pending requests with the wrong container size\n    if (!inflightRequests.isEmpty()) {\n      handler.acquired(launchers);\n    }\n\n    // If no process has been launched through the given launcher, return the container.\n    for (ProcessLauncher<YarnContainerInfo> l : launchers) {\n      // This cast always works.\n      RunnableProcessLauncher launcher = (RunnableProcessLauncher) l;\n      if (!launcher.isLaunched()) {\n        YarnContainerInfo containerInfo = launcher.getContainerInfo();\n        // Casting is needed in Java 8, otherwise it complains about ambiguous method over the info(String, Throwable)\n        releaseAssignedContainer(containerInfo);\n      }\n    }\n  }\n\n  List<YarnContainerStatus> completed = allocateResponse.getCompletedStatus();\n  if (!completed.isEmpty()) {\n    handler.completed(completed);\n  }\n}",
        "accept_response": "@Override\npublic final synchronized void allocate(float progress, AllocateHandler handler) throws Exception {\n  // In one allocate cycle, either only do new container request or removal of requests.\n  // This is a workaround for YARN-314.\n  // When remove a container request, AMRMClient will send a container request with size = 0\n  // With bug YARN-314, if we mix the allocate call with new container request of the same priority,\n  // in some cases the RM would not see the new request (based on sorting of resource capability),\n  // but rather only see the one with size = 0.\n  if (pendingRemoves.isEmpty()) {\n    for (Map.Entry<String, T> entry : pendingRequests.entries()) {\n      addContainerRequest(entry.getValue());\n    }\n    inflightRequests.putAll(pendingRequests);\n    pendingRequests.clear();\n  } else {\n    for (T request : pendingRemoves) {\n      removeContainerRequest(request);\n    }\n    pendingRemoves.clear();\n  }\n\n  if (!blacklistAdditions.isEmpty() || !blacklistRemovals.isEmpty()) {\n    updateBlacklist(blacklistAdditions, blacklistRemovals);\n    blacklistAdditions.clear();\n    blacklistRemovals.clear();\n  }\n\n  AllocateResult allocateResponse = doAllocate(progress);\n  List<RunnableProcessLauncher> launchers = allocateResponse.getLaunchers();\n\n  if (!launchers.isEmpty()) {\n    // Only call handler acquire if there is actually inflight requests.\n    // This is to workaround the YARN behavior that it can return more containers being asked,\n    // such that it causes us to launch process in the pending requests with the wrong container size\n    if (!inflightRequests.isEmpty()) {\n      handler.acquired(launchers);\n    }\n\n    // If no process has been launched through the given launcher, return the container.\n    for (ProcessLauncher<YarnContainerInfo> l : launchers) {\n      // This cast always works.\n      RunnableProcessLauncher launcher = (RunnableProcessLauncher) l;\n      if (!launcher.isLaunched()) {\n        YarnContainerInfo containerInfo = launcher.getContainerInfo();\n        // Casting is needed in Java 8, otherwise it complains about ambiguous method over the info(String, Throwable)\n        LOG.info(\"Nothing to run in container, releasing it: {}\", containerInfo.getContainer());\n        releaseAssignedContainer(containerInfo);\n      }\n    }\n  }\n\n  List<YarnContainerStatus> completed = allocateResponse.getCompletedStatus();\n  if (!completed.isEmpty()) {\n    handler.completed(completed);\n  }\n}",
        "reject_response": "@Override\npublic final synchronized void allocate(float progress, AllocateHandler handler) throws Exception {\n  // In one allocate cycle, either only do new container request or removal of requests.\n  // This is a workaround for YARN-314.\n  // When remove a container request, AMRMClient will send a container request with size = 0\n  // With bug YARN-314, if we mix the allocate call with new container request of the same priority,\n  // in some cases the RM would not see the new request (based on sorting of resource capability),\n  // but rather only see the one with size = 0.\n  if (pendingRemoves.isEmpty()) {\n    for (Map.Entry<String, T> entry : pendingRequests.entries()) {\n      addContainerRequest(entry.getValue());\n    }\n    inflightRequests.putAll(pendingRequests);\n    pendingRequests.clear();\n  } else {\n    for (T request : pendingRemoves) {\n      removeContainerRequest(request);\n    }\n    pendingRemoves.clear();\n  }\n\n  if (!blacklistAdditions.isEmpty() || !blacklistRemovals.isEmpty()) {\n    updateBlacklist(blacklistAdditions, blacklistRemovals);\n    blacklistAdditions.clear();\n    blacklistRemovals.clear();\n  }\n\n  AllocateResult allocateResponse = doAllocate(progress);\n  List<RunnableProcessLauncher> launchers = allocateResponse.getLaunchers();\n\n  if (!launchers.isEmpty()) {\n    // Only call handler acquire if there is actually inflight requests.\n    // This is to workaround the YARN behavior that it can return more containers being asked,\n    // such that it causes us to launch process in the pending requests with the wrong container size\n    if (!inflightRequests.isEmpty()) {\n      handler.acquired(launchers);\n    }\n\n    // If no process has been launched through the given launcher, return the container.\n    for (ProcessLauncher<YarnContainerInfo> l : launchers) {\n      // This cast always works.\n      RunnableProcessLauncher launcher = (RunnableProcessLauncher) l;\n      if (!launcher.isLaunched()) {\n        YarnContainerInfo containerInfo = launcher.getContainerInfo();\n        // Casting is needed in Java 8, otherwise it complains about ambiguous method over the info(String, Throwable)\n        LOG.info(\"Nothing to run in container, releasing it: {}\", (Object) containerInfo.getContainer());\n        releaseAssignedContainer(containerInfo);\n      }\n    }\n  }\n\n  List<YarnContainerStatus> completed = allocateResponse.getCompletedStatus();\n  if (!completed.isEmpty()) {\n    handler.completed(completed);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2514,
        "instruction": "void start()\n{\n    // The run loop for the manager thread\n    Runnable runnable = new WrappedRunnable()\n    {\n        public void runMayThrow() throws Exception\n        {\n            while (!shutdown)\n            {\n                try\n                {\n                    assert availableSegment == null;\n                    availableSegment = createSegment();\n                    if (shutdown)\n                    {\n                        // If shutdown() started and finished during segment creation, we are now left with a\n                        // segment that no one will consume. Discard it.\n                        discardAvailableSegment();\n                        return;\n                    }\n\n                    segmentPrepared.signalAll();\n                    Thread.yield();\n\n                    if (availableSegment == null && !atSegmentBufferLimit())\n                        // Writing threads need another segment now.\n                        continue;\n\n                    // Writing threads are not waiting for new segments, we can spend time on other tasks.\n                    // flush old Cfs if we're full\n                    maybeFlushToReclaim();\n                }\n                catch (Throwable t)\n                {\n                    JVMStabilityInspector.inspectThrowable(t);\n                    if (!CommitLog.handleCommitError(\"Failed managing commit log segments\", t))\n                        return;\n                    // sleep some arbitrary period to avoid spamming CL\n                    Uninterruptibles.sleepUninterruptibly(1, TimeUnit.SECONDS);\n\n                    // If we offered a segment, wait for it to be taken before reentering the loop.\n                    // There could be a new segment in next not offered, but only on failure to discard it while\n                    // shutting down-- nothing more can or needs to be done in that case.\n                }\n\n                WaitQueue.waitOnCondition(managerThreadWaitCondition, managerThreadWaitQueue);\n            }\n        }\n    };\n\n    shutdown = false;\n    managerThread = NamedThreadFactory.createThread(runnable, \"COMMIT-LOG-ALLOCATOR\");\n    managerThread.start();\n\n    // for simplicity, ensure the first segment is allocated before continuing\n    advanceAllocatingFrom(null);\n}",
        "accept_response": "void start()\n{\n    // The run loop for the manager thread\n    Runnable runnable = new WrappedRunnable()\n    {\n        public void runMayThrow() throws Exception\n        {\n            while (!shutdown)\n            {\n                try\n                {\n                    assert availableSegment == null;\n                    logger.trace(\"No segments in reserve; creating a fresh one\");\n                    availableSegment = createSegment();\n                    if (shutdown)\n                    {\n                        // If shutdown() started and finished during segment creation, we are now left with a\n                        // segment that no one will consume. Discard it.\n                        discardAvailableSegment();\n                        return;\n                    }\n\n                    segmentPrepared.signalAll();\n                    Thread.yield();\n\n                    if (availableSegment == null && !atSegmentBufferLimit())\n                        // Writing threads need another segment now.\n                        continue;\n\n                    // Writing threads are not waiting for new segments, we can spend time on other tasks.\n                    // flush old Cfs if we're full\n                    maybeFlushToReclaim();\n                }\n                catch (Throwable t)\n                {\n                    JVMStabilityInspector.inspectThrowable(t);\n                    if (!CommitLog.handleCommitError(\"Failed managing commit log segments\", t))\n                        return;\n                    // sleep some arbitrary period to avoid spamming CL\n                    Uninterruptibles.sleepUninterruptibly(1, TimeUnit.SECONDS);\n\n                    // If we offered a segment, wait for it to be taken before reentering the loop.\n                    // There could be a new segment in next not offered, but only on failure to discard it while\n                    // shutting down-- nothing more can or needs to be done in that case.\n                }\n\n                WaitQueue.waitOnCondition(managerThreadWaitCondition, managerThreadWaitQueue);\n            }\n        }\n    };\n\n    shutdown = false;\n    managerThread = NamedThreadFactory.createThread(runnable, \"COMMIT-LOG-ALLOCATOR\");\n    managerThread.start();\n\n    // for simplicity, ensure the first segment is allocated before continuing\n    advanceAllocatingFrom(null);\n}",
        "reject_response": "void start()\n{\n    // The run loop for the manager thread\n    Runnable runnable = new WrappedRunnable()\n    {\n        public void runMayThrow() throws Exception\n        {\n            while (!shutdown)\n            {\n                try\n                {\n                    assert availableSegment == null;\n                    logger.debug(\"No segments in reserve; creating a fresh one\");\n                    availableSegment = createSegment();\n                    if (shutdown)\n                    {\n                        // If shutdown() started and finished during segment creation, we are now left with a\n                        // segment that no one will consume. Discard it.\n                        discardAvailableSegment();\n                        return;\n                    }\n\n                    segmentPrepared.signalAll();\n                    Thread.yield();\n\n                    if (availableSegment == null && !atSegmentBufferLimit())\n                        // Writing threads need another segment now.\n                        continue;\n\n                    // Writing threads are not waiting for new segments, we can spend time on other tasks.\n                    // flush old Cfs if we're full\n                    maybeFlushToReclaim();\n                }\n                catch (Throwable t)\n                {\n                    JVMStabilityInspector.inspectThrowable(t);\n                    if (!CommitLog.handleCommitError(\"Failed managing commit log segments\", t))\n                        return;\n                    // sleep some arbitrary period to avoid spamming CL\n                    Uninterruptibles.sleepUninterruptibly(1, TimeUnit.SECONDS);\n\n                    // If we offered a segment, wait for it to be taken before reentering the loop.\n                    // There could be a new segment in next not offered, but only on failure to discard it while\n                    // shutting down-- nothing more can or needs to be done in that case.\n                }\n\n                WaitQueue.waitOnCondition(managerThreadWaitCondition, managerThreadWaitQueue);\n            }\n        }\n    };\n\n    shutdown = false;\n    managerThread = NamedThreadFactory.createThread(runnable, \"COMMIT-LOG-ALLOCATOR\");\n    managerThread.start();\n\n    // for simplicity, ensure the first segment is allocated before continuing\n    advanceAllocatingFrom(null);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3133,
        "instruction": "public TaskHeartbeatResponse heartbeat(TaskHeartbeatRequest request)\n    throws IOException, TezException {\n  ContainerId containerId = ConverterUtils.toContainerId(request\n      .getContainerIdentifier());\n\n  if (!registeredContainers.containsKey(containerId)) {\n    LOG.warn(\"Received task heartbeat from unknown container with id: \" + containerId +\n        \", asking it to die\");\n    return RESPONSE_SHOULD_DIE;\n  }\n\n  // A heartbeat can come in anytime. The AM may have made a decision to kill a running task/container\n  // meanwhile. If the decision is processed through the pipeline before the heartbeat is processed,\n  // the heartbeat will be dropped. Otherwise the heartbeat will be processed - and the system\n  // know how to handle this - via FailedInputEvents for example (relevant only if the heartbeat has events).\n  // So - avoiding synchronization.\n\n  pingContainerHeartbeatHandler(containerId);\n  TaskAttemptEventInfo eventInfo = new TaskAttemptEventInfo(0, null, 0);\n  TezTaskAttemptID taskAttemptID = request.getTaskAttemptId();\n  if (taskAttemptID != null) {\n    ContainerId containerIdFromMap = registeredAttempts.get(taskAttemptID);\n    if (containerIdFromMap == null || !containerIdFromMap.equals(containerId)) {\n      // This can happen when a task heartbeats. Meanwhile the container is unregistered.\n      // The information will eventually make it through to the plugin via a corresponding unregister.\n      // There's a race in that case between the unregister making it through, and this method returning.\n      // TODO TEZ-2003 (post) TEZ-2666. An exception back is likely a better approach than sending a shouldDie = true,\n      // so that the plugin can handle the scenario. Alternately augment the response with error codes.\n      // Error codes would be better than exceptions.\n      LOG.info(\"Attempt: \" + taskAttemptID + \" is not recognized for heartbeats\");\n      return RESPONSE_SHOULD_DIE;\n    }\n\n    List<TezEvent> inEvents = request.getEvents();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Ping from \" + taskAttemptID.toString() +\n          \" events: \" + (inEvents != null ? inEvents.size() : -1));\n    }\n\n    long currTime = context.getClock().getTime();\n    // taFinishedEvents - means the TaskAttemptFinishedEvent\n    // taGeneratedEvents - for recovery, means the events generated by this task attempt and is needed by its downstream vertices\n    // eventsForVertex - including all the taGeneratedEvents and other events such as INPUT_READ_ERROR_EVENT/INPUT_FAILED_EVENT\n    // taGeneratedEvents is routed both to TaskAttempt & Vertex. Route to Vertex is for performance consideration\n    // taFinishedEvents must be routed before taGeneratedEvents\n    List<TezEvent> taFinishedEvents = new ArrayList<TezEvent>();\n    List<TezEvent> taGeneratedEvents = new ArrayList<TezEvent>();\n    List<TezEvent> eventsForVertex = new ArrayList<TezEvent>();\n    TaskAttemptEventStatusUpdate taskAttemptEvent = null;\n    boolean readErrorReported = false;\n    for (TezEvent tezEvent : ListUtils.emptyIfNull(inEvents)) {\n      // for now, set the event time on the AM when it is received.\n      // this avoids any time disparity between machines.\n      tezEvent.setEventReceivedTime(currTime);\n      final EventType eventType = tezEvent.getEventType();\n      if (eventType == EventType.TASK_STATUS_UPDATE_EVENT) {\n        // send TA_STATUS_UPDATE before TA_DONE/TA_FAILED/TA_KILLED otherwise Status may be missed\n        taskAttemptEvent = new TaskAttemptEventStatusUpdate(taskAttemptID,\n            (TaskStatusUpdateEvent) tezEvent.getEvent());\n      } else if (eventType == EventType.TASK_ATTEMPT_COMPLETED_EVENT\n         || eventType == EventType.TASK_ATTEMPT_FAILED_EVENT\n         || eventType == EventType.TASK_ATTEMPT_KILLED_EVENT) {\n        taFinishedEvents.add(tezEvent);\n      } else {\n        if (eventType == EventType.INPUT_READ_ERROR_EVENT) {\n          readErrorReported = true;\n        }\n        if (eventType == EventType.DATA_MOVEMENT_EVENT\n          || eventType == EventType.COMPOSITE_DATA_MOVEMENT_EVENT\n          || eventType == EventType.ROOT_INPUT_INITIALIZER_EVENT\n          || eventType == EventType.VERTEX_MANAGER_EVENT) {\n          taGeneratedEvents.add(tezEvent);\n        }\n        eventsForVertex.add(tezEvent);\n      }\n    }\n    if (taskAttemptEvent != null) {\n      taskAttemptEvent.setReadErrorReported(readErrorReported);\n      sendEvent(taskAttemptEvent);\n    }\n    // route taGeneratedEvents to TaskAttempt\n    if (!taGeneratedEvents.isEmpty()) {\n      sendEvent(new TaskAttemptEventTezEventUpdate(taskAttemptID, taGeneratedEvents));\n    }\n    // route events to TaskAttempt\n    Preconditions.checkArgument(taFinishedEvents.size() <= 1, \"Multiple TaskAttemptFinishedEvent\");\n    for (TezEvent e : taFinishedEvents) {\n      EventMetaData sourceMeta = e.getSourceInfo();\n      switch (e.getEventType()) {\n      case TASK_ATTEMPT_FAILED_EVENT:\n      case TASK_ATTEMPT_KILLED_EVENT:\n        TaskAttemptTerminationCause errCause = null;\n        switch (sourceMeta.getEventGenerator()) {\n        case INPUT:\n          errCause = TaskAttemptTerminationCause.INPUT_READ_ERROR;\n          break;\n        case PROCESSOR:\n          errCause = TaskAttemptTerminationCause.APPLICATION_ERROR;\n          break;\n        case OUTPUT:\n          errCause = TaskAttemptTerminationCause.OUTPUT_WRITE_ERROR;\n          break;\n        case SYSTEM:\n          errCause = TaskAttemptTerminationCause.FRAMEWORK_ERROR;\n          break;\n        default:\n          throw new TezUncheckedException(\"Unknown EventProducerConsumerType: \" +\n              sourceMeta.getEventGenerator());\n        }\n        if (e.getEventType() == EventType.TASK_ATTEMPT_FAILED_EVENT) {\n          TaskAttemptFailedEvent taskFailedEvent = (TaskAttemptFailedEvent) e.getEvent();\n          sendEvent(\n              new TaskAttemptEventAttemptFailed(sourceMeta.getTaskAttemptID(),\n                  TaskAttemptEventType.TA_FAILED, taskFailedEvent.getTaskFailureType(),\n                  \"Error: \" + taskFailedEvent.getDiagnostics(),\n                  errCause));\n        } else { // Killed\n          TaskAttemptKilledEvent taskKilledEvent = (TaskAttemptKilledEvent) e.getEvent();\n          sendEvent(\n              new TaskAttemptEventAttemptKilled(sourceMeta.getTaskAttemptID(),\n                  \"Error: \" + taskKilledEvent.getDiagnostics(), errCause));\n        }\n        break;\n      case TASK_ATTEMPT_COMPLETED_EVENT:\n        sendEvent(\n            new TaskAttemptEvent(sourceMeta.getTaskAttemptID(), TaskAttemptEventType.TA_DONE));\n        break;\n      default:\n        throw new TezUncheckedException(\"Unhandled tez event type: \"\n           + e.getEventType());\n      }\n    }\n    if (!eventsForVertex.isEmpty()) {\n      TezVertexID vertexId = taskAttemptID.getTaskID().getVertexID();\n      sendEvent(\n          new VertexEventRouteEvent(vertexId, Collections.unmodifiableList(eventsForVertex)));\n    }\n    taskHeartbeatHandler.pinged(taskAttemptID);\n    eventInfo = context\n        .getCurrentDAG()\n        .getVertex(taskAttemptID.getTaskID().getVertexID())\n        .getTaskAttemptTezEvents(taskAttemptID, request.getStartIndex(), request.getPreRoutedStartIndex(),\n            request.getMaxEvents());\n  }\n  return new TaskHeartbeatResponse(false, eventInfo.getEvents(), eventInfo.getNextFromEventId(), eventInfo.getNextPreRoutedFromEventId());\n}",
        "accept_response": "public TaskHeartbeatResponse heartbeat(TaskHeartbeatRequest request)\n    throws IOException, TezException {\n  ContainerId containerId = ConverterUtils.toContainerId(request\n      .getContainerIdentifier());\n  LOG.debug(\"Received heartbeat from container, request={}\", request);\n\n  if (!registeredContainers.containsKey(containerId)) {\n    LOG.warn(\"Received task heartbeat from unknown container with id: \" + containerId +\n        \", asking it to die\");\n    return RESPONSE_SHOULD_DIE;\n  }\n\n  // A heartbeat can come in anytime. The AM may have made a decision to kill a running task/container\n  // meanwhile. If the decision is processed through the pipeline before the heartbeat is processed,\n  // the heartbeat will be dropped. Otherwise the heartbeat will be processed - and the system\n  // know how to handle this - via FailedInputEvents for example (relevant only if the heartbeat has events).\n  // So - avoiding synchronization.\n\n  pingContainerHeartbeatHandler(containerId);\n  TaskAttemptEventInfo eventInfo = new TaskAttemptEventInfo(0, null, 0);\n  TezTaskAttemptID taskAttemptID = request.getTaskAttemptId();\n  if (taskAttemptID != null) {\n    ContainerId containerIdFromMap = registeredAttempts.get(taskAttemptID);\n    if (containerIdFromMap == null || !containerIdFromMap.equals(containerId)) {\n      // This can happen when a task heartbeats. Meanwhile the container is unregistered.\n      // The information will eventually make it through to the plugin via a corresponding unregister.\n      // There's a race in that case between the unregister making it through, and this method returning.\n      // TODO TEZ-2003 (post) TEZ-2666. An exception back is likely a better approach than sending a shouldDie = true,\n      // so that the plugin can handle the scenario. Alternately augment the response with error codes.\n      // Error codes would be better than exceptions.\n      LOG.info(\"Attempt: \" + taskAttemptID + \" is not recognized for heartbeats\");\n      return RESPONSE_SHOULD_DIE;\n    }\n\n    List<TezEvent> inEvents = request.getEvents();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Ping from \" + taskAttemptID.toString() +\n          \" events: \" + (inEvents != null ? inEvents.size() : -1));\n    }\n\n    long currTime = context.getClock().getTime();\n    // taFinishedEvents - means the TaskAttemptFinishedEvent\n    // taGeneratedEvents - for recovery, means the events generated by this task attempt and is needed by its downstream vertices\n    // eventsForVertex - including all the taGeneratedEvents and other events such as INPUT_READ_ERROR_EVENT/INPUT_FAILED_EVENT\n    // taGeneratedEvents is routed both to TaskAttempt & Vertex. Route to Vertex is for performance consideration\n    // taFinishedEvents must be routed before taGeneratedEvents\n    List<TezEvent> taFinishedEvents = new ArrayList<TezEvent>();\n    List<TezEvent> taGeneratedEvents = new ArrayList<TezEvent>();\n    List<TezEvent> eventsForVertex = new ArrayList<TezEvent>();\n    TaskAttemptEventStatusUpdate taskAttemptEvent = null;\n    boolean readErrorReported = false;\n    for (TezEvent tezEvent : ListUtils.emptyIfNull(inEvents)) {\n      // for now, set the event time on the AM when it is received.\n      // this avoids any time disparity between machines.\n      tezEvent.setEventReceivedTime(currTime);\n      final EventType eventType = tezEvent.getEventType();\n      if (eventType == EventType.TASK_STATUS_UPDATE_EVENT) {\n        // send TA_STATUS_UPDATE before TA_DONE/TA_FAILED/TA_KILLED otherwise Status may be missed\n        taskAttemptEvent = new TaskAttemptEventStatusUpdate(taskAttemptID,\n            (TaskStatusUpdateEvent) tezEvent.getEvent());\n      } else if (eventType == EventType.TASK_ATTEMPT_COMPLETED_EVENT\n         || eventType == EventType.TASK_ATTEMPT_FAILED_EVENT\n         || eventType == EventType.TASK_ATTEMPT_KILLED_EVENT) {\n        taFinishedEvents.add(tezEvent);\n      } else {\n        if (eventType == EventType.INPUT_READ_ERROR_EVENT) {\n          readErrorReported = true;\n        }\n        if (eventType == EventType.DATA_MOVEMENT_EVENT\n          || eventType == EventType.COMPOSITE_DATA_MOVEMENT_EVENT\n          || eventType == EventType.ROOT_INPUT_INITIALIZER_EVENT\n          || eventType == EventType.VERTEX_MANAGER_EVENT) {\n          taGeneratedEvents.add(tezEvent);\n        }\n        eventsForVertex.add(tezEvent);\n      }\n    }\n    if (taskAttemptEvent != null) {\n      taskAttemptEvent.setReadErrorReported(readErrorReported);\n      sendEvent(taskAttemptEvent);\n    }\n    // route taGeneratedEvents to TaskAttempt\n    if (!taGeneratedEvents.isEmpty()) {\n      sendEvent(new TaskAttemptEventTezEventUpdate(taskAttemptID, taGeneratedEvents));\n    }\n    // route events to TaskAttempt\n    Preconditions.checkArgument(taFinishedEvents.size() <= 1, \"Multiple TaskAttemptFinishedEvent\");\n    for (TezEvent e : taFinishedEvents) {\n      EventMetaData sourceMeta = e.getSourceInfo();\n      switch (e.getEventType()) {\n      case TASK_ATTEMPT_FAILED_EVENT:\n      case TASK_ATTEMPT_KILLED_EVENT:\n        TaskAttemptTerminationCause errCause = null;\n        switch (sourceMeta.getEventGenerator()) {\n        case INPUT:\n          errCause = TaskAttemptTerminationCause.INPUT_READ_ERROR;\n          break;\n        case PROCESSOR:\n          errCause = TaskAttemptTerminationCause.APPLICATION_ERROR;\n          break;\n        case OUTPUT:\n          errCause = TaskAttemptTerminationCause.OUTPUT_WRITE_ERROR;\n          break;\n        case SYSTEM:\n          errCause = TaskAttemptTerminationCause.FRAMEWORK_ERROR;\n          break;\n        default:\n          throw new TezUncheckedException(\"Unknown EventProducerConsumerType: \" +\n              sourceMeta.getEventGenerator());\n        }\n        if (e.getEventType() == EventType.TASK_ATTEMPT_FAILED_EVENT) {\n          TaskAttemptFailedEvent taskFailedEvent = (TaskAttemptFailedEvent) e.getEvent();\n          sendEvent(\n              new TaskAttemptEventAttemptFailed(sourceMeta.getTaskAttemptID(),\n                  TaskAttemptEventType.TA_FAILED, taskFailedEvent.getTaskFailureType(),\n                  \"Error: \" + taskFailedEvent.getDiagnostics(),\n                  errCause));\n        } else { // Killed\n          TaskAttemptKilledEvent taskKilledEvent = (TaskAttemptKilledEvent) e.getEvent();\n          sendEvent(\n              new TaskAttemptEventAttemptKilled(sourceMeta.getTaskAttemptID(),\n                  \"Error: \" + taskKilledEvent.getDiagnostics(), errCause));\n        }\n        break;\n      case TASK_ATTEMPT_COMPLETED_EVENT:\n        sendEvent(\n            new TaskAttemptEvent(sourceMeta.getTaskAttemptID(), TaskAttemptEventType.TA_DONE));\n        break;\n      default:\n        throw new TezUncheckedException(\"Unhandled tez event type: \"\n           + e.getEventType());\n      }\n    }\n    if (!eventsForVertex.isEmpty()) {\n      TezVertexID vertexId = taskAttemptID.getTaskID().getVertexID();\n      sendEvent(\n          new VertexEventRouteEvent(vertexId, Collections.unmodifiableList(eventsForVertex)));\n    }\n    taskHeartbeatHandler.pinged(taskAttemptID);\n    eventInfo = context\n        .getCurrentDAG()\n        .getVertex(taskAttemptID.getTaskID().getVertexID())\n        .getTaskAttemptTezEvents(taskAttemptID, request.getStartIndex(), request.getPreRoutedStartIndex(),\n            request.getMaxEvents());\n  }\n  return new TaskHeartbeatResponse(false, eventInfo.getEvents(), eventInfo.getNextFromEventId(), eventInfo.getNextPreRoutedFromEventId());\n}",
        "reject_response": "public TaskHeartbeatResponse heartbeat(TaskHeartbeatRequest request)\n    throws IOException, TezException {\n  ContainerId containerId = ConverterUtils.toContainerId(request\n      .getContainerIdentifier());\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"Received heartbeat from container\"\n        + \", request=\" + request);\n  }\n\n  if (!registeredContainers.containsKey(containerId)) {\n    LOG.warn(\"Received task heartbeat from unknown container with id: \" + containerId +\n        \", asking it to die\");\n    return RESPONSE_SHOULD_DIE;\n  }\n\n  // A heartbeat can come in anytime. The AM may have made a decision to kill a running task/container\n  // meanwhile. If the decision is processed through the pipeline before the heartbeat is processed,\n  // the heartbeat will be dropped. Otherwise the heartbeat will be processed - and the system\n  // know how to handle this - via FailedInputEvents for example (relevant only if the heartbeat has events).\n  // So - avoiding synchronization.\n\n  pingContainerHeartbeatHandler(containerId);\n  TaskAttemptEventInfo eventInfo = new TaskAttemptEventInfo(0, null, 0);\n  TezTaskAttemptID taskAttemptID = request.getTaskAttemptId();\n  if (taskAttemptID != null) {\n    ContainerId containerIdFromMap = registeredAttempts.get(taskAttemptID);\n    if (containerIdFromMap == null || !containerIdFromMap.equals(containerId)) {\n      // This can happen when a task heartbeats. Meanwhile the container is unregistered.\n      // The information will eventually make it through to the plugin via a corresponding unregister.\n      // There's a race in that case between the unregister making it through, and this method returning.\n      // TODO TEZ-2003 (post) TEZ-2666. An exception back is likely a better approach than sending a shouldDie = true,\n      // so that the plugin can handle the scenario. Alternately augment the response with error codes.\n      // Error codes would be better than exceptions.\n      LOG.info(\"Attempt: \" + taskAttemptID + \" is not recognized for heartbeats\");\n      return RESPONSE_SHOULD_DIE;\n    }\n\n    List<TezEvent> inEvents = request.getEvents();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Ping from \" + taskAttemptID.toString() +\n          \" events: \" + (inEvents != null ? inEvents.size() : -1));\n    }\n\n    long currTime = context.getClock().getTime();\n    // taFinishedEvents - means the TaskAttemptFinishedEvent\n    // taGeneratedEvents - for recovery, means the events generated by this task attempt and is needed by its downstream vertices\n    // eventsForVertex - including all the taGeneratedEvents and other events such as INPUT_READ_ERROR_EVENT/INPUT_FAILED_EVENT\n    // taGeneratedEvents is routed both to TaskAttempt & Vertex. Route to Vertex is for performance consideration\n    // taFinishedEvents must be routed before taGeneratedEvents\n    List<TezEvent> taFinishedEvents = new ArrayList<TezEvent>();\n    List<TezEvent> taGeneratedEvents = new ArrayList<TezEvent>();\n    List<TezEvent> eventsForVertex = new ArrayList<TezEvent>();\n    TaskAttemptEventStatusUpdate taskAttemptEvent = null;\n    boolean readErrorReported = false;\n    for (TezEvent tezEvent : ListUtils.emptyIfNull(inEvents)) {\n      // for now, set the event time on the AM when it is received.\n      // this avoids any time disparity between machines.\n      tezEvent.setEventReceivedTime(currTime);\n      final EventType eventType = tezEvent.getEventType();\n      if (eventType == EventType.TASK_STATUS_UPDATE_EVENT) {\n        // send TA_STATUS_UPDATE before TA_DONE/TA_FAILED/TA_KILLED otherwise Status may be missed\n        taskAttemptEvent = new TaskAttemptEventStatusUpdate(taskAttemptID,\n            (TaskStatusUpdateEvent) tezEvent.getEvent());\n      } else if (eventType == EventType.TASK_ATTEMPT_COMPLETED_EVENT\n         || eventType == EventType.TASK_ATTEMPT_FAILED_EVENT\n         || eventType == EventType.TASK_ATTEMPT_KILLED_EVENT) {\n        taFinishedEvents.add(tezEvent);\n      } else {\n        if (eventType == EventType.INPUT_READ_ERROR_EVENT) {\n          readErrorReported = true;\n        }\n        if (eventType == EventType.DATA_MOVEMENT_EVENT\n          || eventType == EventType.COMPOSITE_DATA_MOVEMENT_EVENT\n          || eventType == EventType.ROOT_INPUT_INITIALIZER_EVENT\n          || eventType == EventType.VERTEX_MANAGER_EVENT) {\n          taGeneratedEvents.add(tezEvent);\n        }\n        eventsForVertex.add(tezEvent);\n      }\n    }\n    if (taskAttemptEvent != null) {\n      taskAttemptEvent.setReadErrorReported(readErrorReported);\n      sendEvent(taskAttemptEvent);\n    }\n    // route taGeneratedEvents to TaskAttempt\n    if (!taGeneratedEvents.isEmpty()) {\n      sendEvent(new TaskAttemptEventTezEventUpdate(taskAttemptID, taGeneratedEvents));\n    }\n    // route events to TaskAttempt\n    Preconditions.checkArgument(taFinishedEvents.size() <= 1, \"Multiple TaskAttemptFinishedEvent\");\n    for (TezEvent e : taFinishedEvents) {\n      EventMetaData sourceMeta = e.getSourceInfo();\n      switch (e.getEventType()) {\n      case TASK_ATTEMPT_FAILED_EVENT:\n      case TASK_ATTEMPT_KILLED_EVENT:\n        TaskAttemptTerminationCause errCause = null;\n        switch (sourceMeta.getEventGenerator()) {\n        case INPUT:\n          errCause = TaskAttemptTerminationCause.INPUT_READ_ERROR;\n          break;\n        case PROCESSOR:\n          errCause = TaskAttemptTerminationCause.APPLICATION_ERROR;\n          break;\n        case OUTPUT:\n          errCause = TaskAttemptTerminationCause.OUTPUT_WRITE_ERROR;\n          break;\n        case SYSTEM:\n          errCause = TaskAttemptTerminationCause.FRAMEWORK_ERROR;\n          break;\n        default:\n          throw new TezUncheckedException(\"Unknown EventProducerConsumerType: \" +\n              sourceMeta.getEventGenerator());\n        }\n        if (e.getEventType() == EventType.TASK_ATTEMPT_FAILED_EVENT) {\n          TaskAttemptFailedEvent taskFailedEvent = (TaskAttemptFailedEvent) e.getEvent();\n          sendEvent(\n              new TaskAttemptEventAttemptFailed(sourceMeta.getTaskAttemptID(),\n                  TaskAttemptEventType.TA_FAILED, taskFailedEvent.getTaskFailureType(),\n                  \"Error: \" + taskFailedEvent.getDiagnostics(),\n                  errCause));\n        } else { // Killed\n          TaskAttemptKilledEvent taskKilledEvent = (TaskAttemptKilledEvent) e.getEvent();\n          sendEvent(\n              new TaskAttemptEventAttemptKilled(sourceMeta.getTaskAttemptID(),\n                  \"Error: \" + taskKilledEvent.getDiagnostics(), errCause));\n        }\n        break;\n      case TASK_ATTEMPT_COMPLETED_EVENT:\n        sendEvent(\n            new TaskAttemptEvent(sourceMeta.getTaskAttemptID(), TaskAttemptEventType.TA_DONE));\n        break;\n      default:\n        throw new TezUncheckedException(\"Unhandled tez event type: \"\n           + e.getEventType());\n      }\n    }\n    if (!eventsForVertex.isEmpty()) {\n      TezVertexID vertexId = taskAttemptID.getTaskID().getVertexID();\n      sendEvent(\n          new VertexEventRouteEvent(vertexId, Collections.unmodifiableList(eventsForVertex)));\n    }\n    taskHeartbeatHandler.pinged(taskAttemptID);\n    eventInfo = context\n        .getCurrentDAG()\n        .getVertex(taskAttemptID.getTaskID().getVertexID())\n        .getTaskAttemptTezEvents(taskAttemptID, request.getStartIndex(), request.getPreRoutedStartIndex(),\n            request.getMaxEvents());\n  }\n  return new TaskHeartbeatResponse(false, eventInfo.getEvents(), eventInfo.getNextFromEventId(), eventInfo.getNextPreRoutedFromEventId());\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3114,
        "instruction": "@SuppressWarnings({ \"rawtypes\", \"unchecked\" })\n@Override\npublic List<SelfDiagnosisRule> getDefinedRules() {\n  Set<Class> classSet = ClassUtil.findClasses(SelfDiagnosisRule.class,\n      getClass().getPackage().getName());\n  List<SelfDiagnosisRule> ruleList = new ArrayList<SelfDiagnosisRule>(classSet.size());\n\n  for (Class<SelfDiagnosisRule> ruleClazz: classSet) {\n    try {\n      ruleList.add(ruleClazz.newInstance());\n    } catch (Exception e) {\n      continue;\n    }\n  }\n  return ruleList;\n}",
        "accept_response": "@SuppressWarnings({ \"rawtypes\", \"unchecked\" })\n@Override\npublic List<SelfDiagnosisRule> getDefinedRules() {\n  Set<Class> classSet = ClassUtil.findClasses(SelfDiagnosisRule.class,\n      getClass().getPackage().getName());\n  List<SelfDiagnosisRule> ruleList = new ArrayList<SelfDiagnosisRule>(classSet.size());\n\n  for (Class<SelfDiagnosisRule> ruleClazz: classSet) {\n    try {\n      ruleList.add(ruleClazz.newInstance());\n    } catch (Exception e) {\n      LOG.warn(\"Cannot instantiate \" + ruleClazz.getName() + \" class.\", e);\n      continue;\n    }\n  }\n  return ruleList;\n}",
        "reject_response": "@SuppressWarnings({ \"rawtypes\", \"unchecked\" })\n@Override\npublic List<SelfDiagnosisRule> getDefinedRules() {\n  Set<Class> classSet = ClassUtil.findClasses(SelfDiagnosisRule.class,\n      getClass().getPackage().getName());\n  List<SelfDiagnosisRule> ruleList = new ArrayList<SelfDiagnosisRule>(classSet.size());\n\n  for (Class<SelfDiagnosisRule> ruleClazz: classSet) {\n    try {\n      ruleList.add(ruleClazz.newInstance());\n    } catch (Exception e) {\n      LOG.warn(\"Cannot instantiate \" + ruleClazz.getName() + \" class.\");\n      continue;\n    }\n  }\n  return ruleList;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2499,
        "instruction": "@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\npublic <T extends Location> T createLocation(LocationSpec<T> spec) {\n    if (spec.getFlags().containsKey(\"parent\")) {\n        throw new IllegalArgumentException(\"Spec's flags must not contain parent; use spec.parent() instead for \"+spec);\n    }\n    if (spec.getFlags().containsKey(\"id\")) {\n        throw new IllegalArgumentException(\"Spec's flags must not contain id; use spec.id() instead for \"+spec);\n    }\n\n    try {\n        Class<? extends T> clazz = spec.getType();\n\n        T loc = construct(clazz, spec, null);\n\n        if (Locations.isManaged(loc)) {\n            // Construct can return an existing instance, if using SpecialBrooklynObjectConstructor.Config.SPECIAL_CONSTRUCTOR.\n            // In which case, don't reconfigure it (don't change its parent, etc).\n            return loc;\n        }\n\n        managementContext.prePreManage(loc);\n\n        final AbstractLocation location = (AbstractLocation) loc;\n        if (spec.getDisplayName()!=null)\n            location.setDisplayName(spec.getDisplayName());\n\n        if (spec.getCatalogItemId()!=null) {\n            location.setCatalogItemIdAndSearchPath(spec.getCatalogItemId(), spec.getCatalogItemIdSearchPath());\n        }\n\n        loc.tags().addTags(spec.getTags());\n\n        if (isNewStyle(clazz)) {\n            location.setManagementContext(managementContext);\n            location.configure(ConfigBag.newInstance().putAll(spec.getFlags()).putAll(spec.getConfig()).getAllConfig());\n        }\n\n        for (Map.Entry<ConfigKey<?>, Object> entry : spec.getConfig().entrySet()) {\n            location.config().set((ConfigKey)entry.getKey(), entry.getValue());\n        }\n        for (Entry<Class<?>, Object> entry : spec.getExtensions().entrySet()) {\n            ((LocationInternal)loc).addExtension((Class)entry.getKey(), entry.getValue());\n        }\n        location.init();\n\n        Location parent = spec.getParent();\n        if (parent != null) {\n            loc.setParent(parent);\n        }\n        return loc;\n\n    } catch (Exception e) {\n        throw Exceptions.propagate(e);\n    }\n}",
        "accept_response": "@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\npublic <T extends Location> T createLocation(LocationSpec<T> spec) {\n    if (spec.getFlags().containsKey(\"parent\")) {\n        throw new IllegalArgumentException(\"Spec's flags must not contain parent; use spec.parent() instead for \"+spec);\n    }\n    if (spec.getFlags().containsKey(\"id\")) {\n        throw new IllegalArgumentException(\"Spec's flags must not contain id; use spec.id() instead for \"+spec);\n    }\n\n    try {\n        Class<? extends T> clazz = spec.getType();\n\n        T loc = construct(clazz, spec, null);\n\n        if (Locations.isManaged(loc)) {\n            // Construct can return an existing instance, if using SpecialBrooklynObjectConstructor.Config.SPECIAL_CONSTRUCTOR.\n            // In which case, don't reconfigure it (don't change its parent, etc).\n            LOG.trace(\"Location-factory returning pre-existing location; skipping initialization of {}\", loc);\n            return loc;\n        }\n\n        managementContext.prePreManage(loc);\n\n        final AbstractLocation location = (AbstractLocation) loc;\n        if (spec.getDisplayName()!=null)\n            location.setDisplayName(spec.getDisplayName());\n\n        if (spec.getCatalogItemId()!=null) {\n            location.setCatalogItemIdAndSearchPath(spec.getCatalogItemId(), spec.getCatalogItemIdSearchPath());\n        }\n\n        loc.tags().addTags(spec.getTags());\n\n        if (isNewStyle(clazz)) {\n            location.setManagementContext(managementContext);\n            location.configure(ConfigBag.newInstance().putAll(spec.getFlags()).putAll(spec.getConfig()).getAllConfig());\n        }\n\n        for (Map.Entry<ConfigKey<?>, Object> entry : spec.getConfig().entrySet()) {\n            location.config().set((ConfigKey)entry.getKey(), entry.getValue());\n        }\n        for (Entry<Class<?>, Object> entry : spec.getExtensions().entrySet()) {\n            ((LocationInternal)loc).addExtension((Class)entry.getKey(), entry.getValue());\n        }\n        location.init();\n\n        Location parent = spec.getParent();\n        if (parent != null) {\n            loc.setParent(parent);\n        }\n        return loc;\n\n    } catch (Exception e) {\n        throw Exceptions.propagate(e);\n    }\n}",
        "reject_response": "@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\npublic <T extends Location> T createLocation(LocationSpec<T> spec) {\n    if (spec.getFlags().containsKey(\"parent\")) {\n        throw new IllegalArgumentException(\"Spec's flags must not contain parent; use spec.parent() instead for \"+spec);\n    }\n    if (spec.getFlags().containsKey(\"id\")) {\n        throw new IllegalArgumentException(\"Spec's flags must not contain id; use spec.id() instead for \"+spec);\n    }\n\n    try {\n        Class<? extends T> clazz = spec.getType();\n\n        T loc = construct(clazz, spec, null);\n\n        if (Locations.isManaged(loc)) {\n            // Construct can return an existing instance, if using SpecialBrooklynObjectConstructor.Config.SPECIAL_CONSTRUCTOR.\n            // In which case, don't reconfigure it (don't change its parent, etc).\n            LOG.debug(\"Location-factory returning pre-existing location; skipping initialization of {}\", loc);\n            return loc;\n        }\n\n        managementContext.prePreManage(loc);\n\n        final AbstractLocation location = (AbstractLocation) loc;\n        if (spec.getDisplayName()!=null)\n            location.setDisplayName(spec.getDisplayName());\n\n        if (spec.getCatalogItemId()!=null) {\n            location.setCatalogItemIdAndSearchPath(spec.getCatalogItemId(), spec.getCatalogItemIdSearchPath());\n        }\n\n        loc.tags().addTags(spec.getTags());\n\n        if (isNewStyle(clazz)) {\n            location.setManagementContext(managementContext);\n            location.configure(ConfigBag.newInstance().putAll(spec.getFlags()).putAll(spec.getConfig()).getAllConfig());\n        }\n\n        for (Map.Entry<ConfigKey<?>, Object> entry : spec.getConfig().entrySet()) {\n            location.config().set((ConfigKey)entry.getKey(), entry.getValue());\n        }\n        for (Entry<Class<?>, Object> entry : spec.getExtensions().entrySet()) {\n            ((LocationInternal)loc).addExtension((Class)entry.getKey(), entry.getValue());\n        }\n        location.init();\n\n        Location parent = spec.getParent();\n        if (parent != null) {\n            loc.setParent(parent);\n        }\n        return loc;\n\n    } catch (Exception e) {\n        throw Exceptions.propagate(e);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2849,
        "instruction": "@Override public void apply(UUID nodeId, M msg) {\n    ClusterNode node = cctx.node(nodeId);\n\n    if (node == null) {\n        if (log.isTraceEnabled())\n\n        return;\n    }\n\n    if (log.isTraceEnabled())\n        log.trace(\"Received message from node [node=\" + nodeId + \", msg=\" + msg + ']');\n\n    onMessage(node, msg);\n}",
        "accept_response": "@Override public void apply(UUID nodeId, M msg) {\n    ClusterNode node = cctx.node(nodeId);\n\n    if (node == null) {\n        if (log.isTraceEnabled())\n            log.trace(\"Received message from failed node [node=\" + nodeId + \", msg=\" + msg + ']');\n\n        return;\n    }\n\n    if (log.isTraceEnabled())\n        log.trace(\"Received message from node [node=\" + nodeId + \", msg=\" + msg + ']');\n\n    onMessage(node, msg);\n}",
        "reject_response": "@Override public void apply(UUID nodeId, M msg) {\n    ClusterNode node = cctx.node(nodeId);\n\n    if (node == null) {\n        if (log.isTraceEnabled())\n        if (log.isDebugEnabled())\n            log.debug(\"Received message from failed node [node=\" + nodeId + \", msg=\" + msg + ']');\n\n        return;\n    }\n\n    if (log.isTraceEnabled())\n        log.trace(\"Received message from node [node=\" + nodeId + \", msg=\" + msg + ']');\n\n    onMessage(node, msg);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2798,
        "instruction": "@Override\npublic int read(final ByteBuffer dst) throws IOException {\n    final int bytesRead = session.read(dst);\n    if (bytesRead > 0 && wireLog.isEnabled()) {\n        final ByteBuffer b = dst.duplicate();\n        final int p = b.position();\n        b.limit(p);\n        b.position(p - bytesRead);\n        wireLog.input(b);\n    }\n    return bytesRead;\n}",
        "accept_response": "@Override\npublic int read(final ByteBuffer dst) throws IOException {\n    final int bytesRead = session.read(dst);\n    if (log.isDebugEnabled()) {\n        log.debug(\"{} {} bytes read\", session, bytesRead);\n    }\n    if (bytesRead > 0 && wireLog.isEnabled()) {\n        final ByteBuffer b = dst.duplicate();\n        final int p = b.position();\n        b.limit(p);\n        b.position(p - bytesRead);\n        wireLog.input(b);\n    }\n    return bytesRead;\n}",
        "reject_response": "@Override\npublic int read(final ByteBuffer dst) throws IOException {\n    final int bytesRead = session.read(dst);\n    if (log.isDebugEnabled()) {\n        log.debug(session + \" \" + bytesRead + \" bytes read\");\n    }\n    if (bytesRead > 0 && wireLog.isEnabled()) {\n        final ByteBuffer b = dst.duplicate();\n        final int p = b.position();\n        b.limit(p);\n        b.position(p - bytesRead);\n        wireLog.input(b);\n    }\n    return bytesRead;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3015,
        "instruction": "@Override\npublic void onNext(final RemoteEvent<T> value) {\n  try {\n\n    LOG.log(Level.FINEST, \"Link: {0} event: {1}\", new Object[] {linkRef, value});\n\n    if (linkRef.get() == null) {\n      queue.add(value);\n\n      final Link<byte[]> link = transport.get(value.remoteAddress());\n      if (link != null) {\n        LOG.log(Level.FINEST, \"transport get link: {0}\", link);\n        setLink(link);\n        return;\n      }\n\n      final ConnectFutureTask<Link<byte[]>> cf = new ConnectFutureTask<>(\n          new ConnectCallable(transport, value.localAddress(), value.remoteAddress()),\n          new ConnectEventHandler<>(this));\n      executor.submit(cf);\n\n    } else {\n      // encode and write bytes\n      // consumeQueue();\n      linkRef.get().write(encoder.encode(value));\n    }\n\n  } catch (final RemoteRuntimeException ex) {\n    LOG.log(Level.SEVERE, \"Remote Exception\", ex);\n    throw ex;\n  }\n}",
        "accept_response": "@Override\npublic void onNext(final RemoteEvent<T> value) {\n  try {\n\n    LOG.log(Level.FINEST, \"Link: {0} event: {1}\", new Object[] {linkRef, value});\n\n    if (linkRef.get() == null) {\n      queue.add(value);\n\n      final Link<byte[]> link = transport.get(value.remoteAddress());\n      if (link != null) {\n        LOG.log(Level.FINEST, \"transport get link: {0}\", link);\n        setLink(link);\n        return;\n      }\n\n      final ConnectFutureTask<Link<byte[]>> cf = new ConnectFutureTask<>(\n          new ConnectCallable(transport, value.localAddress(), value.remoteAddress()),\n          new ConnectEventHandler<>(this));\n      executor.submit(cf);\n\n    } else {\n      // encode and write bytes\n      // consumeQueue();\n      LOG.log(Level.FINEST, \"Send: {0} event: {1}\", new Object[] {linkRef, value});\n      linkRef.get().write(encoder.encode(value));\n    }\n\n  } catch (final RemoteRuntimeException ex) {\n    LOG.log(Level.SEVERE, \"Remote Exception\", ex);\n    throw ex;\n  }\n}",
        "reject_response": "@Override\npublic void onNext(final RemoteEvent<T> value) {\n  try {\n\n    LOG.log(Level.FINEST, \"Link: {0} event: {1}\", new Object[] {linkRef, value});\n\n    if (linkRef.get() == null) {\n      queue.add(value);\n\n      final Link<byte[]> link = transport.get(value.remoteAddress());\n      if (link != null) {\n        LOG.log(Level.FINEST, \"transport get link: {0}\", link);\n        setLink(link);\n        return;\n      }\n\n      final ConnectFutureTask<Link<byte[]>> cf = new ConnectFutureTask<>(\n          new ConnectCallable(transport, value.localAddress(), value.remoteAddress()),\n          new ConnectEventHandler<>(this));\n      executor.submit(cf);\n\n    } else {\n      // encode and write bytes\n      // consumeQueue();\n\n      if (LOG.isLoggable(Level.FINEST)) {\n        LOG.log(Level.FINEST, \"Send an event from \" + linkRef.get().getLocalAddress() + \" to \" +\n            linkRef.get().getRemoteAddress() + \" value \" + value);\n      }\n      linkRef.get().write(encoder.encode(value));\n    }\n\n  } catch (final RemoteRuntimeException ex) {\n    LOG.log(Level.SEVERE, \"Remote Exception\", ex);\n    throw ex;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2729,
        "instruction": "private void loadInternal(RandomAccessFile raFile, FileInputStream fin)\n    throws IOException {\n  if (!FSImageUtil.checkFileFormat(raFile)) {\n    throw new IOException(\"Unrecognized file format\");\n  }\n  FileSummary summary = FSImageUtil.loadSummary(raFile);\n  if (requireSameLayoutVersion && summary.getLayoutVersion() !=\n      HdfsConstants.NAMENODE_LAYOUT_VERSION) {\n    throw new IOException(\"Image version \" + summary.getLayoutVersion() +\n        \" is not equal to the software version \" +\n        HdfsConstants.NAMENODE_LAYOUT_VERSION);\n  }\n\n  FileChannel channel = fin.getChannel();\n\n  FSImageFormatPBINode.Loader inodeLoader = new FSImageFormatPBINode.Loader(\n      fsn, this);\n  FSImageFormatPBSnapshot.Loader snapshotLoader = new FSImageFormatPBSnapshot.Loader(\n      fsn, this);\n\n  ArrayList<FileSummary.Section> sections = Lists.newArrayList(summary\n      .getSectionsList());\n  Collections.sort(sections, new Comparator<FileSummary.Section>() {\n    @Override\n    public int compare(FileSummary.Section s1, FileSummary.Section s2) {\n      SectionName n1 = SectionName.fromString(s1.getName());\n      SectionName n2 = SectionName.fromString(s2.getName());\n      if (n1 == null) {\n        return n2 == null ? 0 : -1;\n      } else if (n2 == null) {\n        return -1;\n      } else {\n        return n1.ordinal() - n2.ordinal();\n      }\n    }\n  });\n\n  StartupProgress prog = NameNode.getStartupProgress();\n  /**\n   * beginStep() and the endStep() calls do not match the boundary of the\n   * sections. This is because that the current implementation only allows\n   * a particular step to be started for once.\n   */\n  Step currentStep = null;\n\n  for (FileSummary.Section s : sections) {\n    channel.position(s.getOffset());\n    InputStream in = new BufferedInputStream(new LimitInputStream(fin,\n        s.getLength()));\n\n    in = FSImageUtil.wrapInputStreamForCompression(conf,\n        summary.getCodec(), in);\n\n    String n = s.getName();\n\n    switch (SectionName.fromString(n)) {\n    case NS_INFO:\n      loadNameSystemSection(in);\n      break;\n    case STRING_TABLE:\n      loadStringTableSection(in);\n      break;\n    case INODE: {\n      currentStep = new Step(StepType.INODES);\n      prog.beginStep(Phase.LOADING_FSIMAGE, currentStep);\n      inodeLoader.loadINodeSection(in);\n    }\n      break;\n    case INODE_REFERENCE:\n      snapshotLoader.loadINodeReferenceSection(in);\n      break;\n    case INODE_DIR:\n      inodeLoader.loadINodeDirectorySection(in);\n      break;\n    case FILES_UNDERCONSTRUCTION:\n      inodeLoader.loadFilesUnderConstructionSection(in);\n      break;\n    case SNAPSHOT:\n      snapshotLoader.loadSnapshotSection(in);\n      break;\n    case SNAPSHOT_DIFF:\n      snapshotLoader.loadSnapshotDiffSection(in);\n      break;\n    case SECRET_MANAGER: {\n      prog.endStep(Phase.LOADING_FSIMAGE, currentStep);\n      Step step = new Step(StepType.DELEGATION_TOKENS);\n      prog.beginStep(Phase.LOADING_FSIMAGE, step);\n      loadSecretManagerSection(in);\n      prog.endStep(Phase.LOADING_FSIMAGE, step);\n    }\n      break;\n    case CACHE_MANAGER: {\n      Step step = new Step(StepType.CACHE_POOLS);\n      prog.beginStep(Phase.LOADING_FSIMAGE, step);\n      loadCacheManagerSection(in);\n      prog.endStep(Phase.LOADING_FSIMAGE, step);\n    }\n      break;\n    default:\n      break;\n    }\n  }\n}",
        "accept_response": "private void loadInternal(RandomAccessFile raFile, FileInputStream fin)\n    throws IOException {\n  if (!FSImageUtil.checkFileFormat(raFile)) {\n    throw new IOException(\"Unrecognized file format\");\n  }\n  FileSummary summary = FSImageUtil.loadSummary(raFile);\n  if (requireSameLayoutVersion && summary.getLayoutVersion() !=\n      HdfsConstants.NAMENODE_LAYOUT_VERSION) {\n    throw new IOException(\"Image version \" + summary.getLayoutVersion() +\n        \" is not equal to the software version \" +\n        HdfsConstants.NAMENODE_LAYOUT_VERSION);\n  }\n\n  FileChannel channel = fin.getChannel();\n\n  FSImageFormatPBINode.Loader inodeLoader = new FSImageFormatPBINode.Loader(\n      fsn, this);\n  FSImageFormatPBSnapshot.Loader snapshotLoader = new FSImageFormatPBSnapshot.Loader(\n      fsn, this);\n\n  ArrayList<FileSummary.Section> sections = Lists.newArrayList(summary\n      .getSectionsList());\n  Collections.sort(sections, new Comparator<FileSummary.Section>() {\n    @Override\n    public int compare(FileSummary.Section s1, FileSummary.Section s2) {\n      SectionName n1 = SectionName.fromString(s1.getName());\n      SectionName n2 = SectionName.fromString(s2.getName());\n      if (n1 == null) {\n        return n2 == null ? 0 : -1;\n      } else if (n2 == null) {\n        return -1;\n      } else {\n        return n1.ordinal() - n2.ordinal();\n      }\n    }\n  });\n\n  StartupProgress prog = NameNode.getStartupProgress();\n  /**\n   * beginStep() and the endStep() calls do not match the boundary of the\n   * sections. This is because that the current implementation only allows\n   * a particular step to be started for once.\n   */\n  Step currentStep = null;\n\n  for (FileSummary.Section s : sections) {\n    channel.position(s.getOffset());\n    InputStream in = new BufferedInputStream(new LimitInputStream(fin,\n        s.getLength()));\n\n    in = FSImageUtil.wrapInputStreamForCompression(conf,\n        summary.getCodec(), in);\n\n    String n = s.getName();\n\n    switch (SectionName.fromString(n)) {\n    case NS_INFO:\n      loadNameSystemSection(in);\n      break;\n    case STRING_TABLE:\n      loadStringTableSection(in);\n      break;\n    case INODE: {\n      currentStep = new Step(StepType.INODES);\n      prog.beginStep(Phase.LOADING_FSIMAGE, currentStep);\n      inodeLoader.loadINodeSection(in);\n    }\n      break;\n    case INODE_REFERENCE:\n      snapshotLoader.loadINodeReferenceSection(in);\n      break;\n    case INODE_DIR:\n      inodeLoader.loadINodeDirectorySection(in);\n      break;\n    case FILES_UNDERCONSTRUCTION:\n      inodeLoader.loadFilesUnderConstructionSection(in);\n      break;\n    case SNAPSHOT:\n      snapshotLoader.loadSnapshotSection(in);\n      break;\n    case SNAPSHOT_DIFF:\n      snapshotLoader.loadSnapshotDiffSection(in);\n      break;\n    case SECRET_MANAGER: {\n      prog.endStep(Phase.LOADING_FSIMAGE, currentStep);\n      Step step = new Step(StepType.DELEGATION_TOKENS);\n      prog.beginStep(Phase.LOADING_FSIMAGE, step);\n      loadSecretManagerSection(in);\n      prog.endStep(Phase.LOADING_FSIMAGE, step);\n    }\n      break;\n    case CACHE_MANAGER: {\n      Step step = new Step(StepType.CACHE_POOLS);\n      prog.beginStep(Phase.LOADING_FSIMAGE, step);\n      loadCacheManagerSection(in);\n      prog.endStep(Phase.LOADING_FSIMAGE, step);\n    }\n      break;\n    default:\n      LOG.warn(\"Unrecognized section {}\", n);\n      break;\n    }\n  }\n}",
        "reject_response": "private void loadInternal(RandomAccessFile raFile, FileInputStream fin)\n    throws IOException {\n  if (!FSImageUtil.checkFileFormat(raFile)) {\n    throw new IOException(\"Unrecognized file format\");\n  }\n  FileSummary summary = FSImageUtil.loadSummary(raFile);\n  if (requireSameLayoutVersion && summary.getLayoutVersion() !=\n      HdfsConstants.NAMENODE_LAYOUT_VERSION) {\n    throw new IOException(\"Image version \" + summary.getLayoutVersion() +\n        \" is not equal to the software version \" +\n        HdfsConstants.NAMENODE_LAYOUT_VERSION);\n  }\n\n  FileChannel channel = fin.getChannel();\n\n  FSImageFormatPBINode.Loader inodeLoader = new FSImageFormatPBINode.Loader(\n      fsn, this);\n  FSImageFormatPBSnapshot.Loader snapshotLoader = new FSImageFormatPBSnapshot.Loader(\n      fsn, this);\n\n  ArrayList<FileSummary.Section> sections = Lists.newArrayList(summary\n      .getSectionsList());\n  Collections.sort(sections, new Comparator<FileSummary.Section>() {\n    @Override\n    public int compare(FileSummary.Section s1, FileSummary.Section s2) {\n      SectionName n1 = SectionName.fromString(s1.getName());\n      SectionName n2 = SectionName.fromString(s2.getName());\n      if (n1 == null) {\n        return n2 == null ? 0 : -1;\n      } else if (n2 == null) {\n        return -1;\n      } else {\n        return n1.ordinal() - n2.ordinal();\n      }\n    }\n  });\n\n  StartupProgress prog = NameNode.getStartupProgress();\n  /**\n   * beginStep() and the endStep() calls do not match the boundary of the\n   * sections. This is because that the current implementation only allows\n   * a particular step to be started for once.\n   */\n  Step currentStep = null;\n\n  for (FileSummary.Section s : sections) {\n    channel.position(s.getOffset());\n    InputStream in = new BufferedInputStream(new LimitInputStream(fin,\n        s.getLength()));\n\n    in = FSImageUtil.wrapInputStreamForCompression(conf,\n        summary.getCodec(), in);\n\n    String n = s.getName();\n\n    switch (SectionName.fromString(n)) {\n    case NS_INFO:\n      loadNameSystemSection(in);\n      break;\n    case STRING_TABLE:\n      loadStringTableSection(in);\n      break;\n    case INODE: {\n      currentStep = new Step(StepType.INODES);\n      prog.beginStep(Phase.LOADING_FSIMAGE, currentStep);\n      inodeLoader.loadINodeSection(in);\n    }\n      break;\n    case INODE_REFERENCE:\n      snapshotLoader.loadINodeReferenceSection(in);\n      break;\n    case INODE_DIR:\n      inodeLoader.loadINodeDirectorySection(in);\n      break;\n    case FILES_UNDERCONSTRUCTION:\n      inodeLoader.loadFilesUnderConstructionSection(in);\n      break;\n    case SNAPSHOT:\n      snapshotLoader.loadSnapshotSection(in);\n      break;\n    case SNAPSHOT_DIFF:\n      snapshotLoader.loadSnapshotDiffSection(in);\n      break;\n    case SECRET_MANAGER: {\n      prog.endStep(Phase.LOADING_FSIMAGE, currentStep);\n      Step step = new Step(StepType.DELEGATION_TOKENS);\n      prog.beginStep(Phase.LOADING_FSIMAGE, step);\n      loadSecretManagerSection(in);\n      prog.endStep(Phase.LOADING_FSIMAGE, step);\n    }\n      break;\n    case CACHE_MANAGER: {\n      Step step = new Step(StepType.CACHE_POOLS);\n      prog.beginStep(Phase.LOADING_FSIMAGE, step);\n      loadCacheManagerSection(in);\n      prog.endStep(Phase.LOADING_FSIMAGE, step);\n    }\n      break;\n    default:\n      LOG.warn(\"Unrecognized section \" + n);\n      break;\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3161,
        "instruction": "protected final void renderComponentTag(ComponentTag tag)\n{\n\tif (needToRenderTag(tag))\n\t{\n\t\t// apply behaviors that are attached to the component tag.\n\t\tif (tag.hasBehaviors())\n\t\t{\n\t\t\tIterator<? extends Behavior> tagBehaviors = tag.getBehaviors();\n\t\t\twhile (tagBehaviors.hasNext())\n\t\t\t{\n\t\t\t\tfinal Behavior behavior = tagBehaviors.next();\n\t\t\t\tif (behavior.isEnabled(this))\n\t\t\t\t{\n\t\t\t\t\tbehavior.onComponentTag(this, tag);\n\t\t\t\t}\n\t\t\t\tbehavior.detach(this);\n\t\t\t}\n\t\t}\n\n\t\t// Apply behavior modifiers\n\t\tList<? extends Behavior> behaviors = getBehaviors();\n\t\tif ((behaviors != null) && !behaviors.isEmpty() && !tag.isClose() &&\n\t\t\t(isIgnoreAttributeModifier() == false))\n\t\t{\n\t\t\ttag = tag.mutable();\n\t\t\tfor (Behavior behavior : behaviors)\n\t\t\t{\n\t\t\t\t// Components may reject some behavior components\n\t\t\t\tif (isBehaviorAccepted(behavior))\n\t\t\t\t{\n\t\t\t\t\tbehavior.onComponentTag(this, tag);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif ((tag instanceof WicketTag) && !tag.isClose() &&\n\t\t\t!getFlag(FLAG_IGNORE_ATTRIBUTE_MODIFIER))\n\t\t{\n\t\t\tExceptionSettings.NotRenderableErrorStrategy notRenderableErrorStrategy = ExceptionSettings.NotRenderableErrorStrategy.LOG_WARNING;\n\t\t\tif (Application.exists())\n\t\t\t{\n\t\t\t\tnotRenderableErrorStrategy = getApplication().getExceptionSettings().getNotRenderableErrorStrategy();\n\t\t\t}\n\n\t\t\tString tagName = tag.getNamespace() + \":\" + tag.getName();\n\t\t\tString componentId = getId();\n\t\t\tif (getFlag(FLAG_OUTPUT_MARKUP_ID))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Markup id set on a component that is usually not rendered into markup. \" +\n\t\t\t\t                               \"Markup id: %s, component id: %s, component tag: %s.\",\n\t\t\t\t                               getMarkupId(), componentId, tagName);\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t\tif (getFlag(FLAG_PLACEHOLDER))\n\t\t\t{\n\t\t\t\tString message = String.format(\n\t\t\t\t\t\t\"Placeholder tag set on a component that is usually not rendered into markup. \" +\n\t\t\t\t\t\t\"Component id: %s, component tag: %s.\", componentId, tagName);\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Write the tag\n\t\ttag.writeOutput(getResponse(), !needToRenderTag(null),\n\t\t\tgetMarkup().getMarkupResourceStream().getWicketNamespace());\n\t}\n}",
        "accept_response": "protected final void renderComponentTag(ComponentTag tag)\n{\n\tif (needToRenderTag(tag))\n\t{\n\t\t// apply behaviors that are attached to the component tag.\n\t\tif (tag.hasBehaviors())\n\t\t{\n\t\t\tIterator<? extends Behavior> tagBehaviors = tag.getBehaviors();\n\t\t\twhile (tagBehaviors.hasNext())\n\t\t\t{\n\t\t\t\tfinal Behavior behavior = tagBehaviors.next();\n\t\t\t\tif (behavior.isEnabled(this))\n\t\t\t\t{\n\t\t\t\t\tbehavior.onComponentTag(this, tag);\n\t\t\t\t}\n\t\t\t\tbehavior.detach(this);\n\t\t\t}\n\t\t}\n\n\t\t// Apply behavior modifiers\n\t\tList<? extends Behavior> behaviors = getBehaviors();\n\t\tif ((behaviors != null) && !behaviors.isEmpty() && !tag.isClose() &&\n\t\t\t(isIgnoreAttributeModifier() == false))\n\t\t{\n\t\t\ttag = tag.mutable();\n\t\t\tfor (Behavior behavior : behaviors)\n\t\t\t{\n\t\t\t\t// Components may reject some behavior components\n\t\t\t\tif (isBehaviorAccepted(behavior))\n\t\t\t\t{\n\t\t\t\t\tbehavior.onComponentTag(this, tag);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif ((tag instanceof WicketTag) && !tag.isClose() &&\n\t\t\t!getFlag(FLAG_IGNORE_ATTRIBUTE_MODIFIER))\n\t\t{\n\t\t\tExceptionSettings.NotRenderableErrorStrategy notRenderableErrorStrategy = ExceptionSettings.NotRenderableErrorStrategy.LOG_WARNING;\n\t\t\tif (Application.exists())\n\t\t\t{\n\t\t\t\tnotRenderableErrorStrategy = getApplication().getExceptionSettings().getNotRenderableErrorStrategy();\n\t\t\t}\n\n\t\t\tString tagName = tag.getNamespace() + \":\" + tag.getName();\n\t\t\tString componentId = getId();\n\t\t\tif (getFlag(FLAG_OUTPUT_MARKUP_ID))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Markup id set on a component that is usually not rendered into markup. \" +\n\t\t\t\t                               \"Markup id: %s, component id: %s, component tag: %s.\",\n\t\t\t\t                               getMarkupId(), componentId, tagName);\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t\tif (getFlag(FLAG_PLACEHOLDER))\n\t\t\t{\n\t\t\t\tString message = String.format(\n\t\t\t\t\t\t\"Placeholder tag set on a component that is usually not rendered into markup. \" +\n\t\t\t\t\t\t\"Component id: %s, component tag: %s.\", componentId, tagName);\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t}\n\n\t\t// Write the tag\n\t\ttag.writeOutput(getResponse(), !needToRenderTag(null),\n\t\t\tgetMarkup().getMarkupResourceStream().getWicketNamespace());\n\t}\n}",
        "reject_response": "protected final void renderComponentTag(ComponentTag tag)\n{\n\tif (needToRenderTag(tag))\n\t{\n\t\t// apply behaviors that are attached to the component tag.\n\t\tif (tag.hasBehaviors())\n\t\t{\n\t\t\tIterator<? extends Behavior> tagBehaviors = tag.getBehaviors();\n\t\t\twhile (tagBehaviors.hasNext())\n\t\t\t{\n\t\t\t\tfinal Behavior behavior = tagBehaviors.next();\n\t\t\t\tif (behavior.isEnabled(this))\n\t\t\t\t{\n\t\t\t\t\tbehavior.onComponentTag(this, tag);\n\t\t\t\t}\n\t\t\t\tbehavior.detach(this);\n\t\t\t}\n\t\t}\n\n\t\t// Apply behavior modifiers\n\t\tList<? extends Behavior> behaviors = getBehaviors();\n\t\tif ((behaviors != null) && !behaviors.isEmpty() && !tag.isClose() &&\n\t\t\t(isIgnoreAttributeModifier() == false))\n\t\t{\n\t\t\ttag = tag.mutable();\n\t\t\tfor (Behavior behavior : behaviors)\n\t\t\t{\n\t\t\t\t// Components may reject some behavior components\n\t\t\t\tif (isBehaviorAccepted(behavior))\n\t\t\t\t{\n\t\t\t\t\tbehavior.onComponentTag(this, tag);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif ((tag instanceof WicketTag) && !tag.isClose() &&\n\t\t\t!getFlag(FLAG_IGNORE_ATTRIBUTE_MODIFIER))\n\t\t{\n\t\t\tExceptionSettings.NotRenderableErrorStrategy notRenderableErrorStrategy = ExceptionSettings.NotRenderableErrorStrategy.LOG_WARNING;\n\t\t\tif (Application.exists())\n\t\t\t{\n\t\t\t\tnotRenderableErrorStrategy = getApplication().getExceptionSettings().getNotRenderableErrorStrategy();\n\t\t\t}\n\n\t\t\tString tagName = tag.getNamespace() + \":\" + tag.getName();\n\t\t\tString componentId = getId();\n\t\t\tif (getFlag(FLAG_OUTPUT_MARKUP_ID))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Markup id set on a component that is usually not rendered into markup. \" +\n\t\t\t\t                               \"Markup id: %s, component id: %s, component tag: %s.\",\n\t\t\t\t                               getMarkupId(), componentId, tagName);\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t\tif (getFlag(FLAG_PLACEHOLDER))\n\t\t\t{\n\t\t\t\tString message = String.format(\n\t\t\t\t\t\t\"Placeholder tag set on a component that is usually not rendered into markup. \" +\n\t\t\t\t\t\t\"Component id: %s, component tag: %s.\", componentId, tagName);\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(String.format(\n\t\t\t\t\t\"Placeholder tag set on a component that is usually not rendered into markup. \"\n\t\t\t\t\t\t+ \"Component id: %s, component tag: %s.\", getId(), tag.getName()));\n\t\t\t}\n\t\t}\n\n\t\t// Write the tag\n\t\ttag.writeOutput(getResponse(), !needToRenderTag(null),\n\t\t\tgetMarkup().getMarkupResourceStream().getWicketNamespace());\n\t}\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2431,
        "instruction": "protected void checkStoreSystemUsageLimits() throws Exception {\n    final SystemUsage usage = getSystemUsage();\n\n    //Check the persistent store and temp store limits if they exist\n    //and schedule a periodic check to update disk limits if\n    //schedulePeriodForDiskLimitCheck is set\n    checkStoreUsageLimits();\n    checkTmpStoreUsageLimits();\n    scheduleDiskUsageLimitsCheck();\n\n    if (getJobSchedulerStore() != null) {\n        JobSchedulerStore scheduler = getJobSchedulerStore();\n        File schedulerDir = scheduler.getDirectory();\n        if (schedulerDir != null) {\n\n            String schedulerDirPath = schedulerDir.getAbsolutePath();\n            if (!schedulerDir.isAbsolute()) {\n                schedulerDir = new File(schedulerDirPath);\n            }\n\n            while (schedulerDir != null && !schedulerDir.isDirectory()) {\n                schedulerDir = schedulerDir.getParentFile();\n            }\n            long schedulerLimit = usage.getJobSchedulerUsage().getLimit();\n            long dirFreeSpace = schedulerDir.getUsableSpace();\n            if (schedulerLimit > dirFreeSpace) {\n                usage.getJobSchedulerUsage().setLimit(dirFreeSpace);\n            }\n        }\n    }\n}",
        "accept_response": "protected void checkStoreSystemUsageLimits() throws Exception {\n    final SystemUsage usage = getSystemUsage();\n\n    //Check the persistent store and temp store limits if they exist\n    //and schedule a periodic check to update disk limits if\n    //schedulePeriodForDiskLimitCheck is set\n    checkStoreUsageLimits();\n    checkTmpStoreUsageLimits();\n    scheduleDiskUsageLimitsCheck();\n\n    if (getJobSchedulerStore() != null) {\n        JobSchedulerStore scheduler = getJobSchedulerStore();\n        File schedulerDir = scheduler.getDirectory();\n        if (schedulerDir != null) {\n\n            String schedulerDirPath = schedulerDir.getAbsolutePath();\n            if (!schedulerDir.isAbsolute()) {\n                schedulerDir = new File(schedulerDirPath);\n            }\n\n            while (schedulerDir != null && !schedulerDir.isDirectory()) {\n                schedulerDir = schedulerDir.getParentFile();\n            }\n            long schedulerLimit = usage.getJobSchedulerUsage().getLimit();\n            long dirFreeSpace = schedulerDir.getUsableSpace();\n            if (schedulerLimit > dirFreeSpace) {\n                LOG.warn(\"Job Scheduler Store limit is {} mb, whilst the data directory: {} \" +\n                         \"only has {} mb of usable space - resetting to {} mb.\",\n                         schedulerLimit / (1024 * 1024), schedulerDir.getAbsolutePath(),\n                         dirFreeSpace / (1024 * 1024), dirFreeSpace / (1024 * 1024));\n                usage.getJobSchedulerUsage().setLimit(dirFreeSpace);\n            }\n        }\n    }\n}",
        "reject_response": "protected void checkStoreSystemUsageLimits() throws Exception {\n    final SystemUsage usage = getSystemUsage();\n\n    //Check the persistent store and temp store limits if they exist\n    //and schedule a periodic check to update disk limits if\n    //schedulePeriodForDiskLimitCheck is set\n    checkStoreUsageLimits();\n    checkTmpStoreUsageLimits();\n    scheduleDiskUsageLimitsCheck();\n\n    if (getJobSchedulerStore() != null) {\n        JobSchedulerStore scheduler = getJobSchedulerStore();\n        File schedulerDir = scheduler.getDirectory();\n        if (schedulerDir != null) {\n\n            String schedulerDirPath = schedulerDir.getAbsolutePath();\n            if (!schedulerDir.isAbsolute()) {\n                schedulerDir = new File(schedulerDirPath);\n            }\n\n            while (schedulerDir != null && !schedulerDir.isDirectory()) {\n                schedulerDir = schedulerDir.getParentFile();\n            }\n            long schedulerLimit = usage.getJobSchedulerUsage().getLimit();\n            long dirFreeSpace = schedulerDir.getUsableSpace();\n            if (schedulerLimit > dirFreeSpace) {\n                LOG.warn(\"Job Scheduler Store limit is \" + schedulerLimit / (1024 * 1024) +\n                         \" mb, whilst the data directory: \" + schedulerDir.getAbsolutePath() +\n                         \" only has \" + dirFreeSpace / (1024 * 1024) + \" mb of usable space - resetting to \" +\n                        dirFreeSpace / (1024 * 1024) + \" mb.\");\n                usage.getJobSchedulerUsage().setLimit(dirFreeSpace);\n            }\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2793,
        "instruction": "public void revalidateCacheEntry(\n        final String cacheKey,\n        final RevalidationCall call) {\n    scheduleRevalidation(cacheKey, new Runnable() {\n\n                    @Override\n                    public void run() {\n                        try (ClassicHttpResponse httpResponse = call.execute()) {\n                            if (httpResponse.getCode() < HttpStatus.SC_SERVER_ERROR && !isStale(httpResponse)) {\n                                jobSuccessful(cacheKey);\n                            } else {\n                                jobFailed(cacheKey);\n                            }\n                        } catch (final IOException ex) {\n                            jobFailed(cacheKey);\n                            LOG.debug(\"Asynchronous revalidation failed due to I/O error\", ex);\n                        } catch (final HttpException ex) {\n                            jobFailed(cacheKey);\n                        } catch (final RuntimeException ex) {\n                            jobFailed(cacheKey);\n                            LOG.error(\"Unexpected runtime exception thrown during asynchronous revalidation\", ex);\n                        }\n\n                    }\n\n                });\n}",
        "accept_response": "public void revalidateCacheEntry(\n        final String cacheKey,\n        final RevalidationCall call) {\n    scheduleRevalidation(cacheKey, new Runnable() {\n\n                    @Override\n                    public void run() {\n                        try (ClassicHttpResponse httpResponse = call.execute()) {\n                            if (httpResponse.getCode() < HttpStatus.SC_SERVER_ERROR && !isStale(httpResponse)) {\n                                jobSuccessful(cacheKey);\n                            } else {\n                                jobFailed(cacheKey);\n                            }\n                        } catch (final IOException ex) {\n                            jobFailed(cacheKey);\n                            LOG.debug(\"Asynchronous revalidation failed due to I/O error\", ex);\n                        } catch (final HttpException ex) {\n                            jobFailed(cacheKey);\n                            LOG.error(\"HTTP protocol exception during asynchronous revalidation\", ex);\n                        } catch (final RuntimeException ex) {\n                            jobFailed(cacheKey);\n                            LOG.error(\"Unexpected runtime exception thrown during asynchronous revalidation\", ex);\n                        }\n\n                    }\n\n                });\n}",
        "reject_response": "public void revalidateCacheEntry(\n        final String cacheKey,\n        final RevalidationCall call) {\n    scheduleRevalidation(cacheKey, new Runnable() {\n\n                    @Override\n                    public void run() {\n                        try (ClassicHttpResponse httpResponse = call.execute()) {\n                            if (httpResponse.getCode() < HttpStatus.SC_SERVER_ERROR && !isStale(httpResponse)) {\n                                jobSuccessful(cacheKey);\n                            } else {\n                                jobFailed(cacheKey);\n                            }\n                        } catch (final IOException ex) {\n                            jobFailed(cacheKey);\n                            LOG.debug(\"Asynchronous revalidation failed due to I/O error\", ex);\n                        } catch (final HttpException ex) {\n                            jobFailed(cacheKey);\n                            log.error(\"HTTP protocol exception during asynchronous revalidation\", ex);\n                        } catch (final RuntimeException ex) {\n                            jobFailed(cacheKey);\n                            LOG.error(\"Unexpected runtime exception thrown during asynchronous revalidation\", ex);\n                        }\n\n                    }\n\n                });\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2589,
        "instruction": "public static SingleInputGate create(\n\tString owningTaskName,\n\tJobID jobId,\n\tExecutionAttemptID executionId,\n\tInputGateDeploymentDescriptor igdd,\n\tNetworkEnvironment networkEnvironment,\n\tTaskActions taskActions,\n\tTaskIOMetricGroup metrics) {\n\n\tfinal IntermediateDataSetID consumedResultId = checkNotNull(igdd.getConsumedResultId());\n\tfinal ResultPartitionType consumedPartitionType = checkNotNull(igdd.getConsumedPartitionType());\n\n\tfinal int consumedSubpartitionIndex = igdd.getConsumedSubpartitionIndex();\n\tcheckArgument(consumedSubpartitionIndex >= 0);\n\n\tfinal InputChannelDeploymentDescriptor[] icdd = checkNotNull(igdd.getInputChannelDeploymentDescriptors());\n\n\tfinal SingleInputGate inputGate = new SingleInputGate(\n\t\towningTaskName, jobId, consumedResultId, consumedPartitionType, consumedSubpartitionIndex,\n\t\ticdd.length, taskActions, metrics, networkEnvironment.isCreditBased());\n\n\t// Create the input channels. There is one input channel for each consumed partition.\n\tfinal InputChannel[] inputChannels = new InputChannel[icdd.length];\n\n\tint numLocalChannels = 0;\n\tint numRemoteChannels = 0;\n\tint numUnknownChannels = 0;\n\n\tfor (int i = 0; i < inputChannels.length; i++) {\n\t\tfinal ResultPartitionID partitionId = icdd[i].getConsumedPartitionId();\n\t\tfinal ResultPartitionLocation partitionLocation = icdd[i].getConsumedPartitionLocation();\n\n\t\tif (partitionLocation.isLocal()) {\n\t\t\tinputChannels[i] = new LocalInputChannel(inputGate, i, partitionId,\n\t\t\t\tnetworkEnvironment.getResultPartitionManager(),\n\t\t\t\tnetworkEnvironment.getTaskEventDispatcher(),\n\t\t\t\tnetworkEnvironment.getPartitionRequestInitialBackoff(),\n\t\t\t\tnetworkEnvironment.getPartitionRequestMaxBackoff(),\n\t\t\t\tmetrics\n\t\t\t);\n\n\t\t\tnumLocalChannels++;\n\t\t}\n\t\telse if (partitionLocation.isRemote()) {\n\t\t\tinputChannels[i] = new RemoteInputChannel(inputGate, i, partitionId,\n\t\t\t\tpartitionLocation.getConnectionId(),\n\t\t\t\tnetworkEnvironment.getConnectionManager(),\n\t\t\t\tnetworkEnvironment.getPartitionRequestInitialBackoff(),\n\t\t\t\tnetworkEnvironment.getPartitionRequestMaxBackoff(),\n\t\t\t\tmetrics\n\t\t\t);\n\n\t\t\tnumRemoteChannels++;\n\t\t}\n\t\telse if (partitionLocation.isUnknown()) {\n\t\t\tinputChannels[i] = new UnknownInputChannel(inputGate, i, partitionId,\n\t\t\t\tnetworkEnvironment.getResultPartitionManager(),\n\t\t\t\tnetworkEnvironment.getTaskEventDispatcher(),\n\t\t\t\tnetworkEnvironment.getConnectionManager(),\n\t\t\t\tnetworkEnvironment.getPartitionRequestInitialBackoff(),\n\t\t\t\tnetworkEnvironment.getPartitionRequestMaxBackoff(),\n\t\t\t\tmetrics\n\t\t\t);\n\n\t\t\tnumUnknownChannels++;\n\t\t}\n\t\telse {\n\t\t\tthrow new IllegalStateException(\"Unexpected partition location.\");\n\t\t}\n\n\t\tinputGate.setInputChannel(partitionId.getPartitionId(), inputChannels[i]);\n\t}\n\n\n\treturn inputGate;\n}",
        "accept_response": "public static SingleInputGate create(\n\tString owningTaskName,\n\tJobID jobId,\n\tExecutionAttemptID executionId,\n\tInputGateDeploymentDescriptor igdd,\n\tNetworkEnvironment networkEnvironment,\n\tTaskActions taskActions,\n\tTaskIOMetricGroup metrics) {\n\n\tfinal IntermediateDataSetID consumedResultId = checkNotNull(igdd.getConsumedResultId());\n\tfinal ResultPartitionType consumedPartitionType = checkNotNull(igdd.getConsumedPartitionType());\n\n\tfinal int consumedSubpartitionIndex = igdd.getConsumedSubpartitionIndex();\n\tcheckArgument(consumedSubpartitionIndex >= 0);\n\n\tfinal InputChannelDeploymentDescriptor[] icdd = checkNotNull(igdd.getInputChannelDeploymentDescriptors());\n\n\tfinal SingleInputGate inputGate = new SingleInputGate(\n\t\towningTaskName, jobId, consumedResultId, consumedPartitionType, consumedSubpartitionIndex,\n\t\ticdd.length, taskActions, metrics, networkEnvironment.isCreditBased());\n\n\t// Create the input channels. There is one input channel for each consumed partition.\n\tfinal InputChannel[] inputChannels = new InputChannel[icdd.length];\n\n\tint numLocalChannels = 0;\n\tint numRemoteChannels = 0;\n\tint numUnknownChannels = 0;\n\n\tfor (int i = 0; i < inputChannels.length; i++) {\n\t\tfinal ResultPartitionID partitionId = icdd[i].getConsumedPartitionId();\n\t\tfinal ResultPartitionLocation partitionLocation = icdd[i].getConsumedPartitionLocation();\n\n\t\tif (partitionLocation.isLocal()) {\n\t\t\tinputChannels[i] = new LocalInputChannel(inputGate, i, partitionId,\n\t\t\t\tnetworkEnvironment.getResultPartitionManager(),\n\t\t\t\tnetworkEnvironment.getTaskEventDispatcher(),\n\t\t\t\tnetworkEnvironment.getPartitionRequestInitialBackoff(),\n\t\t\t\tnetworkEnvironment.getPartitionRequestMaxBackoff(),\n\t\t\t\tmetrics\n\t\t\t);\n\n\t\t\tnumLocalChannels++;\n\t\t}\n\t\telse if (partitionLocation.isRemote()) {\n\t\t\tinputChannels[i] = new RemoteInputChannel(inputGate, i, partitionId,\n\t\t\t\tpartitionLocation.getConnectionId(),\n\t\t\t\tnetworkEnvironment.getConnectionManager(),\n\t\t\t\tnetworkEnvironment.getPartitionRequestInitialBackoff(),\n\t\t\t\tnetworkEnvironment.getPartitionRequestMaxBackoff(),\n\t\t\t\tmetrics\n\t\t\t);\n\n\t\t\tnumRemoteChannels++;\n\t\t}\n\t\telse if (partitionLocation.isUnknown()) {\n\t\t\tinputChannels[i] = new UnknownInputChannel(inputGate, i, partitionId,\n\t\t\t\tnetworkEnvironment.getResultPartitionManager(),\n\t\t\t\tnetworkEnvironment.getTaskEventDispatcher(),\n\t\t\t\tnetworkEnvironment.getConnectionManager(),\n\t\t\t\tnetworkEnvironment.getPartitionRequestInitialBackoff(),\n\t\t\t\tnetworkEnvironment.getPartitionRequestMaxBackoff(),\n\t\t\t\tmetrics\n\t\t\t);\n\n\t\t\tnumUnknownChannels++;\n\t\t}\n\t\telse {\n\t\t\tthrow new IllegalStateException(\"Unexpected partition location.\");\n\t\t}\n\n\t\tinputGate.setInputChannel(partitionId.getPartitionId(), inputChannels[i]);\n\t}\n\n\tLOG.debug(\"{}: Created {} input channels (local: {}, remote: {}, unknown: {}).\",\n\t\towningTaskName,\n\t\tinputChannels.length,\n\t\tnumLocalChannels,\n\t\tnumRemoteChannels,\n\t\tnumUnknownChannels);\n\n\treturn inputGate;\n}",
        "reject_response": "public static SingleInputGate create(\n\tString owningTaskName,\n\tJobID jobId,\n\tExecutionAttemptID executionId,\n\tInputGateDeploymentDescriptor igdd,\n\tNetworkEnvironment networkEnvironment,\n\tTaskActions taskActions,\n\tTaskIOMetricGroup metrics) {\n\n\tfinal IntermediateDataSetID consumedResultId = checkNotNull(igdd.getConsumedResultId());\n\tfinal ResultPartitionType consumedPartitionType = checkNotNull(igdd.getConsumedPartitionType());\n\n\tfinal int consumedSubpartitionIndex = igdd.getConsumedSubpartitionIndex();\n\tcheckArgument(consumedSubpartitionIndex >= 0);\n\n\tfinal InputChannelDeploymentDescriptor[] icdd = checkNotNull(igdd.getInputChannelDeploymentDescriptors());\n\n\tfinal SingleInputGate inputGate = new SingleInputGate(\n\t\towningTaskName, jobId, consumedResultId, consumedPartitionType, consumedSubpartitionIndex,\n\t\ticdd.length, taskActions, metrics, networkEnvironment.isCreditBased());\n\n\t// Create the input channels. There is one input channel for each consumed partition.\n\tfinal InputChannel[] inputChannels = new InputChannel[icdd.length];\n\n\tint numLocalChannels = 0;\n\tint numRemoteChannels = 0;\n\tint numUnknownChannels = 0;\n\n\tfor (int i = 0; i < inputChannels.length; i++) {\n\t\tfinal ResultPartitionID partitionId = icdd[i].getConsumedPartitionId();\n\t\tfinal ResultPartitionLocation partitionLocation = icdd[i].getConsumedPartitionLocation();\n\n\t\tif (partitionLocation.isLocal()) {\n\t\t\tinputChannels[i] = new LocalInputChannel(inputGate, i, partitionId,\n\t\t\t\tnetworkEnvironment.getResultPartitionManager(),\n\t\t\t\tnetworkEnvironment.getTaskEventDispatcher(),\n\t\t\t\tnetworkEnvironment.getPartitionRequestInitialBackoff(),\n\t\t\t\tnetworkEnvironment.getPartitionRequestMaxBackoff(),\n\t\t\t\tmetrics\n\t\t\t);\n\n\t\t\tnumLocalChannels++;\n\t\t}\n\t\telse if (partitionLocation.isRemote()) {\n\t\t\tinputChannels[i] = new RemoteInputChannel(inputGate, i, partitionId,\n\t\t\t\tpartitionLocation.getConnectionId(),\n\t\t\t\tnetworkEnvironment.getConnectionManager(),\n\t\t\t\tnetworkEnvironment.getPartitionRequestInitialBackoff(),\n\t\t\t\tnetworkEnvironment.getPartitionRequestMaxBackoff(),\n\t\t\t\tmetrics\n\t\t\t);\n\n\t\t\tnumRemoteChannels++;\n\t\t}\n\t\telse if (partitionLocation.isUnknown()) {\n\t\t\tinputChannels[i] = new UnknownInputChannel(inputGate, i, partitionId,\n\t\t\t\tnetworkEnvironment.getResultPartitionManager(),\n\t\t\t\tnetworkEnvironment.getTaskEventDispatcher(),\n\t\t\t\tnetworkEnvironment.getConnectionManager(),\n\t\t\t\tnetworkEnvironment.getPartitionRequestInitialBackoff(),\n\t\t\t\tnetworkEnvironment.getPartitionRequestMaxBackoff(),\n\t\t\t\tmetrics\n\t\t\t);\n\n\t\t\tnumUnknownChannels++;\n\t\t}\n\t\telse {\n\t\t\tthrow new IllegalStateException(\"Unexpected partition location.\");\n\t\t}\n\n\t\tinputGate.setInputChannel(partitionId.getPartitionId(), inputChannels[i]);\n\t}\n\n\tLOG.debug(\"Created {} input channels (local: {}, remote: {}, unknown: {}).\",\n\n\treturn inputGate;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2916,
        "instruction": "private void maybeUpdateStandbyTasks() {\n    if (!standbyTasks.isEmpty()) {\n        if (processStandbyRecords) {\n            if (!standbyRecords.isEmpty()) {\n                Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> remainingStandbyRecords = new HashMap<>();\n\n                for (TopicPartition partition : standbyRecords.keySet()) {\n                    List<ConsumerRecord<byte[], byte[]>> remaining = standbyRecords.get(partition);\n                    if (remaining != null) {\n                        StandbyTask task = standbyTasksByPartition.get(partition);\n                        remaining = task.update(partition, remaining);\n                        if (remaining != null) {\n                            remainingStandbyRecords.put(partition, remaining);\n                        } else {\n                            restoreConsumer.resume(singleton(partition));\n                        }\n                    }\n                }\n\n                standbyRecords = remainingStandbyRecords;\n            }\n            processStandbyRecords = false;\n        }\n\n        ConsumerRecords<byte[], byte[]> records = restoreConsumer.poll(0);\n\n        if (!records.isEmpty()) {\n            for (TopicPartition partition : records.partitions()) {\n                StandbyTask task = standbyTasksByPartition.get(partition);\n\n                if (task == null) {\n                    throw new StreamsException(String.format(\"stream-thread [%s] missing standby task for partition %s\", this.getName(), partition));\n                }\n\n                List<ConsumerRecord<byte[], byte[]>> remaining = task.update(partition, records.records(partition));\n                if (remaining != null) {\n                    restoreConsumer.pause(singleton(partition));\n                    standbyRecords.put(partition, remaining);\n                }\n            }\n        }\n    }\n}",
        "accept_response": "private void maybeUpdateStandbyTasks() {\n    if (!standbyTasks.isEmpty()) {\n        if (processStandbyRecords) {\n            if (!standbyRecords.isEmpty()) {\n                Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> remainingStandbyRecords = new HashMap<>();\n\n                for (TopicPartition partition : standbyRecords.keySet()) {\n                    List<ConsumerRecord<byte[], byte[]>> remaining = standbyRecords.get(partition);\n                    if (remaining != null) {\n                        StandbyTask task = standbyTasksByPartition.get(partition);\n                        remaining = task.update(partition, remaining);\n                        if (remaining != null) {\n                            remainingStandbyRecords.put(partition, remaining);\n                        } else {\n                            restoreConsumer.resume(singleton(partition));\n                        }\n                    }\n                }\n\n                standbyRecords = remainingStandbyRecords;\n            }\n            processStandbyRecords = false;\n        }\n\n        ConsumerRecords<byte[], byte[]> records = restoreConsumer.poll(0);\n\n        if (!records.isEmpty()) {\n            for (TopicPartition partition : records.partitions()) {\n                StandbyTask task = standbyTasksByPartition.get(partition);\n\n                if (task == null) {\n                    log.error(\"stream-thread [{}]  missing standby task for partition {} \", this.getName(), partition);\n                    throw new StreamsException(String.format(\"stream-thread [%s] missing standby task for partition %s\", this.getName(), partition));\n                }\n\n                List<ConsumerRecord<byte[], byte[]>> remaining = task.update(partition, records.records(partition));\n                if (remaining != null) {\n                    restoreConsumer.pause(singleton(partition));\n                    standbyRecords.put(partition, remaining);\n                }\n            }\n        }\n    }\n}",
        "reject_response": "private void maybeUpdateStandbyTasks() {\n    if (!standbyTasks.isEmpty()) {\n        if (processStandbyRecords) {\n            if (!standbyRecords.isEmpty()) {\n                Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> remainingStandbyRecords = new HashMap<>();\n\n                for (TopicPartition partition : standbyRecords.keySet()) {\n                    List<ConsumerRecord<byte[], byte[]>> remaining = standbyRecords.get(partition);\n                    if (remaining != null) {\n                        StandbyTask task = standbyTasksByPartition.get(partition);\n                        remaining = task.update(partition, remaining);\n                        if (remaining != null) {\n                            remainingStandbyRecords.put(partition, remaining);\n                        } else {\n                            restoreConsumer.resume(singleton(partition));\n                        }\n                    }\n                }\n\n                standbyRecords = remainingStandbyRecords;\n            }\n            processStandbyRecords = false;\n        }\n\n        ConsumerRecords<byte[], byte[]> records = restoreConsumer.poll(0);\n\n        if (!records.isEmpty()) {\n            for (TopicPartition partition : records.partitions()) {\n                StandbyTask task = standbyTasksByPartition.get(partition);\n\n                if (task == null) {\n                    log.error(\"missing standby task for partition {}\", partition);\n                    throw new StreamsException(\"missing standby task for partition \" + partition);\n                    throw new StreamsException(String.format(\"stream-thread [%s] missing standby task for partition %s\", this.getName(), partition));\n                }\n\n                List<ConsumerRecord<byte[], byte[]>> remaining = task.update(partition, records.records(partition));\n                if (remaining != null) {\n                    restoreConsumer.pause(singleton(partition));\n                    standbyRecords.put(partition, remaining);\n                }\n            }\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3040,
        "instruction": "@Override\npublic void run() {\n    TopologyView oldView = viewManager.updateView();\n    TopologyView newView = viewManager.getCurrentView();\n    if (! newView.equals(oldView)) {\n        topologyChangeHandler.changed(oldView, newView);\n    }\n}",
        "accept_response": "@Override\npublic void run() {\n    TopologyView oldView = viewManager.updateView();\n    TopologyView newView = viewManager.getCurrentView();\n    if (! newView.equals(oldView)) {\n        LOG.info(\"TopologyView changed from {} to {}\", oldView, newView);\n        topologyChangeHandler.changed(oldView, newView);\n    }\n}",
        "reject_response": "@Override\npublic void run() {\n    TopologyView oldView = viewManager.updateView();\n    TopologyView newView = viewManager.getCurrentView();\n    if (! newView.equals(oldView)) {\n        LOG.debug(String.format(\"TopologyView changed from %s to %s\", oldView, newView));\n        topologyChangeHandler.changed(oldView, newView);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3098,
        "instruction": "@Override\npublic void map(LongWritable key, Text value, OutputCollector<Writable, Writable> out, Reporter reporter)\n\tthrows IOException\n{\n\tLOG.trace(\"execute RemoteParWorkerMapper \"+_stringID+\" (\"+_workerID+\")\");\n\n\t//state for jvm reuse and multiple iterations\n\tlong numIters = getExecutedIterations();\n\n\ttry\n\t{\n\t\t//parse input task\n\t\tTask lTask = Task.parseCompactString( value.toString() );\n\n\t\t//execute task (on error: re-try via Hadoop)\n\t\texecuteTask( lTask );\n\n\t\t//write output if required (matrix indexed write)\n\t\tRemoteParForUtils.exportResultVariables( _workerID, _ec.getVariables(), _resultVars, _rvarFnames, out );\n\t}\n\tcatch(Exception ex)\n\t{\n\t\t//throw IO exception to adhere to API specification\n\t\tthrow new IOException(\"ParFOR: Failed to execute task.\",ex);\n\t}\n\n\t//statistic maintenance\n\tRemoteParForUtils.incrementParForMRCounters(reporter, 1, getExecutedIterations()-numIters);\n\n\t//print heaver hitter per task\n\tJobConf job = ConfigurationManager.getCachedJobConf();\n\tif( DMLScript.STATISTICS && !InfrastructureAnalyzer.isLocalMode(job) )\n}",
        "accept_response": "@Override\npublic void map(LongWritable key, Text value, OutputCollector<Writable, Writable> out, Reporter reporter)\n\tthrows IOException\n{\n\tLOG.trace(\"execute RemoteParWorkerMapper \"+_stringID+\" (\"+_workerID+\")\");\n\n\t//state for jvm reuse and multiple iterations\n\tlong numIters = getExecutedIterations();\n\n\ttry\n\t{\n\t\t//parse input task\n\t\tTask lTask = Task.parseCompactString( value.toString() );\n\n\t\t//execute task (on error: re-try via Hadoop)\n\t\texecuteTask( lTask );\n\n\t\t//write output if required (matrix indexed write)\n\t\tRemoteParForUtils.exportResultVariables( _workerID, _ec.getVariables(), _resultVars, _rvarFnames, out );\n\t}\n\tcatch(Exception ex)\n\t{\n\t\t//throw IO exception to adhere to API specification\n\t\tthrow new IOException(\"ParFOR: Failed to execute task.\",ex);\n\t}\n\n\t//statistic maintenance\n\tRemoteParForUtils.incrementParForMRCounters(reporter, 1, getExecutedIterations()-numIters);\n\n\t//print heaver hitter per task\n\tJobConf job = ConfigurationManager.getCachedJobConf();\n\tif( DMLScript.STATISTICS && !InfrastructureAnalyzer.isLocalMode(job) )\n\t\tLOG.info(\"\\nSystemML Statistics:\\nHeavy hitter instructions (name, time, count):\\n\" + Statistics.getHeavyHitters(DMLScript.STATISTICS_COUNT));\n}",
        "reject_response": "@Override\npublic void map(LongWritable key, Text value, OutputCollector<Writable, Writable> out, Reporter reporter)\n\tthrows IOException\n{\n\tLOG.trace(\"execute RemoteParWorkerMapper \"+_stringID+\" (\"+_workerID+\")\");\n\n\t//state for jvm reuse and multiple iterations\n\tlong numIters = getExecutedIterations();\n\n\ttry\n\t{\n\t\t//parse input task\n\t\tTask lTask = Task.parseCompactString( value.toString() );\n\n\t\t//execute task (on error: re-try via Hadoop)\n\t\texecuteTask( lTask );\n\n\t\t//write output if required (matrix indexed write)\n\t\tRemoteParForUtils.exportResultVariables( _workerID, _ec.getVariables(), _resultVars, _rvarFnames, out );\n\t}\n\tcatch(Exception ex)\n\t{\n\t\t//throw IO exception to adhere to API specification\n\t\tthrow new IOException(\"ParFOR: Failed to execute task.\",ex);\n\t}\n\n\t//statistic maintenance\n\tRemoteParForUtils.incrementParForMRCounters(reporter, 1, getExecutedIterations()-numIters);\n\n\t//print heaver hitter per task\n\tJobConf job = ConfigurationManager.getCachedJobConf();\n\tif( DMLScript.STATISTICS && !InfrastructureAnalyzer.isLocalMode(job) )\n\t\tLOG.info(\"\\nSystemML Statistics:\\nHeavy hitter instructions (name, time, count):\\n\" + Statistics.getHeavyHitters(10));\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2851,
        "instruction": "@Override\npublic TSExecuteStatementResp executeQueryStatement(TSExecuteStatementReq req) {\n  if (!checkLogin()) {\n    logger.info(INFO_NOT_LOGIN, IoTDBConstant.GLOBAL_DB_NAME);\n    return getTSExecuteStatementResp(getStatus(TSStatusCode.NOT_LOGIN_ERROR));\n  }\n\n  String statement = req.getStatement();\n  PhysicalPlan physicalPlan;\n  try {\n    physicalPlan = processor.parseSQLToPhysicalPlan(statement, zoneIds.get());\n  } catch (QueryProcessException | MetadataException e) {\n    return getTSExecuteStatementResp(getStatus(TSStatusCode.SQL_PARSE_ERROR, e.getMessage()));\n  }\n\n  if (!physicalPlan.isQuery()) {\n    return getTSExecuteStatementResp(getStatus(TSStatusCode.EXECUTE_STATEMENT_ERROR,\n        \"Statement is not a query statement.\"));\n  }\n  return executeQueryStatement(req.statementId, physicalPlan);\n}",
        "accept_response": "@Override\npublic TSExecuteStatementResp executeQueryStatement(TSExecuteStatementReq req) {\n  if (!checkLogin()) {\n    logger.info(INFO_NOT_LOGIN, IoTDBConstant.GLOBAL_DB_NAME);\n    return getTSExecuteStatementResp(getStatus(TSStatusCode.NOT_LOGIN_ERROR));\n  }\n\n  String statement = req.getStatement();\n  PhysicalPlan physicalPlan;\n  try {\n    physicalPlan = processor.parseSQLToPhysicalPlan(statement, zoneIds.get());\n  } catch (QueryProcessException | MetadataException e) {\n    logger.info(\"meet error while parsing SQL to physical plan: {}\", e.getMessage());\n    return getTSExecuteStatementResp(getStatus(TSStatusCode.SQL_PARSE_ERROR, e.getMessage()));\n  }\n\n  if (!physicalPlan.isQuery()) {\n    return getTSExecuteStatementResp(getStatus(TSStatusCode.EXECUTE_STATEMENT_ERROR,\n        \"Statement is not a query statement.\"));\n  }\n  return executeQueryStatement(req.statementId, physicalPlan);\n}",
        "reject_response": "@Override\npublic TSExecuteStatementResp executeQueryStatement(TSExecuteStatementReq req) {\n  if (!checkLogin()) {\n    logger.info(INFO_NOT_LOGIN, IoTDBConstant.GLOBAL_DB_NAME);\n    return getTSExecuteStatementResp(getStatus(TSStatusCode.NOT_LOGIN_ERROR));\n  }\n\n  String statement = req.getStatement();\n  PhysicalPlan physicalPlan;\n  try {\n    physicalPlan = processor.parseSQLToPhysicalPlan(statement, zoneIds.get());\n  } catch (QueryProcessException | MetadataException e) {\n    logger.error(\"meet error while parsing SQL to physical plan!\", e);\n    return getTSExecuteStatementResp(getStatus(TSStatusCode.SQL_PARSE_ERROR, e.getMessage()));\n  }\n\n  if (!physicalPlan.isQuery()) {\n    return getTSExecuteStatementResp(getStatus(TSStatusCode.EXECUTE_STATEMENT_ERROR,\n        \"Statement is not a query statement.\"));\n  }\n  return executeQueryStatement(req.statementId, physicalPlan);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3238,
        "instruction": "boolean haveDelivered() {\n    for (ArrayBlockingQueue<ByteBuffer> queue : queueSendMap.values()) {\n        if (queue.size() == 0) {\n            return true;\n        }\n    }\n\n    return false;\n}",
        "accept_response": "boolean haveDelivered() {\n    for (ArrayBlockingQueue<ByteBuffer> queue : queueSendMap.values()) {\n        LOG.debug(\"Queue size: {}\", queue.size());\n        if (queue.size() == 0) {\n            return true;\n        }\n    }\n\n    return false;\n}",
        "reject_response": "boolean haveDelivered() {\n    for (ArrayBlockingQueue<ByteBuffer> queue : queueSendMap.values()) {\n        LOG.debug(\"Queue size: \" + queue.size());\n        if (queue.size() == 0) {\n            return true;\n        }\n    }\n\n    return false;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3142,
        "instruction": "private CookieContainerRequest getMatchingRequestWithoutPriority(\n    Container container,\n    String location,\n    boolean considerContainerAffinity) {\n  Resource capability = container.getResource();\n  List<? extends Collection<CookieContainerRequest>> pRequestsList =\n    amRmClient.getMatchingRequestsForTopPriority(location, capability);\n  if (considerContainerAffinity &&\n      !priorityHasAffinity.contains(amRmClient.getTopPriority())) {\n    considerContainerAffinity = false;\n  }\n  if (pRequestsList == null || pRequestsList.isEmpty()) {\n    return null;\n  }\n  CookieContainerRequest firstMatch = null;\n  for (Collection<CookieContainerRequest> requests : pRequestsList) {\n    for (CookieContainerRequest cookieContainerRequest : requests) {\n      if (firstMatch == null || // we dont have a match. So look for one\n          // we have a match but are looking for a better container level match.\n          // skip the expensive canAssignTaskToContainer() if the request is\n          // not affinitized to the container\n          container.getId().equals(cookieContainerRequest.getAffinitizedContainer())\n          ) {\n        if (canAssignTaskToContainer(cookieContainerRequest, container)) {\n          // request matched to container\n          if (!considerContainerAffinity) {\n            return cookieContainerRequest;\n          }\n          ContainerId affCId = cookieContainerRequest.getAffinitizedContainer();\n          boolean canMatchTaskWithAffinity = true;\n          if (affCId == null ||\n              !heldContainers.containsKey(affCId) ||\n              inUseContainers.contains(affCId)) {\n            // affinity not specified\n            // affinitized container is no longer held\n            // affinitized container is in use\n            canMatchTaskWithAffinity = false;\n          }\n          if (canMatchTaskWithAffinity) {\n            if (container.getId().equals(\n                cookieContainerRequest.getAffinitizedContainer())) {\n              // container level match\n              return cookieContainerRequest;\n            }\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Skipping request for container \" + container.getId()\n                  + \" due to affinity. Request: \" + cookieContainerRequest\n                  + \" affContainer: \" + affCId);\n            }\n          } else {\n            firstMatch = cookieContainerRequest;\n          }\n        }\n      }\n    }\n  }\n\n  return firstMatch;\n}",
        "accept_response": "private CookieContainerRequest getMatchingRequestWithoutPriority(\n    Container container,\n    String location,\n    boolean considerContainerAffinity) {\n  Resource capability = container.getResource();\n  List<? extends Collection<CookieContainerRequest>> pRequestsList =\n    amRmClient.getMatchingRequestsForTopPriority(location, capability);\n  if (considerContainerAffinity &&\n      !priorityHasAffinity.contains(amRmClient.getTopPriority())) {\n    considerContainerAffinity = false;\n  }\n  if (pRequestsList == null || pRequestsList.isEmpty()) {\n    return null;\n  }\n  CookieContainerRequest firstMatch = null;\n  for (Collection<CookieContainerRequest> requests : pRequestsList) {\n    for (CookieContainerRequest cookieContainerRequest : requests) {\n      if (firstMatch == null || // we dont have a match. So look for one\n          // we have a match but are looking for a better container level match.\n          // skip the expensive canAssignTaskToContainer() if the request is\n          // not affinitized to the container\n          container.getId().equals(cookieContainerRequest.getAffinitizedContainer())\n          ) {\n        if (canAssignTaskToContainer(cookieContainerRequest, container)) {\n          // request matched to container\n          if (!considerContainerAffinity) {\n            return cookieContainerRequest;\n          }\n          ContainerId affCId = cookieContainerRequest.getAffinitizedContainer();\n          boolean canMatchTaskWithAffinity = true;\n          if (affCId == null ||\n              !heldContainers.containsKey(affCId) ||\n              inUseContainers.contains(affCId)) {\n            // affinity not specified\n            // affinitized container is no longer held\n            // affinitized container is in use\n            canMatchTaskWithAffinity = false;\n          }\n          if (canMatchTaskWithAffinity) {\n            if (container.getId().equals(\n                cookieContainerRequest.getAffinitizedContainer())) {\n              // container level match\n                LOG.debug(\"Matching with affinity for request: {} container: {}\",\n                  cookieContainerRequest, affCId);\n              return cookieContainerRequest;\n            }\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Skipping request for container \" + container.getId()\n                  + \" due to affinity. Request: \" + cookieContainerRequest\n                  + \" affContainer: \" + affCId);\n            }\n          } else {\n            firstMatch = cookieContainerRequest;\n          }\n        }\n      }\n    }\n  }\n\n  return firstMatch;\n}",
        "reject_response": "private CookieContainerRequest getMatchingRequestWithoutPriority(\n    Container container,\n    String location,\n    boolean considerContainerAffinity) {\n  Resource capability = container.getResource();\n  List<? extends Collection<CookieContainerRequest>> pRequestsList =\n    amRmClient.getMatchingRequestsForTopPriority(location, capability);\n  if (considerContainerAffinity &&\n      !priorityHasAffinity.contains(amRmClient.getTopPriority())) {\n    considerContainerAffinity = false;\n  }\n  if (pRequestsList == null || pRequestsList.isEmpty()) {\n    return null;\n  }\n  CookieContainerRequest firstMatch = null;\n  for (Collection<CookieContainerRequest> requests : pRequestsList) {\n    for (CookieContainerRequest cookieContainerRequest : requests) {\n      if (firstMatch == null || // we dont have a match. So look for one\n          // we have a match but are looking for a better container level match.\n          // skip the expensive canAssignTaskToContainer() if the request is\n          // not affinitized to the container\n          container.getId().equals(cookieContainerRequest.getAffinitizedContainer())\n          ) {\n        if (canAssignTaskToContainer(cookieContainerRequest, container)) {\n          // request matched to container\n          if (!considerContainerAffinity) {\n            return cookieContainerRequest;\n          }\n          ContainerId affCId = cookieContainerRequest.getAffinitizedContainer();\n          boolean canMatchTaskWithAffinity = true;\n          if (affCId == null ||\n              !heldContainers.containsKey(affCId) ||\n              inUseContainers.contains(affCId)) {\n            // affinity not specified\n            // affinitized container is no longer held\n            // affinitized container is in use\n            canMatchTaskWithAffinity = false;\n          }\n          if (canMatchTaskWithAffinity) {\n            if (container.getId().equals(\n                cookieContainerRequest.getAffinitizedContainer())) {\n              // container level match\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Matching with affinity for request: \"\n                    + cookieContainerRequest + \" container: \" + affCId);\n              }\n              return cookieContainerRequest;\n            }\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Skipping request for container \" + container.getId()\n                  + \" due to affinity. Request: \" + cookieContainerRequest\n                  + \" affContainer: \" + affCId);\n            }\n          } else {\n            firstMatch = cookieContainerRequest;\n          }\n        }\n      }\n    }\n  }\n\n  return firstMatch;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3077,
        "instruction": "@Override\n/** {@inheritDoc} */\npublic int run(SqoopOptions options) {\n\n  if (!init(options)) {\n    return 1;\n  }\n\n  codeGenerator.setManager(manager);\n\n  if (options.getUpdateKeyCol() != null) {\n    manager.configureDbOutputColumns(options);\n  }\n\n  try {\n    exportTable(options, options.getTableName());\n  } catch (IOException ioe) {\n    LOG.error(\"Encountered IOException running export job: \", ioe);\n    if (System.getProperty(Sqoop.SQOOP_RETHROW_PROPERTY) != null) {\n      throw new RuntimeException(ioe);\n    } else {\n      return 1;\n    }\n  } catch (ExportException ee) {\n    if (System.getProperty(Sqoop.SQOOP_RETHROW_PROPERTY) != null) {\n      throw new RuntimeException(ee);\n    } else {\n      return 1;\n    }\n  } finally {\n    destroy(options);\n  }\n\n  return 0;\n}",
        "accept_response": "@Override\n/** {@inheritDoc} */\npublic int run(SqoopOptions options) {\n\n  if (!init(options)) {\n    return 1;\n  }\n\n  codeGenerator.setManager(manager);\n\n  if (options.getUpdateKeyCol() != null) {\n    manager.configureDbOutputColumns(options);\n  }\n\n  try {\n    exportTable(options, options.getTableName());\n  } catch (IOException ioe) {\n    LOG.error(\"Encountered IOException running export job: \", ioe);\n    if (System.getProperty(Sqoop.SQOOP_RETHROW_PROPERTY) != null) {\n      throw new RuntimeException(ioe);\n    } else {\n      return 1;\n    }\n  } catch (ExportException ee) {\n    LOG.error(\"Error during export: \", ee);\n    if (System.getProperty(Sqoop.SQOOP_RETHROW_PROPERTY) != null) {\n      throw new RuntimeException(ee);\n    } else {\n      return 1;\n    }\n  } finally {\n    destroy(options);\n  }\n\n  return 0;\n}",
        "reject_response": "@Override\n/** {@inheritDoc} */\npublic int run(SqoopOptions options) {\n\n  if (!init(options)) {\n    return 1;\n  }\n\n  codeGenerator.setManager(manager);\n\n  if (options.getUpdateKeyCol() != null) {\n    manager.configureDbOutputColumns(options);\n  }\n\n  try {\n    exportTable(options, options.getTableName());\n  } catch (IOException ioe) {\n    LOG.error(\"Encountered IOException running export job: \", ioe);\n    if (System.getProperty(Sqoop.SQOOP_RETHROW_PROPERTY) != null) {\n      throw new RuntimeException(ioe);\n    } else {\n      return 1;\n    }\n  } catch (ExportException ee) {\n    LOG.error(\"Error during export: \" + ee.toString());\n    if (System.getProperty(Sqoop.SQOOP_RETHROW_PROPERTY) != null) {\n      throw new RuntimeException(ee);\n    } else {\n      return 1;\n    }\n  } finally {\n    destroy(options);\n  }\n\n  return 0;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2583,
        "instruction": "public String getAddSqlWithScore(final List<ResultsetColumnHeaderData> columnHeaders, final String datatable, final String fkName,\n        final Long appTableId, final Map<String, String> queryParams) {\n\n    final Map<String, String> affectedColumns = getAffectedColumns(columnHeaders, queryParams, fkName);\n\n    String pValueWrite = \"\";\n    String scoresId = \" \";\n    final String singleQuote = \"'\";\n\n    String insertColumns = \"\";\n    String selectColumns = \"\";\n    String columnName = \"\";\n    String pValue = null;\n    for (final String key : affectedColumns.keySet()) {\n        pValue = affectedColumns.get(key);\n\n        if (StringUtils.isEmpty(pValue)) {\n            pValueWrite = \"null\";\n        } else {\n            pValueWrite = singleQuote + this.genericDataService.replace(pValue, singleQuote, singleQuote + singleQuote) + singleQuote;\n\n            scoresId += pValueWrite + \" ,\";\n\n        }\n        columnName = \"`\" + key + \"`\";\n        insertColumns += \", \" + columnName;\n        selectColumns += \",\" + pValueWrite + \" as \" + columnName;\n    }\n\n    scoresId = scoresId.replaceAll(\" ,$\", \"\");\n\n    String vaddSql = \"insert into `\" + datatable + \"` (`\" + fkName + \"` \" + insertColumns + \", `score` )\" + \" select \" + appTableId\n            + \" as id\" + selectColumns + \" , ( SELECT SUM( code_score ) FROM m_code_value WHERE m_code_value.id IN (\" + scoresId\n            + \" ) ) as score\";\n\n\n    return vaddSql;\n}",
        "accept_response": "public String getAddSqlWithScore(final List<ResultsetColumnHeaderData> columnHeaders, final String datatable, final String fkName,\n        final Long appTableId, final Map<String, String> queryParams) {\n\n    final Map<String, String> affectedColumns = getAffectedColumns(columnHeaders, queryParams, fkName);\n\n    String pValueWrite = \"\";\n    String scoresId = \" \";\n    final String singleQuote = \"'\";\n\n    String insertColumns = \"\";\n    String selectColumns = \"\";\n    String columnName = \"\";\n    String pValue = null;\n    for (final String key : affectedColumns.keySet()) {\n        pValue = affectedColumns.get(key);\n\n        if (StringUtils.isEmpty(pValue)) {\n            pValueWrite = \"null\";\n        } else {\n            pValueWrite = singleQuote + this.genericDataService.replace(pValue, singleQuote, singleQuote + singleQuote) + singleQuote;\n\n            scoresId += pValueWrite + \" ,\";\n\n        }\n        columnName = \"`\" + key + \"`\";\n        insertColumns += \", \" + columnName;\n        selectColumns += \",\" + pValueWrite + \" as \" + columnName;\n    }\n\n    scoresId = scoresId.replaceAll(\" ,$\", \"\");\n\n    String vaddSql = \"insert into `\" + datatable + \"` (`\" + fkName + \"` \" + insertColumns + \", `score` )\" + \" select \" + appTableId\n            + \" as id\" + selectColumns + \" , ( SELECT SUM( code_score ) FROM m_code_value WHERE m_code_value.id IN (\" + scoresId\n            + \" ) ) as score\";\n\n    logger.info(\"{}\", vaddSql);\n\n    return vaddSql;\n}",
        "reject_response": "public String getAddSqlWithScore(final List<ResultsetColumnHeaderData> columnHeaders, final String datatable, final String fkName,\n        final Long appTableId, final Map<String, String> queryParams) {\n\n    final Map<String, String> affectedColumns = getAffectedColumns(columnHeaders, queryParams, fkName);\n\n    String pValueWrite = \"\";\n    String scoresId = \" \";\n    final String singleQuote = \"'\";\n\n    String insertColumns = \"\";\n    String selectColumns = \"\";\n    String columnName = \"\";\n    String pValue = null;\n    for (final String key : affectedColumns.keySet()) {\n        pValue = affectedColumns.get(key);\n\n        if (StringUtils.isEmpty(pValue)) {\n            pValueWrite = \"null\";\n        } else {\n            pValueWrite = singleQuote + this.genericDataService.replace(pValue, singleQuote, singleQuote + singleQuote) + singleQuote;\n\n            scoresId += pValueWrite + \" ,\";\n\n        }\n        columnName = \"`\" + key + \"`\";\n        insertColumns += \", \" + columnName;\n        selectColumns += \",\" + pValueWrite + \" as \" + columnName;\n    }\n\n    scoresId = scoresId.replaceAll(\" ,$\", \"\");\n\n    String vaddSql = \"insert into `\" + datatable + \"` (`\" + fkName + \"` \" + insertColumns + \", `score` )\" + \" select \" + appTableId\n            + \" as id\" + selectColumns + \" , ( SELECT SUM( code_score ) FROM m_code_value WHERE m_code_value.id IN (\" + scoresId\n            + \" ) ) as score\";\n\n    logger.info(vaddSql);\n\n    return vaddSql;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3164,
        "instruction": "public final void internalRenderComponent()\n{\n\tfinal IMarkupFragment markup = getMarkup();\n\tif (markup == null)\n\t{\n\t\tthrow new MarkupException(\"Markup not found. Component: \" + toString());\n\t}\n\n\tfinal MarkupStream markupStream = new MarkupStream(markup);\n\n\t// Get mutable copy of next tag\n\tfinal ComponentTag openTag = markupStream.getTag();\n\tfinal ComponentTag tag = openTag.mutable();\n\n\t// Call any tag handler\n\tonComponentTag(tag);\n\n\t// If we're an openclose tag\n\tif (!tag.isOpenClose() && !tag.isOpen())\n\t{\n\t\t// We were something other than <tag> or <tag/>\n\t\tmarkupStream.throwMarkupException(\"Method renderComponent called on bad markup element: \" +\n\t\t\ttag);\n\t}\n\n\tif (tag.isOpenClose() && openTag.isOpen())\n\t{\n\t\tmarkupStream.throwMarkupException(\"You can not modify a open tag to open-close: \" + tag);\n\t}\n\n\ttry\n\t{\n\t\t// Render open tag\n\t\tboolean renderBodyOnly = getRenderBodyOnly();\n\t\tif (renderBodyOnly)\n\t\t{\n\t\t\tExceptionSettings.NotRenderableErrorStrategy notRenderableErrorStrategy = ExceptionSettings.NotRenderableErrorStrategy.LOG_WARNING;\n\t\t\tif (Application.exists())\n\t\t\t{\n\t\t\t\tnotRenderableErrorStrategy = getApplication().getExceptionSettings().getNotRenderableErrorStrategy();\n\t\t\t}\n\n\t\t\tif (getFlag(FLAG_OUTPUT_MARKUP_ID))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Markup id set on a component that renders its body only. \" +\n\t\t\t\t                               \"Markup id: %s, component id: %s.\", getMarkupId(), getId());\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (getFlag(FLAG_PLACEHOLDER))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Placeholder tag set on a component that renders its body only. \" +\n\t\t\t\t                               \"Component id: %s.\", getId());\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t}\n\t\telse\n\t\t{\n\t\t\trenderComponentTag(tag);\n\t\t}\n\t\tmarkupStream.next();\n\n\t\t// Render the body only if open-body-close. Do not render if open-close.\n\t\tif (tag.isOpen())\n\t\t{\n\t\t\t// Render the body. The default strategy will simply call the component's\n\t\t\t// onComponentTagBody() implementation.\n\t\t\tgetMarkupSourcingStrategy().onComponentTagBody(this, markupStream, tag);\n\n\t\t\t// Render close tag\n\t\t\tif (openTag.isOpen())\n\t\t\t{\n\t\t\t\trenderClosingComponentTag(markupStream, tag, renderBodyOnly);\n\t\t\t}\n\t\t\telse if (renderBodyOnly == false)\n\t\t\t{\n\t\t\t\tif (needToRenderTag(openTag))\n\t\t\t\t{\n\t\t\t\t\t// Close the manually opened tag. And since the user might have changed the\n\t\t\t\t\t// tag name ...\n\t\t\t\t\tgetResponse().write(tag.syntheticCloseTagString());\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tcatch (WicketRuntimeException wre)\n\t{\n\t\tthrow wre;\n\t}\n\tcatch (RuntimeException re)\n\t{\n\t\tthrow new WicketRuntimeException(\"Exception in rendering component: \" + this, re);\n\t}\n}",
        "accept_response": "public final void internalRenderComponent()\n{\n\tfinal IMarkupFragment markup = getMarkup();\n\tif (markup == null)\n\t{\n\t\tthrow new MarkupException(\"Markup not found. Component: \" + toString());\n\t}\n\n\tfinal MarkupStream markupStream = new MarkupStream(markup);\n\n\t// Get mutable copy of next tag\n\tfinal ComponentTag openTag = markupStream.getTag();\n\tfinal ComponentTag tag = openTag.mutable();\n\n\t// Call any tag handler\n\tonComponentTag(tag);\n\n\t// If we're an openclose tag\n\tif (!tag.isOpenClose() && !tag.isOpen())\n\t{\n\t\t// We were something other than <tag> or <tag/>\n\t\tmarkupStream.throwMarkupException(\"Method renderComponent called on bad markup element: \" +\n\t\t\ttag);\n\t}\n\n\tif (tag.isOpenClose() && openTag.isOpen())\n\t{\n\t\tmarkupStream.throwMarkupException(\"You can not modify a open tag to open-close: \" + tag);\n\t}\n\n\ttry\n\t{\n\t\t// Render open tag\n\t\tboolean renderBodyOnly = getRenderBodyOnly();\n\t\tif (renderBodyOnly)\n\t\t{\n\t\t\tExceptionSettings.NotRenderableErrorStrategy notRenderableErrorStrategy = ExceptionSettings.NotRenderableErrorStrategy.LOG_WARNING;\n\t\t\tif (Application.exists())\n\t\t\t{\n\t\t\t\tnotRenderableErrorStrategy = getApplication().getExceptionSettings().getNotRenderableErrorStrategy();\n\t\t\t}\n\n\t\t\tif (getFlag(FLAG_OUTPUT_MARKUP_ID))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Markup id set on a component that renders its body only. \" +\n\t\t\t\t                               \"Markup id: %s, component id: %s.\", getMarkupId(), getId());\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t\tif (getFlag(FLAG_PLACEHOLDER))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Placeholder tag set on a component that renders its body only. \" +\n\t\t\t\t                               \"Component id: %s.\", getId());\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t}\n\t\telse\n\t\t{\n\t\t\trenderComponentTag(tag);\n\t\t}\n\t\tmarkupStream.next();\n\n\t\t// Render the body only if open-body-close. Do not render if open-close.\n\t\tif (tag.isOpen())\n\t\t{\n\t\t\t// Render the body. The default strategy will simply call the component's\n\t\t\t// onComponentTagBody() implementation.\n\t\t\tgetMarkupSourcingStrategy().onComponentTagBody(this, markupStream, tag);\n\n\t\t\t// Render close tag\n\t\t\tif (openTag.isOpen())\n\t\t\t{\n\t\t\t\trenderClosingComponentTag(markupStream, tag, renderBodyOnly);\n\t\t\t}\n\t\t\telse if (renderBodyOnly == false)\n\t\t\t{\n\t\t\t\tif (needToRenderTag(openTag))\n\t\t\t\t{\n\t\t\t\t\t// Close the manually opened tag. And since the user might have changed the\n\t\t\t\t\t// tag name ...\n\t\t\t\t\tgetResponse().write(tag.syntheticCloseTagString());\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tcatch (WicketRuntimeException wre)\n\t{\n\t\tthrow wre;\n\t}\n\tcatch (RuntimeException re)\n\t{\n\t\tthrow new WicketRuntimeException(\"Exception in rendering component: \" + this, re);\n\t}\n}",
        "reject_response": "public final void internalRenderComponent()\n{\n\tfinal IMarkupFragment markup = getMarkup();\n\tif (markup == null)\n\t{\n\t\tthrow new MarkupException(\"Markup not found. Component: \" + toString());\n\t}\n\n\tfinal MarkupStream markupStream = new MarkupStream(markup);\n\n\t// Get mutable copy of next tag\n\tfinal ComponentTag openTag = markupStream.getTag();\n\tfinal ComponentTag tag = openTag.mutable();\n\n\t// Call any tag handler\n\tonComponentTag(tag);\n\n\t// If we're an openclose tag\n\tif (!tag.isOpenClose() && !tag.isOpen())\n\t{\n\t\t// We were something other than <tag> or <tag/>\n\t\tmarkupStream.throwMarkupException(\"Method renderComponent called on bad markup element: \" +\n\t\t\ttag);\n\t}\n\n\tif (tag.isOpenClose() && openTag.isOpen())\n\t{\n\t\tmarkupStream.throwMarkupException(\"You can not modify a open tag to open-close: \" + tag);\n\t}\n\n\ttry\n\t{\n\t\t// Render open tag\n\t\tboolean renderBodyOnly = getRenderBodyOnly();\n\t\tif (renderBodyOnly)\n\t\t{\n\t\t\tExceptionSettings.NotRenderableErrorStrategy notRenderableErrorStrategy = ExceptionSettings.NotRenderableErrorStrategy.LOG_WARNING;\n\t\t\tif (Application.exists())\n\t\t\t{\n\t\t\t\tnotRenderableErrorStrategy = getApplication().getExceptionSettings().getNotRenderableErrorStrategy();\n\t\t\t}\n\n\t\t\tif (getFlag(FLAG_OUTPUT_MARKUP_ID))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Markup id set on a component that renders its body only. \" +\n\t\t\t\t                               \"Markup id: %s, component id: %s.\", getMarkupId(), getId());\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(String.format(\n\t\t\t\t\t\"Markup id set on a component that renders its body only. \"\n\t\t\t\t\t\t+ \"Markup id: %s, component id: %s.\", getMarkupId(), getId()));\n\t\t\t}\n\t\t\tif (getFlag(FLAG_PLACEHOLDER))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Placeholder tag set on a component that renders its body only. \" +\n\t\t\t\t                               \"Component id: %s.\", getId());\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t}\n\t\telse\n\t\t{\n\t\t\trenderComponentTag(tag);\n\t\t}\n\t\tmarkupStream.next();\n\n\t\t// Render the body only if open-body-close. Do not render if open-close.\n\t\tif (tag.isOpen())\n\t\t{\n\t\t\t// Render the body. The default strategy will simply call the component's\n\t\t\t// onComponentTagBody() implementation.\n\t\t\tgetMarkupSourcingStrategy().onComponentTagBody(this, markupStream, tag);\n\n\t\t\t// Render close tag\n\t\t\tif (openTag.isOpen())\n\t\t\t{\n\t\t\t\trenderClosingComponentTag(markupStream, tag, renderBodyOnly);\n\t\t\t}\n\t\t\telse if (renderBodyOnly == false)\n\t\t\t{\n\t\t\t\tif (needToRenderTag(openTag))\n\t\t\t\t{\n\t\t\t\t\t// Close the manually opened tag. And since the user might have changed the\n\t\t\t\t\t// tag name ...\n\t\t\t\t\tgetResponse().write(tag.syntheticCloseTagString());\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tcatch (WicketRuntimeException wre)\n\t{\n\t\tthrow wre;\n\t}\n\tcatch (RuntimeException re)\n\t{\n\t\tthrow new WicketRuntimeException(\"Exception in rendering component: \" + this, re);\n\t}\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3120,
        "instruction": "@Private\npublic static DAGClientAMProtocolBlockingPB getAMProxy(final Configuration conf, String amHost,\n    int amRpcPort, org.apache.hadoop.yarn.api.records.Token clientToAMToken,\n    UserGroupInformation userUgi) throws IOException {\n\n  final InetSocketAddress serviceAddr = NetUtils.createSocketAddrForHost(amHost, amRpcPort);\n  if (clientToAMToken != null) {\n    Token<ClientToAMTokenIdentifier> token = ConverterUtils.convertFromYarn(clientToAMToken,\n        serviceAddr);\n    userUgi.addToken(token);\n  }\n  DAGClientAMProtocolBlockingPB proxy = null;\n  try {\n    proxy = userUgi.doAs(new PrivilegedExceptionAction<DAGClientAMProtocolBlockingPB>() {\n      @Override\n      public DAGClientAMProtocolBlockingPB run() throws IOException {\n        RPC.setProtocolEngine(conf, DAGClientAMProtocolBlockingPB.class, ProtobufRpcEngine.class);\n        return (DAGClientAMProtocolBlockingPB) RPC.getProxy(DAGClientAMProtocolBlockingPB.class,\n            0, serviceAddr, conf);\n      }\n    });\n  } catch (InterruptedException e) {\n    throw new IOException(\"Failed to connect to AM\", e);\n  }\n  return proxy;\n}",
        "accept_response": "@Private\npublic static DAGClientAMProtocolBlockingPB getAMProxy(final Configuration conf, String amHost,\n    int amRpcPort, org.apache.hadoop.yarn.api.records.Token clientToAMToken,\n    UserGroupInformation userUgi) throws IOException {\n\n  final InetSocketAddress serviceAddr = NetUtils.createSocketAddrForHost(amHost, amRpcPort);\n  if (clientToAMToken != null) {\n    Token<ClientToAMTokenIdentifier> token = ConverterUtils.convertFromYarn(clientToAMToken,\n        serviceAddr);\n    userUgi.addToken(token);\n  }\n  LOG.debug(\"Connecting to Tez AM at {}\", serviceAddr);\n  DAGClientAMProtocolBlockingPB proxy = null;\n  try {\n    proxy = userUgi.doAs(new PrivilegedExceptionAction<DAGClientAMProtocolBlockingPB>() {\n      @Override\n      public DAGClientAMProtocolBlockingPB run() throws IOException {\n        RPC.setProtocolEngine(conf, DAGClientAMProtocolBlockingPB.class, ProtobufRpcEngine.class);\n        return (DAGClientAMProtocolBlockingPB) RPC.getProxy(DAGClientAMProtocolBlockingPB.class,\n            0, serviceAddr, conf);\n      }\n    });\n  } catch (InterruptedException e) {\n    throw new IOException(\"Failed to connect to AM\", e);\n  }\n  return proxy;\n}",
        "reject_response": "@Private\npublic static DAGClientAMProtocolBlockingPB getAMProxy(final Configuration conf, String amHost,\n    int amRpcPort, org.apache.hadoop.yarn.api.records.Token clientToAMToken,\n    UserGroupInformation userUgi) throws IOException {\n\n  final InetSocketAddress serviceAddr = NetUtils.createSocketAddrForHost(amHost, amRpcPort);\n  if (clientToAMToken != null) {\n    Token<ClientToAMTokenIdentifier> token = ConverterUtils.convertFromYarn(clientToAMToken,\n        serviceAddr);\n    userUgi.addToken(token);\n  }\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"Connecting to Tez AM at \" + serviceAddr);\n  }\n  DAGClientAMProtocolBlockingPB proxy = null;\n  try {\n    proxy = userUgi.doAs(new PrivilegedExceptionAction<DAGClientAMProtocolBlockingPB>() {\n      @Override\n      public DAGClientAMProtocolBlockingPB run() throws IOException {\n        RPC.setProtocolEngine(conf, DAGClientAMProtocolBlockingPB.class, ProtobufRpcEngine.class);\n        return (DAGClientAMProtocolBlockingPB) RPC.getProxy(DAGClientAMProtocolBlockingPB.class,\n            0, serviceAddr, conf);\n      }\n    });\n  } catch (InterruptedException e) {\n    throw new IOException(\"Failed to connect to AM\", e);\n  }\n  return proxy;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2448,
        "instruction": "private boolean send(final Packet packet, final int reconnectID, final boolean flush, final boolean batch) {\n   if (invokeInterceptors(packet, interceptors, connection) != null) {\n      return false;\n   }\n\n   synchronized (sendLock) {\n      packet.setChannelID(id);\n\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" Sending packet nonblocking \" + packet + \" on channelID=\" + id);\n      }\n\n      ActiveMQBuffer buffer = packet.encode(connection);\n\n      lock.lock();\n\n      try {\n         if (failingOver) {\n            waitForFailOver(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" timed-out waiting for fail-over condition on non-blocking send\");\n         }\n\n         // Sanity check\n         if (transferring) {\n            throw ActiveMQClientMessageBundle.BUNDLE.cannotSendPacketDuringFailover();\n         }\n\n         if (resendCache != null && packet.isRequiresConfirmations()) {\n            addResendPacket(packet);\n         }\n      } finally {\n         lock.unlock();\n      }\n\n      checkReconnectID(reconnectID);\n\n      // The actual send must be outside the lock, or with OIO transport, the write can block if the tcp\n      // buffer is full, preventing any incoming buffers being handled and blocking failover\n      connection.getTransportConnection().write(buffer, flush, batch);\n      return true;\n   }\n}",
        "accept_response": "private boolean send(final Packet packet, final int reconnectID, final boolean flush, final boolean batch) {\n   if (invokeInterceptors(packet, interceptors, connection) != null) {\n      return false;\n   }\n\n   synchronized (sendLock) {\n      packet.setChannelID(id);\n\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" Sending packet nonblocking \" + packet + \" on channelID=\" + id);\n      }\n\n      ActiveMQBuffer buffer = packet.encode(connection);\n\n      lock.lock();\n\n      try {\n         if (failingOver) {\n            waitForFailOver(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" timed-out waiting for fail-over condition on non-blocking send\");\n         }\n\n         // Sanity check\n         if (transferring) {\n            throw ActiveMQClientMessageBundle.BUNDLE.cannotSendPacketDuringFailover();\n         }\n\n         if (resendCache != null && packet.isRequiresConfirmations()) {\n            addResendPacket(packet);\n         }\n      } finally {\n         lock.unlock();\n      }\n\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" Writing buffer for channelID=\" + id);\n      }\n\n      checkReconnectID(reconnectID);\n\n      // The actual send must be outside the lock, or with OIO transport, the write can block if the tcp\n      // buffer is full, preventing any incoming buffers being handled and blocking failover\n      connection.getTransportConnection().write(buffer, flush, batch);\n      return true;\n   }\n}",
        "reject_response": "private boolean send(final Packet packet, final int reconnectID, final boolean flush, final boolean batch) {\n   if (invokeInterceptors(packet, interceptors, connection) != null) {\n      return false;\n   }\n\n   synchronized (sendLock) {\n      packet.setChannelID(id);\n\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" Sending packet nonblocking \" + packet + \" on channelID=\" + id);\n      }\n\n      ActiveMQBuffer buffer = packet.encode(connection);\n\n      lock.lock();\n\n      try {\n         if (failingOver) {\n            waitForFailOver(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" timed-out waiting for fail-over condition on non-blocking send\");\n         }\n\n         // Sanity check\n         if (transferring) {\n            throw ActiveMQClientMessageBundle.BUNDLE.cannotSendPacketDuringFailover();\n         }\n\n         if (resendCache != null && packet.isRequiresConfirmations()) {\n            addResendPacket(packet);\n         }\n      } finally {\n         lock.unlock();\n      }\n\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"Writing buffer for channelID=\" + id);\n      }\n\n      checkReconnectID(reconnectID);\n\n      // The actual send must be outside the lock, or with OIO transport, the write can block if the tcp\n      // buffer is full, preventing any incoming buffers being handled and blocking failover\n      connection.getTransportConnection().write(buffer, flush, batch);\n      return true;\n   }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2903,
        "instruction": "public SPF1Record getSPFRecord(String currentDomain, SPF1Record spfRecord) throws PermErrorException, TempErrorException, NoneException, NeutralException {\n    if (spfRecord == null) return null;\n    String mechanism = ((Directive) spfRecord.getDirectives().get(spfRecord.getDirectives().size())).toString();\n    if (mechanism.equals(\"-all\") || mechanism.equals(\"?all\")) {\n        try {\n            IncludeMechanism trusted = new IncludeMechanism() {\n                /**\n                 * Set the host to use\n                 *\n                 * @param host the host to include\n                 */\n                public synchronized IncludeMechanism setHost(String host) {\n                    this.host = host;\n                    return this;\n                }\n            }.setHost(TRUSTED_FORWARDER_HOST);\n            spfRecord.getDirectives().add(spfRecord.getDirectives().size()-1, new Directive(null, trusted));\n        } catch (PermErrorException e) {\n            // will never happen\n        }\n    }\n    return spfRecord;\n}",
        "accept_response": "public SPF1Record getSPFRecord(String currentDomain, SPF1Record spfRecord) throws PermErrorException, TempErrorException, NoneException, NeutralException {\n    if (spfRecord == null) return null;\n    String mechanism = ((Directive) spfRecord.getDirectives().get(spfRecord.getDirectives().size())).toString();\n    if (mechanism.equals(\"-all\") || mechanism.equals(\"?all\")) {\n        LOGGER.debug(\"Add TrustedForwarderPolicy = include:{}\", TRUSTED_FORWARDER_HOST);\n        try {\n            IncludeMechanism trusted = new IncludeMechanism() {\n                /**\n                 * Set the host to use\n                 *\n                 * @param host the host to include\n                 */\n                public synchronized IncludeMechanism setHost(String host) {\n                    this.host = host;\n                    return this;\n                }\n            }.setHost(TRUSTED_FORWARDER_HOST);\n            spfRecord.getDirectives().add(spfRecord.getDirectives().size()-1, new Directive(null, trusted));\n        } catch (PermErrorException e) {\n            // will never happen\n        }\n    }\n    return spfRecord;\n}",
        "reject_response": "public SPF1Record getSPFRecord(String currentDomain, SPF1Record spfRecord) throws PermErrorException, TempErrorException, NoneException, NeutralException {\n    if (spfRecord == null) return null;\n    String mechanism = ((Directive) spfRecord.getDirectives().get(spfRecord.getDirectives().size())).toString();\n    if (mechanism.equals(\"-all\") || mechanism.equals(\"?all\")) {\n        log.debug(\"Add TrustedForwarderPolicy = include:\"+TRUSTED_FORWARDER_HOST);\n        try {\n            IncludeMechanism trusted = new IncludeMechanism() {\n                /**\n                 * Set the host to use\n                 *\n                 * @param host the host to include\n                 */\n                public synchronized IncludeMechanism setHost(String host) {\n                    this.host = host;\n                    return this;\n                }\n            }.setHost(TRUSTED_FORWARDER_HOST);\n            spfRecord.getDirectives().add(spfRecord.getDirectives().size()-1, new Directive(null, trusted));\n        } catch (PermErrorException e) {\n            // will never happen\n        }\n    }\n    return spfRecord;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2710,
        "instruction": "@Override\npublic boolean isFileDeletable(FileStatus fStat) {\n  try {\n    // if its a directory, then it can be deleted\n    if (fStat.isDirectory()) return true;\n\n    Path file = fStat.getPath();\n    // check to see if\n    FileStatus[] deleteStatus = FSUtils.listStatus(this.fs, file, null);\n    // if the file doesn't exist, then it can be deleted (but should never\n    // happen since deleted files shouldn't get passed in)\n    if (deleteStatus == null) return true;\n\n    // otherwise, we need to check the file's table and see its being archived\n    Path family = file.getParent();\n    Path region = family.getParent();\n    Path table = region.getParent();\n\n    String tableName = table.getName();\n    boolean ret = !archiveTracker.keepHFiles(tableName);\n    LOG.debug(\"Archiver says to [\" + (ret ? \"delete\" : \"keep\") + \"] files for table:\" + tableName);\n    return ret;\n  } catch (IOException e) {\n    return false;\n  }\n}",
        "accept_response": "@Override\npublic boolean isFileDeletable(FileStatus fStat) {\n  try {\n    // if its a directory, then it can be deleted\n    if (fStat.isDirectory()) return true;\n\n    Path file = fStat.getPath();\n    // check to see if\n    FileStatus[] deleteStatus = FSUtils.listStatus(this.fs, file, null);\n    // if the file doesn't exist, then it can be deleted (but should never\n    // happen since deleted files shouldn't get passed in)\n    if (deleteStatus == null) return true;\n\n    // otherwise, we need to check the file's table and see its being archived\n    Path family = file.getParent();\n    Path region = family.getParent();\n    Path table = region.getParent();\n\n    String tableName = table.getName();\n    boolean ret = !archiveTracker.keepHFiles(tableName);\n    LOG.debug(\"Archiver says to [\" + (ret ? \"delete\" : \"keep\") + \"] files for table:\" + tableName);\n    return ret;\n  } catch (IOException e) {\n    LOG.error(\"Failed to lookup status of:\" + file + \", keeping it just incase.\", e);\n    return false;\n  }\n}",
        "reject_response": "@Override\npublic boolean isFileDeletable(FileStatus fStat) {\n  try {\n    // if its a directory, then it can be deleted\n    if (fStat.isDirectory()) return true;\n\n    Path file = fStat.getPath();\n    // check to see if\n    FileStatus[] deleteStatus = FSUtils.listStatus(this.fs, file, null);\n    // if the file doesn't exist, then it can be deleted (but should never\n    // happen since deleted files shouldn't get passed in)\n    if (deleteStatus == null) return true;\n\n    // otherwise, we need to check the file's table and see its being archived\n    Path family = file.getParent();\n    Path region = family.getParent();\n    Path table = region.getParent();\n\n    String tableName = table.getName();\n    boolean ret = !archiveTracker.keepHFiles(tableName);\n    LOG.debug(\"Archiver says to [\" + (ret ? \"delete\" : \"keep\") + \"] files for table:\" + tableName);\n    return ret;\n  } catch (IOException e) {\n    LOG.error(\"Failed to lookup status of:\" + fStat.getPath() + \", keeping it just incase.\", e);\n    return false;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2928,
        "instruction": "private void suspendTasksAndState()  {\n\n    final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n\n    firstException.compareAndSet(null, performOnStreamTasks(new StreamTaskAction() {\n        @Override\n        public String name() {\n            return \"suspend\";\n        }\n\n        @Override\n        public void apply(final StreamTask task) {\n            task.suspend();\n        }\n    }));\n\n    for (final StandbyTask task : standbyTasks.values()) {\n        try {\n            task.suspend();\n        } catch (final RuntimeException e) {\n            firstException.compareAndSet(null, e);\n        }\n    }\n\n    // remove the changelog partitions from restore consumer\n    firstException.compareAndSet(null, unAssignChangeLogPartitions());\n\n    updateSuspendedTasks();\n\n    if (firstException.get() != null) {\n        throw new StreamsException(logPrefix + \" failed to suspend stream tasks\", firstException.get());\n    }\n}",
        "accept_response": "private void suspendTasksAndState()  {\n    log.debug(\"{} Suspending all active tasks {} and standby tasks {}\",\n        logPrefix, activeTasks.keySet(), standbyTasks.keySet());\n\n    final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n\n    firstException.compareAndSet(null, performOnStreamTasks(new StreamTaskAction() {\n        @Override\n        public String name() {\n            return \"suspend\";\n        }\n\n        @Override\n        public void apply(final StreamTask task) {\n            task.suspend();\n        }\n    }));\n\n    for (final StandbyTask task : standbyTasks.values()) {\n        try {\n            task.suspend();\n        } catch (final RuntimeException e) {\n            firstException.compareAndSet(null, e);\n        }\n    }\n\n    // remove the changelog partitions from restore consumer\n    firstException.compareAndSet(null, unAssignChangeLogPartitions());\n\n    updateSuspendedTasks();\n\n    if (firstException.get() != null) {\n        throw new StreamsException(logPrefix + \" failed to suspend stream tasks\", firstException.get());\n    }\n}",
        "reject_response": "private void suspendTasksAndState()  {\n    log.debug(\"{} suspendTasksAndState: suspending all active tasks {} and standby tasks {}\",\n\n    final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n\n    firstException.compareAndSet(null, performOnStreamTasks(new StreamTaskAction() {\n        @Override\n        public String name() {\n            return \"suspend\";\n        }\n\n        @Override\n        public void apply(final StreamTask task) {\n            task.suspend();\n        }\n    }));\n\n    for (final StandbyTask task : standbyTasks.values()) {\n        try {\n            task.suspend();\n        } catch (final RuntimeException e) {\n            firstException.compareAndSet(null, e);\n        }\n    }\n\n    // remove the changelog partitions from restore consumer\n    firstException.compareAndSet(null, unAssignChangeLogPartitions());\n\n    updateSuspendedTasks();\n\n    if (firstException.get() != null) {\n        throw new StreamsException(logPrefix + \" failed to suspend stream tasks\", firstException.get());\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2688,
        "instruction": "public static FileSplit parseFragmentMetadata(InputData inputData) {\n    try {\n        byte[] serializedLocation = inputData.getFragmentMetadata();\n        if (serializedLocation == null) {\n            throw new IllegalArgumentException(\n                    \"Missing fragment location information\");\n        }\n\n        ByteArrayInputStream bytesStream = new ByteArrayInputStream(\n                serializedLocation);\n        ObjectInputStream objectStream = new ObjectInputStream(bytesStream);\n\n        long start = objectStream.readLong();\n        long end = objectStream.readLong();\n\n        String[] hosts = (String[]) objectStream.readObject();\n\n        FileSplit fileSplit = new FileSplit(new Path(\n                inputData.getDataSource()), start, end, hosts);\n\n\n        return fileSplit;\n\n    } catch (Exception e) {\n        throw new RuntimeException(\n                \"Exception while reading expected fragment metadata\", e);\n    }\n}",
        "accept_response": "public static FileSplit parseFragmentMetadata(InputData inputData) {\n    try {\n        byte[] serializedLocation = inputData.getFragmentMetadata();\n        if (serializedLocation == null) {\n            throw new IllegalArgumentException(\n                    \"Missing fragment location information\");\n        }\n\n        ByteArrayInputStream bytesStream = new ByteArrayInputStream(\n                serializedLocation);\n        ObjectInputStream objectStream = new ObjectInputStream(bytesStream);\n\n        long start = objectStream.readLong();\n        long end = objectStream.readLong();\n\n        String[] hosts = (String[]) objectStream.readObject();\n\n        FileSplit fileSplit = new FileSplit(new Path(\n                inputData.getDataSource()), start, end, hosts);\n\n        LOG.debug(\"parsed file split: path \" + inputData.getDataSource()\n                + \", start \" + start + \", end \" + end + \", hosts \"\n                + ArrayUtils.toString(hosts));\n\n        return fileSplit;\n\n    } catch (Exception e) {\n        throw new RuntimeException(\n                \"Exception while reading expected fragment metadata\", e);\n    }\n}",
        "reject_response": "public static FileSplit parseFragmentMetadata(InputData inputData) {\n    try {\n        byte[] serializedLocation = inputData.getFragmentMetadata();\n        if (serializedLocation == null) {\n            throw new IllegalArgumentException(\n                    \"Missing fragment location information\");\n        }\n\n        ByteArrayInputStream bytesStream = new ByteArrayInputStream(\n                serializedLocation);\n        ObjectInputStream objectStream = new ObjectInputStream(bytesStream);\n\n        long start = objectStream.readLong();\n        long end = objectStream.readLong();\n\n        String[] hosts = (String[]) objectStream.readObject();\n\n        FileSplit fileSplit = new FileSplit(new Path(\n                inputData.getDataSource()), start, end, hosts);\n\n        Log.debug(\"parsed file split: path \" + inputData.getDataSource()\n\n        return fileSplit;\n\n    } catch (Exception e) {\n        throw new RuntimeException(\n                \"Exception while reading expected fragment metadata\", e);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2595,
        "instruction": "@Override\npublic LocatableInputSplit getNextInputSplit(String host) {\n\t// for a null host, we return an arbitrary split\n\tif (host == null) {\n\n\t\tsynchronized (this.unassigned) {\n\t\t\tIterator<LocatableInputSplit> iter = this.unassigned.iterator();\n\t\t\tif (iter.hasNext()) {\n\t\t\t\tLocatableInputSplit next = iter.next();\n\t\t\t\titer.remove();\n\n\t\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\t\tLOG.info(\"Assigning split to null host (random assignment).\");\n\t\t\t\t}\n\n\t\t\t\tremoteAssignments++;\n\t\t\t\treturn next;\n\t\t\t} else {\n\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\tLOG.debug(\"No more unassigned input splits remaining.\");\n\t\t\t\t}\n\t\t\t\treturn null;\n\t\t\t}\n\t\t}\n\t}\n\n\thost = host.toLowerCase(Locale.US);\n\n\t// for any non-null host, we take the list of non-null splits\n\tList<LocatableInputSplit> localSplits = this.localPerHost.get(host);\n\n\t// if we have no list for this host yet, create one\n\tif (localSplits == null) {\n\t\tlocalSplits = new ArrayList<LocatableInputSplit>(16);\n\n\t\t// lock the list, to be sure that others have to wait for that host's local list\n\t\tsynchronized (localSplits) {\n\t\t\tList<LocatableInputSplit> prior = this.localPerHost.putIfAbsent(host, localSplits);\n\n\t\t\t// if someone else beat us in the case to create this list, then we do not populate this one, but\n\t\t\t// simply work with that other list\n\t\t\tif (prior == null) {\n\t\t\t\t// we are the first, we populate\n\n\t\t\t\t// first, copy the remaining splits to release the lock on the set early\n\t\t\t\t// because that is shared among threads\n\t\t\t\tLocatableInputSplit[] remaining;\n\t\t\t\tsynchronized (this.unassigned) {\n\t\t\t\t\tremaining = (LocatableInputSplit[]) this.unassigned.toArray(new LocatableInputSplit[this.unassigned.size()]);\n\t\t\t\t}\n\n\t\t\t\tfor (LocatableInputSplit is : remaining) {\n\t\t\t\t\tif (isLocal(host, is.getHostnames())) {\n\t\t\t\t\t\tlocalSplits.add(is);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\telse {\n\t\t\t\t// someone else was faster\n\t\t\t\tlocalSplits = prior;\n\t\t\t}\n\t\t}\n\t}\n\n\n\t// at this point, we have a list of local splits (possibly empty)\n\t// we need to make sure no one else operates in the current list (that protects against\n\t// list creation races) and that the unassigned set is consistent\n\t// NOTE: we need to obtain the locks in this order, strictly!!!\n\tsynchronized (localSplits) {\n\t\tint size = localSplits.size();\n\t\tif (size > 0) {\n\t\t\tsynchronized (this.unassigned) {\n\t\t\t\tdo {\n\t\t\t\t\t--size;\n\t\t\t\t\tLocatableInputSplit split = localSplits.remove(size);\n\t\t\t\t\tif (this.unassigned.remove(split)) {\n\n\t\t\t\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\t\t\t\tLOG.info(\"Assigning local split to host \" + host);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tlocalAssignments++;\n\t\t\t\t\t\treturn split;\n\t\t\t\t\t}\n\t\t\t\t} while (size > 0);\n\t\t\t}\n\t\t}\n\t}\n\n\t// we did not find a local split, return any\n\tsynchronized (this.unassigned) {\n\t\tIterator<LocatableInputSplit> iter = this.unassigned.iterator();\n\t\tif (iter.hasNext()) {\n\t\t\tLocatableInputSplit next = iter.next();\n\t\t\titer.remove();\n\n\t\t\tremoteAssignments++;\n\t\t\treturn next;\n\t\t} else {\n\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n\t\t\t}\n\t\t\treturn null;\n\t\t}\n\t}\n}",
        "accept_response": "@Override\npublic LocatableInputSplit getNextInputSplit(String host) {\n\t// for a null host, we return an arbitrary split\n\tif (host == null) {\n\n\t\tsynchronized (this.unassigned) {\n\t\t\tIterator<LocatableInputSplit> iter = this.unassigned.iterator();\n\t\t\tif (iter.hasNext()) {\n\t\t\t\tLocatableInputSplit next = iter.next();\n\t\t\t\titer.remove();\n\n\t\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\t\tLOG.info(\"Assigning split to null host (random assignment).\");\n\t\t\t\t}\n\n\t\t\t\tremoteAssignments++;\n\t\t\t\treturn next;\n\t\t\t} else {\n\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\tLOG.debug(\"No more unassigned input splits remaining.\");\n\t\t\t\t}\n\t\t\t\treturn null;\n\t\t\t}\n\t\t}\n\t}\n\n\thost = host.toLowerCase(Locale.US);\n\n\t// for any non-null host, we take the list of non-null splits\n\tList<LocatableInputSplit> localSplits = this.localPerHost.get(host);\n\n\t// if we have no list for this host yet, create one\n\tif (localSplits == null) {\n\t\tlocalSplits = new ArrayList<LocatableInputSplit>(16);\n\n\t\t// lock the list, to be sure that others have to wait for that host's local list\n\t\tsynchronized (localSplits) {\n\t\t\tList<LocatableInputSplit> prior = this.localPerHost.putIfAbsent(host, localSplits);\n\n\t\t\t// if someone else beat us in the case to create this list, then we do not populate this one, but\n\t\t\t// simply work with that other list\n\t\t\tif (prior == null) {\n\t\t\t\t// we are the first, we populate\n\n\t\t\t\t// first, copy the remaining splits to release the lock on the set early\n\t\t\t\t// because that is shared among threads\n\t\t\t\tLocatableInputSplit[] remaining;\n\t\t\t\tsynchronized (this.unassigned) {\n\t\t\t\t\tremaining = (LocatableInputSplit[]) this.unassigned.toArray(new LocatableInputSplit[this.unassigned.size()]);\n\t\t\t\t}\n\n\t\t\t\tfor (LocatableInputSplit is : remaining) {\n\t\t\t\t\tif (isLocal(host, is.getHostnames())) {\n\t\t\t\t\t\tlocalSplits.add(is);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\telse {\n\t\t\t\t// someone else was faster\n\t\t\t\tlocalSplits = prior;\n\t\t\t}\n\t\t}\n\t}\n\n\n\t// at this point, we have a list of local splits (possibly empty)\n\t// we need to make sure no one else operates in the current list (that protects against\n\t// list creation races) and that the unassigned set is consistent\n\t// NOTE: we need to obtain the locks in this order, strictly!!!\n\tsynchronized (localSplits) {\n\t\tint size = localSplits.size();\n\t\tif (size > 0) {\n\t\t\tsynchronized (this.unassigned) {\n\t\t\t\tdo {\n\t\t\t\t\t--size;\n\t\t\t\t\tLocatableInputSplit split = localSplits.remove(size);\n\t\t\t\t\tif (this.unassigned.remove(split)) {\n\n\t\t\t\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\t\t\t\tLOG.info(\"Assigning local split to host \" + host);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tlocalAssignments++;\n\t\t\t\t\t\treturn split;\n\t\t\t\t\t}\n\t\t\t\t} while (size > 0);\n\t\t\t}\n\t\t}\n\t}\n\n\t// we did not find a local split, return any\n\tsynchronized (this.unassigned) {\n\t\tIterator<LocatableInputSplit> iter = this.unassigned.iterator();\n\t\tif (iter.hasNext()) {\n\t\t\tLocatableInputSplit next = iter.next();\n\t\t\titer.remove();\n\n\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\tLOG.info(\"Assigning remote split to host \" + host);\n\t\t\t}\n\n\t\t\tremoteAssignments++;\n\t\t\treturn next;\n\t\t} else {\n\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n\t\t\t}\n\t\t\treturn null;\n\t\t}\n\t}\n}",
        "reject_response": "@Override\npublic LocatableInputSplit getNextInputSplit(String host) {\n\t// for a null host, we return an arbitrary split\n\tif (host == null) {\n\n\t\tsynchronized (this.unassigned) {\n\t\t\tIterator<LocatableInputSplit> iter = this.unassigned.iterator();\n\t\t\tif (iter.hasNext()) {\n\t\t\t\tLocatableInputSplit next = iter.next();\n\t\t\t\titer.remove();\n\n\t\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\t\tLOG.info(\"Assigning split to null host (random assignment).\");\n\t\t\t\t}\n\n\t\t\t\tremoteAssignments++;\n\t\t\t\treturn next;\n\t\t\t} else {\n\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\tLOG.debug(\"No more unassigned input splits remaining.\");\n\t\t\t\t}\n\t\t\t\treturn null;\n\t\t\t}\n\t\t}\n\t}\n\n\thost = host.toLowerCase(Locale.US);\n\n\t// for any non-null host, we take the list of non-null splits\n\tList<LocatableInputSplit> localSplits = this.localPerHost.get(host);\n\n\t// if we have no list for this host yet, create one\n\tif (localSplits == null) {\n\t\tlocalSplits = new ArrayList<LocatableInputSplit>(16);\n\n\t\t// lock the list, to be sure that others have to wait for that host's local list\n\t\tsynchronized (localSplits) {\n\t\t\tList<LocatableInputSplit> prior = this.localPerHost.putIfAbsent(host, localSplits);\n\n\t\t\t// if someone else beat us in the case to create this list, then we do not populate this one, but\n\t\t\t// simply work with that other list\n\t\t\tif (prior == null) {\n\t\t\t\t// we are the first, we populate\n\n\t\t\t\t// first, copy the remaining splits to release the lock on the set early\n\t\t\t\t// because that is shared among threads\n\t\t\t\tLocatableInputSplit[] remaining;\n\t\t\t\tsynchronized (this.unassigned) {\n\t\t\t\t\tremaining = (LocatableInputSplit[]) this.unassigned.toArray(new LocatableInputSplit[this.unassigned.size()]);\n\t\t\t\t}\n\n\t\t\t\tfor (LocatableInputSplit is : remaining) {\n\t\t\t\t\tif (isLocal(host, is.getHostnames())) {\n\t\t\t\t\t\tlocalSplits.add(is);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\telse {\n\t\t\t\t// someone else was faster\n\t\t\t\tlocalSplits = prior;\n\t\t\t}\n\t\t}\n\t}\n\n\n\t// at this point, we have a list of local splits (possibly empty)\n\t// we need to make sure no one else operates in the current list (that protects against\n\t// list creation races) and that the unassigned set is consistent\n\t// NOTE: we need to obtain the locks in this order, strictly!!!\n\tsynchronized (localSplits) {\n\t\tint size = localSplits.size();\n\t\tif (size > 0) {\n\t\t\tsynchronized (this.unassigned) {\n\t\t\t\tdo {\n\t\t\t\t\t--size;\n\t\t\t\t\tLocatableInputSplit split = localSplits.remove(size);\n\t\t\t\t\tif (this.unassigned.remove(split)) {\n\n\t\t\t\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\t\t\t\tLOG.info(\"Assigning local split to host \" + host);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tlocalAssignments++;\n\t\t\t\t\t\treturn split;\n\t\t\t\t\t}\n\t\t\t\t} while (size > 0);\n\t\t\t}\n\t\t}\n\t}\n\n\t// we did not find a local split, return any\n\tsynchronized (this.unassigned) {\n\t\tIterator<LocatableInputSplit> iter = this.unassigned.iterator();\n\t\tif (iter.hasNext()) {\n\t\t\tLocatableInputSplit next = iter.next();\n\t\t\titer.remove();\n\n\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\tLOG.debug(\"Assigning remote split to host \" + host);\n\t\t\t}\n\n\t\t\tremoteAssignments++;\n\t\t\treturn next;\n\t\t} else {\n\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n\t\t\t}\n\t\t\treturn null;\n\t\t}\n\t}\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3002,
        "instruction": "public VXPortalUser changeEmailAddress(XXPortalUser gjUser, VXPasswordChange changeEmail) {\n\tcheckAccessForUpdate(gjUser);\n\trangerBizUtil.blockAuditorRoleUser();\n\tif (StringUtils.isEmpty(changeEmail.getEmailAddress())) {\n\t\tchangeEmail.setEmailAddress(null);\n\t}\n\n\tif (!StringUtils.isEmpty(changeEmail.getEmailAddress()) && !stringUtil.validateEmail(changeEmail.getEmailAddress())) {\n\t\tlogger.info(\"Invalid email address.\" + changeEmail);\n\t\tthrow restErrorUtil.createRESTException(\"serverMsg.userMgrInvalidEmail\",\n\t\t\t\tMessageEnums.INVALID_INPUT_DATA, changeEmail.getId(), \"emailAddress\", changeEmail.toString());\n\t}\n\n\tif (this.isFipsEnabled) {\n\t\tif (!isPasswordValid(changeEmail.getLoginId(), gjUser.getPassword(), changeEmail.getOldPassword())) {\n\t\t\tlogger.info(\"changeEmailAddress(). Invalid  password. changeEmail=\" + changeEmail);\n\t\t\tthrow restErrorUtil.createRESTException(\"serverMsg.userMgrWrongPassword\",\n\t\t\t\t\t\t\t\t\t\t\tMessageEnums.OPER_NO_PERMISSION, null, null, \"\" + changeEmail);\n\t\t}\n\t} else {\n\t\tString encryptedOldPwd = encrypt(gjUser.getLoginId(), changeEmail.getOldPassword());\n\t\tif (!stringUtil.equals(encryptedOldPwd, gjUser.getPassword())) {\n\t\t\tthrow restErrorUtil.createRESTException(\"serverMsg.userMgrWrongPassword\",\n\t\t\t\t\tMessageEnums.OPER_NO_PERMISSION, null, null, \"\" + changeEmail);\n\t\t}\n\t}\n\n\t// Normalize email. Make it lower case\n\tgjUser.setEmailAddress(stringUtil.normalizeEmail(changeEmail.getEmailAddress()));\n\n\tString saltEncodedpasswd = encrypt(gjUser.getLoginId(), changeEmail.getOldPassword());\n\tif (gjUser.getUserSource() == RangerCommonEnums.USER_APP) {\n\t\tgjUser.setPassword(saltEncodedpasswd);\n\t} else if (gjUser.getUserSource() == RangerCommonEnums.USER_EXTERNAL) {\n\t\tgjUser.setPassword(gjUser.getPassword());\n\t}\n\tdaoManager.getXXPortalUser().update(gjUser);\n\treturn mapXXPortalUserVXPortalUser(gjUser);\n}",
        "accept_response": "public VXPortalUser changeEmailAddress(XXPortalUser gjUser, VXPasswordChange changeEmail) {\n\tcheckAccessForUpdate(gjUser);\n\trangerBizUtil.blockAuditorRoleUser();\n\tif (StringUtils.isEmpty(changeEmail.getEmailAddress())) {\n\t\tchangeEmail.setEmailAddress(null);\n\t}\n\n\tif (!StringUtils.isEmpty(changeEmail.getEmailAddress()) && !stringUtil.validateEmail(changeEmail.getEmailAddress())) {\n\t\tlogger.info(\"Invalid email address.\" + changeEmail);\n\t\tthrow restErrorUtil.createRESTException(\"serverMsg.userMgrInvalidEmail\",\n\t\t\t\tMessageEnums.INVALID_INPUT_DATA, changeEmail.getId(), \"emailAddress\", changeEmail.toString());\n\t}\n\n\tif (this.isFipsEnabled) {\n\t\tif (!isPasswordValid(changeEmail.getLoginId(), gjUser.getPassword(), changeEmail.getOldPassword())) {\n\t\t\tlogger.info(\"changeEmailAddress(). Invalid  password. changeEmail=\" + changeEmail);\n\t\t\tthrow restErrorUtil.createRESTException(\"serverMsg.userMgrWrongPassword\",\n\t\t\t\t\t\t\t\t\t\t\tMessageEnums.OPER_NO_PERMISSION, null, null, \"\" + changeEmail);\n\t\t}\n\t} else {\n\t\tString encryptedOldPwd = encrypt(gjUser.getLoginId(), changeEmail.getOldPassword());\n\t\tif (!stringUtil.equals(encryptedOldPwd, gjUser.getPassword())) {\n\t\t\tlogger.info(\"changeEmailAddress(). Invalid  password. changeEmail=\" + changeEmail);\n\t\t\tthrow restErrorUtil.createRESTException(\"serverMsg.userMgrWrongPassword\",\n\t\t\t\t\tMessageEnums.OPER_NO_PERMISSION, null, null, \"\" + changeEmail);\n\t\t}\n\t}\n\n\t// Normalize email. Make it lower case\n\tgjUser.setEmailAddress(stringUtil.normalizeEmail(changeEmail.getEmailAddress()));\n\n\tString saltEncodedpasswd = encrypt(gjUser.getLoginId(), changeEmail.getOldPassword());\n\tif (gjUser.getUserSource() == RangerCommonEnums.USER_APP) {\n\t\tgjUser.setPassword(saltEncodedpasswd);\n\t} else if (gjUser.getUserSource() == RangerCommonEnums.USER_EXTERNAL) {\n\t\tgjUser.setPassword(gjUser.getPassword());\n\t}\n\tdaoManager.getXXPortalUser().update(gjUser);\n\treturn mapXXPortalUserVXPortalUser(gjUser);\n}",
        "reject_response": "public VXPortalUser changeEmailAddress(XXPortalUser gjUser, VXPasswordChange changeEmail) {\n\tcheckAccessForUpdate(gjUser);\n\trangerBizUtil.blockAuditorRoleUser();\n\tif (StringUtils.isEmpty(changeEmail.getEmailAddress())) {\n\t\tchangeEmail.setEmailAddress(null);\n\t}\n\n\tif (!StringUtils.isEmpty(changeEmail.getEmailAddress()) && !stringUtil.validateEmail(changeEmail.getEmailAddress())) {\n\t\tlogger.info(\"Invalid email address.\" + changeEmail);\n\t\tthrow restErrorUtil.createRESTException(\"serverMsg.userMgrInvalidEmail\",\n\t\t\t\tMessageEnums.INVALID_INPUT_DATA, changeEmail.getId(), \"emailAddress\", changeEmail.toString());\n\t}\n\n\tif (this.isFipsEnabled) {\n\t\tif (!isPasswordValid(changeEmail.getLoginId(), gjUser.getPassword(), changeEmail.getOldPassword())) {\n\t\t\tlogger.info(\"changeEmailAddress(). Invalid  password. changeEmail=\" + changeEmail);\n\t\t\tthrow restErrorUtil.createRESTException(\"serverMsg.userMgrWrongPassword\",\n\t\t\t\t\t\t\t\t\t\t\tMessageEnums.OPER_NO_PERMISSION, null, null, \"\" + changeEmail);\n\t\t}\n\t} else {\n\t\tString encryptedOldPwd = encrypt(gjUser.getLoginId(), changeEmail.getOldPassword());\n\t\tif (!stringUtil.equals(encryptedOldPwd, gjUser.getPassword())) {\n\t\t\tlogger.info(\"changeEmailAddress(). Invalid  password. changeEmail=\"\n\t\t\t\t\t\t\t+ changeEmail);\n\t\t\t\t\t\t\tthrow restErrorUtil.createRESTException(\n\t\t\t\t\t\t\t\t\t\t\"serverMsg.userMgrWrongPassword\",\n\t\t\t\t\t\t\t\t\t\t\tMessageEnums.OPER_NO_PERMISSION, null, null, \"\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t+ changeEmail);\n\t\t\t\t}\n\t\t} else {\n\t\t\tString encryptedOldPwd = encrypt(gjUser.getLoginId(), changeEmail.getOldPassword());\n\t\t\tif (!stringUtil.equals(encryptedOldPwd, gjUser.getPassword())) {\n\t\t\t\tlogger.info(\"changeEmailAddress(). Invalid  password. changeEmail=\"\n\t\t\t\t\t\t+ changeEmail);\n\t\t\t\tthrow restErrorUtil.createRESTException(\n\t\t\t\t\t\t\"serverMsg.userMgrWrongPassword\",\n\t\t\t\t\t\tMessageEnums.OPER_NO_PERMISSION, null, null, \"\"\n\t\t\t\t\t\t\t\t+ changeEmail);\n\t\t\t}\n\t\t\tthrow restErrorUtil.createRESTException(\"serverMsg.userMgrWrongPassword\",\n\t\t\t\t\tMessageEnums.OPER_NO_PERMISSION, null, null, \"\" + changeEmail);\n\t\t}\n\t}\n\n\t// Normalize email. Make it lower case\n\tgjUser.setEmailAddress(stringUtil.normalizeEmail(changeEmail.getEmailAddress()));\n\n\tString saltEncodedpasswd = encrypt(gjUser.getLoginId(), changeEmail.getOldPassword());\n\tif (gjUser.getUserSource() == RangerCommonEnums.USER_APP) {\n\t\tgjUser.setPassword(saltEncodedpasswd);\n\t} else if (gjUser.getUserSource() == RangerCommonEnums.USER_EXTERNAL) {\n\t\tgjUser.setPassword(gjUser.getPassword());\n\t}\n\tdaoManager.getXXPortalUser().update(gjUser);\n\treturn mapXXPortalUserVXPortalUser(gjUser);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3257,
        "instruction": "private int getHandshakeDetectionTimeoutMillis(final ZKConfig config) {\n    String propertyString = config.getProperty(x509Util.getSslHandshakeDetectionTimeoutMillisProperty());\n    int result;\n    if (propertyString == null) {\n        result = X509Util.DEFAULT_HANDSHAKE_DETECTION_TIMEOUT_MILLIS;\n    } else {\n        result = Integer.parseInt(propertyString);\n        if (result < 1) {\n            // Timeout of 0 is not allowed, since an infinite timeout can permanently lock up an\n            // accept() thread.\n            result = X509Util.DEFAULT_HANDSHAKE_DETECTION_TIMEOUT_MILLIS;\n        }\n    }\n    return result;\n}",
        "accept_response": "private int getHandshakeDetectionTimeoutMillis(final ZKConfig config) {\n    String propertyString = config.getProperty(x509Util.getSslHandshakeDetectionTimeoutMillisProperty());\n    int result;\n    if (propertyString == null) {\n        result = X509Util.DEFAULT_HANDSHAKE_DETECTION_TIMEOUT_MILLIS;\n    } else {\n        result = Integer.parseInt(propertyString);\n        if (result < 1) {\n            // Timeout of 0 is not allowed, since an infinite timeout can permanently lock up an\n            // accept() thread.\n            LOG.warn(\n                \"Invalid value for {}: {}, using the default value of {}\",\n                x509Util.getSslHandshakeDetectionTimeoutMillisProperty(),\n                result,\n                X509Util.DEFAULT_HANDSHAKE_DETECTION_TIMEOUT_MILLIS);\n            result = X509Util.DEFAULT_HANDSHAKE_DETECTION_TIMEOUT_MILLIS;\n        }\n    }\n    return result;\n}",
        "reject_response": "private int getHandshakeDetectionTimeoutMillis(final ZKConfig config) {\n    String propertyString = config.getProperty(x509Util.getSslHandshakeDetectionTimeoutMillisProperty());\n    int result;\n    if (propertyString == null) {\n        result = X509Util.DEFAULT_HANDSHAKE_DETECTION_TIMEOUT_MILLIS;\n    } else {\n        result = Integer.parseInt(propertyString);\n        if (result < 1) {\n            // Timeout of 0 is not allowed, since an infinite timeout can permanently lock up an\n            // accept() thread.\n            LOG.warn(\"Invalid value for {}: {}, using the default value of {}\", x509Util.getSslHandshakeDetectionTimeoutMillisProperty(), result, X509Util.DEFAULT_HANDSHAKE_DETECTION_TIMEOUT_MILLIS);\n            result = X509Util.DEFAULT_HANDSHAKE_DETECTION_TIMEOUT_MILLIS;\n        }\n    }\n    return result;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2911,
        "instruction": "public List<String> getRecords(DNSRequest request) throws TimeoutException {\n    try {\n        List<String> result = dnsService.getRecords(request);\n        StringBuffer logBuff = new StringBuffer();\n        logBuff.append(\"getRecords(\" + request.getHostname() + \",\" + request.getRecordType() + \") = \");\n        if (result != null) {\n            for (int i = 0; i < result.size(); i++) {\n                logBuff.append(result.get(i));\n                if (i == result.size() - 1) {\n                    logBuff.append(\"\");\n                } else {\n                    logBuff.append(\",\");\n                }\n            }\n        } else {\n            logBuff.append(\"getRecords-ret: null\");\n        }\n        LOGGER.debug(logBuff.toString());\n        return result;\n    } catch (TimeoutException e) {\n        throw e;\n    }\n}",
        "accept_response": "public List<String> getRecords(DNSRequest request) throws TimeoutException {\n    try {\n        List<String> result = dnsService.getRecords(request);\n        StringBuffer logBuff = new StringBuffer();\n        logBuff.append(\"getRecords(\" + request.getHostname() + \",\" + request.getRecordType() + \") = \");\n        if (result != null) {\n            for (int i = 0; i < result.size(); i++) {\n                logBuff.append(result.get(i));\n                if (i == result.size() - 1) {\n                    logBuff.append(\"\");\n                } else {\n                    logBuff.append(\",\");\n                }\n            }\n        } else {\n            logBuff.append(\"getRecords-ret: null\");\n        }\n        LOGGER.debug(logBuff.toString());\n        return result;\n    } catch (TimeoutException e) {\n        LOGGER.debug(\"getRecords({}) = TempErrorException[{}]\",\n            request.getHostname(), e.getMessage());\n        throw e;\n    }\n}",
        "reject_response": "public List<String> getRecords(DNSRequest request) throws TimeoutException {\n    try {\n        List<String> result = dnsService.getRecords(request);\n        StringBuffer logBuff = new StringBuffer();\n        logBuff.append(\"getRecords(\" + request.getHostname() + \",\" + request.getRecordType() + \") = \");\n        if (result != null) {\n            for (int i = 0; i < result.size(); i++) {\n                logBuff.append(result.get(i));\n                if (i == result.size() - 1) {\n                    logBuff.append(\"\");\n                } else {\n                    logBuff.append(\",\");\n                }\n            }\n        } else {\n            logBuff.append(\"getRecords-ret: null\");\n        }\n        LOGGER.debug(logBuff.toString());\n        return result;\n    } catch (TimeoutException e) {\n        logger.debug(\"getRecords(\" + request.getHostname()\n                + \") = TempErrorException[\" + e.getMessage() + \"]\");\n        throw e;\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2797,
        "instruction": "@Override\npublic void execute(\n        final HttpRequest request,\n        final AsyncEntityProducer entityProducer,\n        final AsyncExecChain.Scope scope,\n        final AsyncExecChain chain,\n        final AsyncExecCallback asyncExecCallback) throws HttpException, IOException {\n    Args.notNull(request, \"HTTP request\");\n    Args.notNull(scope, \"Scope\");\n\n    final String exchangeId = scope.exchangeId;\n    final HttpRoute route = scope.route;\n    final CancellableDependency cancellableDependency = scope.cancellableDependency;\n    final HttpClientContext clientContext = scope.clientContext;\n    final AsyncExecRuntime execRuntime = scope.execRuntime;\n    final State state = new State(route);\n\n    if (!execRuntime.isEndpointAcquired()) {\n        final Object userToken = clientContext.getUserToken();\n        cancellableDependency.setDependency(execRuntime.acquireEndpoint(\n                exchangeId, route, userToken, clientContext, new FutureCallback<AsyncExecRuntime>() {\n\n                    @Override\n                    public void completed(final AsyncExecRuntime execRuntime) {\n                        if (execRuntime.isEndpointConnected()) {\n                            try {\n                                chain.proceed(request, entityProducer, scope, asyncExecCallback);\n                            } catch (final HttpException | IOException ex) {\n                                asyncExecCallback.failed(ex);\n                            }\n                        } else {\n                            proceedToNextHop(state, request, entityProducer, scope, chain, asyncExecCallback);\n                        }\n                    }\n\n                    @Override\n                    public void failed(final Exception ex) {\n                        asyncExecCallback.failed(ex);\n                    }\n\n                    @Override\n                    public void cancelled() {\n                        asyncExecCallback.failed(new InterruptedIOException());\n                    }\n\n                }));\n    } else {\n        if (execRuntime.isEndpointConnected()) {\n            try {\n                chain.proceed(request, entityProducer, scope, asyncExecCallback);\n            } catch (final HttpException | IOException ex) {\n                asyncExecCallback.failed(ex);\n            }\n        } else {\n            proceedToNextHop(state, request, entityProducer, scope, chain, asyncExecCallback);\n        }\n    }\n\n}",
        "accept_response": "@Override\npublic void execute(\n        final HttpRequest request,\n        final AsyncEntityProducer entityProducer,\n        final AsyncExecChain.Scope scope,\n        final AsyncExecChain chain,\n        final AsyncExecCallback asyncExecCallback) throws HttpException, IOException {\n    Args.notNull(request, \"HTTP request\");\n    Args.notNull(scope, \"Scope\");\n\n    final String exchangeId = scope.exchangeId;\n    final HttpRoute route = scope.route;\n    final CancellableDependency cancellableDependency = scope.cancellableDependency;\n    final HttpClientContext clientContext = scope.clientContext;\n    final AsyncExecRuntime execRuntime = scope.execRuntime;\n    final State state = new State(route);\n\n    if (!execRuntime.isEndpointAcquired()) {\n        final Object userToken = clientContext.getUserToken();\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"{}: acquiring connection with route {}\", exchangeId, route);\n        }\n        cancellableDependency.setDependency(execRuntime.acquireEndpoint(\n                exchangeId, route, userToken, clientContext, new FutureCallback<AsyncExecRuntime>() {\n\n                    @Override\n                    public void completed(final AsyncExecRuntime execRuntime) {\n                        if (execRuntime.isEndpointConnected()) {\n                            try {\n                                chain.proceed(request, entityProducer, scope, asyncExecCallback);\n                            } catch (final HttpException | IOException ex) {\n                                asyncExecCallback.failed(ex);\n                            }\n                        } else {\n                            proceedToNextHop(state, request, entityProducer, scope, chain, asyncExecCallback);\n                        }\n                    }\n\n                    @Override\n                    public void failed(final Exception ex) {\n                        asyncExecCallback.failed(ex);\n                    }\n\n                    @Override\n                    public void cancelled() {\n                        asyncExecCallback.failed(new InterruptedIOException());\n                    }\n\n                }));\n    } else {\n        if (execRuntime.isEndpointConnected()) {\n            try {\n                chain.proceed(request, entityProducer, scope, asyncExecCallback);\n            } catch (final HttpException | IOException ex) {\n                asyncExecCallback.failed(ex);\n            }\n        } else {\n            proceedToNextHop(state, request, entityProducer, scope, chain, asyncExecCallback);\n        }\n    }\n\n}",
        "reject_response": "@Override\npublic void execute(\n        final HttpRequest request,\n        final AsyncEntityProducer entityProducer,\n        final AsyncExecChain.Scope scope,\n        final AsyncExecChain chain,\n        final AsyncExecCallback asyncExecCallback) throws HttpException, IOException {\n    Args.notNull(request, \"HTTP request\");\n    Args.notNull(scope, \"Scope\");\n\n    final String exchangeId = scope.exchangeId;\n    final HttpRoute route = scope.route;\n    final CancellableDependency cancellableDependency = scope.cancellableDependency;\n    final HttpClientContext clientContext = scope.clientContext;\n    final AsyncExecRuntime execRuntime = scope.execRuntime;\n    final State state = new State(route);\n\n    if (!execRuntime.isEndpointAcquired()) {\n        final Object userToken = clientContext.getUserToken();\n        if (LOG.isDebugEnabled()) {\n        if (log.isDebugEnabled()) {\n            log.debug(\"{}: acquiring connection with route {}\", exchangeId, route);\n        }\n        cancellableDependency.setDependency(execRuntime.acquireEndpoint(\n                exchangeId, route, userToken, clientContext, new FutureCallback<AsyncExecRuntime>() {\n\n                    @Override\n                    public void completed(final AsyncExecRuntime execRuntime) {\n                        if (execRuntime.isEndpointConnected()) {\n                            try {\n                                chain.proceed(request, entityProducer, scope, asyncExecCallback);\n                            } catch (final HttpException | IOException ex) {\n                                asyncExecCallback.failed(ex);\n                            }\n                        } else {\n                            proceedToNextHop(state, request, entityProducer, scope, chain, asyncExecCallback);\n                        }\n                    }\n\n                    @Override\n                    public void failed(final Exception ex) {\n                        asyncExecCallback.failed(ex);\n                    }\n\n                    @Override\n                    public void cancelled() {\n                        asyncExecCallback.failed(new InterruptedIOException());\n                    }\n\n                }));\n    } else {\n        if (execRuntime.isEndpointConnected()) {\n            try {\n                chain.proceed(request, entityProducer, scope, asyncExecCallback);\n            } catch (final HttpException | IOException ex) {\n                asyncExecCallback.failed(ex);\n            }\n        } else {\n            proceedToNextHop(state, request, entityProducer, scope, chain, asyncExecCallback);\n        }\n    }\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2734,
        "instruction": "private RpcSaslProto buildSaslResponse(SaslState state, byte[] replyToken) {\n  RpcSaslProto.Builder response = RpcSaslProto.newBuilder();\n  response.setState(state);\n  if (replyToken != null) {\n    response.setToken(ByteString.copyFrom(replyToken));\n  }\n  return response.build();\n}",
        "accept_response": "private RpcSaslProto buildSaslResponse(SaslState state, byte[] replyToken) {\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"Will send {} token of size {} from saslServer.\", state,\n        ((replyToken != null) ? replyToken.length : null));\n  }\n  RpcSaslProto.Builder response = RpcSaslProto.newBuilder();\n  response.setState(state);\n  if (replyToken != null) {\n    response.setToken(ByteString.copyFrom(replyToken));\n  }\n  return response.build();\n}",
        "reject_response": "private RpcSaslProto buildSaslResponse(SaslState state, byte[] replyToken) {\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"Will send \" + state + \" token of size \"\n        + ((replyToken != null) ? replyToken.length : null)\n        + \" from saslServer.\");\n  }\n  RpcSaslProto.Builder response = RpcSaslProto.newBuilder();\n  response.setState(state);\n  if (replyToken != null) {\n    response.setToken(ByteString.copyFrom(replyToken));\n  }\n  return response.build();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2882,
        "instruction": "public static FileRef get(int fileId)\n{\n    FileRef f = id2name.get(fileId) ;\n    if ( f == null )\n    {\n        throw new TDBException(\"No FileRef registered for id: \"+fileId) ;\n    }\n    return f ;\n}",
        "accept_response": "public static FileRef get(int fileId)\n{\n    FileRef f = id2name.get(fileId) ;\n    if ( f == null )\n    {\n        Log.error(FileRef.class, \"No FileRef registered for id: \"+fileId) ;\n        throw new TDBException(\"No FileRef registered for id: \"+fileId) ;\n    }\n    return f ;\n}",
        "reject_response": "public static FileRef get(int fileId)\n{\n    FileRef f = id2name.get(fileId) ;\n    if ( f == null )\n    {\n        Log.fatal(FileRef.class, \"No FileRef registered for id: \"+fileId) ;\n        throw new TDBException(\"No FileRef registered for id: \"+fileId) ;\n    }\n    return f ;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3124,
        "instruction": "void startServices() {\n  try {\n    Throwable firstError = null;\n    List<ServiceThread> threads = new ArrayList<ServiceThread>();\n\n    for(ServiceWithDependency sd : services.values()) {\n      // start the service. If this fails that service\n      // will be stopped and an exception raised\n      ServiceThread st = new ServiceThread(sd, services);\n      threads.add(st);\n    }\n\n    for(ServiceThread st : threads) {\n      st.start();\n    }\n    for(ServiceThread st : threads) {\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"Waiting for service thread to join for \" + st.getName());\n      }\n      st.join();\n      if(st.error != null && firstError == null) {\n        firstError = st.error;\n      }\n    }\n\n    if(firstError != null) {\n      throw ServiceStateException.convert(firstError);\n    }\n    LOG.debug(\"End parallel start\");\n  } catch (InterruptedException e) {\n    e.printStackTrace();\n  }\n}",
        "accept_response": "void startServices() {\n  try {\n    Throwable firstError = null;\n    List<ServiceThread> threads = new ArrayList<ServiceThread>();\n    LOG.debug(\"Begin parallel start\");\n\n    for(ServiceWithDependency sd : services.values()) {\n      // start the service. If this fails that service\n      // will be stopped and an exception raised\n      ServiceThread st = new ServiceThread(sd, services);\n      threads.add(st);\n    }\n\n    for(ServiceThread st : threads) {\n      st.start();\n    }\n    for(ServiceThread st : threads) {\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"Waiting for service thread to join for \" + st.getName());\n      }\n      st.join();\n      if(st.error != null && firstError == null) {\n        firstError = st.error;\n      }\n    }\n\n    if(firstError != null) {\n      throw ServiceStateException.convert(firstError);\n    }\n    LOG.debug(\"End parallel start\");\n  } catch (InterruptedException e) {\n    e.printStackTrace();\n  }\n}",
        "reject_response": "void startServices() {\n  try {\n    Throwable firstError = null;\n    List<ServiceThread> threads = new ArrayList<ServiceThread>();\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"Begin parallel start\");\n    }\n\n    for(ServiceWithDependency sd : services.values()) {\n      // start the service. If this fails that service\n      // will be stopped and an exception raised\n      ServiceThread st = new ServiceThread(sd, services);\n      threads.add(st);\n    }\n\n    for(ServiceThread st : threads) {\n      st.start();\n    }\n    for(ServiceThread st : threads) {\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"Waiting for service thread to join for \" + st.getName());\n      }\n      st.join();\n      if(st.error != null && firstError == null) {\n        firstError = st.error;\n      }\n    }\n\n    if(firstError != null) {\n      throw ServiceStateException.convert(firstError);\n    }\n    LOG.debug(\"End parallel start\");\n  } catch (InterruptedException e) {\n    e.printStackTrace();\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3204,
        "instruction": "T runWithRetries() throws IOException, InterruptedException {\n  int retry = 0;\n  while (true) {\n    try {\n      return run();\n    } catch (IOException e) {\n      LOG.info(\"Exception while executing a FS operation.\", e);\n      if (++retry > fsNumRetries) {\n        LOG.info(\"Maxed out FS retries. Giving up!\");\n        throw e;\n      }\n      Thread.sleep(fsRetryInterval);\n    }\n  }\n}",
        "accept_response": "T runWithRetries() throws IOException, InterruptedException {\n  int retry = 0;\n  while (true) {\n    try {\n      return run();\n    } catch (IOException e) {\n      LOG.info(\"Exception while executing a FS operation.\", e);\n      if (++retry > fsNumRetries) {\n        LOG.info(\"Maxed out FS retries. Giving up!\");\n        throw e;\n      }\n      LOG.info(\"Will retry operation on FS. Retry no. {}\" +\n          \" after sleeping for {} seconds\", retry, fsRetryInterval);\n      Thread.sleep(fsRetryInterval);\n    }\n  }\n}",
        "reject_response": "T runWithRetries() throws IOException, InterruptedException {\n  int retry = 0;\n  while (true) {\n    try {\n      return run();\n    } catch (IOException e) {\n      LOG.info(\"Exception while executing a FS operation.\", e);\n      if (++retry > fsNumRetries) {\n        LOG.info(\"Maxed out FS retries. Giving up!\");\n        throw e;\n      }\n      LOG.info(\"Will retry operation on FS. Retry no. \" + retry +\n          \" after sleeping for \" + fsRetryInterval + \" seconds\");\n      Thread.sleep(fsRetryInterval);\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3023,
        "instruction": "public void executeRequestWhenWakeup(final Channel channel, final RemotingCommand request) throws RemotingCommandException {\n    Runnable run = new Runnable() {\n        @Override\n        public void run() {\n            try {\n                final RemotingCommand response = PullMessageProcessor.this.processRequest(channel, request, false);\n\n                if (response != null) {\n                    response.setOpaque(request.getOpaque());\n                    response.markResponseType();\n                    try {\n                        channel.writeAndFlush(response).addListener(new ChannelFutureListener() {\n                            @Override\n                            public void operationComplete(ChannelFuture future) throws Exception {\n                                if (!future.isSuccess()) {\n                                    log.error(request.toString());\n                                    log.error(response.toString());\n                                }\n                            }\n                        });\n                    } catch (Throwable e) {\n                        log.error(\"processRequestWrapper process request over, but response failed\", e);\n                        log.error(request.toString());\n                        log.error(response.toString());\n                    }\n                }\n            } catch (RemotingCommandException e1) {\n                log.error(\"excuteRequestWhenWakeup run\", e1);\n            }\n        }\n    };\n    this.brokerController.getPullMessageExecutor().submit(new RequestTask(run, channel, request));\n}",
        "accept_response": "public void executeRequestWhenWakeup(final Channel channel, final RemotingCommand request) throws RemotingCommandException {\n    Runnable run = new Runnable() {\n        @Override\n        public void run() {\n            try {\n                final RemotingCommand response = PullMessageProcessor.this.processRequest(channel, request, false);\n\n                if (response != null) {\n                    response.setOpaque(request.getOpaque());\n                    response.markResponseType();\n                    try {\n                        channel.writeAndFlush(response).addListener(new ChannelFutureListener() {\n                            @Override\n                            public void operationComplete(ChannelFuture future) throws Exception {\n                                if (!future.isSuccess()) {\n                                    log.error(\"processRequestWrapper response to {} failed\",\n                                        future.channel().remoteAddress(), future.cause());\n                                    log.error(request.toString());\n                                    log.error(response.toString());\n                                }\n                            }\n                        });\n                    } catch (Throwable e) {\n                        log.error(\"processRequestWrapper process request over, but response failed\", e);\n                        log.error(request.toString());\n                        log.error(response.toString());\n                    }\n                }\n            } catch (RemotingCommandException e1) {\n                log.error(\"excuteRequestWhenWakeup run\", e1);\n            }\n        }\n    };\n    this.brokerController.getPullMessageExecutor().submit(new RequestTask(run, channel, request));\n}",
        "reject_response": "public void executeRequestWhenWakeup(final Channel channel, final RemotingCommand request) throws RemotingCommandException {\n    Runnable run = new Runnable() {\n        @Override\n        public void run() {\n            try {\n                final RemotingCommand response = PullMessageProcessor.this.processRequest(channel, request, false);\n\n                if (response != null) {\n                    response.setOpaque(request.getOpaque());\n                    response.markResponseType();\n                    try {\n                        channel.writeAndFlush(response).addListener(new ChannelFutureListener() {\n                            @Override\n                            public void operationComplete(ChannelFuture future) throws Exception {\n                                if (!future.isSuccess()) {\n                                    LOG.error(\"ProcessRequestWrapper response to {} failed\", future.channel().remoteAddress(), future.cause());\n                                    LOG.error(request.toString());\n                                    LOG.error(response.toString());\n                                    log.error(request.toString());\n                                    log.error(response.toString());\n                                }\n                            }\n                        });\n                    } catch (Throwable e) {\n                        log.error(\"processRequestWrapper process request over, but response failed\", e);\n                        log.error(request.toString());\n                        log.error(response.toString());\n                    }\n                }\n            } catch (RemotingCommandException e1) {\n                log.error(\"excuteRequestWhenWakeup run\", e1);\n            }\n        }\n    };\n    this.brokerController.getPullMessageExecutor().submit(new RequestTask(run, channel, request));\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2623,
        "instruction": "private void initializeSSL() {\n\n\n  this.sysPulseUseSSLLocator = Boolean.valueOf(\n      pulseProperties.getProperty(PulseConstants.SYSTEM_PROPERTY_PULSE_USESSL_LOCATOR, \"false\"));\n\n  this.sysPulseUseSSLManager = Boolean.valueOf(\n      pulseProperties.getProperty(PulseConstants.SYSTEM_PROPERTY_PULSE_USESSL_MANAGER, \"false\"));\n\n\n  if ((sysPulseUseSSLLocator || sysPulseUseSSLManager)) {\n    Properties sslProperties = new Properties();\n    if (!pulseSecurityProperties.isEmpty()) {\n      Set entrySet = pulseSecurityProperties.entrySet();\n      for (Iterator it = entrySet.iterator(); it.hasNext();) {\n        Entry<String, String> entry = (Entry<String, String>) it.next();\n        String key = entry.getKey();\n        if (key.startsWith(\"javax.net.ssl.\")) {\n\n          String val = entry.getValue();\n          System.setProperty(key, val);\n          sslProperties.setProperty(key, val);\n        }\n      }\n    }\n    if (sslProperties.isEmpty()) {\n      logger.warn(resourceBundle.getString(\"LOG_MSG_SSL_NOT_SET\"));\n    }\n  }\n\n}",
        "accept_response": "private void initializeSSL() {\n  logger.info(resourceBundle.getString(\"LOG_MSG_GET_SSL_DETAILS\"));\n\n\n  this.sysPulseUseSSLLocator = Boolean.valueOf(\n      pulseProperties.getProperty(PulseConstants.SYSTEM_PROPERTY_PULSE_USESSL_LOCATOR, \"false\"));\n\n  this.sysPulseUseSSLManager = Boolean.valueOf(\n      pulseProperties.getProperty(PulseConstants.SYSTEM_PROPERTY_PULSE_USESSL_MANAGER, \"false\"));\n\n\n  if ((sysPulseUseSSLLocator || sysPulseUseSSLManager)) {\n    Properties sslProperties = new Properties();\n    if (!pulseSecurityProperties.isEmpty()) {\n      Set entrySet = pulseSecurityProperties.entrySet();\n      for (Iterator it = entrySet.iterator(); it.hasNext();) {\n        Entry<String, String> entry = (Entry<String, String>) it.next();\n        String key = entry.getKey();\n        if (key.startsWith(\"javax.net.ssl.\")) {\n\n          String val = entry.getValue();\n          System.setProperty(key, val);\n          sslProperties.setProperty(key, val);\n        }\n      }\n    }\n    if (sslProperties.isEmpty()) {\n      logger.warn(resourceBundle.getString(\"LOG_MSG_SSL_NOT_SET\"));\n    }\n  }\n\n}",
        "reject_response": "private void initializeSSL() {\n  if (LOGGER.infoEnabled()) {\n    LOGGER.info(resourceBundle.getString(\"LOG_MSG_GET_SSL_DETAILS\"));\n  }\n\n\n  this.sysPulseUseSSLLocator = Boolean.valueOf(\n      pulseProperties.getProperty(PulseConstants.SYSTEM_PROPERTY_PULSE_USESSL_LOCATOR, \"false\"));\n\n  this.sysPulseUseSSLManager = Boolean.valueOf(\n      pulseProperties.getProperty(PulseConstants.SYSTEM_PROPERTY_PULSE_USESSL_MANAGER, \"false\"));\n\n\n  if ((sysPulseUseSSLLocator || sysPulseUseSSLManager)) {\n    Properties sslProperties = new Properties();\n    if (!pulseSecurityProperties.isEmpty()) {\n      Set entrySet = pulseSecurityProperties.entrySet();\n      for (Iterator it = entrySet.iterator(); it.hasNext();) {\n        Entry<String, String> entry = (Entry<String, String>) it.next();\n        String key = entry.getKey();\n        if (key.startsWith(\"javax.net.ssl.\")) {\n\n          String val = entry.getValue();\n          System.setProperty(key, val);\n          sslProperties.setProperty(key, val);\n        }\n      }\n    }\n    if (sslProperties.isEmpty()) {\n      logger.warn(resourceBundle.getString(\"LOG_MSG_SSL_NOT_SET\"));\n    }\n  }\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2917,
        "instruction": "private boolean stillRunning() {\n    if (!running.get()) {\n        return false;\n    }\n\n    return true;\n}",
        "accept_response": "private boolean stillRunning() {\n    if (!running.get()) {\n        log.debug(\"stream-thread [{}] Shutting down at user request\", this.getName());\n        return false;\n    }\n\n    return true;\n}",
        "reject_response": "private boolean stillRunning() {\n    if (!running.get()) {\n        log.debug(\"Shutting down at user request.\");\n        return false;\n    }\n\n    return true;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2643,
        "instruction": "@Override\npublic ObjectNode executeQuery(String queryText, String members, int limit) {\n\n  ObjectNode queryResult = mapper.createObjectNode();\n\n  if (this.mbs != null && this.systemMBeans != null) {\n    Object opParams[] = {queryText, members, limit};\n    for (ObjectName sysMBean : this.systemMBeans) {\n      try {\n        String resultString = (String) (this.mbs.invoke(sysMBean,\n            PulseConstants.MBEAN_OPERATION_QUERYDATABROWSER, opParams, this.opSignature));\n\n        // Convert result into JSON\n        queryResult = (ObjectNode) mapper.readTree(resultString);\n\n      } catch (Exception e) {\n        // Send error into result\n        queryResult.put(\"error\", e.getMessage());\n      }\n    }\n  }\n\n  return queryResult;\n}",
        "accept_response": "@Override\npublic ObjectNode executeQuery(String queryText, String members, int limit) {\n\n  ObjectNode queryResult = mapper.createObjectNode();\n\n  if (this.mbs != null && this.systemMBeans != null) {\n    Object opParams[] = {queryText, members, limit};\n    for (ObjectName sysMBean : this.systemMBeans) {\n      try {\n        String resultString = (String) (this.mbs.invoke(sysMBean,\n            PulseConstants.MBEAN_OPERATION_QUERYDATABROWSER, opParams, this.opSignature));\n\n        // Convert result into JSON\n        queryResult = (ObjectNode) mapper.readTree(resultString);\n\n      } catch (Exception e) {\n        // Send error into result\n        queryResult.put(\"error\", e.getMessage());\n        logger.debug(e);\n      }\n    }\n  }\n\n  return queryResult;\n}",
        "reject_response": "@Override\npublic ObjectNode executeQuery(String queryText, String members, int limit) {\n\n  ObjectNode queryResult = mapper.createObjectNode();\n\n  if (this.mbs != null && this.systemMBeans != null) {\n    Object opParams[] = {queryText, members, limit};\n    for (ObjectName sysMBean : this.systemMBeans) {\n      try {\n        String resultString = (String) (this.mbs.invoke(sysMBean,\n            PulseConstants.MBEAN_OPERATION_QUERYDATABROWSER, opParams, this.opSignature));\n\n        // Convert result into JSON\n        queryResult = (ObjectNode) mapper.readTree(resultString);\n\n      } catch (Exception e) {\n        // Send error into result\n        queryResult.put(\"error\", e.getMessage());\n        if (LOGGER.fineEnabled()) {\n          LOGGER.fine(e.getMessage());\n        }\n      }\n    }\n  }\n\n  return queryResult;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3148,
        "instruction": "protected void handleSummaryEvent(TezDAGID dagID,\n    HistoryEventType eventType,\n    SummaryEvent summaryEvent) throws IOException {\n\n  if (summaryStream == null) {\n    Path summaryPath = TezCommonUtils.getSummaryRecoveryPath(recoveryPath);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"AppId :\" + appContext.getApplicationID() + \" summaryPath \" + summaryPath);\n    }\n    try {\n      summaryStream = recoveryDirFS.create(summaryPath, false, bufferSize);\n    } catch (IOException e) {\n      LOG.error(\"Error handling summary event, eventType=\" + eventType, e);\n      createFatalErrorFlagDir();\n      return;\n    }\n  }\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"Writing recovery event to summary stream\"\n        + \", dagId=\" + dagID\n        + \", eventType=\" + eventType);\n  }\n  summaryEvent.toSummaryProtoStream(summaryStream);\n  summaryStream.hflush();\n}",
        "accept_response": "protected void handleSummaryEvent(TezDAGID dagID,\n    HistoryEventType eventType,\n    SummaryEvent summaryEvent) throws IOException {\n    LOG.debug(\"Handling summary event, dagID={}, eventType={}\", dagID, eventType);\n\n  if (summaryStream == null) {\n    Path summaryPath = TezCommonUtils.getSummaryRecoveryPath(recoveryPath);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"AppId :\" + appContext.getApplicationID() + \" summaryPath \" + summaryPath);\n    }\n    try {\n      summaryStream = recoveryDirFS.create(summaryPath, false, bufferSize);\n    } catch (IOException e) {\n      LOG.error(\"Error handling summary event, eventType=\" + eventType, e);\n      createFatalErrorFlagDir();\n      return;\n    }\n  }\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"Writing recovery event to summary stream\"\n        + \", dagId=\" + dagID\n        + \", eventType=\" + eventType);\n  }\n  summaryEvent.toSummaryProtoStream(summaryStream);\n  summaryStream.hflush();\n}",
        "reject_response": "protected void handleSummaryEvent(TezDAGID dagID,\n    HistoryEventType eventType,\n    SummaryEvent summaryEvent) throws IOException {\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"Handling summary event\"\n        + \", dagID=\" + dagID\n        + \", eventType=\" + eventType);\n  }\n\n  if (summaryStream == null) {\n    Path summaryPath = TezCommonUtils.getSummaryRecoveryPath(recoveryPath);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"AppId :\" + appContext.getApplicationID() + \" summaryPath \" + summaryPath);\n    }\n    try {\n      summaryStream = recoveryDirFS.create(summaryPath, false, bufferSize);\n    } catch (IOException e) {\n      LOG.error(\"Error handling summary event, eventType=\" + eventType, e);\n      createFatalErrorFlagDir();\n      return;\n    }\n  }\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"Writing recovery event to summary stream\"\n        + \", dagId=\" + dagID\n        + \", eventType=\" + eventType);\n  }\n  summaryEvent.toSummaryProtoStream(summaryStream);\n  summaryStream.hflush();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3105,
        "instruction": "public void initialize(MessageType requestedSchema, MessageType fileSchema,\n                       Map<String, String> extraMetadata, Map<String, String> readSupportMetadata,\n                       Path file, List<BlockMetaData> blocks, Configuration configuration)\n    throws IOException {\n  this.requestedSchema = requestedSchema;\n  this.fileSchema = fileSchema;\n  this.file = file;\n  this.columnCount = this.requestedSchema.getPaths().size();\n  this.recordConverter = readSupport.prepareForRead(\n      configuration, extraMetadata, fileSchema,\n      new ReadSupport.ReadContext(requestedSchema, readSupportMetadata));\n\n  List<ColumnDescriptor> columns = requestedSchema.getColumns();\n  reader = new ParquetFileReader(configuration, file, blocks, columns);\n  for (BlockMetaData block : blocks) {\n    total += block.getRowCount();\n  }\n}",
        "accept_response": "public void initialize(MessageType requestedSchema, MessageType fileSchema,\n                       Map<String, String> extraMetadata, Map<String, String> readSupportMetadata,\n                       Path file, List<BlockMetaData> blocks, Configuration configuration)\n    throws IOException {\n  this.requestedSchema = requestedSchema;\n  this.fileSchema = fileSchema;\n  this.file = file;\n  this.columnCount = this.requestedSchema.getPaths().size();\n  this.recordConverter = readSupport.prepareForRead(\n      configuration, extraMetadata, fileSchema,\n      new ReadSupport.ReadContext(requestedSchema, readSupportMetadata));\n\n  List<ColumnDescriptor> columns = requestedSchema.getColumns();\n  reader = new ParquetFileReader(configuration, file, blocks, columns);\n  for (BlockMetaData block : blocks) {\n    total += block.getRowCount();\n  }\n  if (DEBUG) LOG.debug(\"RecordReader initialized will read a total of \" + total + \" records.\");\n}",
        "reject_response": "public void initialize(MessageType requestedSchema, MessageType fileSchema,\n                       Map<String, String> extraMetadata, Map<String, String> readSupportMetadata,\n                       Path file, List<BlockMetaData> blocks, Configuration configuration)\n    throws IOException {\n  this.requestedSchema = requestedSchema;\n  this.fileSchema = fileSchema;\n  this.file = file;\n  this.columnCount = this.requestedSchema.getPaths().size();\n  this.recordConverter = readSupport.prepareForRead(\n      configuration, extraMetadata, fileSchema,\n      new ReadSupport.ReadContext(requestedSchema, readSupportMetadata));\n\n  List<ColumnDescriptor> columns = requestedSchema.getColumns();\n  reader = new ParquetFileReader(configuration, file, blocks, columns);\n  for (BlockMetaData block : blocks) {\n    total += block.getRowCount();\n  }\n  LOG.info(\"RecordReader initialized will read a total of \" + total + \" records.\");\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3147,
        "instruction": "private void doAssignAll() {\n  // The allocatedContainers queue should not be modified in the middle of an iteration over it.\n  // Synchronizing here on TaskScheduler.this to prevent this from happening.\n  // The call to assignAll from within this method should NOT add any\n  // elements back to the allocatedContainers list. Since they're all\n  // delayed elements, de-allocation should not happen either - leaving the\n  // list of delayed containers intact, except for the contaienrs which end\n  // up getting assigned.\n  if (delayedContainers.isEmpty()) {\n    return;\n  }\n\n  Map<CookieContainerRequest, Container> assignedContainers;\n  synchronized(YarnTaskSchedulerService.this) {\n    // honor reuse-locality flags (container not timed out yet), Don't queue\n    // (already in queue), don't release (release happens when containers\n    // time-out)\n    Iterator<HeldContainer> iter = delayedContainers.iterator();\n    while(iter.hasNext()) {\n      HeldContainer delayedContainer = iter.next();\n      if (!heldContainers.containsKey(delayedContainer.getContainer().getId())) {\n        // this container is no longer held by us\n        // non standard scenario\n        LOG.info(\"AssignAll - Skipping delayed container as container is no longer\"\n              + \" running, containerId=\"\n              + delayedContainer.getContainer().getId());\n        iter.remove();\n      }\n    }\n    assignedContainers = tryAssignReUsedContainers(\n      new ContainerIterable(delayedContainers));\n  }\n  // Inform app\n  informAppAboutAssignments(assignedContainers);\n}",
        "accept_response": "private void doAssignAll() {\n  // The allocatedContainers queue should not be modified in the middle of an iteration over it.\n  // Synchronizing here on TaskScheduler.this to prevent this from happening.\n  // The call to assignAll from within this method should NOT add any\n  // elements back to the allocatedContainers list. Since they're all\n  // delayed elements, de-allocation should not happen either - leaving the\n  // list of delayed containers intact, except for the contaienrs which end\n  // up getting assigned.\n  if (delayedContainers.isEmpty()) {\n    return;\n  }\n\n  Map<CookieContainerRequest, Container> assignedContainers;\n  synchronized(YarnTaskSchedulerService.this) {\n    // honor reuse-locality flags (container not timed out yet), Don't queue\n    // (already in queue), don't release (release happens when containers\n    // time-out)\n    LOG.debug(\"Trying to assign all delayed containers to newly received tasks\");\n    Iterator<HeldContainer> iter = delayedContainers.iterator();\n    while(iter.hasNext()) {\n      HeldContainer delayedContainer = iter.next();\n      if (!heldContainers.containsKey(delayedContainer.getContainer().getId())) {\n        // this container is no longer held by us\n        // non standard scenario\n        LOG.info(\"AssignAll - Skipping delayed container as container is no longer\"\n              + \" running, containerId=\"\n              + delayedContainer.getContainer().getId());\n        iter.remove();\n      }\n    }\n    assignedContainers = tryAssignReUsedContainers(\n      new ContainerIterable(delayedContainers));\n  }\n  // Inform app\n  informAppAboutAssignments(assignedContainers);\n}",
        "reject_response": "private void doAssignAll() {\n  // The allocatedContainers queue should not be modified in the middle of an iteration over it.\n  // Synchronizing here on TaskScheduler.this to prevent this from happening.\n  // The call to assignAll from within this method should NOT add any\n  // elements back to the allocatedContainers list. Since they're all\n  // delayed elements, de-allocation should not happen either - leaving the\n  // list of delayed containers intact, except for the contaienrs which end\n  // up getting assigned.\n  if (delayedContainers.isEmpty()) {\n    return;\n  }\n\n  Map<CookieContainerRequest, Container> assignedContainers;\n  synchronized(YarnTaskSchedulerService.this) {\n    // honor reuse-locality flags (container not timed out yet), Don't queue\n    // (already in queue), don't release (release happens when containers\n    // time-out)\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Trying to assign all delayed containers to newly received\"\n        + \" tasks\");\n    }\n    Iterator<HeldContainer> iter = delayedContainers.iterator();\n    while(iter.hasNext()) {\n      HeldContainer delayedContainer = iter.next();\n      if (!heldContainers.containsKey(delayedContainer.getContainer().getId())) {\n        // this container is no longer held by us\n        // non standard scenario\n        LOG.info(\"AssignAll - Skipping delayed container as container is no longer\"\n              + \" running, containerId=\"\n              + delayedContainer.getContainer().getId());\n        iter.remove();\n      }\n    }\n    assignedContainers = tryAssignReUsedContainers(\n      new ContainerIterable(delayedContainers));\n  }\n  // Inform app\n  informAppAboutAssignments(assignedContainers);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2791,
        "instruction": "public void revalidateCacheEntry(\n        final String cacheKey,\n        final RevalidationCall call) {\n    scheduleRevalidation(cacheKey, new Runnable() {\n\n                    @Override\n                    public void run() {\n                        try (ClassicHttpResponse httpResponse = call.execute()) {\n                            if (httpResponse.getCode() < HttpStatus.SC_SERVER_ERROR && !isStale(httpResponse)) {\n                                jobSuccessful(cacheKey);\n                            } else {\n                                jobFailed(cacheKey);\n                            }\n                        } catch (final IOException ex) {\n                            jobFailed(cacheKey);\n                        } catch (final HttpException ex) {\n                            jobFailed(cacheKey);\n                            LOG.error(\"HTTP protocol exception during asynchronous revalidation\", ex);\n                        } catch (final RuntimeException ex) {\n                            jobFailed(cacheKey);\n                            LOG.error(\"Unexpected runtime exception thrown during asynchronous revalidation\", ex);\n                        }\n\n                    }\n\n                });\n}",
        "accept_response": "public void revalidateCacheEntry(\n        final String cacheKey,\n        final RevalidationCall call) {\n    scheduleRevalidation(cacheKey, new Runnable() {\n\n                    @Override\n                    public void run() {\n                        try (ClassicHttpResponse httpResponse = call.execute()) {\n                            if (httpResponse.getCode() < HttpStatus.SC_SERVER_ERROR && !isStale(httpResponse)) {\n                                jobSuccessful(cacheKey);\n                            } else {\n                                jobFailed(cacheKey);\n                            }\n                        } catch (final IOException ex) {\n                            jobFailed(cacheKey);\n                            LOG.debug(\"Asynchronous revalidation failed due to I/O error\", ex);\n                        } catch (final HttpException ex) {\n                            jobFailed(cacheKey);\n                            LOG.error(\"HTTP protocol exception during asynchronous revalidation\", ex);\n                        } catch (final RuntimeException ex) {\n                            jobFailed(cacheKey);\n                            LOG.error(\"Unexpected runtime exception thrown during asynchronous revalidation\", ex);\n                        }\n\n                    }\n\n                });\n}",
        "reject_response": "public void revalidateCacheEntry(\n        final String cacheKey,\n        final RevalidationCall call) {\n    scheduleRevalidation(cacheKey, new Runnable() {\n\n                    @Override\n                    public void run() {\n                        try (ClassicHttpResponse httpResponse = call.execute()) {\n                            if (httpResponse.getCode() < HttpStatus.SC_SERVER_ERROR && !isStale(httpResponse)) {\n                                jobSuccessful(cacheKey);\n                            } else {\n                                jobFailed(cacheKey);\n                            }\n                        } catch (final IOException ex) {\n                            jobFailed(cacheKey);\n                            log.debug(\"Asynchronous revalidation failed due to I/O error\", ex);\n                        } catch (final HttpException ex) {\n                            jobFailed(cacheKey);\n                            LOG.error(\"HTTP protocol exception during asynchronous revalidation\", ex);\n                        } catch (final RuntimeException ex) {\n                            jobFailed(cacheKey);\n                            LOG.error(\"Unexpected runtime exception thrown during asynchronous revalidation\", ex);\n                        }\n\n                    }\n\n                });\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3112,
        "instruction": "@Override\npublic WorkerAllocatedResource allocateQueryMaster(QueryInProgress queryInProgress) {\n  // Create a resource request for a query master\n  WorkerResourceAllocationRequest qmResourceRequest = createQMResourceRequest(queryInProgress.getQueryId());\n\n  // call future for async call\n  CallFuture<WorkerResourceAllocationResponse> callFuture = new CallFuture<WorkerResourceAllocationResponse>();\n  allocateWorkerResources(qmResourceRequest, callFuture);\n\n  // Wait for 3 seconds\n  WorkerResourceAllocationResponse response = null;\n  try {\n    response = callFuture.get(3, TimeUnit.SECONDS);\n  } catch (Throwable t) {\n    return null;\n  }\n\n  if (response.getWorkerAllocatedResourceList().size() == 0) {\n    return null;\n  }\n\n  WorkerAllocatedResource resource = response.getWorkerAllocatedResource(0);\n  registerQueryMaster(queryInProgress.getQueryId(), resource.getContainerId());\n  return resource;\n}",
        "accept_response": "@Override\npublic WorkerAllocatedResource allocateQueryMaster(QueryInProgress queryInProgress) {\n  // Create a resource request for a query master\n  WorkerResourceAllocationRequest qmResourceRequest = createQMResourceRequest(queryInProgress.getQueryId());\n\n  // call future for async call\n  CallFuture<WorkerResourceAllocationResponse> callFuture = new CallFuture<WorkerResourceAllocationResponse>();\n  allocateWorkerResources(qmResourceRequest, callFuture);\n\n  // Wait for 3 seconds\n  WorkerResourceAllocationResponse response = null;\n  try {\n    response = callFuture.get(3, TimeUnit.SECONDS);\n  } catch (Throwable t) {\n    LOG.error(t, t);\n    return null;\n  }\n\n  if (response.getWorkerAllocatedResourceList().size() == 0) {\n    return null;\n  }\n\n  WorkerAllocatedResource resource = response.getWorkerAllocatedResource(0);\n  registerQueryMaster(queryInProgress.getQueryId(), resource.getContainerId());\n  return resource;\n}",
        "reject_response": "@Override\npublic WorkerAllocatedResource allocateQueryMaster(QueryInProgress queryInProgress) {\n  // Create a resource request for a query master\n  WorkerResourceAllocationRequest qmResourceRequest = createQMResourceRequest(queryInProgress.getQueryId());\n\n  // call future for async call\n  CallFuture<WorkerResourceAllocationResponse> callFuture = new CallFuture<WorkerResourceAllocationResponse>();\n  allocateWorkerResources(qmResourceRequest, callFuture);\n\n  // Wait for 3 seconds\n  WorkerResourceAllocationResponse response = null;\n  try {\n    response = callFuture.get(3, TimeUnit.SECONDS);\n  } catch (Throwable t) {\n    LOG.error(t);\n    return null;\n  }\n\n  if (response.getWorkerAllocatedResourceList().size() == 0) {\n    return null;\n  }\n\n  WorkerAllocatedResource resource = response.getWorkerAllocatedResource(0);\n  registerQueryMaster(queryInProgress.getQueryId(), resource.getContainerId());\n  return resource;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2577,
        "instruction": "private void handleCodeDataIntegrityIssues(final JsonCommand command, final Throwable realCause, final Exception dve) {\n    if (realCause.getMessage().contains(\"code_name\")) {\n        final String name = command.stringValueOfParameterNamed(\"name\");\n        throw new PlatformDataIntegrityException(\"error.msg.code.duplicate.name\", \"A code with name '\" + name + \"' already exists\",\n                \"name\", name);\n    }\n\n    throw new PlatformDataIntegrityException(\"error.msg.cund.unknown.data.integrity.issue\",\n            \"Unknown data integrity issue with resource: \" + realCause.getMessage());\n}",
        "accept_response": "private void handleCodeDataIntegrityIssues(final JsonCommand command, final Throwable realCause, final Exception dve) {\n    if (realCause.getMessage().contains(\"code_name\")) {\n        final String name = command.stringValueOfParameterNamed(\"name\");\n        throw new PlatformDataIntegrityException(\"error.msg.code.duplicate.name\", \"A code with name '\" + name + \"' already exists\",\n                \"name\", name);\n    }\n\n    logger.error(\"Error occured.\", dve);\n    throw new PlatformDataIntegrityException(\"error.msg.cund.unknown.data.integrity.issue\",\n            \"Unknown data integrity issue with resource: \" + realCause.getMessage());\n}",
        "reject_response": "private void handleCodeDataIntegrityIssues(final JsonCommand command, final Throwable realCause, final Exception dve) {\n    if (realCause.getMessage().contains(\"code_name\")) {\n        final String name = command.stringValueOfParameterNamed(\"name\");\n        throw new PlatformDataIntegrityException(\"error.msg.code.duplicate.name\", \"A code with name '\" + name + \"' already exists\",\n                \"name\", name);\n    }\n\n    logger.error(dve.getMessage(), dve);\n    throw new PlatformDataIntegrityException(\"error.msg.cund.unknown.data.integrity.issue\",\n            \"Unknown data integrity issue with resource: \" + realCause.getMessage());\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2683,
        "instruction": "private void getCompressionCodec(InputData inputData) {\n\n    String userCompressCodec = inputData.getUserProperty(\"COMPRESSION_CODEC\");\n    String userCompressType = inputData.getUserProperty(\"COMPRESSION_TYPE\");\n    String parsedCompressType = parseCompressionType(userCompressType);\n\n    compressionType = SequenceFile.CompressionType.NONE;\n    codec = null;\n    if (userCompressCodec != null) {\n        codec = HdfsUtilities.getCodec(conf, userCompressCodec);\n\n        try {\n            compressionType = CompressionType.valueOf(parsedCompressType);\n        } catch (IllegalArgumentException e) {\n            throw new IllegalArgumentException(\n                    \"Illegal value for compression type \" + \"'\"\n                            + parsedCompressType + \"'\");\n        }\n        if (compressionType == null) {\n            throw new IllegalArgumentException(\n                    \"Compression type must be defined\");\n        }\n\n    }\n}",
        "accept_response": "private void getCompressionCodec(InputData inputData) {\n\n    String userCompressCodec = inputData.getUserProperty(\"COMPRESSION_CODEC\");\n    String userCompressType = inputData.getUserProperty(\"COMPRESSION_TYPE\");\n    String parsedCompressType = parseCompressionType(userCompressType);\n\n    compressionType = SequenceFile.CompressionType.NONE;\n    codec = null;\n    if (userCompressCodec != null) {\n        codec = HdfsUtilities.getCodec(conf, userCompressCodec);\n\n        try {\n            compressionType = CompressionType.valueOf(parsedCompressType);\n        } catch (IllegalArgumentException e) {\n            throw new IllegalArgumentException(\n                    \"Illegal value for compression type \" + \"'\"\n                            + parsedCompressType + \"'\");\n        }\n        if (compressionType == null) {\n            throw new IllegalArgumentException(\n                    \"Compression type must be defined\");\n        }\n\n        LOG.debug(\"Compression ON: \" + \"compression codec: \"\n                + userCompressCodec + \", compression type: \"\n                + compressionType);\n    }\n}",
        "reject_response": "private void getCompressionCodec(InputData inputData) {\n\n    String userCompressCodec = inputData.getUserProperty(\"COMPRESSION_CODEC\");\n    String userCompressType = inputData.getUserProperty(\"COMPRESSION_TYPE\");\n    String parsedCompressType = parseCompressionType(userCompressType);\n\n    compressionType = SequenceFile.CompressionType.NONE;\n    codec = null;\n    if (userCompressCodec != null) {\n        codec = HdfsUtilities.getCodec(conf, userCompressCodec);\n\n        try {\n            compressionType = CompressionType.valueOf(parsedCompressType);\n        } catch (IllegalArgumentException e) {\n            throw new IllegalArgumentException(\n                    \"Illegal value for compression type \" + \"'\"\n                            + parsedCompressType + \"'\");\n        }\n        if (compressionType == null) {\n            throw new IllegalArgumentException(\n                    \"Compression type must be defined\");\n        }\n\n        Log.debug(\"Compression ON: \" + \"compression codec: \"\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2964,
        "instruction": "private Path partitionSegment(Path segmentsDir, Path inputDir, int numLists)\n    throws IOException, ClassNotFoundException, InterruptedException {\n  // invert again, partition by host/domain/IP, sort by url hash\n\n  Path segment = new Path(segmentsDir, generateSegmentName());\n  Path output = new Path(segment, CrawlDatum.GENERATE_DIR_NAME);\n\n  LOG.info(\"Generator: segment: \" + segment);\n\n  Job job = NutchJob.getInstance(getConf());\n  job.setJobName(\"generate: partition \" + segment);\n  Configuration conf = job.getConfiguration();\n  conf.setInt(\"partition.url.seed\", new Random().nextInt());\n\n  FileInputFormat.addInputPath(job, inputDir);\n  job.setInputFormatClass(SequenceFileInputFormat.class);\n\n  job.setJarByClass(Generator.class);\n  job.setMapperClass(SelectorInverseMapper.class);\n  job.setMapOutputKeyClass(Text.class);\n  job.setMapOutputValueClass(SelectorEntry.class);\n  job.setPartitionerClass(URLPartitioner.class);\n  job.setReducerClass(PartitionReducer.class);\n  job.setNumReduceTasks(numLists);\n\n  FileOutputFormat.setOutputPath(job, output);\n  job.setOutputFormatClass(SequenceFileOutputFormat.class);\n  job.setOutputKeyClass(Text.class);\n  job.setOutputValueClass(CrawlDatum.class);\n  job.setSortComparatorClass(HashComparator.class);\n  try {\n    boolean success = job.waitForCompletion(true);\n    if (!success) {\n      String message = \"Generator job did not succeed, job status:\"\n          + job.getStatus().getState() + \", reason: \"\n          + job.getStatus().getFailureInfo();\n      LOG.error(message);\n      throw new RuntimeException(message);\n    }\n  } catch (IOException | InterruptedException | ClassNotFoundException e) {\n    LOG.error(StringUtils.stringifyException(e));\n    throw e;\n  }\n\n  return segment;\n}",
        "accept_response": "private Path partitionSegment(Path segmentsDir, Path inputDir, int numLists)\n    throws IOException, ClassNotFoundException, InterruptedException {\n  // invert again, partition by host/domain/IP, sort by url hash\n  LOG.info(\"Generator: Partitioning selected urls for politeness.\");\n\n  Path segment = new Path(segmentsDir, generateSegmentName());\n  Path output = new Path(segment, CrawlDatum.GENERATE_DIR_NAME);\n\n  LOG.info(\"Generator: segment: \" + segment);\n\n  Job job = NutchJob.getInstance(getConf());\n  job.setJobName(\"generate: partition \" + segment);\n  Configuration conf = job.getConfiguration();\n  conf.setInt(\"partition.url.seed\", new Random().nextInt());\n\n  FileInputFormat.addInputPath(job, inputDir);\n  job.setInputFormatClass(SequenceFileInputFormat.class);\n\n  job.setJarByClass(Generator.class);\n  job.setMapperClass(SelectorInverseMapper.class);\n  job.setMapOutputKeyClass(Text.class);\n  job.setMapOutputValueClass(SelectorEntry.class);\n  job.setPartitionerClass(URLPartitioner.class);\n  job.setReducerClass(PartitionReducer.class);\n  job.setNumReduceTasks(numLists);\n\n  FileOutputFormat.setOutputPath(job, output);\n  job.setOutputFormatClass(SequenceFileOutputFormat.class);\n  job.setOutputKeyClass(Text.class);\n  job.setOutputValueClass(CrawlDatum.class);\n  job.setSortComparatorClass(HashComparator.class);\n  try {\n    boolean success = job.waitForCompletion(true);\n    if (!success) {\n      String message = \"Generator job did not succeed, job status:\"\n          + job.getStatus().getState() + \", reason: \"\n          + job.getStatus().getFailureInfo();\n      LOG.error(message);\n      throw new RuntimeException(message);\n    }\n  } catch (IOException | InterruptedException | ClassNotFoundException e) {\n    LOG.error(StringUtils.stringifyException(e));\n    throw e;\n  }\n\n  return segment;\n}",
        "reject_response": "private Path partitionSegment(Path segmentsDir, Path inputDir, int numLists)\n    throws IOException, ClassNotFoundException, InterruptedException {\n  // invert again, partition by host/domain/IP, sort by url hash\n  if (LOG.isInfoEnabled()) {\n    LOG.info(\"Generator: Partitioning selected urls for politeness.\");\n  }\n\n  Path segment = new Path(segmentsDir, generateSegmentName());\n  Path output = new Path(segment, CrawlDatum.GENERATE_DIR_NAME);\n\n  LOG.info(\"Generator: segment: \" + segment);\n\n  Job job = NutchJob.getInstance(getConf());\n  job.setJobName(\"generate: partition \" + segment);\n  Configuration conf = job.getConfiguration();\n  conf.setInt(\"partition.url.seed\", new Random().nextInt());\n\n  FileInputFormat.addInputPath(job, inputDir);\n  job.setInputFormatClass(SequenceFileInputFormat.class);\n\n  job.setJarByClass(Generator.class);\n  job.setMapperClass(SelectorInverseMapper.class);\n  job.setMapOutputKeyClass(Text.class);\n  job.setMapOutputValueClass(SelectorEntry.class);\n  job.setPartitionerClass(URLPartitioner.class);\n  job.setReducerClass(PartitionReducer.class);\n  job.setNumReduceTasks(numLists);\n\n  FileOutputFormat.setOutputPath(job, output);\n  job.setOutputFormatClass(SequenceFileOutputFormat.class);\n  job.setOutputKeyClass(Text.class);\n  job.setOutputValueClass(CrawlDatum.class);\n  job.setSortComparatorClass(HashComparator.class);\n  try {\n    boolean success = job.waitForCompletion(true);\n    if (!success) {\n      String message = \"Generator job did not succeed, job status:\"\n          + job.getStatus().getState() + \", reason: \"\n          + job.getStatus().getFailureInfo();\n      LOG.error(message);\n      throw new RuntimeException(message);\n    }\n  } catch (IOException | InterruptedException | ClassNotFoundException e) {\n    LOG.error(StringUtils.stringifyException(e));\n    throw e;\n  }\n\n  return segment;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3117,
        "instruction": "@Override\npublic boolean isValid(int timeout) throws SQLException {\n  try {\n    if (tajoClient.isConnected()) {\n      ResultSet resultSet = tajoClient.executeQueryAndGetResult(\"SELECT 1;\");\n      boolean next = resultSet.next();\n      boolean valid = next && resultSet.getLong(1) == 1;\n      resultSet.close();\n      return valid;\n    } else {\n      return false;\n    }\n  } catch (ServiceException e) {\n    LOG.error(\"TajoMaster is not available.\", e);\n    return false;\n  } catch (IOException e) {\n    return false;\n  }\n}",
        "accept_response": "@Override\npublic boolean isValid(int timeout) throws SQLException {\n  try {\n    if (tajoClient.isConnected()) {\n      ResultSet resultSet = tajoClient.executeQueryAndGetResult(\"SELECT 1;\");\n      boolean next = resultSet.next();\n      boolean valid = next && resultSet.getLong(1) == 1;\n      resultSet.close();\n      return valid;\n    } else {\n      return false;\n    }\n  } catch (ServiceException e) {\n    LOG.error(\"TajoMaster is not available.\", e);\n    return false;\n  } catch (IOException e) {\n    LOG.error(\"JDBC connection is not valid.\", e);\n    return false;\n  }\n}",
        "reject_response": "@Override\npublic boolean isValid(int timeout) throws SQLException {\n  try {\n    if (tajoClient.isConnected()) {\n      ResultSet resultSet = tajoClient.executeQueryAndGetResult(\"SELECT 1;\");\n      boolean next = resultSet.next();\n      boolean valid = next && resultSet.getLong(1) == 1;\n      resultSet.close();\n      return valid;\n    } else {\n      return false;\n    }\n  } catch (ServiceException e) {\n    LOG.error(\"TajoMaster is not available.\", e);\n    return false;\n  } catch (IOException e) {\n    LOG.error(\"JDBC connection is not valid.\");\n    return false;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2558,
        "instruction": "public void makeFeedInstanceAvailable(String feedName, String clusterName, Date nominalTime)\n    throws FalconException {\n    List<Date> instances = (MONITORING_JDBC_STATE_STORE.getNominalInstances(feedName, clusterName,\n            EntityType.FEED.toString()));\n    // Slas for feeds not having sla tag are not stored.\n    if (CollectionUtils.isEmpty(instances)){\n        MONITORING_JDBC_STATE_STORE.deletePendingInstance(feedName, clusterName, nominalTime,\n                EntityType.FEED.toString());\n    }\n}",
        "accept_response": "public void makeFeedInstanceAvailable(String feedName, String clusterName, Date nominalTime)\n    throws FalconException {\n    LOG.debug(\"Removing {} feed's instance {} in cluster {} from pendingSLA\", feedName,\n            clusterName, nominalTime);\n    List<Date> instances = (MONITORING_JDBC_STATE_STORE.getNominalInstances(feedName, clusterName,\n            EntityType.FEED.toString()));\n    // Slas for feeds not having sla tag are not stored.\n    if (CollectionUtils.isEmpty(instances)){\n        MONITORING_JDBC_STATE_STORE.deletePendingInstance(feedName, clusterName, nominalTime,\n                EntityType.FEED.toString());\n    }\n}",
        "reject_response": "public void makeFeedInstanceAvailable(String feedName, String clusterName, Date nominalTime)\n    throws FalconException {\n    LOG.info(\"Removing {} feed's instance {} in cluster {} from pendingSLA\", feedName,\n    List<Date> instances = (MONITORING_JDBC_STATE_STORE.getNominalInstances(feedName, clusterName,\n            EntityType.FEED.toString()));\n    // Slas for feeds not having sla tag are not stored.\n    if (CollectionUtils.isEmpty(instances)){\n        MONITORING_JDBC_STATE_STORE.deletePendingInstance(feedName, clusterName, nominalTime,\n                EntityType.FEED.toString());\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2853,
        "instruction": "@Override\npublic void storeMail(MailAddress recipient, Mail mail) throws MessagingException {\n    String username = computeUsername(recipient);\n\n    String locatedFolder = locateFolder(username, mail);\n    ComposedMessageId composedMessageId = mailboxAppender.append(mail.getMessage(), username, locatedFolder);\n\n    metric.increment();\n}",
        "accept_response": "@Override\npublic void storeMail(MailAddress recipient, Mail mail) throws MessagingException {\n    String username = computeUsername(recipient);\n\n    String locatedFolder = locateFolder(username, mail);\n    ComposedMessageId composedMessageId = mailboxAppender.append(mail.getMessage(), username, locatedFolder);\n\n    metric.increment();\n    LOGGER.info(\"Local delivered mail {} successfully from {} to {} in folder {} with composedMessageId {}\", mail.getName(),\n        DeliveryUtils.prettyPrint(mail.getSender()), DeliveryUtils.prettyPrint(recipient), locatedFolder, composedMessageId);\n}",
        "reject_response": "@Override\npublic void storeMail(MailAddress recipient, Mail mail) throws MessagingException {\n    String username = computeUsername(recipient);\n\n    String locatedFolder = locateFolder(username, mail);\n    ComposedMessageId composedMessageId = mailboxAppender.append(mail.getMessage(), username, locatedFolder);\n\n    metric.increment();\n    LOGGER.info(\"Local delivered mail {} successfully from {} to {} in folder {}\", mail.getName(),\n        DeliveryUtils.prettyPrint(mail.getSender()), DeliveryUtils.prettyPrint(recipient), locatedFolder);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2679,
        "instruction": "@Override\npublic boolean openForWrite() throws Exception {\n\n    String fileName = inputData.getDataSource();\n    String compressCodec = inputData.getUserProperty(\"COMPRESSION_CODEC\");\n    CompressionCodec codec = null;\n\n    conf = new Configuration();\n    fs = FileSystem.get(conf);\n\n    // get compression codec\n    if (compressCodec != null) {\n        codec = HdfsUtilities.getCodec(conf, compressCodec);\n        String extension = codec.getDefaultExtension();\n        fileName += extension;\n    }\n\n    file = new Path(fileName);\n\n    if (fs.exists(file)) {\n        throw new IOException(\"file \" + file.toString()\n                + \" already exists, can't write data\");\n    }\n    org.apache.hadoop.fs.Path parent = file.getParent();\n    if (!fs.exists(parent)) {\n        fs.mkdirs(parent);\n    }\n\n    // create output stream - do not allow overwriting existing file\n    createOutputStream(file, codec);\n\n    return true;\n}",
        "accept_response": "@Override\npublic boolean openForWrite() throws Exception {\n\n    String fileName = inputData.getDataSource();\n    String compressCodec = inputData.getUserProperty(\"COMPRESSION_CODEC\");\n    CompressionCodec codec = null;\n\n    conf = new Configuration();\n    fs = FileSystem.get(conf);\n\n    // get compression codec\n    if (compressCodec != null) {\n        codec = HdfsUtilities.getCodec(conf, compressCodec);\n        String extension = codec.getDefaultExtension();\n        fileName += extension;\n    }\n\n    file = new Path(fileName);\n\n    if (fs.exists(file)) {\n        throw new IOException(\"file \" + file.toString()\n                + \" already exists, can't write data\");\n    }\n    org.apache.hadoop.fs.Path parent = file.getParent();\n    if (!fs.exists(parent)) {\n        fs.mkdirs(parent);\n        LOG.debug(\"Created new dir \" + parent.toString());\n    }\n\n    // create output stream - do not allow overwriting existing file\n    createOutputStream(file, codec);\n\n    return true;\n}",
        "reject_response": "@Override\npublic boolean openForWrite() throws Exception {\n\n    String fileName = inputData.getDataSource();\n    String compressCodec = inputData.getUserProperty(\"COMPRESSION_CODEC\");\n    CompressionCodec codec = null;\n\n    conf = new Configuration();\n    fs = FileSystem.get(conf);\n\n    // get compression codec\n    if (compressCodec != null) {\n        codec = HdfsUtilities.getCodec(conf, compressCodec);\n        String extension = codec.getDefaultExtension();\n        fileName += extension;\n    }\n\n    file = new Path(fileName);\n\n    if (fs.exists(file)) {\n        throw new IOException(\"file \" + file.toString()\n                + \" already exists, can't write data\");\n    }\n    org.apache.hadoop.fs.Path parent = file.getParent();\n    if (!fs.exists(parent)) {\n        fs.mkdirs(parent);\n        Log.debug(\"Created new dir \" + parent.toString());\n    }\n\n    // create output stream - do not allow overwriting existing file\n    createOutputStream(file, codec);\n\n    return true;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2814,
        "instruction": "public void inject(Object obj, GridResourceIoc.AnnotationSet annSet, Object... params)\n    throws IgniteCheckedException {\n    assert obj != null;\n\n    if (log.isDebugEnabled())\n\n    // Unwrap Proxy object.\n    obj = unwrapTarget(obj);\n\n    inject(obj, annSet, null, null, params);\n}",
        "accept_response": "public void inject(Object obj, GridResourceIoc.AnnotationSet annSet, Object... params)\n    throws IgniteCheckedException {\n    assert obj != null;\n\n    if (log.isDebugEnabled())\n        log.debug(S.toString(\"Injecting resources\", \"obj\", obj, true));\n\n    // Unwrap Proxy object.\n    obj = unwrapTarget(obj);\n\n    inject(obj, annSet, null, null, params);\n}",
        "reject_response": "public void inject(Object obj, GridResourceIoc.AnnotationSet annSet, Object... params)\n    throws IgniteCheckedException {\n    assert obj != null;\n\n    if (log.isDebugEnabled())\n        log.debug(\"Injecting resources: \" + obj);\n\n    // Unwrap Proxy object.\n    obj = unwrapTarget(obj);\n\n    inject(obj, annSet, null, null, params);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2373,
        "instruction": "private void transitionToFailed(long tid, Repo<T> op, Exception e) {\n  String tidStr = String.format(\"%016x\", tid);\n  log.warn(\"Failed to execute Repo, tid=\" + tidStr, e);\n  store.setProperty(tid, EXCEPTION_PROP, e);\n  store.setStatus(tid, TStatus.FAILED_IN_PROGRESS);\n}",
        "accept_response": "private void transitionToFailed(long tid, Repo<T> op, Exception e) {\n  String tidStr = String.format(\"%016x\", tid);\n  log.warn(\"Failed to execute Repo, tid=\" + tidStr, e);\n  store.setProperty(tid, EXCEPTION_PROP, e);\n  store.setStatus(tid, TStatus.FAILED_IN_PROGRESS);\n  log.info(\"Updated status for Repo with tid=\" + tidStr + \" to FAILED_IN_PROGRESS\");\n}",
        "reject_response": "private void transitionToFailed(long tid, Repo<T> op, Exception e) {\n  String tidStr = String.format(\"%016x\", tid);\n  log.warn(\"Failed to execute Repo, tid=\" + tidStr, e);\n  store.setProperty(tid, EXCEPTION_PROP, e);\n  store.setStatus(tid, TStatus.FAILED_IN_PROGRESS);\n  log.warn(\"Failed to execute Repo, tid=\" + String.format(\"%016x\", tid), e);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2593,
        "instruction": "private void onBarrier(int channelIndex) throws IOException {\n\tif (!blockedChannels[channelIndex]) {\n\t\tblockedChannels[channelIndex] = true;\n\n\t\tnumBarriersReceived++;\n\t}\n\telse {\n\t\tthrow new IOException(\"Stream corrupt: Repeated barrier for same checkpoint on input \" + channelIndex);\n\t}\n}",
        "accept_response": "private void onBarrier(int channelIndex) throws IOException {\n\tif (!blockedChannels[channelIndex]) {\n\t\tblockedChannels[channelIndex] = true;\n\n\t\tnumBarriersReceived++;\n\n\t\tif (LOG.isDebugEnabled()) {\n\t\t\tLOG.debug(\"{}: Received barrier from channel {}.\",\n\t\t\t\tinputGate.getOwningTaskName(),\n\t\t\t\tchannelIndex);\n\t\t}\n\t}\n\telse {\n\t\tthrow new IOException(\"Stream corrupt: Repeated barrier for same checkpoint on input \" + channelIndex);\n\t}\n}",
        "reject_response": "private void onBarrier(int channelIndex) throws IOException {\n\tif (!blockedChannels[channelIndex]) {\n\t\tblockedChannels[channelIndex] = true;\n\n\t\tnumBarriersReceived++;\n\n\t\tif (LOG.isDebugEnabled()) {\n\t\t\tLOG.debug(\"Received barrier from channel \" + channelIndex);\n\t\t}\n\t}\n\telse {\n\t\tthrow new IOException(\"Stream corrupt: Repeated barrier for same checkpoint on input \" + channelIndex);\n\t}\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2776,
        "instruction": "private ClassicHttpResponse handleCacheHit(\n        final HttpHost target,\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain,\n        final HttpCacheEntry entry) throws IOException, HttpException {\n    final HttpClientContext context  = scope.clientContext;\n    context.setAttribute(HttpCoreContext.HTTP_REQUEST, request);\n    recordCacheHit(target, request);\n    final Date now = getCurrentDate();\n    if (suitabilityChecker.canCachedResponseBeUsed(target, request, entry, now)) {\n        try {\n            return convert(generateCachedResponse(request, context, entry, now), scope);\n        } catch (final ResourceIOException ex) {\n            recordCacheFailure(target, request);\n            if (!mayCallBackend(request)) {\n                return convert(generateGatewayTimeout(context), scope);\n            }\n            setResponseStatus(scope.clientContext, CacheResponseStatus.FAILURE);\n            return chain.proceed(request, scope);\n        }\n    } else if (!mayCallBackend(request)) {\n        LOG.debug(\"Cache entry not suitable but only-if-cached requested\");\n        return convert(generateGatewayTimeout(context), scope);\n    } else if (!(entry.getStatus() == HttpStatus.SC_NOT_MODIFIED && !suitabilityChecker.isConditional(request))) {\n        LOG.debug(\"Revalidating cache entry\");\n        try {\n            if (cacheRevalidator != null\n                    && !staleResponseNotAllowed(request, entry, now)\n                    && validityPolicy.mayReturnStaleWhileRevalidating(entry, now)) {\n                LOG.debug(\"Serving stale with asynchronous revalidation\");\n                final String exchangeId = ExecSupport.getNextExchangeId();\n                final ExecChain.Scope fork = new ExecChain.Scope(\n                        exchangeId,\n                        scope.route,\n                        scope.originalRequest,\n                        scope.execRuntime.fork(null),\n                        HttpClientContext.create());\n                final SimpleHttpResponse response = generateCachedResponse(request, context, entry, now);\n                cacheRevalidator.revalidateCacheEntry(\n                        responseCache.generateKey(target, request, entry),\n                        new DefaultCacheRevalidator.RevalidationCall() {\n\n                    @Override\n                    public ClassicHttpResponse execute() throws HttpException, IOException {\n                        return revalidateCacheEntry(target, request, fork, chain, entry);\n                    }\n\n                });\n                return convert(response, scope);\n            }\n            return revalidateCacheEntry(target, request, scope, chain, entry);\n        } catch (final IOException ioex) {\n            return convert(handleRevalidationFailure(request, context, entry, now), scope);\n        }\n    } else {\n        LOG.debug(\"Cache entry not usable; calling backend\");\n        return callBackend(target, request, scope, chain);\n    }\n}",
        "accept_response": "private ClassicHttpResponse handleCacheHit(\n        final HttpHost target,\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain,\n        final HttpCacheEntry entry) throws IOException, HttpException {\n    final HttpClientContext context  = scope.clientContext;\n    context.setAttribute(HttpCoreContext.HTTP_REQUEST, request);\n    recordCacheHit(target, request);\n    final Date now = getCurrentDate();\n    if (suitabilityChecker.canCachedResponseBeUsed(target, request, entry, now)) {\n        LOG.debug(\"Cache hit\");\n        try {\n            return convert(generateCachedResponse(request, context, entry, now), scope);\n        } catch (final ResourceIOException ex) {\n            recordCacheFailure(target, request);\n            if (!mayCallBackend(request)) {\n                return convert(generateGatewayTimeout(context), scope);\n            }\n            setResponseStatus(scope.clientContext, CacheResponseStatus.FAILURE);\n            return chain.proceed(request, scope);\n        }\n    } else if (!mayCallBackend(request)) {\n        LOG.debug(\"Cache entry not suitable but only-if-cached requested\");\n        return convert(generateGatewayTimeout(context), scope);\n    } else if (!(entry.getStatus() == HttpStatus.SC_NOT_MODIFIED && !suitabilityChecker.isConditional(request))) {\n        LOG.debug(\"Revalidating cache entry\");\n        try {\n            if (cacheRevalidator != null\n                    && !staleResponseNotAllowed(request, entry, now)\n                    && validityPolicy.mayReturnStaleWhileRevalidating(entry, now)) {\n                LOG.debug(\"Serving stale with asynchronous revalidation\");\n                final String exchangeId = ExecSupport.getNextExchangeId();\n                final ExecChain.Scope fork = new ExecChain.Scope(\n                        exchangeId,\n                        scope.route,\n                        scope.originalRequest,\n                        scope.execRuntime.fork(null),\n                        HttpClientContext.create());\n                final SimpleHttpResponse response = generateCachedResponse(request, context, entry, now);\n                cacheRevalidator.revalidateCacheEntry(\n                        responseCache.generateKey(target, request, entry),\n                        new DefaultCacheRevalidator.RevalidationCall() {\n\n                    @Override\n                    public ClassicHttpResponse execute() throws HttpException, IOException {\n                        return revalidateCacheEntry(target, request, fork, chain, entry);\n                    }\n\n                });\n                return convert(response, scope);\n            }\n            return revalidateCacheEntry(target, request, scope, chain, entry);\n        } catch (final IOException ioex) {\n            return convert(handleRevalidationFailure(request, context, entry, now), scope);\n        }\n    } else {\n        LOG.debug(\"Cache entry not usable; calling backend\");\n        return callBackend(target, request, scope, chain);\n    }\n}",
        "reject_response": "private ClassicHttpResponse handleCacheHit(\n        final HttpHost target,\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain,\n        final HttpCacheEntry entry) throws IOException, HttpException {\n    final HttpClientContext context  = scope.clientContext;\n    context.setAttribute(HttpCoreContext.HTTP_REQUEST, request);\n    recordCacheHit(target, request);\n    final Date now = getCurrentDate();\n    if (suitabilityChecker.canCachedResponseBeUsed(target, request, entry, now)) {\n        log.debug(\"Cache hit\");\n        try {\n            return convert(generateCachedResponse(request, context, entry, now), scope);\n        } catch (final ResourceIOException ex) {\n            recordCacheFailure(target, request);\n            if (!mayCallBackend(request)) {\n                return convert(generateGatewayTimeout(context), scope);\n            }\n            setResponseStatus(scope.clientContext, CacheResponseStatus.FAILURE);\n            return chain.proceed(request, scope);\n        }\n    } else if (!mayCallBackend(request)) {\n        LOG.debug(\"Cache entry not suitable but only-if-cached requested\");\n        return convert(generateGatewayTimeout(context), scope);\n    } else if (!(entry.getStatus() == HttpStatus.SC_NOT_MODIFIED && !suitabilityChecker.isConditional(request))) {\n        LOG.debug(\"Revalidating cache entry\");\n        try {\n            if (cacheRevalidator != null\n                    && !staleResponseNotAllowed(request, entry, now)\n                    && validityPolicy.mayReturnStaleWhileRevalidating(entry, now)) {\n                LOG.debug(\"Serving stale with asynchronous revalidation\");\n                final String exchangeId = ExecSupport.getNextExchangeId();\n                final ExecChain.Scope fork = new ExecChain.Scope(\n                        exchangeId,\n                        scope.route,\n                        scope.originalRequest,\n                        scope.execRuntime.fork(null),\n                        HttpClientContext.create());\n                final SimpleHttpResponse response = generateCachedResponse(request, context, entry, now);\n                cacheRevalidator.revalidateCacheEntry(\n                        responseCache.generateKey(target, request, entry),\n                        new DefaultCacheRevalidator.RevalidationCall() {\n\n                    @Override\n                    public ClassicHttpResponse execute() throws HttpException, IOException {\n                        return revalidateCacheEntry(target, request, fork, chain, entry);\n                    }\n\n                });\n                return convert(response, scope);\n            }\n            return revalidateCacheEntry(target, request, scope, chain, entry);\n        } catch (final IOException ioex) {\n            return convert(handleRevalidationFailure(request, context, entry, now), scope);\n        }\n    } else {\n        LOG.debug(\"Cache entry not usable; calling backend\");\n        return callBackend(target, request, scope, chain);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2534,
        "instruction": "public void publish(DiagnosticEvent event)\n{\n    if (!DatabaseDescriptor.diagnosticEventsEnabled())\n        return;\n\n\n    // event class + type\n    ImmutableMultimap<Enum<?>, Consumer<DiagnosticEvent>> consumersByType = subscribersByClassAndType.get(event.getClass());\n    if (consumersByType != null)\n    {\n        ImmutableCollection<Consumer<DiagnosticEvent>> consumers = consumersByType.get(event.getType());\n        if (consumers != null)\n        {\n            for (Consumer<DiagnosticEvent> consumer : consumers)\n                consumer.accept(event);\n        }\n    }\n\n    // event class\n    Set<Consumer<DiagnosticEvent>> consumersByEvents = subscribersByClass.get(event.getClass());\n    if (consumersByEvents != null)\n    {\n        for (Consumer<DiagnosticEvent> consumer : consumersByEvents)\n            consumer.accept(event);\n    }\n\n    // all events\n    for (Consumer<DiagnosticEvent> consumer : subscribersAll)\n        consumer.accept(event);\n}",
        "accept_response": "public void publish(DiagnosticEvent event)\n{\n    if (!DatabaseDescriptor.diagnosticEventsEnabled())\n        return;\n\n    logger.trace(\"Publishing: {}={}\", event.getClass().getName(), event.toMap());\n\n    // event class + type\n    ImmutableMultimap<Enum<?>, Consumer<DiagnosticEvent>> consumersByType = subscribersByClassAndType.get(event.getClass());\n    if (consumersByType != null)\n    {\n        ImmutableCollection<Consumer<DiagnosticEvent>> consumers = consumersByType.get(event.getType());\n        if (consumers != null)\n        {\n            for (Consumer<DiagnosticEvent> consumer : consumers)\n                consumer.accept(event);\n        }\n    }\n\n    // event class\n    Set<Consumer<DiagnosticEvent>> consumersByEvents = subscribersByClass.get(event.getClass());\n    if (consumersByEvents != null)\n    {\n        for (Consumer<DiagnosticEvent> consumer : consumersByEvents)\n            consumer.accept(event);\n    }\n\n    // all events\n    for (Consumer<DiagnosticEvent> consumer : subscribersAll)\n        consumer.accept(event);\n}",
        "reject_response": "public void publish(DiagnosticEvent event)\n{\n    if (!DatabaseDescriptor.diagnosticEventsEnabled())\n        return;\n\n    logger.trace(\"Publishing: {}\", event);\n\n    // event class + type\n    ImmutableMultimap<Enum<?>, Consumer<DiagnosticEvent>> consumersByType = subscribersByClassAndType.get(event.getClass());\n    if (consumersByType != null)\n    {\n        ImmutableCollection<Consumer<DiagnosticEvent>> consumers = consumersByType.get(event.getType());\n        if (consumers != null)\n        {\n            for (Consumer<DiagnosticEvent> consumer : consumers)\n                consumer.accept(event);\n        }\n    }\n\n    // event class\n    Set<Consumer<DiagnosticEvent>> consumersByEvents = subscribersByClass.get(event.getClass());\n    if (consumersByEvents != null)\n    {\n        for (Consumer<DiagnosticEvent> consumer : consumersByEvents)\n            consumer.accept(event);\n    }\n\n    // all events\n    for (Consumer<DiagnosticEvent> consumer : subscribersAll)\n        consumer.accept(event);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2812,
        "instruction": "@Override public void write(Entry<? extends K, ? extends V> entry) {\n    try {\n        if (log.isDebugEnabled())\n\n        updateCache(entry.getKey(), entry, StoreOperation.PUT);\n    }\n    catch (IgniteInterruptedCheckedException e) {\n        throw new CacheWriterException(U.convertExceptionNoWrap(e));\n    }\n}",
        "accept_response": "@Override public void write(Entry<? extends K, ? extends V> entry) {\n    try {\n        if (log.isDebugEnabled())\n            log.debug(S.toString(\"Store put\",\n                \"key\", entry.getKey(), true,\n                \"val\", entry.getValue(), true));\n\n        updateCache(entry.getKey(), entry, StoreOperation.PUT);\n    }\n    catch (IgniteInterruptedCheckedException e) {\n        throw new CacheWriterException(U.convertExceptionNoWrap(e));\n    }\n}",
        "reject_response": "@Override public void write(Entry<? extends K, ? extends V> entry) {\n    try {\n        if (log.isDebugEnabled())\n            log.debug(\"Store put [key=\" + entry.getKey() + \", val=\" + entry.getValue() + ']');\n\n        updateCache(entry.getKey(), entry, StoreOperation.PUT);\n    }\n    catch (IgniteInterruptedCheckedException e) {\n        throw new CacheWriterException(U.convertExceptionNoWrap(e));\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2554,
        "instruction": "@Override\npublic void run() {\n  // rename the thread we're using for debugging purposes\n  final Thread currentThread = Thread.currentThread();\n  final String originalName = currentThread.getName();\n  currentThread.setName(queryIdString + \":foreman\");\n  try {\n    /*\n     Check if the foreman is ONLINE. If not don't accept any new queries.\n    */\n    if (!drillbitContext.isForemanOnline()) {\n      throw new ForemanException(\"Query submission failed since Foreman is shutting down.\");\n    }\n  } catch (ForemanException e) {\n    logger.debug(\"Failure while submitting query\", e);\n    queryStateProcessor.addToEventQueue(QueryState.FAILED, e);\n  }\n\n  queryText = queryRequest.getPlan();\n  queryStateProcessor.moveToState(QueryState.PLANNING, null);\n\n  try {\n    injector.injectChecked(queryContext.getExecutionControls(), \"run-try-beginning\", ForemanException.class);\n\n    // convert a run query request into action\n    switch (queryRequest.getType()) {\n    case LOGICAL:\n      parseAndRunLogicalPlan(queryRequest.getPlan());\n      break;\n    case PHYSICAL:\n      parseAndRunPhysicalPlan(queryRequest.getPlan());\n      break;\n    case SQL:\n      final String sql = queryRequest.getPlan();\n      // log query id, username and query text before starting any real work. Also, put\n      // them together such that it is easy to search based on query id\n      runSQL(sql);\n      break;\n    case EXECUTION:\n      runFragment(queryRequest.getFragmentsList());\n      break;\n    case PREPARED_STATEMENT:\n      runPreparedStatement(queryRequest.getPreparedStatementHandle());\n      break;\n    default:\n      throw new IllegalStateException();\n    }\n    injector.injectChecked(queryContext.getExecutionControls(), \"run-try-end\", ForemanException.class);\n  } catch (final ForemanException e) {\n    queryStateProcessor.moveToState(QueryState.FAILED, e);\n  } catch (final OutOfMemoryError | OutOfMemoryException e) {\n    if (FailureUtils.isDirectMemoryOOM(e)) {\n      queryStateProcessor.moveToState(QueryState.FAILED, UserException.memoryError(e).build(logger));\n    } else {\n      /*\n       * FragmentExecutors use a DrillbitStatusListener to watch out for the death of their query's Foreman. So, if we\n       * die here, they should get notified about that, and cancel themselves; we don't have to attempt to notify\n       * them, which might not work under these conditions.\n       */\n      FailureUtils.unrecoverableFailure(e, \"Unable to handle out of memory condition in Foreman.\", EXIT_CODE_HEAP_OOM);\n    }\n\n  } catch (AssertionError | Exception ex) {\n    queryStateProcessor.moveToState(QueryState.FAILED,\n        new ForemanException(\"Unexpected exception during fragment initialization: \" + ex.getMessage(), ex));\n  } finally {\n    // restore the thread's original name\n    currentThread.setName(originalName);\n  }\n\n  /*\n   * Note that despite the run() completing, the Foreman continues to exist, and receives\n   * events (indirectly, through the QueryManager's use of stateListener), about fragment\n   * completions. It won't go away until everything is completed, failed, or cancelled.\n   */\n}",
        "accept_response": "@Override\npublic void run() {\n  // rename the thread we're using for debugging purposes\n  final Thread currentThread = Thread.currentThread();\n  final String originalName = currentThread.getName();\n  currentThread.setName(queryIdString + \":foreman\");\n  try {\n    /*\n     Check if the foreman is ONLINE. If not don't accept any new queries.\n    */\n    if (!drillbitContext.isForemanOnline()) {\n      throw new ForemanException(\"Query submission failed since Foreman is shutting down.\");\n    }\n  } catch (ForemanException e) {\n    logger.debug(\"Failure while submitting query\", e);\n    queryStateProcessor.addToEventQueue(QueryState.FAILED, e);\n  }\n\n  queryText = queryRequest.getPlan();\n  queryStateProcessor.moveToState(QueryState.PLANNING, null);\n\n  try {\n    injector.injectChecked(queryContext.getExecutionControls(), \"run-try-beginning\", ForemanException.class);\n\n    // convert a run query request into action\n    switch (queryRequest.getType()) {\n    case LOGICAL:\n      parseAndRunLogicalPlan(queryRequest.getPlan());\n      break;\n    case PHYSICAL:\n      parseAndRunPhysicalPlan(queryRequest.getPlan());\n      break;\n    case SQL:\n      final String sql = queryRequest.getPlan();\n      // log query id, username and query text before starting any real work. Also, put\n      // them together such that it is easy to search based on query id\n      logger.info(\"Query text for query with id {} issued by {}: {}\", queryIdString,\n          queryContext.getQueryUserName(), sql);\n      runSQL(sql);\n      break;\n    case EXECUTION:\n      runFragment(queryRequest.getFragmentsList());\n      break;\n    case PREPARED_STATEMENT:\n      runPreparedStatement(queryRequest.getPreparedStatementHandle());\n      break;\n    default:\n      throw new IllegalStateException();\n    }\n    injector.injectChecked(queryContext.getExecutionControls(), \"run-try-end\", ForemanException.class);\n  } catch (final ForemanException e) {\n    queryStateProcessor.moveToState(QueryState.FAILED, e);\n  } catch (final OutOfMemoryError | OutOfMemoryException e) {\n    if (FailureUtils.isDirectMemoryOOM(e)) {\n      queryStateProcessor.moveToState(QueryState.FAILED, UserException.memoryError(e).build(logger));\n    } else {\n      /*\n       * FragmentExecutors use a DrillbitStatusListener to watch out for the death of their query's Foreman. So, if we\n       * die here, they should get notified about that, and cancel themselves; we don't have to attempt to notify\n       * them, which might not work under these conditions.\n       */\n      FailureUtils.unrecoverableFailure(e, \"Unable to handle out of memory condition in Foreman.\", EXIT_CODE_HEAP_OOM);\n    }\n\n  } catch (AssertionError | Exception ex) {\n    queryStateProcessor.moveToState(QueryState.FAILED,\n        new ForemanException(\"Unexpected exception during fragment initialization: \" + ex.getMessage(), ex));\n  } finally {\n    // restore the thread's original name\n    currentThread.setName(originalName);\n  }\n\n  /*\n   * Note that despite the run() completing, the Foreman continues to exist, and receives\n   * events (indirectly, through the QueryManager's use of stateListener), about fragment\n   * completions. It won't go away until everything is completed, failed, or cancelled.\n   */\n}",
        "reject_response": "@Override\npublic void run() {\n  // rename the thread we're using for debugging purposes\n  final Thread currentThread = Thread.currentThread();\n  final String originalName = currentThread.getName();\n  currentThread.setName(queryIdString + \":foreman\");\n  try {\n    /*\n     Check if the foreman is ONLINE. If not don't accept any new queries.\n    */\n    if (!drillbitContext.isForemanOnline()) {\n      throw new ForemanException(\"Query submission failed since Foreman is shutting down.\");\n    }\n  } catch (ForemanException e) {\n    logger.debug(\"Failure while submitting query\", e);\n    queryStateProcessor.addToEventQueue(QueryState.FAILED, e);\n  }\n\n  queryText = queryRequest.getPlan();\n  queryStateProcessor.moveToState(QueryState.PLANNING, null);\n\n  try {\n    injector.injectChecked(queryContext.getExecutionControls(), \"run-try-beginning\", ForemanException.class);\n\n    // convert a run query request into action\n    switch (queryRequest.getType()) {\n    case LOGICAL:\n      parseAndRunLogicalPlan(queryRequest.getPlan());\n      break;\n    case PHYSICAL:\n      parseAndRunPhysicalPlan(queryRequest.getPlan());\n      break;\n    case SQL:\n      final String sql = queryRequest.getPlan();\n      // log query id, username and query text before starting any real work. Also, put\n      // them together such that it is easy to search based on query id\n      logger.info(\"Query text for query id {}: {}\", this.queryIdString, sql);\n      runSQL(sql);\n      break;\n    case EXECUTION:\n      runFragment(queryRequest.getFragmentsList());\n      break;\n    case PREPARED_STATEMENT:\n      runPreparedStatement(queryRequest.getPreparedStatementHandle());\n      break;\n    default:\n      throw new IllegalStateException();\n    }\n    injector.injectChecked(queryContext.getExecutionControls(), \"run-try-end\", ForemanException.class);\n  } catch (final ForemanException e) {\n    queryStateProcessor.moveToState(QueryState.FAILED, e);\n  } catch (final OutOfMemoryError | OutOfMemoryException e) {\n    if (FailureUtils.isDirectMemoryOOM(e)) {\n      queryStateProcessor.moveToState(QueryState.FAILED, UserException.memoryError(e).build(logger));\n    } else {\n      /*\n       * FragmentExecutors use a DrillbitStatusListener to watch out for the death of their query's Foreman. So, if we\n       * die here, they should get notified about that, and cancel themselves; we don't have to attempt to notify\n       * them, which might not work under these conditions.\n       */\n      FailureUtils.unrecoverableFailure(e, \"Unable to handle out of memory condition in Foreman.\", EXIT_CODE_HEAP_OOM);\n    }\n\n  } catch (AssertionError | Exception ex) {\n    queryStateProcessor.moveToState(QueryState.FAILED,\n        new ForemanException(\"Unexpected exception during fragment initialization: \" + ex.getMessage(), ex));\n  } finally {\n    // restore the thread's original name\n    currentThread.setName(originalName);\n  }\n\n  /*\n   * Note that despite the run() completing, the Foreman continues to exist, and receives\n   * events (indirectly, through the QueryManager's use of stateListener), about fragment\n   * completions. It won't go away until everything is completed, failed, or cancelled.\n   */\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3028,
        "instruction": "public static Object[] restToArgs(HttpServletRequest request,\n    RestOperationMeta restOperation) throws InvocationException {\n  List<RestParam> paramList = restOperation.getParamList();\n\n  try {\n    Object[] paramValues = new Object[paramList.size()];\n    for (int idx = 0; idx < paramList.size(); idx++) {\n      RestParam param = paramList.get(idx);\n      paramValues[idx] = param.getParamProcessor().getValue(request);\n    }\n\n    return paramValues;\n  } catch (Exception e) {\n    // give standard http error code for invalid parameter\n    throw new InvocationException(Status.BAD_REQUEST, \"Parameter is not valid.\");\n  }\n}",
        "accept_response": "public static Object[] restToArgs(HttpServletRequest request,\n    RestOperationMeta restOperation) throws InvocationException {\n  List<RestParam> paramList = restOperation.getParamList();\n\n  try {\n    Object[] paramValues = new Object[paramList.size()];\n    for (int idx = 0; idx < paramList.size(); idx++) {\n      RestParam param = paramList.get(idx);\n      paramValues[idx] = param.getParamProcessor().getValue(request);\n    }\n\n    return paramValues;\n  } catch (Exception e) {\n    LOG.error(\"Parameter is not valid for operation {}. Message is {}.\",\n        restOperation.getOperationMeta().getMicroserviceQualifiedName(),\n        ExceptionUtils.getExceptionMessageWithoutTrace(e));\n    // give standard http error code for invalid parameter\n    throw new InvocationException(Status.BAD_REQUEST, \"Parameter is not valid.\");\n  }\n}",
        "reject_response": "public static Object[] restToArgs(HttpServletRequest request,\n    RestOperationMeta restOperation) throws InvocationException {\n  List<RestParam> paramList = restOperation.getParamList();\n\n  try {\n    Object[] paramValues = new Object[paramList.size()];\n    for (int idx = 0; idx < paramList.size(); idx++) {\n      RestParam param = paramList.get(idx);\n      paramValues[idx] = param.getParamProcessor().getValue(request);\n    }\n\n    return paramValues;\n  } catch (Exception e) {\n    LOG.error(\"Parameter is not valid for operation {}. \",\n    // give standard http error code for invalid parameter\n    throw new InvocationException(Status.BAD_REQUEST, \"Parameter is not valid.\");\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2918,
        "instruction": "private void maybePunctuate(StreamTask task) {\n    try {\n        // check whether we should punctuate based on the task's partition group timestamp;\n        // which are essentially based on record timestamp.\n        if (task.maybePunctuate())\n            sensors.punctuateTimeSensor.record(computeLatency());\n\n    } catch (KafkaException e) {\n        throw e;\n    }\n}",
        "accept_response": "private void maybePunctuate(StreamTask task) {\n    try {\n        // check whether we should punctuate based on the task's partition group timestamp;\n        // which are essentially based on record timestamp.\n        if (task.maybePunctuate())\n            sensors.punctuateTimeSensor.record(computeLatency());\n\n    } catch (KafkaException e) {\n        log.error(\"stream-thread [{}] Failed to punctuate active task #{}\", this.getName(), task.id(), e);\n        throw e;\n    }\n}",
        "reject_response": "private void maybePunctuate(StreamTask task) {\n    try {\n        // check whether we should punctuate based on the task's partition group timestamp;\n        // which are essentially based on record timestamp.\n        if (task.maybePunctuate())\n            sensors.punctuateTimeSensor.record(computeLatency());\n\n    } catch (KafkaException e) {\n        log.error(\"Failed to punctuate active task #\" + task.id() + \" in thread [\" + this.getName() + \"]: \", e);\n        throw e;\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2732,
        "instruction": "private void loadInternal(RandomAccessFile raFile, FileInputStream fin)\n    throws IOException {\n  if (!FSImageUtil.checkFileFormat(raFile)) {\n    throw new IOException(\"Unrecognized file format\");\n  }\n  FileSummary summary = FSImageUtil.loadSummary(raFile);\n  if (requireSameLayoutVersion && summary.getLayoutVersion() !=\n      HdfsServerConstants.NAMENODE_LAYOUT_VERSION) {\n    throw new IOException(\"Image version \" + summary.getLayoutVersion() +\n        \" is not equal to the software version \" +\n        HdfsServerConstants.NAMENODE_LAYOUT_VERSION);\n  }\n\n  FileChannel channel = fin.getChannel();\n\n  FSImageFormatPBINode.Loader inodeLoader = new FSImageFormatPBINode.Loader(\n      fsn, this);\n  FSImageFormatPBSnapshot.Loader snapshotLoader = new FSImageFormatPBSnapshot.Loader(\n      fsn, this);\n\n  ArrayList<FileSummary.Section> sections = Lists.newArrayList(summary\n      .getSectionsList());\n  Collections.sort(sections, new Comparator<FileSummary.Section>() {\n    @Override\n    public int compare(FileSummary.Section s1, FileSummary.Section s2) {\n      SectionName n1 = SectionName.fromString(s1.getName());\n      SectionName n2 = SectionName.fromString(s2.getName());\n      if (n1 == null) {\n        return n2 == null ? 0 : -1;\n      } else if (n2 == null) {\n        return -1;\n      } else {\n        return n1.ordinal() - n2.ordinal();\n      }\n    }\n  });\n\n  StartupProgress prog = NameNode.getStartupProgress();\n  /**\n   * beginStep() and the endStep() calls do not match the boundary of the\n   * sections. This is because that the current implementation only allows\n   * a particular step to be started for once.\n   */\n  Step currentStep = null;\n\n  for (FileSummary.Section s : sections) {\n    channel.position(s.getOffset());\n    InputStream in = new BufferedInputStream(new LimitInputStream(fin,\n        s.getLength()));\n\n    in = FSImageUtil.wrapInputStreamForCompression(conf,\n        summary.getCodec(), in);\n\n    String n = s.getName();\n\n    switch (SectionName.fromString(n)) {\n    case NS_INFO:\n      loadNameSystemSection(in);\n      break;\n    case STRING_TABLE:\n      loadStringTableSection(in);\n      break;\n    case INODE: {\n      currentStep = new Step(StepType.INODES);\n      prog.beginStep(Phase.LOADING_FSIMAGE, currentStep);\n      inodeLoader.loadINodeSection(in, prog, currentStep);\n    }\n      break;\n    case INODE_REFERENCE:\n      snapshotLoader.loadINodeReferenceSection(in);\n      break;\n    case INODE_DIR:\n      inodeLoader.loadINodeDirectorySection(in);\n      break;\n    case FILES_UNDERCONSTRUCTION:\n      inodeLoader.loadFilesUnderConstructionSection(in);\n      break;\n    case SNAPSHOT:\n      snapshotLoader.loadSnapshotSection(in);\n      break;\n    case SNAPSHOT_DIFF:\n      snapshotLoader.loadSnapshotDiffSection(in);\n      break;\n    case SECRET_MANAGER: {\n      prog.endStep(Phase.LOADING_FSIMAGE, currentStep);\n      Step step = new Step(StepType.DELEGATION_TOKENS);\n      prog.beginStep(Phase.LOADING_FSIMAGE, step);\n      loadSecretManagerSection(in, prog, step);\n      prog.endStep(Phase.LOADING_FSIMAGE, step);\n    }\n      break;\n    case CACHE_MANAGER: {\n      Step step = new Step(StepType.CACHE_POOLS);\n      prog.beginStep(Phase.LOADING_FSIMAGE, step);\n      loadCacheManagerSection(in, prog, step);\n      prog.endStep(Phase.LOADING_FSIMAGE, step);\n    }\n      break;\n    default:\n      break;\n    }\n  }\n}",
        "accept_response": "private void loadInternal(RandomAccessFile raFile, FileInputStream fin)\n    throws IOException {\n  if (!FSImageUtil.checkFileFormat(raFile)) {\n    throw new IOException(\"Unrecognized file format\");\n  }\n  FileSummary summary = FSImageUtil.loadSummary(raFile);\n  if (requireSameLayoutVersion && summary.getLayoutVersion() !=\n      HdfsServerConstants.NAMENODE_LAYOUT_VERSION) {\n    throw new IOException(\"Image version \" + summary.getLayoutVersion() +\n        \" is not equal to the software version \" +\n        HdfsServerConstants.NAMENODE_LAYOUT_VERSION);\n  }\n\n  FileChannel channel = fin.getChannel();\n\n  FSImageFormatPBINode.Loader inodeLoader = new FSImageFormatPBINode.Loader(\n      fsn, this);\n  FSImageFormatPBSnapshot.Loader snapshotLoader = new FSImageFormatPBSnapshot.Loader(\n      fsn, this);\n\n  ArrayList<FileSummary.Section> sections = Lists.newArrayList(summary\n      .getSectionsList());\n  Collections.sort(sections, new Comparator<FileSummary.Section>() {\n    @Override\n    public int compare(FileSummary.Section s1, FileSummary.Section s2) {\n      SectionName n1 = SectionName.fromString(s1.getName());\n      SectionName n2 = SectionName.fromString(s2.getName());\n      if (n1 == null) {\n        return n2 == null ? 0 : -1;\n      } else if (n2 == null) {\n        return -1;\n      } else {\n        return n1.ordinal() - n2.ordinal();\n      }\n    }\n  });\n\n  StartupProgress prog = NameNode.getStartupProgress();\n  /**\n   * beginStep() and the endStep() calls do not match the boundary of the\n   * sections. This is because that the current implementation only allows\n   * a particular step to be started for once.\n   */\n  Step currentStep = null;\n\n  for (FileSummary.Section s : sections) {\n    channel.position(s.getOffset());\n    InputStream in = new BufferedInputStream(new LimitInputStream(fin,\n        s.getLength()));\n\n    in = FSImageUtil.wrapInputStreamForCompression(conf,\n        summary.getCodec(), in);\n\n    String n = s.getName();\n\n    switch (SectionName.fromString(n)) {\n    case NS_INFO:\n      loadNameSystemSection(in);\n      break;\n    case STRING_TABLE:\n      loadStringTableSection(in);\n      break;\n    case INODE: {\n      currentStep = new Step(StepType.INODES);\n      prog.beginStep(Phase.LOADING_FSIMAGE, currentStep);\n      inodeLoader.loadINodeSection(in, prog, currentStep);\n    }\n      break;\n    case INODE_REFERENCE:\n      snapshotLoader.loadINodeReferenceSection(in);\n      break;\n    case INODE_DIR:\n      inodeLoader.loadINodeDirectorySection(in);\n      break;\n    case FILES_UNDERCONSTRUCTION:\n      inodeLoader.loadFilesUnderConstructionSection(in);\n      break;\n    case SNAPSHOT:\n      snapshotLoader.loadSnapshotSection(in);\n      break;\n    case SNAPSHOT_DIFF:\n      snapshotLoader.loadSnapshotDiffSection(in);\n      break;\n    case SECRET_MANAGER: {\n      prog.endStep(Phase.LOADING_FSIMAGE, currentStep);\n      Step step = new Step(StepType.DELEGATION_TOKENS);\n      prog.beginStep(Phase.LOADING_FSIMAGE, step);\n      loadSecretManagerSection(in, prog, step);\n      prog.endStep(Phase.LOADING_FSIMAGE, step);\n    }\n      break;\n    case CACHE_MANAGER: {\n      Step step = new Step(StepType.CACHE_POOLS);\n      prog.beginStep(Phase.LOADING_FSIMAGE, step);\n      loadCacheManagerSection(in, prog, step);\n      prog.endStep(Phase.LOADING_FSIMAGE, step);\n    }\n      break;\n    default:\n      LOG.warn(\"Unrecognized section {}\", n);\n      break;\n    }\n  }\n}",
        "reject_response": "private void loadInternal(RandomAccessFile raFile, FileInputStream fin)\n    throws IOException {\n  if (!FSImageUtil.checkFileFormat(raFile)) {\n    throw new IOException(\"Unrecognized file format\");\n  }\n  FileSummary summary = FSImageUtil.loadSummary(raFile);\n  if (requireSameLayoutVersion && summary.getLayoutVersion() !=\n      HdfsServerConstants.NAMENODE_LAYOUT_VERSION) {\n    throw new IOException(\"Image version \" + summary.getLayoutVersion() +\n        \" is not equal to the software version \" +\n        HdfsServerConstants.NAMENODE_LAYOUT_VERSION);\n  }\n\n  FileChannel channel = fin.getChannel();\n\n  FSImageFormatPBINode.Loader inodeLoader = new FSImageFormatPBINode.Loader(\n      fsn, this);\n  FSImageFormatPBSnapshot.Loader snapshotLoader = new FSImageFormatPBSnapshot.Loader(\n      fsn, this);\n\n  ArrayList<FileSummary.Section> sections = Lists.newArrayList(summary\n      .getSectionsList());\n  Collections.sort(sections, new Comparator<FileSummary.Section>() {\n    @Override\n    public int compare(FileSummary.Section s1, FileSummary.Section s2) {\n      SectionName n1 = SectionName.fromString(s1.getName());\n      SectionName n2 = SectionName.fromString(s2.getName());\n      if (n1 == null) {\n        return n2 == null ? 0 : -1;\n      } else if (n2 == null) {\n        return -1;\n      } else {\n        return n1.ordinal() - n2.ordinal();\n      }\n    }\n  });\n\n  StartupProgress prog = NameNode.getStartupProgress();\n  /**\n   * beginStep() and the endStep() calls do not match the boundary of the\n   * sections. This is because that the current implementation only allows\n   * a particular step to be started for once.\n   */\n  Step currentStep = null;\n\n  for (FileSummary.Section s : sections) {\n    channel.position(s.getOffset());\n    InputStream in = new BufferedInputStream(new LimitInputStream(fin,\n        s.getLength()));\n\n    in = FSImageUtil.wrapInputStreamForCompression(conf,\n        summary.getCodec(), in);\n\n    String n = s.getName();\n\n    switch (SectionName.fromString(n)) {\n    case NS_INFO:\n      loadNameSystemSection(in);\n      break;\n    case STRING_TABLE:\n      loadStringTableSection(in);\n      break;\n    case INODE: {\n      currentStep = new Step(StepType.INODES);\n      prog.beginStep(Phase.LOADING_FSIMAGE, currentStep);\n      inodeLoader.loadINodeSection(in, prog, currentStep);\n    }\n      break;\n    case INODE_REFERENCE:\n      snapshotLoader.loadINodeReferenceSection(in);\n      break;\n    case INODE_DIR:\n      inodeLoader.loadINodeDirectorySection(in);\n      break;\n    case FILES_UNDERCONSTRUCTION:\n      inodeLoader.loadFilesUnderConstructionSection(in);\n      break;\n    case SNAPSHOT:\n      snapshotLoader.loadSnapshotSection(in);\n      break;\n    case SNAPSHOT_DIFF:\n      snapshotLoader.loadSnapshotDiffSection(in);\n      break;\n    case SECRET_MANAGER: {\n      prog.endStep(Phase.LOADING_FSIMAGE, currentStep);\n      Step step = new Step(StepType.DELEGATION_TOKENS);\n      prog.beginStep(Phase.LOADING_FSIMAGE, step);\n      loadSecretManagerSection(in, prog, step);\n      prog.endStep(Phase.LOADING_FSIMAGE, step);\n    }\n      break;\n    case CACHE_MANAGER: {\n      Step step = new Step(StepType.CACHE_POOLS);\n      prog.beginStep(Phase.LOADING_FSIMAGE, step);\n      loadCacheManagerSection(in, prog, step);\n      prog.endStep(Phase.LOADING_FSIMAGE, step);\n    }\n      break;\n    default:\n      LOG.warn(\"Unrecognized section \" + n);\n      break;\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2395,
        "instruction": "private synchronized void updateUnknownErrors(String msg, Throwable t) {\n  somethingFailed = true;\n  unknownErrors++;\n  this.lastUnknownError = t;\n  this.notifyAll();\n  if (t instanceof TableDeletedException || t instanceof TableOfflineException || t instanceof TimedOutException)\n}",
        "accept_response": "private synchronized void updateUnknownErrors(String msg, Throwable t) {\n  somethingFailed = true;\n  unknownErrors++;\n  this.lastUnknownError = t;\n  this.notifyAll();\n  if (t instanceof TableDeletedException || t instanceof TableOfflineException || t instanceof TimedOutException)\n    log.debug(msg, t); // this is not unknown\n  else\n    log.error(msg, t);\n}",
        "reject_response": "private synchronized void updateUnknownErrors(String msg, Throwable t) {\n  somethingFailed = true;\n  unknownErrors++;\n  this.lastUnknownError = t;\n  this.notifyAll();\n  if (t instanceof TableDeletedException || t instanceof TableOfflineException || t instanceof TimedOutException)\n    log.debug(\"{}\", msg, t); // this is not unknown\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2538,
        "instruction": "private void pushGiantRecord(int partitionId, byte[] buffer, int numBytes) throws IOException {\n  long pushStartTime = System.nanoTime();\n  int bytesWritten =\n      rssShuffleClient.pushData(\n          appId,\n          shuffleId,\n          mapId,\n          taskContext.attemptNumber(),\n          partitionId,\n          buffer,\n          0,\n          numBytes,\n          numMappers,\n          numPartitions);\n  mapStatusLengths[partitionId].add(bytesWritten);\n  writeMetrics.incBytesWritten(bytesWritten);\n  writeMetrics.incWriteTime(System.nanoTime() - pushStartTime);\n}",
        "accept_response": "private void pushGiantRecord(int partitionId, byte[] buffer, int numBytes) throws IOException {\n  logger.debug(\"Push giant record, size {}.\", Utils.bytesToString(numBytes));\n  long pushStartTime = System.nanoTime();\n  int bytesWritten =\n      rssShuffleClient.pushData(\n          appId,\n          shuffleId,\n          mapId,\n          taskContext.attemptNumber(),\n          partitionId,\n          buffer,\n          0,\n          numBytes,\n          numMappers,\n          numPartitions);\n  mapStatusLengths[partitionId].add(bytesWritten);\n  writeMetrics.incBytesWritten(bytesWritten);\n  writeMetrics.incWriteTime(System.nanoTime() - pushStartTime);\n}",
        "reject_response": "private void pushGiantRecord(int partitionId, byte[] buffer, int numBytes) throws IOException {\n  logger.debug(\"Push giant record, size {}.\", numBytes);\n  long pushStartTime = System.nanoTime();\n  int bytesWritten =\n      rssShuffleClient.pushData(\n          appId,\n          shuffleId,\n          mapId,\n          taskContext.attemptNumber(),\n          partitionId,\n          buffer,\n          0,\n          numBytes,\n          numMappers,\n          numPartitions);\n  mapStatusLengths[partitionId].add(bytesWritten);\n  writeMetrics.incBytesWritten(bytesWritten);\n  writeMetrics.incWriteTime(System.nanoTime() - pushStartTime);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2750,
        "instruction": "private void renameImageFileInDir(StorageDirectory sd, NameNodeFile fromNnf,\n    NameNodeFile toNnf, long txid, boolean renameMD5) throws IOException {\n  final File fromFile = NNStorage.getStorageFile(sd, fromNnf, txid);\n  final File toFile = NNStorage.getStorageFile(sd, toNnf, txid);\n  // renameTo fails on Windows if the destination file already exists.\n\n  if (!fromFile.renameTo(toFile)) {\n    if (!toFile.delete() || !fromFile.renameTo(toFile)) {\n      throw new IOException(\"renaming  \" + fromFile.getAbsolutePath() + \" to \"  +\n          toFile.getAbsolutePath() + \" FAILED\");\n    }\n  }\n  if (renameMD5) {\n    MD5FileUtils.renameMD5File(fromFile, toFile);\n  }\n}",
        "accept_response": "private void renameImageFileInDir(StorageDirectory sd, NameNodeFile fromNnf,\n    NameNodeFile toNnf, long txid, boolean renameMD5) throws IOException {\n  final File fromFile = NNStorage.getStorageFile(sd, fromNnf, txid);\n  final File toFile = NNStorage.getStorageFile(sd, toNnf, txid);\n  // renameTo fails on Windows if the destination file already exists.\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"renaming  {} to {}\", fromFile.getAbsoluteFile(), toFile.getAbsolutePath());\n  }\n\n  if (!fromFile.renameTo(toFile)) {\n    if (!toFile.delete() || !fromFile.renameTo(toFile)) {\n      throw new IOException(\"renaming  \" + fromFile.getAbsolutePath() + \" to \"  +\n          toFile.getAbsolutePath() + \" FAILED\");\n    }\n  }\n  if (renameMD5) {\n    MD5FileUtils.renameMD5File(fromFile, toFile);\n  }\n}",
        "reject_response": "private void renameImageFileInDir(StorageDirectory sd, NameNodeFile fromNnf,\n    NameNodeFile toNnf, long txid, boolean renameMD5) throws IOException {\n  final File fromFile = NNStorage.getStorageFile(sd, fromNnf, txid);\n  final File toFile = NNStorage.getStorageFile(sd, toNnf, txid);\n  // renameTo fails on Windows if the destination file already exists.\n  if (LOG.isDebugEnabled()) {\n  if(LOG.isDebugEnabled()) {\n    LOG.debug(\"renaming  \" + fromFile.getAbsolutePath() \n              + \" to \" + toFile.getAbsolutePath());\n  }\n\n  if (!fromFile.renameTo(toFile)) {\n    if (!toFile.delete() || !fromFile.renameTo(toFile)) {\n      throw new IOException(\"renaming  \" + fromFile.getAbsolutePath() + \" to \"  +\n          toFile.getAbsolutePath() + \" FAILED\");\n    }\n  }\n  if (renameMD5) {\n    MD5FileUtils.renameMD5File(fromFile, toFile);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2585,
        "instruction": "@Override\n@CronTarget(jobName = JobName.RECALCULATE_INTEREST_FOR_LOAN)\npublic void recalculateInterest(Map<String, String> jobParameters) {\n    // gets the officeId\n    final String officeId = jobParameters.get(\"officeId\");\n    Long officeIdLong = Long.valueOf(officeId);\n\n    // gets the Office object\n    final OfficeData office = this.officeReadPlatformService.retrieveOffice(officeIdLong);\n    if(office == null) {\n        throw new OfficeNotFoundException(officeIdLong);\n    }\n    final int threadPoolSize=Integer.parseInt(jobParameters.get(\"thread-pool-size\"));\n    final int batchSize=Integer.parseInt(jobParameters.get(\"batch-size\"));\n\n    recalculateInterest(office,threadPoolSize,batchSize);\n}",
        "accept_response": "@Override\n@CronTarget(jobName = JobName.RECALCULATE_INTEREST_FOR_LOAN)\npublic void recalculateInterest(Map<String, String> jobParameters) {\n    // gets the officeId\n    final String officeId = jobParameters.get(\"officeId\");\n    logger.info(\"recalculateInterest: officeId={}\", officeId);\n    Long officeIdLong = Long.valueOf(officeId);\n\n    // gets the Office object\n    final OfficeData office = this.officeReadPlatformService.retrieveOffice(officeIdLong);\n    if(office == null) {\n        throw new OfficeNotFoundException(officeIdLong);\n    }\n    final int threadPoolSize=Integer.parseInt(jobParameters.get(\"thread-pool-size\"));\n    final int batchSize=Integer.parseInt(jobParameters.get(\"batch-size\"));\n\n    recalculateInterest(office,threadPoolSize,batchSize);\n}",
        "reject_response": "@Override\n@CronTarget(jobName = JobName.RECALCULATE_INTEREST_FOR_LOAN)\npublic void recalculateInterest(Map<String, String> jobParameters) {\n    // gets the officeId\n    final String officeId = jobParameters.get(\"officeId\");\n    logger.info(officeId);\n    Long officeIdLong=Long.valueOf(officeId);\n    //gets the Office object\n    Long officeIdLong = Long.valueOf(officeId);\n\n    // gets the Office object\n    final OfficeData office = this.officeReadPlatformService.retrieveOffice(officeIdLong);\n    if(office == null) {\n        throw new OfficeNotFoundException(officeIdLong);\n    }\n    final int threadPoolSize=Integer.parseInt(jobParameters.get(\"thread-pool-size\"));\n    final int batchSize=Integer.parseInt(jobParameters.get(\"batch-size\"));\n\n    recalculateInterest(office,threadPoolSize,batchSize);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2789,
        "instruction": "ClassicHttpResponse negotiateResponseFromVariants(\n        final HttpHost target,\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain,\n        final Map<String, Variant> variants) throws IOException, HttpException {\n    final ClassicHttpRequest conditionalRequest = conditionalRequestBuilder.buildConditionalRequestFromVariants(request, variants);\n\n    final Date requestDate = getCurrentDate();\n    final ClassicHttpResponse backendResponse = chain.proceed(conditionalRequest, scope);\n    try {\n        final Date responseDate = getCurrentDate();\n\n        backendResponse.addHeader(\"Via\", generateViaHeader(backendResponse));\n\n        if (backendResponse.getCode() != HttpStatus.SC_NOT_MODIFIED) {\n            return handleBackendResponse(target, request, scope, requestDate, responseDate, backendResponse);\n        }\n\n        final Header resultEtagHeader = backendResponse.getFirstHeader(HeaderConstants.ETAG);\n        if (resultEtagHeader == null) {\n            LOG.warn(\"304 response did not contain ETag\");\n            EntityUtils.consume(backendResponse.getEntity());\n            backendResponse.close();\n            return callBackend(target, request, scope, chain);\n        }\n\n        final String resultEtag = resultEtagHeader.getValue();\n        final Variant matchingVariant = variants.get(resultEtag);\n        if (matchingVariant == null) {\n            EntityUtils.consume(backendResponse.getEntity());\n            backendResponse.close();\n            return callBackend(target, request, scope, chain);\n        }\n\n        if (revalidationResponseIsTooOld(backendResponse, matchingVariant.getEntry())\n                && (request.getEntity() == null || request.getEntity().isRepeatable())) {\n            EntityUtils.consume(backendResponse.getEntity());\n            backendResponse.close();\n            final ClassicHttpRequest unconditional = conditionalRequestBuilder.buildUnconditionalRequest(request);\n            return callBackend(target, unconditional, scope, chain);\n        }\n\n        recordCacheUpdate(scope.clientContext);\n\n        final HttpCacheEntry responseEntry = responseCache.updateVariantCacheEntry(\n                target, conditionalRequest, backendResponse, matchingVariant, requestDate, responseDate);\n        backendResponse.close();\n        if (shouldSendNotModifiedResponse(request, responseEntry)) {\n            return convert(responseGenerator.generateNotModifiedResponse(responseEntry), scope);\n        }\n        final SimpleHttpResponse response = responseGenerator.generateResponse(request, responseEntry);\n        responseCache.reuseVariantEntryFor(target, request, matchingVariant);\n        return convert(response, scope);\n    } catch (final IOException | RuntimeException ex) {\n        backendResponse.close();\n        throw ex;\n    }\n}",
        "accept_response": "ClassicHttpResponse negotiateResponseFromVariants(\n        final HttpHost target,\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain,\n        final Map<String, Variant> variants) throws IOException, HttpException {\n    final ClassicHttpRequest conditionalRequest = conditionalRequestBuilder.buildConditionalRequestFromVariants(request, variants);\n\n    final Date requestDate = getCurrentDate();\n    final ClassicHttpResponse backendResponse = chain.proceed(conditionalRequest, scope);\n    try {\n        final Date responseDate = getCurrentDate();\n\n        backendResponse.addHeader(\"Via\", generateViaHeader(backendResponse));\n\n        if (backendResponse.getCode() != HttpStatus.SC_NOT_MODIFIED) {\n            return handleBackendResponse(target, request, scope, requestDate, responseDate, backendResponse);\n        }\n\n        final Header resultEtagHeader = backendResponse.getFirstHeader(HeaderConstants.ETAG);\n        if (resultEtagHeader == null) {\n            LOG.warn(\"304 response did not contain ETag\");\n            EntityUtils.consume(backendResponse.getEntity());\n            backendResponse.close();\n            return callBackend(target, request, scope, chain);\n        }\n\n        final String resultEtag = resultEtagHeader.getValue();\n        final Variant matchingVariant = variants.get(resultEtag);\n        if (matchingVariant == null) {\n            LOG.debug(\"304 response did not contain ETag matching one sent in If-None-Match\");\n            EntityUtils.consume(backendResponse.getEntity());\n            backendResponse.close();\n            return callBackend(target, request, scope, chain);\n        }\n\n        if (revalidationResponseIsTooOld(backendResponse, matchingVariant.getEntry())\n                && (request.getEntity() == null || request.getEntity().isRepeatable())) {\n            EntityUtils.consume(backendResponse.getEntity());\n            backendResponse.close();\n            final ClassicHttpRequest unconditional = conditionalRequestBuilder.buildUnconditionalRequest(request);\n            return callBackend(target, unconditional, scope, chain);\n        }\n\n        recordCacheUpdate(scope.clientContext);\n\n        final HttpCacheEntry responseEntry = responseCache.updateVariantCacheEntry(\n                target, conditionalRequest, backendResponse, matchingVariant, requestDate, responseDate);\n        backendResponse.close();\n        if (shouldSendNotModifiedResponse(request, responseEntry)) {\n            return convert(responseGenerator.generateNotModifiedResponse(responseEntry), scope);\n        }\n        final SimpleHttpResponse response = responseGenerator.generateResponse(request, responseEntry);\n        responseCache.reuseVariantEntryFor(target, request, matchingVariant);\n        return convert(response, scope);\n    } catch (final IOException | RuntimeException ex) {\n        backendResponse.close();\n        throw ex;\n    }\n}",
        "reject_response": "ClassicHttpResponse negotiateResponseFromVariants(\n        final HttpHost target,\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain,\n        final Map<String, Variant> variants) throws IOException, HttpException {\n    final ClassicHttpRequest conditionalRequest = conditionalRequestBuilder.buildConditionalRequestFromVariants(request, variants);\n\n    final Date requestDate = getCurrentDate();\n    final ClassicHttpResponse backendResponse = chain.proceed(conditionalRequest, scope);\n    try {\n        final Date responseDate = getCurrentDate();\n\n        backendResponse.addHeader(\"Via\", generateViaHeader(backendResponse));\n\n        if (backendResponse.getCode() != HttpStatus.SC_NOT_MODIFIED) {\n            return handleBackendResponse(target, request, scope, requestDate, responseDate, backendResponse);\n        }\n\n        final Header resultEtagHeader = backendResponse.getFirstHeader(HeaderConstants.ETAG);\n        if (resultEtagHeader == null) {\n            LOG.warn(\"304 response did not contain ETag\");\n            EntityUtils.consume(backendResponse.getEntity());\n            backendResponse.close();\n            return callBackend(target, request, scope, chain);\n        }\n\n        final String resultEtag = resultEtagHeader.getValue();\n        final Variant matchingVariant = variants.get(resultEtag);\n        if (matchingVariant == null) {\n            log.debug(\"304 response did not contain ETag matching one sent in If-None-Match\");\n            EntityUtils.consume(backendResponse.getEntity());\n            backendResponse.close();\n            return callBackend(target, request, scope, chain);\n        }\n\n        if (revalidationResponseIsTooOld(backendResponse, matchingVariant.getEntry())\n                && (request.getEntity() == null || request.getEntity().isRepeatable())) {\n            EntityUtils.consume(backendResponse.getEntity());\n            backendResponse.close();\n            final ClassicHttpRequest unconditional = conditionalRequestBuilder.buildUnconditionalRequest(request);\n            return callBackend(target, unconditional, scope, chain);\n        }\n\n        recordCacheUpdate(scope.clientContext);\n\n        final HttpCacheEntry responseEntry = responseCache.updateVariantCacheEntry(\n                target, conditionalRequest, backendResponse, matchingVariant, requestDate, responseDate);\n        backendResponse.close();\n        if (shouldSendNotModifiedResponse(request, responseEntry)) {\n            return convert(responseGenerator.generateNotModifiedResponse(responseEntry), scope);\n        }\n        final SimpleHttpResponse response = responseGenerator.generateResponse(request, responseEntry);\n        responseCache.reuseVariantEntryFor(target, request, matchingVariant);\n        return convert(response, scope);\n    } catch (final IOException | RuntimeException ex) {\n        backendResponse.close();\n        throw ex;\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2900,
        "instruction": "public void execute(SPFSession session, FutureSPFResult result, boolean throttle) {\n    SPFChecker checker;\n    while ((checker = session.popChecker()) != null) {\n        // only execute checkers we added (better recursivity)\n        try {\n            DNSLookupContinuation cont = checker.checkSPF(session);\n            // if the checker returns a continuation we return it\n            if (cont != null) {\n                invokeAsynchService(session, result, cont, throttle);\n                return;\n            }\n        } catch (Exception e) {\n            while (e != null) {\n                while (checker == null || !(checker instanceof SPFCheckerExceptionCatcher)) {\n                    checker = session.popChecker();\n                }\n                try {\n                    ((SPFCheckerExceptionCatcher) checker).onException(e, session);\n                    e = null;\n                } catch (SPFResultException ex) {\n                    e = ex;\n                } finally {\n                    checker = null;\n                }\n            }\n        }\n    }\n    result.setSPFResult(session);\n}",
        "accept_response": "public void execute(SPFSession session, FutureSPFResult result, boolean throttle) {\n    SPFChecker checker;\n    while ((checker = session.popChecker()) != null) {\n        // only execute checkers we added (better recursivity)\n        LOGGER.debug(\"Executing checker: {}\", checker);\n        try {\n            DNSLookupContinuation cont = checker.checkSPF(session);\n            // if the checker returns a continuation we return it\n            if (cont != null) {\n                invokeAsynchService(session, result, cont, throttle);\n                return;\n            }\n        } catch (Exception e) {\n            while (e != null) {\n                while (checker == null || !(checker instanceof SPFCheckerExceptionCatcher)) {\n                    checker = session.popChecker();\n                }\n                try {\n                    ((SPFCheckerExceptionCatcher) checker).onException(e, session);\n                    e = null;\n                } catch (SPFResultException ex) {\n                    e = ex;\n                } finally {\n                    checker = null;\n                }\n            }\n        }\n    }\n    result.setSPFResult(session);\n}",
        "reject_response": "public void execute(SPFSession session, FutureSPFResult result, boolean throttle) {\n    SPFChecker checker;\n    while ((checker = session.popChecker()) != null) {\n        // only execute checkers we added (better recursivity)\n        log.debug(\"Executing checker: \" + checker);\n        try {\n            DNSLookupContinuation cont = checker.checkSPF(session);\n            // if the checker returns a continuation we return it\n            if (cont != null) {\n                invokeAsynchService(session, result, cont, throttle);\n                return;\n            }\n        } catch (Exception e) {\n            while (e != null) {\n                while (checker == null || !(checker instanceof SPFCheckerExceptionCatcher)) {\n                    checker = session.popChecker();\n                }\n                try {\n                    ((SPFCheckerExceptionCatcher) checker).onException(e, session);\n                    e = null;\n                } catch (SPFResultException ex) {\n                    e = ex;\n                } finally {\n                    checker = null;\n                }\n            }\n        }\n    }\n    result.setSPFResult(session);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2678,
        "instruction": "@Override\npublic void closeForWrite() throws Exception {\n    if ((dos != null) && (fsdos != null)) {\n        dos.flush();\n        /*\n         * From release 0.21.0 sync() is deprecated in favor of hflush(),\n         * which only guarantees that new readers will see all data written\n         * to that point, and hsync(), which makes a stronger guarantee that\n         * the operating system has flushed the data to disk (like POSIX\n         * fsync), although data may still be in the disk cache.\n         */\n        fsdos.hsync();\n        dos.close();\n    }\n}",
        "accept_response": "@Override\npublic void closeForWrite() throws Exception {\n    if ((dos != null) && (fsdos != null)) {\n        LOG.debug(\"Closing writing stream for path \" + file);\n        dos.flush();\n        /*\n         * From release 0.21.0 sync() is deprecated in favor of hflush(),\n         * which only guarantees that new readers will see all data written\n         * to that point, and hsync(), which makes a stronger guarantee that\n         * the operating system has flushed the data to disk (like POSIX\n         * fsync), although data may still be in the disk cache.\n         */\n        fsdos.hsync();\n        dos.close();\n    }\n}",
        "reject_response": "@Override\npublic void closeForWrite() throws Exception {\n    if ((dos != null) && (fsdos != null)) {\n        Log.debug(\"Closing writing stream for path \" + file);\n        dos.flush();\n        /*\n         * From release 0.21.0 sync() is deprecated in favor of hflush(),\n         * which only guarantees that new readers will see all data written\n         * to that point, and hsync(), which makes a stronger guarantee that\n         * the operating system has flushed the data to disk (like POSIX\n         * fsync), although data may still be in the disk cache.\n         */\n        fsdos.hsync();\n        dos.close();\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3246,
        "instruction": "void onConnected(\n    int _negotiatedSessionTimeout,\n    long _sessionId,\n    byte[] _sessionPasswd,\n    boolean isRO) throws IOException {\n    negotiatedSessionTimeout = _negotiatedSessionTimeout;\n    if (negotiatedSessionTimeout <= 0) {\n        state = States.CLOSED;\n\n        eventThread.queueEvent(new WatchedEvent(Watcher.Event.EventType.None, Watcher.Event.KeeperState.Expired, null));\n        eventThread.queueEventOfDeath();\n\n        String warnInfo = String.format(\n            \"Unable to reconnect to ZooKeeper service, session 0x%s has expired\",\n            Long.toHexString(sessionId));\n        LOG.warn(warnInfo);\n        throw new SessionExpiredException(warnInfo);\n    }\n\n    if (!readOnly && isRO) {\n        LOG.error(\"Read/write client got connected to read-only server\");\n    }\n\n    readTimeout = negotiatedSessionTimeout * 2 / 3;\n    connectTimeout = negotiatedSessionTimeout / hostProvider.size();\n    hostProvider.onConnected();\n    sessionId = _sessionId;\n    sessionPasswd = _sessionPasswd;\n    state = (isRO) ? States.CONNECTEDREADONLY : States.CONNECTED;\n    seenRwServerBefore |= !isRO;\n    KeeperState eventState = (isRO) ? KeeperState.ConnectedReadOnly : KeeperState.SyncConnected;\n    eventThread.queueEvent(new WatchedEvent(Watcher.Event.EventType.None, eventState, null));\n}",
        "accept_response": "void onConnected(\n    int _negotiatedSessionTimeout,\n    long _sessionId,\n    byte[] _sessionPasswd,\n    boolean isRO) throws IOException {\n    negotiatedSessionTimeout = _negotiatedSessionTimeout;\n    if (negotiatedSessionTimeout <= 0) {\n        state = States.CLOSED;\n\n        eventThread.queueEvent(new WatchedEvent(Watcher.Event.EventType.None, Watcher.Event.KeeperState.Expired, null));\n        eventThread.queueEventOfDeath();\n\n        String warnInfo = String.format(\n            \"Unable to reconnect to ZooKeeper service, session 0x%s has expired\",\n            Long.toHexString(sessionId));\n        LOG.warn(warnInfo);\n        throw new SessionExpiredException(warnInfo);\n    }\n\n    if (!readOnly && isRO) {\n        LOG.error(\"Read/write client got connected to read-only server\");\n    }\n\n    readTimeout = negotiatedSessionTimeout * 2 / 3;\n    connectTimeout = negotiatedSessionTimeout / hostProvider.size();\n    hostProvider.onConnected();\n    sessionId = _sessionId;\n    sessionPasswd = _sessionPasswd;\n    state = (isRO) ? States.CONNECTEDREADONLY : States.CONNECTED;\n    seenRwServerBefore |= !isRO;\n    LOG.info(\n        \"Session establishment complete on server {}, session id = 0x{}, negotiated timeout = {}{}\",\n        clientCnxnSocket.getRemoteSocketAddress(),\n        Long.toHexString(sessionId),\n        negotiatedSessionTimeout,\n        (isRO ? \" (READ-ONLY mode)\" : \"\"));\n    KeeperState eventState = (isRO) ? KeeperState.ConnectedReadOnly : KeeperState.SyncConnected;\n    eventThread.queueEvent(new WatchedEvent(Watcher.Event.EventType.None, eventState, null));\n}",
        "reject_response": "void onConnected(\n    int _negotiatedSessionTimeout,\n    long _sessionId,\n    byte[] _sessionPasswd,\n    boolean isRO) throws IOException {\n    negotiatedSessionTimeout = _negotiatedSessionTimeout;\n    if (negotiatedSessionTimeout <= 0) {\n        state = States.CLOSED;\n\n        eventThread.queueEvent(new WatchedEvent(Watcher.Event.EventType.None, Watcher.Event.KeeperState.Expired, null));\n        eventThread.queueEventOfDeath();\n\n        String warnInfo = String.format(\n            \"Unable to reconnect to ZooKeeper service, session 0x%s has expired\",\n            Long.toHexString(sessionId));\n        LOG.warn(warnInfo);\n        throw new SessionExpiredException(warnInfo);\n    }\n\n    if (!readOnly && isRO) {\n        LOG.error(\"Read/write client got connected to read-only server\");\n    }\n\n    readTimeout = negotiatedSessionTimeout * 2 / 3;\n    connectTimeout = negotiatedSessionTimeout / hostProvider.size();\n    hostProvider.onConnected();\n    sessionId = _sessionId;\n    sessionPasswd = _sessionPasswd;\n    state = (isRO) ? States.CONNECTEDREADONLY : States.CONNECTED;\n    seenRwServerBefore |= !isRO;\n    LOG.info(\"Session establishment complete on server \"\n             + clientCnxnSocket.getRemoteSocketAddress()\n             + \", sessionid = 0x\" + Long.toHexString(sessionId)\n             + \", negotiated timeout = \" + negotiatedSessionTimeout\n             + (isRO ? \" (READ-ONLY mode)\" : \"\"));\n    KeeperState eventState = (isRO) ? KeeperState.ConnectedReadOnly : KeeperState.SyncConnected;\n    eventThread.queueEvent(new WatchedEvent(Watcher.Event.EventType.None, eventState, null));\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2611,
        "instruction": "@Override\npublic void afterBulk(long executionId, BulkRequest request, Throwable failure) {\n\n\ttry {\n\t\tfor (ActionRequest action : request.requests()) {\n\t\t\tfailureHandler.onFailure(action, failure, -1, requestIndexer);\n\t\t}\n\t} catch (Throwable t) {\n\t\t// fail the sink and skip the rest of the items\n\t\t// if the failure handler decides to throw an exception\n\t\tfailureThrowable.compareAndSet(null, t);\n\t}\n\n\tif (flushOnCheckpoint) {\n\t\tnumPendingRequests.getAndAdd(-request.numberOfActions());\n\t}\n}",
        "accept_response": "@Override\npublic void afterBulk(long executionId, BulkRequest request, Throwable failure) {\n\tLOG.error(\"Failed Elasticsearch bulk request: \", failure);\n\n\ttry {\n\t\tfor (ActionRequest action : request.requests()) {\n\t\t\tfailureHandler.onFailure(action, failure, -1, requestIndexer);\n\t\t}\n\t} catch (Throwable t) {\n\t\t// fail the sink and skip the rest of the items\n\t\t// if the failure handler decides to throw an exception\n\t\tfailureThrowable.compareAndSet(null, t);\n\t}\n\n\tif (flushOnCheckpoint) {\n\t\tnumPendingRequests.getAndAdd(-request.numberOfActions());\n\t}\n}",
        "reject_response": "@Override\npublic void afterBulk(long executionId, BulkRequest request, Throwable failure) {\n\tLOG.error(\"Failed Elasticsearch bulk request: {}\", failure.getMessage(), failure.getCause());\n\n\ttry {\n\t\tfor (ActionRequest action : request.requests()) {\n\t\t\tfailureHandler.onFailure(action, failure, -1, requestIndexer);\n\t\t}\n\t} catch (Throwable t) {\n\t\t// fail the sink and skip the rest of the items\n\t\t// if the failure handler decides to throw an exception\n\t\tfailureThrowable.compareAndSet(null, t);\n\t}\n\n\tif (flushOnCheckpoint) {\n\t\tnumPendingRequests.getAndAdd(-request.numberOfActions());\n\t}\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3185,
        "instruction": "@Override\npublic synchronized void requestContainerUpdate(\n    Container container, UpdateContainerRequest updateContainerRequest) {\n  Preconditions.checkNotNull(container, \"Container cannot be null!!\");\n  Preconditions.checkNotNull(updateContainerRequest,\n      \"UpdateContainerRequest cannot be null!!\");\n  if (updateContainerRequest.getCapability() != null &&\n      updateContainerRequest.getExecutionType() == null) {\n    validateContainerResourceChangeRequest(\n        updateContainerRequest.getContainerUpdateType(),\n        container.getId(), container.getResource(),\n        updateContainerRequest.getCapability());\n  } else if (updateContainerRequest.getExecutionType() != null &&\n      updateContainerRequest.getCapability() == null) {\n    validateContainerExecTypeChangeRequest(\n        updateContainerRequest.getContainerUpdateType(),\n        container.getId(), container.getExecutionType(),\n        updateContainerRequest.getExecutionType());\n  } else if (updateContainerRequest.getExecutionType() == null &&\n      updateContainerRequest.getCapability() == null) {\n    throw new IllegalArgumentException(\"Both target Capability and\" +\n        \"target Execution Type are null\");\n  } else {\n    throw new IllegalArgumentException(\"Support currently exists only for\" +\n        \" EITHER update of Capability OR update of Execution Type NOT both\");\n  }\n  if (change.get(container.getId()) == null) {\n    change.put(container.getId(),\n        new SimpleEntry<>(container, updateContainerRequest));\n  } else {\n    change.get(container.getId()).setValue(updateContainerRequest);\n  }\n  if (pendingChange.get(container.getId()) == null) {\n    pendingChange.put(container.getId(),\n        new SimpleEntry<>(container, updateContainerRequest));\n  } else {\n    pendingChange.get(container.getId()).setValue(updateContainerRequest);\n  }\n}",
        "accept_response": "@Override\npublic synchronized void requestContainerUpdate(\n    Container container, UpdateContainerRequest updateContainerRequest) {\n  Preconditions.checkNotNull(container, \"Container cannot be null!!\");\n  Preconditions.checkNotNull(updateContainerRequest,\n      \"UpdateContainerRequest cannot be null!!\");\n  LOG.info(\"Requesting Container update : container={}, updateType={},\" +\n                  \" targetCapability={}, targetExecType={}\",\n          container,\n          updateContainerRequest.getContainerUpdateType(),\n          updateContainerRequest.getCapability(),\n          updateContainerRequest.getExecutionType());\n  if (updateContainerRequest.getCapability() != null &&\n      updateContainerRequest.getExecutionType() == null) {\n    validateContainerResourceChangeRequest(\n        updateContainerRequest.getContainerUpdateType(),\n        container.getId(), container.getResource(),\n        updateContainerRequest.getCapability());\n  } else if (updateContainerRequest.getExecutionType() != null &&\n      updateContainerRequest.getCapability() == null) {\n    validateContainerExecTypeChangeRequest(\n        updateContainerRequest.getContainerUpdateType(),\n        container.getId(), container.getExecutionType(),\n        updateContainerRequest.getExecutionType());\n  } else if (updateContainerRequest.getExecutionType() == null &&\n      updateContainerRequest.getCapability() == null) {\n    throw new IllegalArgumentException(\"Both target Capability and\" +\n        \"target Execution Type are null\");\n  } else {\n    throw new IllegalArgumentException(\"Support currently exists only for\" +\n        \" EITHER update of Capability OR update of Execution Type NOT both\");\n  }\n  if (change.get(container.getId()) == null) {\n    change.put(container.getId(),\n        new SimpleEntry<>(container, updateContainerRequest));\n  } else {\n    change.get(container.getId()).setValue(updateContainerRequest);\n  }\n  if (pendingChange.get(container.getId()) == null) {\n    pendingChange.put(container.getId(),\n        new SimpleEntry<>(container, updateContainerRequest));\n  } else {\n    pendingChange.get(container.getId()).setValue(updateContainerRequest);\n  }\n}",
        "reject_response": "@Override\npublic synchronized void requestContainerUpdate(\n    Container container, UpdateContainerRequest updateContainerRequest) {\n  Preconditions.checkNotNull(container, \"Container cannot be null!!\");\n  Preconditions.checkNotNull(updateContainerRequest,\n      \"UpdateContainerRequest cannot be null!!\");\n  LOG.info(\"Requesting Container update : \" +\n      \"container=\" + container + \", \" +\n      \"updateType=\" + updateContainerRequest.getContainerUpdateType() + \", \" +\n      \"targetCapability=\" + updateContainerRequest.getCapability() + \", \" +\n      \"targetExecType=\" + updateContainerRequest.getExecutionType());\n  if (updateContainerRequest.getCapability() != null &&\n      updateContainerRequest.getExecutionType() == null) {\n    validateContainerResourceChangeRequest(\n        updateContainerRequest.getContainerUpdateType(),\n        container.getId(), container.getResource(),\n        updateContainerRequest.getCapability());\n  } else if (updateContainerRequest.getExecutionType() != null &&\n      updateContainerRequest.getCapability() == null) {\n    validateContainerExecTypeChangeRequest(\n        updateContainerRequest.getContainerUpdateType(),\n        container.getId(), container.getExecutionType(),\n        updateContainerRequest.getExecutionType());\n  } else if (updateContainerRequest.getExecutionType() == null &&\n      updateContainerRequest.getCapability() == null) {\n    throw new IllegalArgumentException(\"Both target Capability and\" +\n        \"target Execution Type are null\");\n  } else {\n    throw new IllegalArgumentException(\"Support currently exists only for\" +\n        \" EITHER update of Capability OR update of Execution Type NOT both\");\n  }\n  if (change.get(container.getId()) == null) {\n    change.put(container.getId(),\n        new SimpleEntry<>(container, updateContainerRequest));\n  } else {\n    change.get(container.getId()).setValue(updateContainerRequest);\n  }\n  if (pendingChange.get(container.getId()) == null) {\n    pendingChange.put(container.getId(),\n        new SimpleEntry<>(container, updateContainerRequest));\n  } else {\n    pendingChange.get(container.getId()).setValue(updateContainerRequest);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2827,
        "instruction": "private SegmentArchiveResult archiveSegment(long absIdx) throws IgniteCheckedException {\n    long segIdx = absIdx % dsCfg.getWalSegments();\n\n    File origFile = new File(walWorkDir, FileDescriptor.fileName(segIdx));\n\n    String name = FileDescriptor.fileName(absIdx);\n\n    File dstTmpFile = new File(walArchiveDir, name + \".tmp\");\n\n    File dstFile = new File(walArchiveDir, name);\n\n    if (log.isInfoEnabled())\n\n    try {\n        Files.deleteIfExists(dstTmpFile.toPath());\n\n        Files.copy(origFile.toPath(), dstTmpFile.toPath());\n\n        Files.move(dstTmpFile.toPath(), dstFile.toPath());\n\n        if (mode != WALMode.NONE) {\n            try (FileIO f0 = ioFactory.create(dstFile, CREATE, READ, WRITE)) {\n                f0.force();\n            }\n        }\n    }\n    catch (IOException e) {\n        throw new IgniteCheckedException(\"Failed to archive WAL segment [\" +\n            \"srcFile=\" + origFile.getAbsolutePath() +\n            \", dstFile=\" + dstTmpFile.getAbsolutePath() + ']', e);\n    }\n\n    if (log.isInfoEnabled())\n        log.info(\"Copied file [src=\" + origFile.getAbsolutePath() +\n            \", dst=\" + dstFile.getAbsolutePath() + ']');\n\n    return new SegmentArchiveResult(absIdx, origFile, dstFile);\n}",
        "accept_response": "private SegmentArchiveResult archiveSegment(long absIdx) throws IgniteCheckedException {\n    long segIdx = absIdx % dsCfg.getWalSegments();\n\n    File origFile = new File(walWorkDir, FileDescriptor.fileName(segIdx));\n\n    String name = FileDescriptor.fileName(absIdx);\n\n    File dstTmpFile = new File(walArchiveDir, name + \".tmp\");\n\n    File dstFile = new File(walArchiveDir, name);\n\n    if (log.isInfoEnabled())\n        log.info(\"Starting to copy WAL segment [absIdx=\" + absIdx + \", segIdx=\" + segIdx +\n            \", origFile=\" + origFile.getAbsolutePath() + \", dstFile=\" + dstFile.getAbsolutePath() + ']');\n\n    try {\n        Files.deleteIfExists(dstTmpFile.toPath());\n\n        Files.copy(origFile.toPath(), dstTmpFile.toPath());\n\n        Files.move(dstTmpFile.toPath(), dstFile.toPath());\n\n        if (mode != WALMode.NONE) {\n            try (FileIO f0 = ioFactory.create(dstFile, CREATE, READ, WRITE)) {\n                f0.force();\n            }\n        }\n    }\n    catch (IOException e) {\n        throw new IgniteCheckedException(\"Failed to archive WAL segment [\" +\n            \"srcFile=\" + origFile.getAbsolutePath() +\n            \", dstFile=\" + dstTmpFile.getAbsolutePath() + ']', e);\n    }\n\n    if (log.isInfoEnabled())\n        log.info(\"Copied file [src=\" + origFile.getAbsolutePath() +\n            \", dst=\" + dstFile.getAbsolutePath() + ']');\n\n    return new SegmentArchiveResult(absIdx, origFile, dstFile);\n}",
        "reject_response": "private SegmentArchiveResult archiveSegment(long absIdx) throws IgniteCheckedException {\n    long segIdx = absIdx % dsCfg.getWalSegments();\n\n    File origFile = new File(walWorkDir, FileDescriptor.fileName(segIdx));\n\n    String name = FileDescriptor.fileName(absIdx);\n\n    File dstTmpFile = new File(walArchiveDir, name + \".tmp\");\n\n    File dstFile = new File(walArchiveDir, name);\n\n    if (log.isInfoEnabled())\n    if (log.isDebugEnabled())\n        log.debug(\"Starting to copy WAL segment [absIdx=\" + absIdx + \", segIdx=\" + segIdx +\n\n    try {\n        Files.deleteIfExists(dstTmpFile.toPath());\n\n        Files.copy(origFile.toPath(), dstTmpFile.toPath());\n\n        Files.move(dstTmpFile.toPath(), dstFile.toPath());\n\n        if (mode != WALMode.NONE) {\n            try (FileIO f0 = ioFactory.create(dstFile, CREATE, READ, WRITE)) {\n                f0.force();\n            }\n        }\n    }\n    catch (IOException e) {\n        throw new IgniteCheckedException(\"Failed to archive WAL segment [\" +\n            \"srcFile=\" + origFile.getAbsolutePath() +\n            \", dstFile=\" + dstTmpFile.getAbsolutePath() + ']', e);\n    }\n\n    if (log.isInfoEnabled())\n        log.info(\"Copied file [src=\" + origFile.getAbsolutePath() +\n            \", dst=\" + dstFile.getAbsolutePath() + ']');\n\n    return new SegmentArchiveResult(absIdx, origFile, dstFile);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2933,
        "instruction": "public static List<BundleInfo> override(List<BundleInfo> infos, String overridesUrl) {\n    List<Clause> overrides = loadOverrides(overridesUrl);\n    if (overrides.isEmpty()) {\n        return infos;\n    }\n    try {\n        Map<String, Manifest> manifests = new HashMap<String, Manifest>();\n        for (Clause override : overrides) {\n            Manifest manifest = getManifest(override.getName());\n            manifests.put(override.getName(), manifest);\n        }\n        List<BundleInfo> newInfos = new ArrayList<BundleInfo>();\n        for (BundleInfo info : infos) {\n            Manifest manifest = getManifest(info.getLocation());\n            if (manifest != null) {\n                String bsn = getBundleSymbolicName(manifest);\n                Version ver = getBundleVersion(manifest);\n                String ven = getBundleVendor(manifest);\n                String url = info.getLocation();\n                for (Clause override : overrides) {\n                    Manifest overMan = manifests.get(override.getName());\n                    if (overMan == null) {\n                        continue;\n                    }\n                    String oBsn = getBundleSymbolicName(overMan);\n                    if (!bsn.equals(oBsn)) {\n                        continue;\n                    }\n\n                    Version oVer = getBundleVersion(overMan);\n                    VersionRange range;\n                    String vr = extractVersionRange(override);\n                    if (vr == null) {\n                        // default to micro version compatibility\n                        Version v2 = new Version(oVer.getMajor(), oVer.getMinor(), 0);\n                        if (v2.equals(oVer)) {\n                            continue;\n                        }\n                        range = new VersionRange(false, v2, oVer, true);\n                    } else {\n                        range = VersionRange.parseVersionRange(vr);\n                    }\n\n                    String vendor = getBundleVendor(overMan);\n\n                    // Before we do a replace, lets check if vendors change\n                    if (ven == null) {\n                         if (vendor != null) {\n                             LOGGER.warn(VENDOR_WARNING);\n                         }\n                    } else {\n                         if (vendor == null) {\n                             LOGGER.warn(VENDOR_WARNING);\n                         } else {\n                              if (!vendor.equals(ven)) {\n                                  LOGGER.warn(VENDOR_WARNING);\n                              }\n                         }\n                    }\n                    // The resource matches, so replace it with the overridden resource\n                    // if the override is actually a newer version than what we currently have\n                    if (range.contains(ver) && ver.compareTo(oVer) < 0) {\n                        ver = oVer;\n                        url = override.getName();\n                    }\n                }\n                if (!info.getLocation().equals(url)) {\n                    Bundle b = new Bundle();\n                    b.setLocation(url);\n                    b.setStartLevel(info.getStartLevel());\n                    b.setStart(info.isStart());\n                    b.setDependency(info.isDependency());\n                    newInfos.add(b);\n                } else {\n                    newInfos.add(info);\n                }\n            } else {\n                newInfos.add(info);\n            }\n        }\n        return newInfos;\n    } catch (Exception e) {\n        LOGGER.info(\"Unable to process bundle overrides\", e);\n        return infos;\n    }\n}",
        "accept_response": "public static List<BundleInfo> override(List<BundleInfo> infos, String overridesUrl) {\n    List<Clause> overrides = loadOverrides(overridesUrl);\n    if (overrides.isEmpty()) {\n        return infos;\n    }\n    try {\n        Map<String, Manifest> manifests = new HashMap<String, Manifest>();\n        for (Clause override : overrides) {\n            Manifest manifest = getManifest(override.getName());\n            manifests.put(override.getName(), manifest);\n        }\n        List<BundleInfo> newInfos = new ArrayList<BundleInfo>();\n        for (BundleInfo info : infos) {\n            Manifest manifest = getManifest(info.getLocation());\n            if (manifest != null) {\n                String bsn = getBundleSymbolicName(manifest);\n                Version ver = getBundleVersion(manifest);\n                String ven = getBundleVendor(manifest);\n                String url = info.getLocation();\n                for (Clause override : overrides) {\n                    Manifest overMan = manifests.get(override.getName());\n                    if (overMan == null) {\n                        continue;\n                    }\n                    String oBsn = getBundleSymbolicName(overMan);\n                    if (!bsn.equals(oBsn)) {\n                        continue;\n                    }\n\n                    Version oVer = getBundleVersion(overMan);\n                    VersionRange range;\n                    String vr = extractVersionRange(override);\n                    if (vr == null) {\n                        // default to micro version compatibility\n                        Version v2 = new Version(oVer.getMajor(), oVer.getMinor(), 0);\n                        if (v2.equals(oVer)) {\n                            continue;\n                        }\n                        range = new VersionRange(false, v2, oVer, true);\n                    } else {\n                        range = VersionRange.parseVersionRange(vr);\n                    }\n\n                    String vendor = getBundleVendor(overMan);\n\n                    // Before we do a replace, lets check if vendors change\n                    if (ven == null) {\n                         if (vendor != null) {\n                             LOGGER.warn(VENDOR_WARNING);\n                         }\n                    } else {\n                         if (vendor == null) {\n                             LOGGER.warn(VENDOR_WARNING);\n                         } else {\n                              if (!vendor.equals(ven)) {\n                                  LOGGER.warn(VENDOR_WARNING);\n                              }\n                         }\n                    }\n                    // The resource matches, so replace it with the overridden resource\n                    // if the override is actually a newer version than what we currently have\n                    if (range.contains(ver) && ver.compareTo(oVer) < 0) {\n                        LOGGER.warn(\"Overriding original bundle \" + url + \" to \" + override.getName());\n                        ver = oVer;\n                        url = override.getName();\n                    }\n                }\n                if (!info.getLocation().equals(url)) {\n                    Bundle b = new Bundle();\n                    b.setLocation(url);\n                    b.setStartLevel(info.getStartLevel());\n                    b.setStart(info.isStart());\n                    b.setDependency(info.isDependency());\n                    newInfos.add(b);\n                } else {\n                    newInfos.add(info);\n                }\n            } else {\n                newInfos.add(info);\n            }\n        }\n        return newInfos;\n    } catch (Exception e) {\n        LOGGER.info(\"Unable to process bundle overrides\", e);\n        return infos;\n    }\n}",
        "reject_response": "public static List<BundleInfo> override(List<BundleInfo> infos, String overridesUrl) {\n    List<Clause> overrides = loadOverrides(overridesUrl);\n    if (overrides.isEmpty()) {\n        return infos;\n    }\n    try {\n        Map<String, Manifest> manifests = new HashMap<String, Manifest>();\n        for (Clause override : overrides) {\n            Manifest manifest = getManifest(override.getName());\n            manifests.put(override.getName(), manifest);\n        }\n        List<BundleInfo> newInfos = new ArrayList<BundleInfo>();\n        for (BundleInfo info : infos) {\n            Manifest manifest = getManifest(info.getLocation());\n            if (manifest != null) {\n                String bsn = getBundleSymbolicName(manifest);\n                Version ver = getBundleVersion(manifest);\n                String ven = getBundleVendor(manifest);\n                String url = info.getLocation();\n                for (Clause override : overrides) {\n                    Manifest overMan = manifests.get(override.getName());\n                    if (overMan == null) {\n                        continue;\n                    }\n                    String oBsn = getBundleSymbolicName(overMan);\n                    if (!bsn.equals(oBsn)) {\n                        continue;\n                    }\n\n                    Version oVer = getBundleVersion(overMan);\n                    VersionRange range;\n                    String vr = extractVersionRange(override);\n                    if (vr == null) {\n                        // default to micro version compatibility\n                        Version v2 = new Version(oVer.getMajor(), oVer.getMinor(), 0);\n                        if (v2.equals(oVer)) {\n                            continue;\n                        }\n                        range = new VersionRange(false, v2, oVer, true);\n                    } else {\n                        range = VersionRange.parseVersionRange(vr);\n                    }\n\n                    String vendor = getBundleVendor(overMan);\n\n                    // Before we do a replace, lets check if vendors change\n                    if (ven == null) {\n                         if (vendor != null) {\n                             LOGGER.warn(VENDOR_WARNING);\n                         }\n                    } else {\n                         if (vendor == null) {\n                             LOGGER.warn(VENDOR_WARNING);\n                         } else {\n                              if (!vendor.equals(ven)) {\n                                  LOGGER.warn(VENDOR_WARNING);\n                              }\n                         }\n                    }\n                    // The resource matches, so replace it with the overridden resource\n                    // if the override is actually a newer version than what we currently have\n                    if (range.contains(ver) && ver.compareTo(oVer) < 0) {\n                        LOGGER.info(\"Overriding original bundle \" + url + \" to \" + override.getName());\n                        ver = oVer;\n                        url = override.getName();\n                    }\n                }\n                if (!info.getLocation().equals(url)) {\n                    Bundle b = new Bundle();\n                    b.setLocation(url);\n                    b.setStartLevel(info.getStartLevel());\n                    b.setStart(info.isStart());\n                    b.setDependency(info.isDependency());\n                    newInfos.add(b);\n                } else {\n                    newInfos.add(info);\n                }\n            } else {\n                newInfos.add(info);\n            }\n        }\n        return newInfos;\n    } catch (Exception e) {\n        LOGGER.info(\"Unable to process bundle overrides\", e);\n        return infos;\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2728,
        "instruction": "void doDrain(final List<RAMQueueEntry> entries) throws InterruptedException {\n  if (entries.isEmpty()) {\n    return;\n  }\n  // This method is a little hard to follow. We run through the passed in entries and for each\n  // successful add, we add a non-null BucketEntry to the below bucketEntries.  Later we must\n  // do cleanup making sure we've cleared ramCache of all entries regardless of whether we\n  // successfully added the item to the bucketcache; if we don't do the cleanup, we'll OOME by\n  // filling ramCache.  We do the clean up by again running through the passed in entries\n  // doing extra work when we find a non-null bucketEntries corresponding entry.\n  final int size = entries.size();\n  BucketEntry[] bucketEntries = new BucketEntry[size];\n  // Index updated inside loop if success or if we can't succeed. We retry if cache is full\n  // when we go to add an entry by going around the loop again without upping the index.\n  int index = 0;\n  while (cacheEnabled && index < size) {\n    RAMQueueEntry re = null;\n    try {\n      re = entries.get(index);\n      if (re == null) {\n        LOG.warn(\"Couldn't get entry or changed on us; who else is messing with it?\");\n        index++;\n        continue;\n      }\n      BucketEntry bucketEntry = re.writeToCache(ioEngine, bucketAllocator, realCacheSize);\n      // Successfully added. Up index and add bucketEntry. Clear io exceptions.\n      bucketEntries[index] = bucketEntry;\n      if (ioErrorStartTime > 0) {\n        ioErrorStartTime = -1;\n      }\n      index++;\n    } catch (BucketAllocatorException fle) {\n      long currTE = System.currentTimeMillis()/1000; // Current time since Epoch in seconds.\n      if (prevRecTE == 0) {\n        // The very first exception.\n        LOG.warn(\"Failed allocation for \" + (re == null ? \"\" : re.getKey()) + \"; \" + fle);\n      } else {\n        // Record the warning.\n        sinceLastTraceAllocFailCount.increment();\n        sinceLastTraceAllocFailSize.add(re.getData().getSerializedLength());\n        if (currTE - prevRecTE > 60) {\n          LOG.warn(\"Failed allocations since last trace: \" + \" count: \" + sinceLastTraceAllocFailCount\n            + \" size: \" + sinceLastTraceAllocFailSize);\n          sinceLastTraceAllocFailCount.reset();\n          sinceLastTraceAllocFailSize.reset();\n        }\n      }\n      // Presume can't add. Too big? Move index on. Entry will be cleared from ramCache below.\n      bucketEntries[index] = null;\n      index++;\n      prevRecTE = currTE;\n    } catch (CacheFullException cfe) {\n      // Cache full when we tried to add. Try freeing space and then retrying (don't up index)\n      if (!freeInProgress) {\n        freeSpace(\"Full!\");\n      } else {\n        Thread.sleep(50);\n      }\n    } catch (IOException ioex) {\n      // Hopefully transient. Retry. checkIOErrorIsTolerated disables cache if problem.\n      LOG.error(\"Failed writing to bucket cache\", ioex);\n      checkIOErrorIsTolerated();\n    }\n  }\n\n  // Make sure data pages are written on media before we update maps.\n  try {\n    ioEngine.sync();\n  } catch (IOException ioex) {\n    LOG.error(\"Failed syncing IO engine\", ioex);\n    checkIOErrorIsTolerated();\n    // Since we failed sync, free the blocks in bucket allocator\n    for (int i = 0; i < entries.size(); ++i) {\n      if (bucketEntries[i] != null) {\n        bucketAllocator.freeBlock(bucketEntries[i].offset());\n        bucketEntries[i] = null;\n      }\n    }\n  }\n\n  // Now add to backingMap if successfully added to bucket cache.  Remove from ramCache if\n  // success or error.\n  for (int i = 0; i < size; ++i) {\n    BlockCacheKey key = entries.get(i).getKey();\n    // Only add if non-null entry.\n    if (bucketEntries[i] != null) {\n      putIntoBackingMap(key, bucketEntries[i]);\n    }\n    // Always remove from ramCache even if we failed adding it to the block cache above.\n    boolean existed = ramCache.remove(key, re -> {\n      if (re != null) {\n        heapSize.add(-1 * re.getData().heapSize());\n      }\n    });\n    if (!existed && bucketEntries[i] != null) {\n      // Block should have already been evicted. Remove it and free space.\n      final BucketEntry bucketEntry = bucketEntries[i];\n      bucketEntry.withWriteLock(offsetLock, () -> {\n        if (backingMap.remove(key, bucketEntry)) {\n          blockEvicted(key, bucketEntry, false);\n        }\n        return null;\n      });\n    }\n  }\n\n  long used = bucketAllocator.getUsedSize();\n  if (used > acceptableSize()) {\n    freeSpace(\"Used=\" + used + \" > acceptable=\" + acceptableSize());\n  }\n  return;\n}",
        "accept_response": "void doDrain(final List<RAMQueueEntry> entries) throws InterruptedException {\n  if (entries.isEmpty()) {\n    return;\n  }\n  // This method is a little hard to follow. We run through the passed in entries and for each\n  // successful add, we add a non-null BucketEntry to the below bucketEntries.  Later we must\n  // do cleanup making sure we've cleared ramCache of all entries regardless of whether we\n  // successfully added the item to the bucketcache; if we don't do the cleanup, we'll OOME by\n  // filling ramCache.  We do the clean up by again running through the passed in entries\n  // doing extra work when we find a non-null bucketEntries corresponding entry.\n  final int size = entries.size();\n  BucketEntry[] bucketEntries = new BucketEntry[size];\n  // Index updated inside loop if success or if we can't succeed. We retry if cache is full\n  // when we go to add an entry by going around the loop again without upping the index.\n  int index = 0;\n  while (cacheEnabled && index < size) {\n    RAMQueueEntry re = null;\n    try {\n      re = entries.get(index);\n      if (re == null) {\n        LOG.warn(\"Couldn't get entry or changed on us; who else is messing with it?\");\n        index++;\n        continue;\n      }\n      BucketEntry bucketEntry = re.writeToCache(ioEngine, bucketAllocator, realCacheSize);\n      // Successfully added. Up index and add bucketEntry. Clear io exceptions.\n      bucketEntries[index] = bucketEntry;\n      if (ioErrorStartTime > 0) {\n        ioErrorStartTime = -1;\n      }\n      index++;\n    } catch (BucketAllocatorException fle) {\n      long currTE = System.currentTimeMillis()/1000; // Current time since Epoch in seconds.\n      if (prevRecTE == 0) {\n        // The very first exception.\n        LOG.warn(\"Failed allocation for \" + (re == null ? \"\" : re.getKey()) + \"; \" + fle);\n      } else {\n        // Record the warning.\n        sinceLastTraceAllocFailCount.increment();\n        sinceLastTraceAllocFailSize.add(re.getData().getSerializedLength());\n        if (currTE - prevRecTE > 60) {\n          LOG.warn(\"Failed allocations since last trace: \" + \" count: \" + sinceLastTraceAllocFailCount\n            + \" size: \" + sinceLastTraceAllocFailSize);\n          LOG.warn(\"Most recent failed allocation: key: \" + (re == null ? \"\" : re.getKey()) + \"; \" + fle);\n          sinceLastTraceAllocFailCount.reset();\n          sinceLastTraceAllocFailSize.reset();\n        }\n      }\n      // Presume can't add. Too big? Move index on. Entry will be cleared from ramCache below.\n      bucketEntries[index] = null;\n      index++;\n      prevRecTE = currTE;\n    } catch (CacheFullException cfe) {\n      // Cache full when we tried to add. Try freeing space and then retrying (don't up index)\n      if (!freeInProgress) {\n        freeSpace(\"Full!\");\n      } else {\n        Thread.sleep(50);\n      }\n    } catch (IOException ioex) {\n      // Hopefully transient. Retry. checkIOErrorIsTolerated disables cache if problem.\n      LOG.error(\"Failed writing to bucket cache\", ioex);\n      checkIOErrorIsTolerated();\n    }\n  }\n\n  // Make sure data pages are written on media before we update maps.\n  try {\n    ioEngine.sync();\n  } catch (IOException ioex) {\n    LOG.error(\"Failed syncing IO engine\", ioex);\n    checkIOErrorIsTolerated();\n    // Since we failed sync, free the blocks in bucket allocator\n    for (int i = 0; i < entries.size(); ++i) {\n      if (bucketEntries[i] != null) {\n        bucketAllocator.freeBlock(bucketEntries[i].offset());\n        bucketEntries[i] = null;\n      }\n    }\n  }\n\n  // Now add to backingMap if successfully added to bucket cache.  Remove from ramCache if\n  // success or error.\n  for (int i = 0; i < size; ++i) {\n    BlockCacheKey key = entries.get(i).getKey();\n    // Only add if non-null entry.\n    if (bucketEntries[i] != null) {\n      putIntoBackingMap(key, bucketEntries[i]);\n    }\n    // Always remove from ramCache even if we failed adding it to the block cache above.\n    boolean existed = ramCache.remove(key, re -> {\n      if (re != null) {\n        heapSize.add(-1 * re.getData().heapSize());\n      }\n    });\n    if (!existed && bucketEntries[i] != null) {\n      // Block should have already been evicted. Remove it and free space.\n      final BucketEntry bucketEntry = bucketEntries[i];\n      bucketEntry.withWriteLock(offsetLock, () -> {\n        if (backingMap.remove(key, bucketEntry)) {\n          blockEvicted(key, bucketEntry, false);\n        }\n        return null;\n      });\n    }\n  }\n\n  long used = bucketAllocator.getUsedSize();\n  if (used > acceptableSize()) {\n    freeSpace(\"Used=\" + used + \" > acceptable=\" + acceptableSize());\n  }\n  return;\n}",
        "reject_response": "void doDrain(final List<RAMQueueEntry> entries) throws InterruptedException {\n  if (entries.isEmpty()) {\n    return;\n  }\n  // This method is a little hard to follow. We run through the passed in entries and for each\n  // successful add, we add a non-null BucketEntry to the below bucketEntries.  Later we must\n  // do cleanup making sure we've cleared ramCache of all entries regardless of whether we\n  // successfully added the item to the bucketcache; if we don't do the cleanup, we'll OOME by\n  // filling ramCache.  We do the clean up by again running through the passed in entries\n  // doing extra work when we find a non-null bucketEntries corresponding entry.\n  final int size = entries.size();\n  BucketEntry[] bucketEntries = new BucketEntry[size];\n  // Index updated inside loop if success or if we can't succeed. We retry if cache is full\n  // when we go to add an entry by going around the loop again without upping the index.\n  int index = 0;\n  while (cacheEnabled && index < size) {\n    RAMQueueEntry re = null;\n    try {\n      re = entries.get(index);\n      if (re == null) {\n        LOG.warn(\"Couldn't get entry or changed on us; who else is messing with it?\");\n        index++;\n        continue;\n      }\n      BucketEntry bucketEntry = re.writeToCache(ioEngine, bucketAllocator, realCacheSize);\n      // Successfully added. Up index and add bucketEntry. Clear io exceptions.\n      bucketEntries[index] = bucketEntry;\n      if (ioErrorStartTime > 0) {\n        ioErrorStartTime = -1;\n      }\n      index++;\n    } catch (BucketAllocatorException fle) {\n      long currTE = System.currentTimeMillis()/1000; // Current time since Epoch in seconds.\n      if (prevRecTE == 0) {\n        // The very first exception.\n        LOG.warn(\"Failed allocation for \" + (re == null ? \"\" : re.getKey()) + \"; \" + fle);\n      } else {\n        // Record the warning.\n        sinceLastTraceAllocFailCount.increment();\n        sinceLastTraceAllocFailSize.add(re.getData().getSerializedLength());\n        if (currTE - prevRecTE > 60) {\n          LOG.warn(\"Failed allocations since last trace: \" + \" count: \" + sinceLastTraceAllocFailCount\n            + \" size: \" + sinceLastTraceAllocFailSize);\n      LOG.warn(\"Failed allocation for \" + (re == null ? \"\" : re.getKey()) + \"; \" + fle);\n          sinceLastTraceAllocFailCount.reset();\n          sinceLastTraceAllocFailSize.reset();\n        }\n      }\n      // Presume can't add. Too big? Move index on. Entry will be cleared from ramCache below.\n      bucketEntries[index] = null;\n      index++;\n      prevRecTE = currTE;\n    } catch (CacheFullException cfe) {\n      // Cache full when we tried to add. Try freeing space and then retrying (don't up index)\n      if (!freeInProgress) {\n        freeSpace(\"Full!\");\n      } else {\n        Thread.sleep(50);\n      }\n    } catch (IOException ioex) {\n      // Hopefully transient. Retry. checkIOErrorIsTolerated disables cache if problem.\n      LOG.error(\"Failed writing to bucket cache\", ioex);\n      checkIOErrorIsTolerated();\n    }\n  }\n\n  // Make sure data pages are written on media before we update maps.\n  try {\n    ioEngine.sync();\n  } catch (IOException ioex) {\n    LOG.error(\"Failed syncing IO engine\", ioex);\n    checkIOErrorIsTolerated();\n    // Since we failed sync, free the blocks in bucket allocator\n    for (int i = 0; i < entries.size(); ++i) {\n      if (bucketEntries[i] != null) {\n        bucketAllocator.freeBlock(bucketEntries[i].offset());\n        bucketEntries[i] = null;\n      }\n    }\n  }\n\n  // Now add to backingMap if successfully added to bucket cache.  Remove from ramCache if\n  // success or error.\n  for (int i = 0; i < size; ++i) {\n    BlockCacheKey key = entries.get(i).getKey();\n    // Only add if non-null entry.\n    if (bucketEntries[i] != null) {\n      putIntoBackingMap(key, bucketEntries[i]);\n    }\n    // Always remove from ramCache even if we failed adding it to the block cache above.\n    boolean existed = ramCache.remove(key, re -> {\n      if (re != null) {\n        heapSize.add(-1 * re.getData().heapSize());\n      }\n    });\n    if (!existed && bucketEntries[i] != null) {\n      // Block should have already been evicted. Remove it and free space.\n      final BucketEntry bucketEntry = bucketEntries[i];\n      bucketEntry.withWriteLock(offsetLock, () -> {\n        if (backingMap.remove(key, bucketEntry)) {\n          blockEvicted(key, bucketEntry, false);\n        }\n        return null;\n      });\n    }\n  }\n\n  long used = bucketAllocator.getUsedSize();\n  if (used > acceptableSize()) {\n    freeSpace(\"Used=\" + used + \" > acceptable=\" + acceptableSize());\n  }\n  return;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2450,
        "instruction": "@Override\npublic void replayCommands(final int otherLastConfirmedCommandID) {\n   if (resendCache != null) {\n      clearUpTo(otherLastConfirmedCommandID);\n\n      for (final Packet packet : resendCache) {\n         doWrite(packet);\n      }\n   }\n}",
        "accept_response": "@Override\npublic void replayCommands(final int otherLastConfirmedCommandID) {\n   if (resendCache != null) {\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" Replaying commands on channelID=\" + id);\n      }\n      clearUpTo(otherLastConfirmedCommandID);\n\n      for (final Packet packet : resendCache) {\n         doWrite(packet);\n      }\n   }\n}",
        "reject_response": "@Override\npublic void replayCommands(final int otherLastConfirmedCommandID) {\n   if (resendCache != null) {\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"Replaying commands on channelID=\" + id);\n      }\n      clearUpTo(otherLastConfirmedCommandID);\n\n      for (final Packet packet : resendCache) {\n         doWrite(packet);\n      }\n   }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2942,
        "instruction": "public static boolean validateIfIssueManagementComplete( MavenProject project, String issueManagementSystem,\n                                                         String mojoResult, Log log )\n{\n    if ( project.getIssueManagement() == null )\n    {\n        log.error( \"No Issue Management set. No \" + mojoResult + \" will be generated.\" );\n\n        return false;\n    }\n    else if ( ( project.getIssueManagement().getUrl() == null )\n        || ( project.getIssueManagement().getUrl().trim().equals( \"\" ) ) )\n    {\n        log.error( \"No URL set in Issue Management. No \" + mojoResult + \" will be generated.\" );\n\n        return false;\n    }\n    else if ( ( project.getIssueManagement().getSystem() == null )\n        || ( project.getIssueManagement().getSystem().trim().equals(\"\") ) )\n    {\n        log.error( \"No System set in Issue Management. No \" + mojoResult + \" will be generated.\" );\n\n        return false;\n    }\n    else if ( !( project.getIssueManagement().getSystem().equalsIgnoreCase( issueManagementSystem ) ) )\n    {\n\n        return false;\n    }\n    return true;\n}",
        "accept_response": "public static boolean validateIfIssueManagementComplete( MavenProject project, String issueManagementSystem,\n                                                         String mojoResult, Log log )\n{\n    if ( project.getIssueManagement() == null )\n    {\n        log.error( \"No Issue Management set. No \" + mojoResult + \" will be generated.\" );\n\n        return false;\n    }\n    else if ( ( project.getIssueManagement().getUrl() == null )\n        || ( project.getIssueManagement().getUrl().trim().equals( \"\" ) ) )\n    {\n        log.error( \"No URL set in Issue Management. No \" + mojoResult + \" will be generated.\" );\n\n        return false;\n    }\n    else if ( ( project.getIssueManagement().getSystem() == null )\n        || ( project.getIssueManagement().getSystem().trim().equals(\"\") ) )\n    {\n        log.error( \"No System set in Issue Management. No \" + mojoResult + \" will be generated.\" );\n\n        return false;\n    }\n    else if ( !( project.getIssueManagement().getSystem().equalsIgnoreCase( issueManagementSystem ) ) )\n    {\n        log.debug( \"The \" + mojoResult + \" only supports \" + issueManagementSystem + \".  No \" + mojoResult\n            + \" will be generated.\" );\n\n        return false;\n    }\n    return true;\n}",
        "reject_response": "public static boolean validateIfIssueManagementComplete( MavenProject project, String issueManagementSystem,\n                                                         String mojoResult, Log log )\n{\n    if ( project.getIssueManagement() == null )\n    {\n        log.error( \"No Issue Management set. No \" + mojoResult + \" will be generated.\" );\n\n        return false;\n    }\n    else if ( ( project.getIssueManagement().getUrl() == null )\n        || ( project.getIssueManagement().getUrl().trim().equals( \"\" ) ) )\n    {\n        log.error( \"No URL set in Issue Management. No \" + mojoResult + \" will be generated.\" );\n\n        return false;\n    }\n    else if ( ( project.getIssueManagement().getSystem() == null )\n        || ( project.getIssueManagement().getSystem().trim().equals(\"\") ) )\n    {\n        log.error( \"No System set in Issue Management. No \" + mojoResult + \" will be generated.\" );\n\n        return false;\n    }\n    else if ( !( project.getIssueManagement().getSystem().equalsIgnoreCase( issueManagementSystem ) ) )\n    {\n        log.error( \"The \" + mojoResult + \" only supports \" + issueManagementSystem + \".  No \" + mojoResult\n\n        return false;\n    }\n    return true;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2380,
        "instruction": "public static MasterClientService.Client getConnection(ClientContext context) {\n  checkArgument(context != null, \"context is null\");\n\n  List<String> locations = context.getInstance().getMasterLocations();\n\n  if (locations.size() == 0) {\n    log.debug(\"No masters...\");\n    return null;\n  }\n\n  HostAndPort master = HostAndPort.fromString(locations.get(0));\n  if (0 == master.getPort())\n    return null;\n\n  try {\n    // Master requests can take a long time: don't ever time out\n    MasterClientService.Client client = ThriftUtil.getClientNoTimeout(new MasterClientService.Client.Factory(), master, context);\n    return client;\n  } catch (TTransportException tte) {\n    Throwable cause = tte.getCause();\n    if (null != cause && cause instanceof UnknownHostException) {\n      // do not expect to recover from this\n      throw new RuntimeException(tte);\n    }\n    return null;\n  }\n}",
        "accept_response": "public static MasterClientService.Client getConnection(ClientContext context) {\n  checkArgument(context != null, \"context is null\");\n\n  List<String> locations = context.getInstance().getMasterLocations();\n\n  if (locations.size() == 0) {\n    log.debug(\"No masters...\");\n    return null;\n  }\n\n  HostAndPort master = HostAndPort.fromString(locations.get(0));\n  if (0 == master.getPort())\n    return null;\n\n  try {\n    // Master requests can take a long time: don't ever time out\n    MasterClientService.Client client = ThriftUtil.getClientNoTimeout(new MasterClientService.Client.Factory(), master, context);\n    return client;\n  } catch (TTransportException tte) {\n    Throwable cause = tte.getCause();\n    if (null != cause && cause instanceof UnknownHostException) {\n      // do not expect to recover from this\n      throw new RuntimeException(tte);\n    }\n    log.debug(\"Failed to connect to master={}, will retry... \", master, tte);\n    return null;\n  }\n}",
        "reject_response": "public static MasterClientService.Client getConnection(ClientContext context) {\n  checkArgument(context != null, \"context is null\");\n\n  List<String> locations = context.getInstance().getMasterLocations();\n\n  if (locations.size() == 0) {\n    log.debug(\"No masters...\");\n    return null;\n  }\n\n  HostAndPort master = HostAndPort.fromString(locations.get(0));\n  if (0 == master.getPort())\n    return null;\n\n  try {\n    // Master requests can take a long time: don't ever time out\n    MasterClientService.Client client = ThriftUtil.getClientNoTimeout(new MasterClientService.Client.Factory(), master, context);\n    return client;\n  } catch (TTransportException tte) {\n    Throwable cause = tte.getCause();\n    if (null != cause && cause instanceof UnknownHostException) {\n      // do not expect to recover from this\n      throw new RuntimeException(tte);\n    }\n    log.debug(\"Failed to connect to master=\" + master + \", will retry... \", tte);\n    return null;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2962,
        "instruction": "@Override\npublic int run(String[] args) throws Exception {\n  if (args.length < 1) {\n    System.err\n        .println(\"Usage: CrawlDb <crawldb> (-dir <segments> | <seg1> <seg2> ...) [-force] [-normalize] [-filter] [-noAdditions]\");\n    System.err.println(\"\\tcrawldb\\tCrawlDb to update\");\n    System.err\n        .println(\"\\t-dir segments\\tparent directory containing all segments to update from\");\n    System.err\n        .println(\"\\tseg1 seg2 ...\\tlist of segment names to update from\");\n    System.err\n        .println(\"\\t-force\\tforce update even if CrawlDb appears to be locked (CAUTION advised)\");\n    System.err\n        .println(\"\\t-normalize\\tuse URLNormalizer on urls in CrawlDb and segment (usually not needed)\");\n    System.err\n        .println(\"\\t-filter\\tuse URLFilters on urls in CrawlDb and segment\");\n    System.err\n        .println(\"\\t-noAdditions\\tonly update already existing URLs, don't add any newly discovered URLs\");\n\n    return -1;\n  }\n  boolean normalize = getConf().getBoolean(CrawlDbFilter.URL_NORMALIZING,\n      false);\n  boolean filter = getConf().getBoolean(CrawlDbFilter.URL_FILTERING, false);\n  boolean additionsAllowed = getConf().getBoolean(CRAWLDB_ADDITIONS_ALLOWED,\n      true);\n  boolean force = false;\n  HashSet<Path> dirs = new HashSet<>();\n  for (int i = 1; i < args.length; i++) {\n    if (args[i].equals(\"-normalize\")) {\n      normalize = true;\n    } else if (args[i].equals(\"-filter\")) {\n      filter = true;\n    } else if (args[i].equals(\"-force\")) {\n      force = true;\n    } else if (args[i].equals(\"-noAdditions\")) {\n      additionsAllowed = false;\n    } else if (args[i].equals(\"-dir\")) {\n      Path dirPath = new Path(args[++i]);\n      FileSystem fs = dirPath.getFileSystem(getConf());\n      FileStatus[] paths = fs.listStatus(dirPath,\n          HadoopFSUtil.getPassDirectoriesFilter(fs));\n      dirs.addAll(Arrays.asList(HadoopFSUtil.getPaths(paths)));\n    } else {\n      dirs.add(new Path(args[i]));\n    }\n  }\n  try {\n    update(new Path(args[0]), dirs.toArray(new Path[dirs.size()]), normalize,\n        filter, additionsAllowed, force);\n    return 0;\n  } catch (Exception e) {\n    return -1;\n  }\n}",
        "accept_response": "@Override\npublic int run(String[] args) throws Exception {\n  if (args.length < 1) {\n    System.err\n        .println(\"Usage: CrawlDb <crawldb> (-dir <segments> | <seg1> <seg2> ...) [-force] [-normalize] [-filter] [-noAdditions]\");\n    System.err.println(\"\\tcrawldb\\tCrawlDb to update\");\n    System.err\n        .println(\"\\t-dir segments\\tparent directory containing all segments to update from\");\n    System.err\n        .println(\"\\tseg1 seg2 ...\\tlist of segment names to update from\");\n    System.err\n        .println(\"\\t-force\\tforce update even if CrawlDb appears to be locked (CAUTION advised)\");\n    System.err\n        .println(\"\\t-normalize\\tuse URLNormalizer on urls in CrawlDb and segment (usually not needed)\");\n    System.err\n        .println(\"\\t-filter\\tuse URLFilters on urls in CrawlDb and segment\");\n    System.err\n        .println(\"\\t-noAdditions\\tonly update already existing URLs, don't add any newly discovered URLs\");\n\n    return -1;\n  }\n  boolean normalize = getConf().getBoolean(CrawlDbFilter.URL_NORMALIZING,\n      false);\n  boolean filter = getConf().getBoolean(CrawlDbFilter.URL_FILTERING, false);\n  boolean additionsAllowed = getConf().getBoolean(CRAWLDB_ADDITIONS_ALLOWED,\n      true);\n  boolean force = false;\n  HashSet<Path> dirs = new HashSet<>();\n  for (int i = 1; i < args.length; i++) {\n    if (args[i].equals(\"-normalize\")) {\n      normalize = true;\n    } else if (args[i].equals(\"-filter\")) {\n      filter = true;\n    } else if (args[i].equals(\"-force\")) {\n      force = true;\n    } else if (args[i].equals(\"-noAdditions\")) {\n      additionsAllowed = false;\n    } else if (args[i].equals(\"-dir\")) {\n      Path dirPath = new Path(args[++i]);\n      FileSystem fs = dirPath.getFileSystem(getConf());\n      FileStatus[] paths = fs.listStatus(dirPath,\n          HadoopFSUtil.getPassDirectoriesFilter(fs));\n      dirs.addAll(Arrays.asList(HadoopFSUtil.getPaths(paths)));\n    } else {\n      dirs.add(new Path(args[i]));\n    }\n  }\n  try {\n    update(new Path(args[0]), dirs.toArray(new Path[dirs.size()]), normalize,\n        filter, additionsAllowed, force);\n    return 0;\n  } catch (Exception e) {\n    LOG.error(\"CrawlDb update: \", e);\n    return -1;\n  }\n}",
        "reject_response": "@Override\npublic int run(String[] args) throws Exception {\n  if (args.length < 1) {\n    System.err\n        .println(\"Usage: CrawlDb <crawldb> (-dir <segments> | <seg1> <seg2> ...) [-force] [-normalize] [-filter] [-noAdditions]\");\n    System.err.println(\"\\tcrawldb\\tCrawlDb to update\");\n    System.err\n        .println(\"\\t-dir segments\\tparent directory containing all segments to update from\");\n    System.err\n        .println(\"\\tseg1 seg2 ...\\tlist of segment names to update from\");\n    System.err\n        .println(\"\\t-force\\tforce update even if CrawlDb appears to be locked (CAUTION advised)\");\n    System.err\n        .println(\"\\t-normalize\\tuse URLNormalizer on urls in CrawlDb and segment (usually not needed)\");\n    System.err\n        .println(\"\\t-filter\\tuse URLFilters on urls in CrawlDb and segment\");\n    System.err\n        .println(\"\\t-noAdditions\\tonly update already existing URLs, don't add any newly discovered URLs\");\n\n    return -1;\n  }\n  boolean normalize = getConf().getBoolean(CrawlDbFilter.URL_NORMALIZING,\n      false);\n  boolean filter = getConf().getBoolean(CrawlDbFilter.URL_FILTERING, false);\n  boolean additionsAllowed = getConf().getBoolean(CRAWLDB_ADDITIONS_ALLOWED,\n      true);\n  boolean force = false;\n  HashSet<Path> dirs = new HashSet<>();\n  for (int i = 1; i < args.length; i++) {\n    if (args[i].equals(\"-normalize\")) {\n      normalize = true;\n    } else if (args[i].equals(\"-filter\")) {\n      filter = true;\n    } else if (args[i].equals(\"-force\")) {\n      force = true;\n    } else if (args[i].equals(\"-noAdditions\")) {\n      additionsAllowed = false;\n    } else if (args[i].equals(\"-dir\")) {\n      Path dirPath = new Path(args[++i]);\n      FileSystem fs = dirPath.getFileSystem(getConf());\n      FileStatus[] paths = fs.listStatus(dirPath,\n          HadoopFSUtil.getPassDirectoriesFilter(fs));\n      dirs.addAll(Arrays.asList(HadoopFSUtil.getPaths(paths)));\n    } else {\n      dirs.add(new Path(args[i]));\n    }\n  }\n  try {\n    update(new Path(args[0]), dirs.toArray(new Path[dirs.size()]), normalize,\n        filter, additionsAllowed, force);\n    return 0;\n  } catch (Exception e) {\n    LOG.error(\"CrawlDb update: \" + StringUtils.stringifyException(e));\n    return -1;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3050,
        "instruction": "public void updateLeaderInitiatedRecoveryState(String collection, String shardId, String coreNodeName,\n    Replica.State state, CoreDescriptor leaderCd, boolean retryOnConnLoss) {\n  if (collection == null || shardId == null || coreNodeName == null) {\n    log.warn(\"Cannot set leader-initiated recovery state znode to \"\n        + state.toString() + \" using: collection=\" + collection\n        + \"; shardId=\" + shardId + \"; coreNodeName=\" + coreNodeName);\n    return; // if we don't have complete data about a core in cloud mode, do nothing\n  }\n\n  assert leaderCd != null;\n  assert leaderCd.getCloudDescriptor() != null;\n\n  String leaderCoreNodeName = leaderCd.getCloudDescriptor().getCoreNodeName();\n\n  String znodePath = getLeaderInitiatedRecoveryZnodePath(collection, shardId, coreNodeName);\n\n  if (state == Replica.State.ACTIVE) {\n    // since we're marking it active, we don't need this znode anymore, so delete instead of update\n    try {\n      zkClient.delete(znodePath, -1, retryOnConnLoss);\n    } catch (Exception justLogIt) {\n      log.warn(\"Failed to delete znode \" + znodePath, justLogIt);\n    }\n    return;\n  }\n\n  Map<String, Object> stateObj = null;\n  try {\n    stateObj = getLeaderInitiatedRecoveryStateObject(collection, shardId, coreNodeName);\n  } catch (Exception exc) {\n    log.warn(exc.getMessage(), exc);\n  }\n  if (stateObj == null) {\n    stateObj = Utils.makeMap();\n  }\n\n  stateObj.put(ZkStateReader.STATE_PROP, state.toString());\n  // only update the createdBy value if it's not set\n  if (stateObj.get(\"createdByNodeName\") == null) {\n    stateObj.put(\"createdByNodeName\", this.nodeName);\n  }\n  if (stateObj.get(\"createdByCoreNodeName\") == null && leaderCoreNodeName != null)  {\n    stateObj.put(\"createdByCoreNodeName\", leaderCoreNodeName);\n  }\n\n  byte[] znodeData = Utils.toJSON(stateObj);\n\n  try {\n    if (state == Replica.State.DOWN) {\n      markShardAsDownIfLeader(collection, shardId, leaderCd, znodePath, znodeData, retryOnConnLoss);\n    } else {\n      // must retry on conn loss otherwise future election attempts may assume wrong LIR state\n      if (zkClient.exists(znodePath, true)) {\n        zkClient.setData(znodePath, znodeData, retryOnConnLoss);\n      } else {\n        zkClient.makePath(znodePath, znodeData, retryOnConnLoss);\n      }\n    }\n  } catch (Exception exc) {\n    if (exc instanceof SolrException) {\n      throw (SolrException) exc;\n    } else {\n      throw new SolrException(ErrorCode.SERVER_ERROR,\n          \"Failed to update data to \" + state.toString() + \" for znode: \" + znodePath, exc);\n    }\n  }\n}",
        "accept_response": "public void updateLeaderInitiatedRecoveryState(String collection, String shardId, String coreNodeName,\n    Replica.State state, CoreDescriptor leaderCd, boolean retryOnConnLoss) {\n  if (collection == null || shardId == null || coreNodeName == null) {\n    log.warn(\"Cannot set leader-initiated recovery state znode to \"\n        + state.toString() + \" using: collection=\" + collection\n        + \"; shardId=\" + shardId + \"; coreNodeName=\" + coreNodeName);\n    return; // if we don't have complete data about a core in cloud mode, do nothing\n  }\n\n  assert leaderCd != null;\n  assert leaderCd.getCloudDescriptor() != null;\n\n  String leaderCoreNodeName = leaderCd.getCloudDescriptor().getCoreNodeName();\n\n  String znodePath = getLeaderInitiatedRecoveryZnodePath(collection, shardId, coreNodeName);\n\n  if (state == Replica.State.ACTIVE) {\n    // since we're marking it active, we don't need this znode anymore, so delete instead of update\n    try {\n      zkClient.delete(znodePath, -1, retryOnConnLoss);\n    } catch (Exception justLogIt) {\n      log.warn(\"Failed to delete znode \" + znodePath, justLogIt);\n    }\n    return;\n  }\n\n  Map<String, Object> stateObj = null;\n  try {\n    stateObj = getLeaderInitiatedRecoveryStateObject(collection, shardId, coreNodeName);\n  } catch (Exception exc) {\n    log.warn(exc.getMessage(), exc);\n  }\n  if (stateObj == null) {\n    stateObj = Utils.makeMap();\n  }\n\n  stateObj.put(ZkStateReader.STATE_PROP, state.toString());\n  // only update the createdBy value if it's not set\n  if (stateObj.get(\"createdByNodeName\") == null) {\n    stateObj.put(\"createdByNodeName\", this.nodeName);\n  }\n  if (stateObj.get(\"createdByCoreNodeName\") == null && leaderCoreNodeName != null)  {\n    stateObj.put(\"createdByCoreNodeName\", leaderCoreNodeName);\n  }\n\n  byte[] znodeData = Utils.toJSON(stateObj);\n\n  try {\n    if (state == Replica.State.DOWN) {\n      markShardAsDownIfLeader(collection, shardId, leaderCd, znodePath, znodeData, retryOnConnLoss);\n    } else {\n      // must retry on conn loss otherwise future election attempts may assume wrong LIR state\n      if (zkClient.exists(znodePath, true)) {\n        zkClient.setData(znodePath, znodeData, retryOnConnLoss);\n      } else {\n        zkClient.makePath(znodePath, znodeData, retryOnConnLoss);\n      }\n    }\n    log.debug(\"Wrote {} to {}\", state, znodePath);\n  } catch (Exception exc) {\n    if (exc instanceof SolrException) {\n      throw (SolrException) exc;\n    } else {\n      throw new SolrException(ErrorCode.SERVER_ERROR,\n          \"Failed to update data to \" + state.toString() + \" for znode: \" + znodePath, exc);\n    }\n  }\n}",
        "reject_response": "public void updateLeaderInitiatedRecoveryState(String collection, String shardId, String coreNodeName,\n    Replica.State state, CoreDescriptor leaderCd, boolean retryOnConnLoss) {\n  if (collection == null || shardId == null || coreNodeName == null) {\n    log.warn(\"Cannot set leader-initiated recovery state znode to \"\n        + state.toString() + \" using: collection=\" + collection\n        + \"; shardId=\" + shardId + \"; coreNodeName=\" + coreNodeName);\n    return; // if we don't have complete data about a core in cloud mode, do nothing\n  }\n\n  assert leaderCd != null;\n  assert leaderCd.getCloudDescriptor() != null;\n\n  String leaderCoreNodeName = leaderCd.getCloudDescriptor().getCoreNodeName();\n\n  String znodePath = getLeaderInitiatedRecoveryZnodePath(collection, shardId, coreNodeName);\n\n  if (state == Replica.State.ACTIVE) {\n    // since we're marking it active, we don't need this znode anymore, so delete instead of update\n    try {\n      zkClient.delete(znodePath, -1, retryOnConnLoss);\n    } catch (Exception justLogIt) {\n      log.warn(\"Failed to delete znode \" + znodePath, justLogIt);\n    }\n    return;\n  }\n\n  Map<String, Object> stateObj = null;\n  try {\n    stateObj = getLeaderInitiatedRecoveryStateObject(collection, shardId, coreNodeName);\n  } catch (Exception exc) {\n    log.warn(exc.getMessage(), exc);\n  }\n  if (stateObj == null) {\n    stateObj = Utils.makeMap();\n  }\n\n  stateObj.put(ZkStateReader.STATE_PROP, state.toString());\n  // only update the createdBy value if it's not set\n  if (stateObj.get(\"createdByNodeName\") == null) {\n    stateObj.put(\"createdByNodeName\", this.nodeName);\n  }\n  if (stateObj.get(\"createdByCoreNodeName\") == null && leaderCoreNodeName != null)  {\n    stateObj.put(\"createdByCoreNodeName\", leaderCoreNodeName);\n  }\n\n  byte[] znodeData = Utils.toJSON(stateObj);\n\n  try {\n    if (state == Replica.State.DOWN) {\n      markShardAsDownIfLeader(collection, shardId, leaderCd, znodePath, znodeData, retryOnConnLoss);\n    } else {\n      // must retry on conn loss otherwise future election attempts may assume wrong LIR state\n      if (zkClient.exists(znodePath, true)) {\n        zkClient.setData(znodePath, znodeData, retryOnConnLoss);\n      } else {\n        zkClient.makePath(znodePath, znodeData, retryOnConnLoss);\n      }\n    }\n    log.debug(\"Wrote {} to {}\", state.toString(), znodePath);\n  } catch (Exception exc) {\n    if (exc instanceof SolrException) {\n      throw (SolrException) exc;\n    } else {\n      throw new SolrException(ErrorCode.SERVER_ERROR,\n          \"Failed to update data to \" + state.toString() + \" for znode: \" + znodePath, exc);\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2385,
        "instruction": "private static SortedMap<String,String> getMap(Instance instance, boolean nameAsKey) {\n  ZooCache zc = getZooCache(instance);\n\n  List<String> tableIds = zc.getChildren(ZooUtil.getRoot(instance) + Constants.ZTABLES);\n  TreeMap<String,String> tableMap = new TreeMap<String,String>();\n  Map<String,String> namespaceIdToNameMap = new HashMap<String,String>();\n\n  for (String tableId : tableIds) {\n    byte[] tableName = zc.get(ZooUtil.getRoot(instance) + Constants.ZTABLES + \"/\" + tableId + Constants.ZTABLE_NAME);\n    byte[] nId = zc.get(ZooUtil.getRoot(instance) + Constants.ZTABLES + \"/\" + tableId + Constants.ZTABLE_NAMESPACE);\n    String namespaceName = Namespaces.DEFAULT_NAMESPACE;\n    // create fully qualified table name\n    if (nId == null) {\n      namespaceName = null;\n    } else {\n      String namespaceId = new String(nId, UTF_8);\n      if (!namespaceId.equals(Namespaces.DEFAULT_NAMESPACE_ID)) {\n        try {\n          namespaceName = namespaceIdToNameMap.get(namespaceId);\n          if (namespaceName == null) {\n            namespaceName = Namespaces.getNamespaceName(instance, namespaceId);\n            namespaceIdToNameMap.put(namespaceId, namespaceName);\n          }\n        } catch (NamespaceNotFoundException e) {\n          continue;\n        }\n      }\n    }\n    if (tableName != null && namespaceName != null) {\n      String tableNameStr = qualified(new String(tableName, UTF_8), namespaceName);\n      if (nameAsKey)\n        tableMap.put(tableNameStr, tableId);\n      else\n        tableMap.put(tableId, tableNameStr);\n    }\n  }\n\n  return tableMap;\n}",
        "accept_response": "private static SortedMap<String,String> getMap(Instance instance, boolean nameAsKey) {\n  ZooCache zc = getZooCache(instance);\n\n  List<String> tableIds = zc.getChildren(ZooUtil.getRoot(instance) + Constants.ZTABLES);\n  TreeMap<String,String> tableMap = new TreeMap<String,String>();\n  Map<String,String> namespaceIdToNameMap = new HashMap<String,String>();\n\n  for (String tableId : tableIds) {\n    byte[] tableName = zc.get(ZooUtil.getRoot(instance) + Constants.ZTABLES + \"/\" + tableId + Constants.ZTABLE_NAME);\n    byte[] nId = zc.get(ZooUtil.getRoot(instance) + Constants.ZTABLES + \"/\" + tableId + Constants.ZTABLE_NAMESPACE);\n    String namespaceName = Namespaces.DEFAULT_NAMESPACE;\n    // create fully qualified table name\n    if (nId == null) {\n      namespaceName = null;\n    } else {\n      String namespaceId = new String(nId, UTF_8);\n      if (!namespaceId.equals(Namespaces.DEFAULT_NAMESPACE_ID)) {\n        try {\n          namespaceName = namespaceIdToNameMap.get(namespaceId);\n          if (namespaceName == null) {\n            namespaceName = Namespaces.getNamespaceName(instance, namespaceId);\n            namespaceIdToNameMap.put(namespaceId, namespaceName);\n          }\n        } catch (NamespaceNotFoundException e) {\n          log.error(\"Table ({}) contains reference to namespace ({}) that doesn't exist \", tableId, namespaceId, e);\n          continue;\n        }\n      }\n    }\n    if (tableName != null && namespaceName != null) {\n      String tableNameStr = qualified(new String(tableName, UTF_8), namespaceName);\n      if (nameAsKey)\n        tableMap.put(tableNameStr, tableId);\n      else\n        tableMap.put(tableId, tableNameStr);\n    }\n  }\n\n  return tableMap;\n}",
        "reject_response": "private static SortedMap<String,String> getMap(Instance instance, boolean nameAsKey) {\n  ZooCache zc = getZooCache(instance);\n\n  List<String> tableIds = zc.getChildren(ZooUtil.getRoot(instance) + Constants.ZTABLES);\n  TreeMap<String,String> tableMap = new TreeMap<String,String>();\n  Map<String,String> namespaceIdToNameMap = new HashMap<String,String>();\n\n  for (String tableId : tableIds) {\n    byte[] tableName = zc.get(ZooUtil.getRoot(instance) + Constants.ZTABLES + \"/\" + tableId + Constants.ZTABLE_NAME);\n    byte[] nId = zc.get(ZooUtil.getRoot(instance) + Constants.ZTABLES + \"/\" + tableId + Constants.ZTABLE_NAMESPACE);\n    String namespaceName = Namespaces.DEFAULT_NAMESPACE;\n    // create fully qualified table name\n    if (nId == null) {\n      namespaceName = null;\n    } else {\n      String namespaceId = new String(nId, UTF_8);\n      if (!namespaceId.equals(Namespaces.DEFAULT_NAMESPACE_ID)) {\n        try {\n          namespaceName = namespaceIdToNameMap.get(namespaceId);\n          if (namespaceName == null) {\n            namespaceName = Namespaces.getNamespaceName(instance, namespaceId);\n            namespaceIdToNameMap.put(namespaceId, namespaceName);\n          }\n        } catch (NamespaceNotFoundException e) {\n          log.error(\"Table (\" + tableId + \") contains reference to namespace (\" + namespaceId + \") that doesn't exist\", e);\n          continue;\n        }\n      }\n    }\n    if (tableName != null && namespaceName != null) {\n      String tableNameStr = qualified(new String(tableName, UTF_8), namespaceName);\n      if (nameAsKey)\n        tableMap.put(tableNameStr, tableId);\n      else\n        tableMap.put(tableId, tableNameStr);\n    }\n  }\n\n  return tableMap;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2481,
        "instruction": "private void addExposedTypes(GenericArrayType type, Class<?> cause) {\n  if (done(type)) {\n    return;\n  }\n  visit(type);\n  addExposedTypes(type.getGenericComponentType(), cause);\n}",
        "accept_response": "private void addExposedTypes(GenericArrayType type, Class<?> cause) {\n  if (done(type)) {\n    return;\n  }\n  visit(type);\n  LOG.debug(\n      \"Adding exposed types from {}, which is the component type on generic array type {}\",\n      type.getGenericComponentType(),\n      type);\n  addExposedTypes(type.getGenericComponentType(), cause);\n}",
        "reject_response": "private void addExposedTypes(GenericArrayType type, Class<?> cause) {\n  if (done(type)) {\n    return;\n  }\n  visit(type);\n  logger.debug(\n  addExposedTypes(type.getGenericComponentType(), cause);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2582,
        "instruction": "private void handleReportDataIntegrityIssues(final JsonCommand command, final Throwable realCause, final Exception dae) {\n\n    if (realCause.getMessage().contains(\"FOREIGN KEY (`x_registered_table_name`)\")) {\n        final String datatableName = command.stringValueOfParameterNamed(\"datatableName\");\n        throw new PlatformDataIntegrityException(\"error.msg.entityDatatableCheck.foreign.key.constraint\", \"datatable with name '\"\n                + datatableName + \"' do not exist\", \"datatableName\", datatableName);\n    }\n\n    if (realCause.getMessage().contains(\"unique_entity_check\")) {\n        final String datatableName = command.stringValueOfParameterNamed(\"datatableName\");\n        final long status = command.longValueOfParameterNamed(\"status\");\n        final String entity = command.stringValueOfParameterNamed(\"entity\");\n        final long productId = command.longValueOfParameterNamed(\"productId\");\n        throw new EntityDatatableCheckAlreadyExistsException(entity, status, datatableName, productId);\n    }\n\n    throw new PlatformDataIntegrityException(\"error.msg.report.unknown.data.integrity.issue\",\n            \"Unknown data integrity issue with resource: \" + realCause.getMessage());\n}",
        "accept_response": "private void handleReportDataIntegrityIssues(final JsonCommand command, final Throwable realCause, final Exception dae) {\n\n    if (realCause.getMessage().contains(\"FOREIGN KEY (`x_registered_table_name`)\")) {\n        final String datatableName = command.stringValueOfParameterNamed(\"datatableName\");\n        throw new PlatformDataIntegrityException(\"error.msg.entityDatatableCheck.foreign.key.constraint\", \"datatable with name '\"\n                + datatableName + \"' do not exist\", \"datatableName\", datatableName);\n    }\n\n    if (realCause.getMessage().contains(\"unique_entity_check\")) {\n        final String datatableName = command.stringValueOfParameterNamed(\"datatableName\");\n        final long status = command.longValueOfParameterNamed(\"status\");\n        final String entity = command.stringValueOfParameterNamed(\"entity\");\n        final long productId = command.longValueOfParameterNamed(\"productId\");\n        throw new EntityDatatableCheckAlreadyExistsException(entity, status, datatableName, productId);\n    }\n\n    logger.error(\"Error occured.\", dae);\n    throw new PlatformDataIntegrityException(\"error.msg.report.unknown.data.integrity.issue\",\n            \"Unknown data integrity issue with resource: \" + realCause.getMessage());\n}",
        "reject_response": "private void handleReportDataIntegrityIssues(final JsonCommand command, final Throwable realCause, final Exception dae) {\n\n    if (realCause.getMessage().contains(\"FOREIGN KEY (`x_registered_table_name`)\")) {\n        final String datatableName = command.stringValueOfParameterNamed(\"datatableName\");\n        throw new PlatformDataIntegrityException(\"error.msg.entityDatatableCheck.foreign.key.constraint\", \"datatable with name '\"\n                + datatableName + \"' do not exist\", \"datatableName\", datatableName);\n    }\n\n    if (realCause.getMessage().contains(\"unique_entity_check\")) {\n        final String datatableName = command.stringValueOfParameterNamed(\"datatableName\");\n        final long status = command.longValueOfParameterNamed(\"status\");\n        final String entity = command.stringValueOfParameterNamed(\"entity\");\n        final long productId = command.longValueOfParameterNamed(\"productId\");\n        throw new EntityDatatableCheckAlreadyExistsException(entity, status, datatableName, productId);\n    }\n\n    logger.error(dae.getMessage(), dae);\n    throw new PlatformDataIntegrityException(\"error.msg.report.unknown.data.integrity.issue\",\n            \"Unknown data integrity issue with resource: \" + realCause.getMessage());\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2645,
        "instruction": "public void removeAllClusters() {\n\n  Iterator<Map.Entry<String, Cluster>> iter = clusterMap.entrySet().iterator();\n\n  while (iter.hasNext()) {\n    Map.Entry<String, Cluster> entry = iter.next();\n    Cluster c = entry.getValue();\n    String clusterKey = entry.getKey();\n    c.stopThread();\n    iter.remove();\n  }\n}",
        "accept_response": "public void removeAllClusters() {\n\n  Iterator<Map.Entry<String, Cluster>> iter = clusterMap.entrySet().iterator();\n\n  while (iter.hasNext()) {\n    Map.Entry<String, Cluster> entry = iter.next();\n    Cluster c = entry.getValue();\n    String clusterKey = entry.getKey();\n    c.stopThread();\n    iter.remove();\n    logger.info(\"{} : {}\", resourceBundle.getString(\"LOG_MSG_REMOVE_THREAD\"), clusterKey);\n  }\n}",
        "reject_response": "public void removeAllClusters() {\n\n  Iterator<Map.Entry<String, Cluster>> iter = clusterMap.entrySet().iterator();\n\n  while (iter.hasNext()) {\n    Map.Entry<String, Cluster> entry = iter.next();\n    Cluster c = entry.getValue();\n    String clusterKey = entry.getKey();\n    c.stopThread();\n    iter.remove();\n    if (LOGGER.infoEnabled()) {\n      LOGGER.info(\n          resourceBundle.getString(\"LOG_MSG_REMOVE_THREAD\") + \" : \" + clusterKey.toString());\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2854,
        "instruction": "public void makePersistent(final ObjectAdapter adapter,\n        final ToPersistObjectSet toPersistObjectSet) {\n    if (alreadyPersistedOrNotPersistable(adapter)) {\n        return;\n    }\n    CallbackUtils.callCallback(adapter, PersistingCallbackFacet.class);\n    // the callback might have caused the adapter to become persistent.\n    if (alreadyPersistedOrNotPersistable(adapter)) {\n        return;\n    }\n    toPersistObjectSet.addCreateObjectCommand(adapter);\n}",
        "accept_response": "public void makePersistent(final ObjectAdapter adapter,\n        final ToPersistObjectSet toPersistObjectSet) {\n    if (alreadyPersistedOrNotPersistable(adapter)) {\n        return;\n    }\n    if (LOG.isDebugEnabled()) {\n        LOG.debug(\"persist \" + adapter);\n    }\n    CallbackUtils.callCallback(adapter, PersistingCallbackFacet.class);\n    // the callback might have caused the adapter to become persistent.\n    if (alreadyPersistedOrNotPersistable(adapter)) {\n        return;\n    }\n    toPersistObjectSet.addCreateObjectCommand(adapter);\n}",
        "reject_response": "public void makePersistent(final ObjectAdapter adapter,\n        final ToPersistObjectSet toPersistObjectSet) {\n    if (alreadyPersistedOrNotPersistable(adapter)) {\n        return;\n    }\n    if (LOG.isDebugEnabled()) {\n    if (LOG.isInfoEnabled()) {\n        LOG.info(\"persist \" + adapter);\n    }\n    CallbackUtils.callCallback(adapter, PersistingCallbackFacet.class);\n    // the callback might have caused the adapter to become persistent.\n    if (alreadyPersistedOrNotPersistable(adapter)) {\n        return;\n    }\n    toPersistObjectSet.addCreateObjectCommand(adapter);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3140,
        "instruction": "@Override\npublic void initialize() throws Exception {\n  configurationDoneEvent = new VertexConfigurationDoneEvent();\n  configurationDoneEvent.fromProtoStream(CodedInputStream.newInstance(getContext().getUserPayload().deepCopyAsArray()));\n  String vertexName = getContext().getVertexName();\n  if (getContext().getVertexNumTasks(vertexName) == -1) {\n    Preconditions.checkArgument(configurationDoneEvent.isSetParallelismCalled(), \"SetParallelism must be called \"\n        + \"when numTasks is -1\");\n    setParallelismInInitializing = true;\n    getContext().registerForVertexStateUpdates(vertexName,\n        Sets.newHashSet(org.apache.tez.dag.api.event.VertexState.INITIALIZING));\n  }\n  getContext().vertexReconfigurationPlanned();\n}",
        "accept_response": "@Override\npublic void initialize() throws Exception {\n  LOG.debug(\"initialize NoOpVertexManager\");\n  configurationDoneEvent = new VertexConfigurationDoneEvent();\n  configurationDoneEvent.fromProtoStream(CodedInputStream.newInstance(getContext().getUserPayload().deepCopyAsArray()));\n  String vertexName = getContext().getVertexName();\n  if (getContext().getVertexNumTasks(vertexName) == -1) {\n    Preconditions.checkArgument(configurationDoneEvent.isSetParallelismCalled(), \"SetParallelism must be called \"\n        + \"when numTasks is -1\");\n    setParallelismInInitializing = true;\n    getContext().registerForVertexStateUpdates(vertexName,\n        Sets.newHashSet(org.apache.tez.dag.api.event.VertexState.INITIALIZING));\n  }\n  getContext().vertexReconfigurationPlanned();\n}",
        "reject_response": "@Override\npublic void initialize() throws Exception {\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"initialize NoOpVertexManager\");\n  }\n  configurationDoneEvent = new VertexConfigurationDoneEvent();\n  configurationDoneEvent.fromProtoStream(CodedInputStream.newInstance(getContext().getUserPayload().deepCopyAsArray()));\n  String vertexName = getContext().getVertexName();\n  if (getContext().getVertexNumTasks(vertexName) == -1) {\n    Preconditions.checkArgument(configurationDoneEvent.isSetParallelismCalled(), \"SetParallelism must be called \"\n        + \"when numTasks is -1\");\n    setParallelismInInitializing = true;\n    getContext().registerForVertexStateUpdates(vertexName,\n        Sets.newHashSet(org.apache.tez.dag.api.event.VertexState.INITIALIZING));\n  }\n  getContext().vertexReconfigurationPlanned();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2544,
        "instruction": "private static void asyncReadRecordFromLogSegment(\n        final String streamName,\n        final LogSegmentRandomAccessEntryReader reader,\n        final LogSegmentMetadata metadata,\n        final ExecutorService executorService,\n        final int scanStartBatchSize,\n        final int scanMaxBatchSize,\n        final boolean includeControl,\n        final boolean includeEndOfStream,\n        final Promise<LogRecordWithDLSN> promise,\n        final AtomicInteger numRecordsScanned,\n        final LogRecordSelector selector,\n        final boolean backward,\n        final long startEntryId) {\n    final long lastAddConfirmed = reader.getLastAddConfirmed();\n    if (lastAddConfirmed < 0) {\n        promise.setValue(null);\n        return;\n    }\n    final ScanContext context = new ScanContext(\n            startEntryId, lastAddConfirmed,\n            scanStartBatchSize, scanMaxBatchSize,\n            includeControl, includeEndOfStream, backward, numRecordsScanned);\n    asyncReadRecordFromEntries(streamName, reader, metadata, executorService,\n                               promise, context, selector);\n}",
        "accept_response": "private static void asyncReadRecordFromLogSegment(\n        final String streamName,\n        final LogSegmentRandomAccessEntryReader reader,\n        final LogSegmentMetadata metadata,\n        final ExecutorService executorService,\n        final int scanStartBatchSize,\n        final int scanMaxBatchSize,\n        final boolean includeControl,\n        final boolean includeEndOfStream,\n        final Promise<LogRecordWithDLSN> promise,\n        final AtomicInteger numRecordsScanned,\n        final LogRecordSelector selector,\n        final boolean backward,\n        final long startEntryId) {\n    final long lastAddConfirmed = reader.getLastAddConfirmed();\n    if (lastAddConfirmed < 0) {\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Log segment {} is empty for {}.\", new Object[] { metadata, streamName });\n        }\n        promise.setValue(null);\n        return;\n    }\n    final ScanContext context = new ScanContext(\n            startEntryId, lastAddConfirmed,\n            scanStartBatchSize, scanMaxBatchSize,\n            includeControl, includeEndOfStream, backward, numRecordsScanned);\n    asyncReadRecordFromEntries(streamName, reader, metadata, executorService,\n                               promise, context, selector);\n}",
        "reject_response": "private static void asyncReadRecordFromLogSegment(\n        final String streamName,\n        final LogSegmentRandomAccessEntryReader reader,\n        final LogSegmentMetadata metadata,\n        final ExecutorService executorService,\n        final int scanStartBatchSize,\n        final int scanMaxBatchSize,\n        final boolean includeControl,\n        final boolean includeEndOfStream,\n        final Promise<LogRecordWithDLSN> promise,\n        final AtomicInteger numRecordsScanned,\n        final LogRecordSelector selector,\n        final boolean backward,\n        final long startEntryId) {\n    final long lastAddConfirmed = reader.getLastAddConfirmed();\n    if (lastAddConfirmed < 0) {\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Ledger {} is empty for {}.\", new Object[] { ledgerDescriptor, streamName });\n        }\n        promise.setValue(null);\n        return;\n    }\n    final ScanContext context = new ScanContext(\n            startEntryId, lastAddConfirmed,\n            scanStartBatchSize, scanMaxBatchSize,\n            includeControl, includeEndOfStream, backward, numRecordsScanned);\n    asyncReadRecordFromEntries(streamName, reader, metadata, executorService,\n                               promise, context, selector);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3123,
        "instruction": "DAGStatus getDAGStatusViaAM(Set<StatusGetOpts> statusOptions, long timeout)\n    throws IOException, TezException {\n  GetDAGStatusRequestProto.Builder requestProtoBuilder =\n      GetDAGStatusRequestProto.newBuilder()\n        .setDagId(dagId).setTimeout(timeout);\n\n  if (statusOptions != null) {\n    requestProtoBuilder.addAllStatusOptions(\n      DagTypeConverters.convertStatusGetOptsToProto(statusOptions));\n  }\n\n  try {\n    return new DAGStatus(\n      proxy.getDAGStatus(null,\n        requestProtoBuilder.build()).getDagStatus(), DagStatusSource.AM);\n  } catch (ServiceException e) {\n    RPCUtil.unwrapAndThrowException(e);\n    // Should not reach here\n    throw new TezException(e);\n  }\n}",
        "accept_response": "DAGStatus getDAGStatusViaAM(Set<StatusGetOpts> statusOptions, long timeout)\n    throws IOException, TezException {\n  LOG.debug(\"GetDAGStatus via AM for app: {} dag:{}\", appId, dagId);\n  GetDAGStatusRequestProto.Builder requestProtoBuilder =\n      GetDAGStatusRequestProto.newBuilder()\n        .setDagId(dagId).setTimeout(timeout);\n\n  if (statusOptions != null) {\n    requestProtoBuilder.addAllStatusOptions(\n      DagTypeConverters.convertStatusGetOptsToProto(statusOptions));\n  }\n\n  try {\n    return new DAGStatus(\n      proxy.getDAGStatus(null,\n        requestProtoBuilder.build()).getDagStatus(), DagStatusSource.AM);\n  } catch (ServiceException e) {\n    RPCUtil.unwrapAndThrowException(e);\n    // Should not reach here\n    throw new TezException(e);\n  }\n}",
        "reject_response": "DAGStatus getDAGStatusViaAM(Set<StatusGetOpts> statusOptions, long timeout)\n    throws IOException, TezException {\n  if(LOG.isDebugEnabled()) {\n    LOG.debug(\"GetDAGStatus via AM for app: \" + appId + \" dag:\" + dagId);\n  }\n  GetDAGStatusRequestProto.Builder requestProtoBuilder =\n      GetDAGStatusRequestProto.newBuilder()\n        .setDagId(dagId).setTimeout(timeout);\n\n  if (statusOptions != null) {\n    requestProtoBuilder.addAllStatusOptions(\n      DagTypeConverters.convertStatusGetOptsToProto(statusOptions));\n  }\n\n  try {\n    return new DAGStatus(\n      proxy.getDAGStatus(null,\n        requestProtoBuilder.build()).getDagStatus(), DagStatusSource.AM);\n  } catch (ServiceException e) {\n    RPCUtil.unwrapAndThrowException(e);\n    // Should not reach here\n    throw new TezException(e);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2419,
        "instruction": "protected synchronized String findPreferredCollectHost() {\n  if (!isInitializedForHA) {\n    init();\n  }\n\n  shardExpired = false;\n  // Auto expire and re-calculate after 1 hour\n  if (targetCollectorHostSupplier != null) {\n    String targetCollector = targetCollectorHostSupplier.get();\n    if (targetCollector != null) {\n      return targetCollector;\n    }\n  }\n\n  // Reach out to all configured collectors before Zookeeper\n  Collection<String> collectorHosts = getConfiguredCollectorHosts();\n  refreshCollectorsFromConfigured(collectorHosts);\n\n  // Lookup Zookeeper for live hosts - max 10 seconds wait time\n  long currentTime = System.currentTimeMillis();\n  if (allKnownLiveCollectors.size() == 0 && getZookeeperQuorum() != null\n    && (currentTime - lastFailedZkRequestTime) > zookeeperBackoffTimeMillis) {\n\n    LOG.debug(\"No live collectors from configuration. Requesting zookeeper...\");\n    allKnownLiveCollectors.addAll(collectorHAHelper.findLiveCollectorHostsFromZNode());\n    boolean noNewCollectorFromZk = true;\n    for (String collectorHostFromZk : allKnownLiveCollectors) {\n      if (!collectorHosts.contains(collectorHostFromZk)) {\n        noNewCollectorFromZk = false;\n        break;\n      }\n    }\n    if (noNewCollectorFromZk) {\n      lastFailedZkRequestTime = System.currentTimeMillis();\n    }\n  }\n\n  if (allKnownLiveCollectors.size() != 0) {\n    targetCollectorHostSupplier = Suppliers.memoizeWithExpiration(\n      new Supplier<String>() {\n        @Override\n        public String get() {\n          //shardExpired flag is used to determine if the Supplier.get() is invoked through the\n          // findPreferredCollectHost method (No need to refresh collector hosts\n          // OR\n          // through Expiry (Refresh needed to pick up dead collectors that might have not become alive).\n          if (shardExpired) {\n            refreshCollectorsFromConfigured(getConfiguredCollectorHosts());\n          }\n          return metricSinkWriteShardStrategy.findCollectorShard(new ArrayList<>(allKnownLiveCollectors));\n        }\n      },  // random.nextInt(max - min + 1) + min # (60 to 75 minutes)\n      rand.nextInt(COLLECTOR_HOST_CACHE_MAX_EXPIRATION_MINUTES\n        - COLLECTOR_HOST_CACHE_MIN_EXPIRATION_MINUTES + 1)\n        + COLLECTOR_HOST_CACHE_MIN_EXPIRATION_MINUTES,\n      TimeUnit.MINUTES\n    );\n\n    String collectorHost = targetCollectorHostSupplier.get();\n    shardExpired = true;\n    return collectorHost;\n  }\n  LOG.debug(\"Couldn't find any live collectors. Returning null\");\n  shardExpired = true;\n  return null;\n}",
        "accept_response": "protected synchronized String findPreferredCollectHost() {\n  if (!isInitializedForHA) {\n    init();\n  }\n\n  shardExpired = false;\n  // Auto expire and re-calculate after 1 hour\n  if (targetCollectorHostSupplier != null) {\n    String targetCollector = targetCollectorHostSupplier.get();\n    if (targetCollector != null) {\n      return targetCollector;\n    }\n  }\n\n  // Reach out to all configured collectors before Zookeeper\n  Collection<String> collectorHosts = getConfiguredCollectorHosts();\n  refreshCollectorsFromConfigured(collectorHosts);\n\n  // Lookup Zookeeper for live hosts - max 10 seconds wait time\n  long currentTime = System.currentTimeMillis();\n  if (allKnownLiveCollectors.size() == 0 && getZookeeperQuorum() != null\n    && (currentTime - lastFailedZkRequestTime) > zookeeperBackoffTimeMillis) {\n\n    LOG.debug(\"No live collectors from configuration. Requesting zookeeper...\");\n    allKnownLiveCollectors.addAll(collectorHAHelper.findLiveCollectorHostsFromZNode());\n    boolean noNewCollectorFromZk = true;\n    for (String collectorHostFromZk : allKnownLiveCollectors) {\n      if (!collectorHosts.contains(collectorHostFromZk)) {\n        noNewCollectorFromZk = false;\n        break;\n      }\n    }\n    if (noNewCollectorFromZk) {\n      LOG.debug(\"No new collector was found from Zookeeper. Will not request zookeeper for \" + zookeeperBackoffTimeMillis + \" millis\");\n      lastFailedZkRequestTime = System.currentTimeMillis();\n    }\n  }\n\n  if (allKnownLiveCollectors.size() != 0) {\n    targetCollectorHostSupplier = Suppliers.memoizeWithExpiration(\n      new Supplier<String>() {\n        @Override\n        public String get() {\n          //shardExpired flag is used to determine if the Supplier.get() is invoked through the\n          // findPreferredCollectHost method (No need to refresh collector hosts\n          // OR\n          // through Expiry (Refresh needed to pick up dead collectors that might have not become alive).\n          if (shardExpired) {\n            refreshCollectorsFromConfigured(getConfiguredCollectorHosts());\n          }\n          return metricSinkWriteShardStrategy.findCollectorShard(new ArrayList<>(allKnownLiveCollectors));\n        }\n      },  // random.nextInt(max - min + 1) + min # (60 to 75 minutes)\n      rand.nextInt(COLLECTOR_HOST_CACHE_MAX_EXPIRATION_MINUTES\n        - COLLECTOR_HOST_CACHE_MIN_EXPIRATION_MINUTES + 1)\n        + COLLECTOR_HOST_CACHE_MIN_EXPIRATION_MINUTES,\n      TimeUnit.MINUTES\n    );\n\n    String collectorHost = targetCollectorHostSupplier.get();\n    shardExpired = true;\n    return collectorHost;\n  }\n  LOG.debug(\"Couldn't find any live collectors. Returning null\");\n  shardExpired = true;\n  return null;\n}",
        "reject_response": "protected synchronized String findPreferredCollectHost() {\n  if (!isInitializedForHA) {\n    init();\n  }\n\n  shardExpired = false;\n  // Auto expire and re-calculate after 1 hour\n  if (targetCollectorHostSupplier != null) {\n    String targetCollector = targetCollectorHostSupplier.get();\n    if (targetCollector != null) {\n      return targetCollector;\n    }\n  }\n\n  // Reach out to all configured collectors before Zookeeper\n  Collection<String> collectorHosts = getConfiguredCollectorHosts();\n  refreshCollectorsFromConfigured(collectorHosts);\n\n  // Lookup Zookeeper for live hosts - max 10 seconds wait time\n  long currentTime = System.currentTimeMillis();\n  if (allKnownLiveCollectors.size() == 0 && getZookeeperQuorum() != null\n    && (currentTime - lastFailedZkRequestTime) > zookeeperBackoffTimeMillis) {\n\n    LOG.debug(\"No live collectors from configuration. Requesting zookeeper...\");\n    allKnownLiveCollectors.addAll(collectorHAHelper.findLiveCollectorHostsFromZNode());\n    boolean noNewCollectorFromZk = true;\n    for (String collectorHostFromZk : allKnownLiveCollectors) {\n      if (!collectorHosts.contains(collectorHostFromZk)) {\n        noNewCollectorFromZk = false;\n        break;\n      }\n    }\n    if (noNewCollectorFromZk) {\n      LOG.info(\"No new collector was found from Zookeeper. Will not request zookeeper for \" + zookeeperBackoffTimeMillis + \" millis\");\n      lastFailedZkRequestTime = System.currentTimeMillis();\n    }\n  }\n\n  if (allKnownLiveCollectors.size() != 0) {\n    targetCollectorHostSupplier = Suppliers.memoizeWithExpiration(\n      new Supplier<String>() {\n        @Override\n        public String get() {\n          //shardExpired flag is used to determine if the Supplier.get() is invoked through the\n          // findPreferredCollectHost method (No need to refresh collector hosts\n          // OR\n          // through Expiry (Refresh needed to pick up dead collectors that might have not become alive).\n          if (shardExpired) {\n            refreshCollectorsFromConfigured(getConfiguredCollectorHosts());\n          }\n          return metricSinkWriteShardStrategy.findCollectorShard(new ArrayList<>(allKnownLiveCollectors));\n        }\n      },  // random.nextInt(max - min + 1) + min # (60 to 75 minutes)\n      rand.nextInt(COLLECTOR_HOST_CACHE_MAX_EXPIRATION_MINUTES\n        - COLLECTOR_HOST_CACHE_MIN_EXPIRATION_MINUTES + 1)\n        + COLLECTOR_HOST_CACHE_MIN_EXPIRATION_MINUTES,\n      TimeUnit.MINUTES\n    );\n\n    String collectorHost = targetCollectorHostSupplier.get();\n    shardExpired = true;\n    return collectorHost;\n  }\n  LOG.debug(\"Couldn't find any live collectors. Returning null\");\n  shardExpired = true;\n  return null;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3095,
        "instruction": "@Override\npublic TTransport getTransport(final TTransport trans) {\n    try {\n        return Subject.doAs(subject,\n                new PrivilegedExceptionAction<TTransport>() {\n            public TTransport run() {\n                try {\n                    return wrapped.getTransport(trans);\n                }\n                catch (Exception e) {\n                    return new NoOpTTrasport(null);\n                }\n            }\n        });\n    } catch (PrivilegedActionException e) {\n        LOG.error(\"Storm server experienced a PrivilegedActionException exception while creating a transport using a JAAS principal context:\" + e, e);\n        return null;\n    }\n}",
        "accept_response": "@Override\npublic TTransport getTransport(final TTransport trans) {\n    try {\n        return Subject.doAs(subject,\n                new PrivilegedExceptionAction<TTransport>() {\n            public TTransport run() {\n                try {\n                    return wrapped.getTransport(trans);\n                }\n                catch (Exception e) {\n                    LOG.debug(\"Storm server failed to open transport \" +\n                            \"to interact with a client during session initiation: \" + e, e);\n                    return new NoOpTTrasport(null);\n                }\n            }\n        });\n    } catch (PrivilegedActionException e) {\n        LOG.error(\"Storm server experienced a PrivilegedActionException exception while creating a transport using a JAAS principal context:\" + e, e);\n        return null;\n    }\n}",
        "reject_response": "@Override\npublic TTransport getTransport(final TTransport trans) {\n    try {\n        return Subject.doAs(subject,\n                new PrivilegedExceptionAction<TTransport>() {\n            public TTransport run() {\n                try {\n                    return wrapped.getTransport(trans);\n                }\n                catch (Exception e) {\n                    LOG.error(\"Storm server failed to open transport to interact with a client during session initiation: \" + e, e);\n                    return null;\n                    return new NoOpTTrasport(null);\n                }\n            }\n        });\n    } catch (PrivilegedActionException e) {\n        LOG.error(\"Storm server experienced a PrivilegedActionException exception while creating a transport using a JAAS principal context:\" + e, e);\n        return null;\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3025,
        "instruction": "private WindowState<WV> applyFoldFunction(WindowState<WV> existingState, M message) {\n  WV wv;\n  long earliestTimestamp;\n\n  if (existingState == null) {\n    wv = window.getInitializer().get();\n    earliestTimestamp = clock.currentTimeMillis();\n  } else {\n    wv = existingState.getWindowValue();\n    earliestTimestamp = existingState.getEarliestTimestamp();\n  }\n\n  WV newVal = window.getFoldLeftFunction().apply(message, wv);\n  WindowState<WV> newState = new WindowState(newVal, earliestTimestamp);\n\n  return newState;\n}",
        "accept_response": "private WindowState<WV> applyFoldFunction(WindowState<WV> existingState, M message) {\n  WV wv;\n  long earliestTimestamp;\n\n  if (existingState == null) {\n    LOG.trace(\"No existing state found for key. Invoking initializer.\");\n    wv = window.getInitializer().get();\n    earliestTimestamp = clock.currentTimeMillis();\n  } else {\n    wv = existingState.getWindowValue();\n    earliestTimestamp = existingState.getEarliestTimestamp();\n  }\n\n  WV newVal = window.getFoldLeftFunction().apply(message, wv);\n  WindowState<WV> newState = new WindowState(newVal, earliestTimestamp);\n\n  return newState;\n}",
        "reject_response": "private WindowState<WV> applyFoldFunction(WindowState<WV> existingState, M message) {\n  WV wv;\n  long earliestTimestamp;\n\n  if (existingState == null) {\n    LOG.trace(\"No existing state found for key\");\n    wv = window.getInitializer().get();\n    earliestTimestamp = clock.currentTimeMillis();\n  } else {\n    wv = existingState.getWindowValue();\n    earliestTimestamp = existingState.getEarliestTimestamp();\n  }\n\n  WV newVal = window.getFoldLeftFunction().apply(message, wv);\n  WindowState<WV> newState = new WindowState(newVal, earliestTimestamp);\n\n  return newState;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2775,
        "instruction": "@Override\npublic ClassicHttpResponse execute(\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain) throws IOException, HttpException {\n    Args.notNull(request, \"HTTP request\");\n    Args.notNull(scope, \"Scope\");\n\n    final HttpRoute route = scope.route;\n    final HttpClientContext context = scope.clientContext;\n    context.setAttribute(HttpClientContext.HTTP_ROUTE, scope.route);\n    context.setAttribute(HttpClientContext.HTTP_REQUEST, request);\n\n    final URIAuthority authority = request.getAuthority();\n    final String scheme = request.getScheme();\n    final HttpHost target = authority != null ? new HttpHost(scheme, authority) : route.getTargetHost();\n    final String via = generateViaHeader(request);\n\n    // default response context\n    setResponseStatus(context, CacheResponseStatus.CACHE_MISS);\n\n    if (clientRequestsOurOptions(request)) {\n        setResponseStatus(context, CacheResponseStatus.CACHE_MODULE_RESPONSE);\n        return new BasicClassicHttpResponse(HttpStatus.SC_NOT_IMPLEMENTED);\n    }\n\n    final SimpleHttpResponse fatalErrorResponse = getFatallyNoncompliantResponse(request, context);\n    if (fatalErrorResponse != null) {\n        return convert(fatalErrorResponse, scope);\n    }\n\n    requestCompliance.makeRequestCompliant(request);\n    request.addHeader(\"Via\",via);\n\n    if (!cacheableRequestPolicy.isServableFromCache(request)) {\n        LOG.debug(\"Request is not servable from cache\");\n        responseCache.flushCacheEntriesInvalidatedByRequest(target, request);\n        return callBackend(target, request, scope, chain);\n    }\n\n    final HttpCacheEntry entry = responseCache.getCacheEntry(target, request);\n    if (entry == null) {\n        return handleCacheMiss(target, request, scope, chain);\n    } else {\n        return handleCacheHit(target, request, scope, chain, entry);\n    }\n}",
        "accept_response": "@Override\npublic ClassicHttpResponse execute(\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain) throws IOException, HttpException {\n    Args.notNull(request, \"HTTP request\");\n    Args.notNull(scope, \"Scope\");\n\n    final HttpRoute route = scope.route;\n    final HttpClientContext context = scope.clientContext;\n    context.setAttribute(HttpClientContext.HTTP_ROUTE, scope.route);\n    context.setAttribute(HttpClientContext.HTTP_REQUEST, request);\n\n    final URIAuthority authority = request.getAuthority();\n    final String scheme = request.getScheme();\n    final HttpHost target = authority != null ? new HttpHost(scheme, authority) : route.getTargetHost();\n    final String via = generateViaHeader(request);\n\n    // default response context\n    setResponseStatus(context, CacheResponseStatus.CACHE_MISS);\n\n    if (clientRequestsOurOptions(request)) {\n        setResponseStatus(context, CacheResponseStatus.CACHE_MODULE_RESPONSE);\n        return new BasicClassicHttpResponse(HttpStatus.SC_NOT_IMPLEMENTED);\n    }\n\n    final SimpleHttpResponse fatalErrorResponse = getFatallyNoncompliantResponse(request, context);\n    if (fatalErrorResponse != null) {\n        return convert(fatalErrorResponse, scope);\n    }\n\n    requestCompliance.makeRequestCompliant(request);\n    request.addHeader(\"Via\",via);\n\n    if (!cacheableRequestPolicy.isServableFromCache(request)) {\n        LOG.debug(\"Request is not servable from cache\");\n        responseCache.flushCacheEntriesInvalidatedByRequest(target, request);\n        return callBackend(target, request, scope, chain);\n    }\n\n    final HttpCacheEntry entry = responseCache.getCacheEntry(target, request);\n    if (entry == null) {\n        LOG.debug(\"Cache miss\");\n        return handleCacheMiss(target, request, scope, chain);\n    } else {\n        return handleCacheHit(target, request, scope, chain, entry);\n    }\n}",
        "reject_response": "@Override\npublic ClassicHttpResponse execute(\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain) throws IOException, HttpException {\n    Args.notNull(request, \"HTTP request\");\n    Args.notNull(scope, \"Scope\");\n\n    final HttpRoute route = scope.route;\n    final HttpClientContext context = scope.clientContext;\n    context.setAttribute(HttpClientContext.HTTP_ROUTE, scope.route);\n    context.setAttribute(HttpClientContext.HTTP_REQUEST, request);\n\n    final URIAuthority authority = request.getAuthority();\n    final String scheme = request.getScheme();\n    final HttpHost target = authority != null ? new HttpHost(scheme, authority) : route.getTargetHost();\n    final String via = generateViaHeader(request);\n\n    // default response context\n    setResponseStatus(context, CacheResponseStatus.CACHE_MISS);\n\n    if (clientRequestsOurOptions(request)) {\n        setResponseStatus(context, CacheResponseStatus.CACHE_MODULE_RESPONSE);\n        return new BasicClassicHttpResponse(HttpStatus.SC_NOT_IMPLEMENTED);\n    }\n\n    final SimpleHttpResponse fatalErrorResponse = getFatallyNoncompliantResponse(request, context);\n    if (fatalErrorResponse != null) {\n        return convert(fatalErrorResponse, scope);\n    }\n\n    requestCompliance.makeRequestCompliant(request);\n    request.addHeader(\"Via\",via);\n\n    if (!cacheableRequestPolicy.isServableFromCache(request)) {\n        LOG.debug(\"Request is not servable from cache\");\n        responseCache.flushCacheEntriesInvalidatedByRequest(target, request);\n        return callBackend(target, request, scope, chain);\n    }\n\n    final HttpCacheEntry entry = responseCache.getCacheEntry(target, request);\n    if (entry == null) {\n        log.debug(\"Cache miss\");\n        return handleCacheMiss(target, request, scope, chain);\n    } else {\n        return handleCacheHit(target, request, scope, chain, entry);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2834,
        "instruction": "private SegmentArchiveResult archiveSegment(long absIdx) throws IgniteCheckedException {\n    long segIdx = absIdx % dsCfg.getWalSegments();\n\n    File origFile = new File(walWorkDir, FileDescriptor.fileName(segIdx));\n\n    String name = FileDescriptor.fileName(absIdx);\n\n    File dstTmpFile = new File(walArchiveDir, name + \".tmp\");\n\n    File dstFile = new File(walArchiveDir, name);\n\n    if (log.isInfoEnabled())\n\n    try {\n        Files.deleteIfExists(dstTmpFile.toPath());\n\n        Files.copy(origFile.toPath(), dstTmpFile.toPath());\n\n        Files.move(dstTmpFile.toPath(), dstFile.toPath());\n\n        if (mode == WALMode.FSYNC) {\n            try (FileIO f0 = ioFactory.create(dstFile, CREATE, READ, WRITE)) {\n                f0.force();\n            }\n        }\n    }\n    catch (IOException e) {\n        throw new IgniteCheckedException(\"Failed to archive WAL segment [\" +\n            \"srcFile=\" + origFile.getAbsolutePath() +\n            \", dstFile=\" + dstTmpFile.getAbsolutePath() + ']', e);\n    }\n\n    if (log.isInfoEnabled())\n        log.info(\"Copied file [src=\" + origFile.getAbsolutePath() +\n            \", dst=\" + dstFile.getAbsolutePath() + ']');\n\n    return new SegmentArchiveResult(absIdx, origFile, dstFile);\n}",
        "accept_response": "private SegmentArchiveResult archiveSegment(long absIdx) throws IgniteCheckedException {\n    long segIdx = absIdx % dsCfg.getWalSegments();\n\n    File origFile = new File(walWorkDir, FileDescriptor.fileName(segIdx));\n\n    String name = FileDescriptor.fileName(absIdx);\n\n    File dstTmpFile = new File(walArchiveDir, name + \".tmp\");\n\n    File dstFile = new File(walArchiveDir, name);\n\n    if (log.isInfoEnabled())\n        log.info(\"Starting to copy WAL segment [absIdx=\" + absIdx + \", segIdx=\" + segIdx +\n            \", origFile=\" + origFile.getAbsolutePath() + \", dstFile=\" + dstFile.getAbsolutePath() + ']');\n\n    try {\n        Files.deleteIfExists(dstTmpFile.toPath());\n\n        Files.copy(origFile.toPath(), dstTmpFile.toPath());\n\n        Files.move(dstTmpFile.toPath(), dstFile.toPath());\n\n        if (mode == WALMode.FSYNC) {\n            try (FileIO f0 = ioFactory.create(dstFile, CREATE, READ, WRITE)) {\n                f0.force();\n            }\n        }\n    }\n    catch (IOException e) {\n        throw new IgniteCheckedException(\"Failed to archive WAL segment [\" +\n            \"srcFile=\" + origFile.getAbsolutePath() +\n            \", dstFile=\" + dstTmpFile.getAbsolutePath() + ']', e);\n    }\n\n    if (log.isInfoEnabled())\n        log.info(\"Copied file [src=\" + origFile.getAbsolutePath() +\n            \", dst=\" + dstFile.getAbsolutePath() + ']');\n\n    return new SegmentArchiveResult(absIdx, origFile, dstFile);\n}",
        "reject_response": "private SegmentArchiveResult archiveSegment(long absIdx) throws IgniteCheckedException {\n    long segIdx = absIdx % dsCfg.getWalSegments();\n\n    File origFile = new File(walWorkDir, FileDescriptor.fileName(segIdx));\n\n    String name = FileDescriptor.fileName(absIdx);\n\n    File dstTmpFile = new File(walArchiveDir, name + \".tmp\");\n\n    File dstFile = new File(walArchiveDir, name);\n\n    if (log.isInfoEnabled())\n    if (log.isDebugEnabled())\n        log.debug(\"Starting to copy WAL segment [absIdx=\" + absIdx + \", segIdx=\" + segIdx +\n\n    try {\n        Files.deleteIfExists(dstTmpFile.toPath());\n\n        Files.copy(origFile.toPath(), dstTmpFile.toPath());\n\n        Files.move(dstTmpFile.toPath(), dstFile.toPath());\n\n        if (mode == WALMode.FSYNC) {\n            try (FileIO f0 = ioFactory.create(dstFile, CREATE, READ, WRITE)) {\n                f0.force();\n            }\n        }\n    }\n    catch (IOException e) {\n        throw new IgniteCheckedException(\"Failed to archive WAL segment [\" +\n            \"srcFile=\" + origFile.getAbsolutePath() +\n            \", dstFile=\" + dstTmpFile.getAbsolutePath() + ']', e);\n    }\n\n    if (log.isInfoEnabled())\n        log.info(\"Copied file [src=\" + origFile.getAbsolutePath() +\n            \", dst=\" + dstFile.getAbsolutePath() + ']');\n\n    return new SegmentArchiveResult(absIdx, origFile, dstFile);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2974,
        "instruction": "private void internalCopyAllGeneratedToDistributedCache() {\n    if (pigContext.getExecType() == ExecType.LOCAL) {\n        String codePath = codeDir.getAbsolutePath();\n        LOG.info(\"Distributed cache not supported or needed in local mode. Setting key [\"\n                + LOCAL_CODE_DIR + \"] with code temp directory: \" + codePath);\n            conf.set(LOCAL_CODE_DIR, codePath);\n        return;\n    } else {\n        // This let's us avoid NPE in some of the non-traditional pipelines\n        String codePath = codeDir.getAbsolutePath();\n        conf.set(LOCAL_CODE_DIR, codePath);\n    }\n    DistributedCache.createSymlink(conf); // we will read using symlinks\n    StringBuilder serialized = new StringBuilder();\n    boolean first = true;\n    // We attempt to copy over every file in the generated code temp directory\n    for (File f : codeDir.listFiles()) {\n        if (first) {\n            first = false;\n        } else {\n            serialized.append(\",\");\n        }\n        String symlink = f.getName(); //the class name will also be the symlink\n        serialized.append(symlink);\n        Path src = new Path(f.toURI());\n        Path dst;\n        try {\n            dst = FileLocalizer.getTemporaryPath(pigContext);\n        } catch (IOException e) {\n            throw new RuntimeException(\"Error getting temporary path in HDFS\", e);\n        }\n        FileSystem fs;\n        try {\n            fs = dst.getFileSystem(conf);\n        } catch (IOException e) {\n            throw new RuntimeException(\"Unable to get FileSystem\", e);\n        }\n        try {\n            fs.copyFromLocalFile(src, dst);\n        } catch (IOException e) {\n            throw new RuntimeException(\"Unable to copy from local filesystem to HDFS, src = \"\n                    + src + \", dst = \" + dst, e);\n        }\n\n        String destination = dst.toString() + \"#\" + symlink;\n\n        try {\n            DistributedCache.addCacheFile(new URI(destination), conf);\n        } catch (URISyntaxException e) {\n            throw new RuntimeException(\"Unable to add file to distributed cache: \" + destination, e);\n        }\n        LOG.info(\"File successfully added to the distributed cache: \" + symlink);\n    }\n    String toSer = serialized.toString();\n    LOG.info(\"Setting key [\" + GENERATED_CLASSES_KEY + \"] with classes to deserialize [\" + toSer + \"]\");\n    // we must set a key in the job conf so individual jobs know to resolve the shipped classes\n    conf.set(GENERATED_CLASSES_KEY, toSer);\n}",
        "accept_response": "private void internalCopyAllGeneratedToDistributedCache() {\n    LOG.info(\"Starting process to move generated code to distributed cache\");\n    if (pigContext.getExecType() == ExecType.LOCAL) {\n        String codePath = codeDir.getAbsolutePath();\n        LOG.info(\"Distributed cache not supported or needed in local mode. Setting key [\"\n                + LOCAL_CODE_DIR + \"] with code temp directory: \" + codePath);\n            conf.set(LOCAL_CODE_DIR, codePath);\n        return;\n    } else {\n        // This let's us avoid NPE in some of the non-traditional pipelines\n        String codePath = codeDir.getAbsolutePath();\n        conf.set(LOCAL_CODE_DIR, codePath);\n    }\n    DistributedCache.createSymlink(conf); // we will read using symlinks\n    StringBuilder serialized = new StringBuilder();\n    boolean first = true;\n    // We attempt to copy over every file in the generated code temp directory\n    for (File f : codeDir.listFiles()) {\n        if (first) {\n            first = false;\n        } else {\n            serialized.append(\",\");\n        }\n        String symlink = f.getName(); //the class name will also be the symlink\n        serialized.append(symlink);\n        Path src = new Path(f.toURI());\n        Path dst;\n        try {\n            dst = FileLocalizer.getTemporaryPath(pigContext);\n        } catch (IOException e) {\n            throw new RuntimeException(\"Error getting temporary path in HDFS\", e);\n        }\n        FileSystem fs;\n        try {\n            fs = dst.getFileSystem(conf);\n        } catch (IOException e) {\n            throw new RuntimeException(\"Unable to get FileSystem\", e);\n        }\n        try {\n            fs.copyFromLocalFile(src, dst);\n        } catch (IOException e) {\n            throw new RuntimeException(\"Unable to copy from local filesystem to HDFS, src = \"\n                    + src + \", dst = \" + dst, e);\n        }\n\n        String destination = dst.toString() + \"#\" + symlink;\n\n        try {\n            DistributedCache.addCacheFile(new URI(destination), conf);\n        } catch (URISyntaxException e) {\n            throw new RuntimeException(\"Unable to add file to distributed cache: \" + destination, e);\n        }\n        LOG.info(\"File successfully added to the distributed cache: \" + symlink);\n    }\n    String toSer = serialized.toString();\n    LOG.info(\"Setting key [\" + GENERATED_CLASSES_KEY + \"] with classes to deserialize [\" + toSer + \"]\");\n    // we must set a key in the job conf so individual jobs know to resolve the shipped classes\n    conf.set(GENERATED_CLASSES_KEY, toSer);\n}",
        "reject_response": "private void internalCopyAllGeneratedToDistributedCache() {\n    LOG.info(\"Starting process to move generated code to distributed cacche\");\n    if (pigContext.getExecType() == ExecType.LOCAL) {\n        String codePath = codeDir.getAbsolutePath();\n        LOG.info(\"Distributed cache not supported or needed in local mode. Setting key [\"\n                + LOCAL_CODE_DIR + \"] with code temp directory: \" + codePath);\n            conf.set(LOCAL_CODE_DIR, codePath);\n        return;\n    } else {\n        // This let's us avoid NPE in some of the non-traditional pipelines\n        String codePath = codeDir.getAbsolutePath();\n        conf.set(LOCAL_CODE_DIR, codePath);\n    }\n    DistributedCache.createSymlink(conf); // we will read using symlinks\n    StringBuilder serialized = new StringBuilder();\n    boolean first = true;\n    // We attempt to copy over every file in the generated code temp directory\n    for (File f : codeDir.listFiles()) {\n        if (first) {\n            first = false;\n        } else {\n            serialized.append(\",\");\n        }\n        String symlink = f.getName(); //the class name will also be the symlink\n        serialized.append(symlink);\n        Path src = new Path(f.toURI());\n        Path dst;\n        try {\n            dst = FileLocalizer.getTemporaryPath(pigContext);\n        } catch (IOException e) {\n            throw new RuntimeException(\"Error getting temporary path in HDFS\", e);\n        }\n        FileSystem fs;\n        try {\n            fs = dst.getFileSystem(conf);\n        } catch (IOException e) {\n            throw new RuntimeException(\"Unable to get FileSystem\", e);\n        }\n        try {\n            fs.copyFromLocalFile(src, dst);\n        } catch (IOException e) {\n            throw new RuntimeException(\"Unable to copy from local filesystem to HDFS, src = \"\n                    + src + \", dst = \" + dst, e);\n        }\n\n        String destination = dst.toString() + \"#\" + symlink;\n\n        try {\n            DistributedCache.addCacheFile(new URI(destination), conf);\n        } catch (URISyntaxException e) {\n            throw new RuntimeException(\"Unable to add file to distributed cache: \" + destination, e);\n        }\n        LOG.info(\"File successfully added to the distributed cache: \" + symlink);\n    }\n    String toSer = serialized.toString();\n    LOG.info(\"Setting key [\" + GENERATED_CLASSES_KEY + \"] with classes to deserialize [\" + toSer + \"]\");\n    // we must set a key in the job conf so individual jobs know to resolve the shipped classes\n    conf.set(GENERATED_CLASSES_KEY, toSer);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2739,
        "instruction": "private void wrapWithSasl(RpcCall call) throws IOException {\n  if (call.connection.saslServer != null) {\n    byte[] token = call.rpcResponse.array();\n    // synchronization may be needed since there can be multiple Handler\n    // threads using saslServer to wrap responses.\n    synchronized (call.connection.saslServer) {\n      token = call.connection.saslServer.wrap(token, 0, token.length);\n    }\n    // rebuild with sasl header and payload\n    RpcResponseHeaderProto saslHeader = RpcResponseHeaderProto.newBuilder()\n        .setCallId(AuthProtocol.SASL.callId)\n        .setStatus(RpcStatusProto.SUCCESS)\n        .build();\n    RpcSaslProto saslMessage = RpcSaslProto.newBuilder()\n        .setState(SaslState.WRAP)\n        .setToken(ByteString.copyFrom(token))\n        .build();\n    setupResponse(call, saslHeader, RpcWritable.wrap(saslMessage));\n  }\n}",
        "accept_response": "private void wrapWithSasl(RpcCall call) throws IOException {\n  if (call.connection.saslServer != null) {\n    byte[] token = call.rpcResponse.array();\n    // synchronization may be needed since there can be multiple Handler\n    // threads using saslServer to wrap responses.\n    synchronized (call.connection.saslServer) {\n      token = call.connection.saslServer.wrap(token, 0, token.length);\n    }\n    LOG.debug(\"Adding saslServer wrapped token of size {} as call response.\", token.length);\n    // rebuild with sasl header and payload\n    RpcResponseHeaderProto saslHeader = RpcResponseHeaderProto.newBuilder()\n        .setCallId(AuthProtocol.SASL.callId)\n        .setStatus(RpcStatusProto.SUCCESS)\n        .build();\n    RpcSaslProto saslMessage = RpcSaslProto.newBuilder()\n        .setState(SaslState.WRAP)\n        .setToken(ByteString.copyFrom(token))\n        .build();\n    setupResponse(call, saslHeader, RpcWritable.wrap(saslMessage));\n  }\n}",
        "reject_response": "private void wrapWithSasl(RpcCall call) throws IOException {\n  if (call.connection.saslServer != null) {\n    byte[] token = call.rpcResponse.array();\n    // synchronization may be needed since there can be multiple Handler\n    // threads using saslServer to wrap responses.\n    synchronized (call.connection.saslServer) {\n      token = call.connection.saslServer.wrap(token, 0, token.length);\n    }\n    if (LOG.isDebugEnabled())\n      LOG.debug(\"Adding saslServer wrapped token of size \" + token.length\n          + \" as call response.\");\n    // rebuild with sasl header and payload\n    RpcResponseHeaderProto saslHeader = RpcResponseHeaderProto.newBuilder()\n        .setCallId(AuthProtocol.SASL.callId)\n        .setStatus(RpcStatusProto.SUCCESS)\n        .build();\n    RpcSaslProto saslMessage = RpcSaslProto.newBuilder()\n        .setState(SaslState.WRAP)\n        .setToken(ByteString.copyFrom(token))\n        .build();\n    setupResponse(call, saslHeader, RpcWritable.wrap(saslMessage));\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2417,
        "instruction": "protected synchronized String findPreferredCollectHost() {\n  if (!isInitializedForHA) {\n    init();\n  }\n\n  shardExpired = false;\n  // Auto expire and re-calculate after 1 hour\n  if (targetCollectorHostSupplier != null) {\n    String targetCollector = targetCollectorHostSupplier.get();\n    if (targetCollector != null) {\n      return targetCollector;\n    }\n  }\n\n  // Reach out to all configured collectors before Zookeeper\n  Collection<String> collectorHosts = getConfiguredCollectorHosts();\n  refreshCollectorsFromConfigured(collectorHosts);\n\n  // Lookup Zookeeper for live hosts - max 10 seconds wait time\n  long currentTime = System.currentTimeMillis();\n  if (allKnownLiveCollectors.size() == 0 && getZookeeperQuorum() != null\n    && (currentTime - lastFailedZkRequestTime) > zookeeperBackoffTimeMillis) {\n\n    allKnownLiveCollectors.addAll(collectorHAHelper.findLiveCollectorHostsFromZNode());\n    boolean noNewCollectorFromZk = true;\n    for (String collectorHostFromZk : allKnownLiveCollectors) {\n      if (!collectorHosts.contains(collectorHostFromZk)) {\n        noNewCollectorFromZk = false;\n        break;\n      }\n    }\n    if (noNewCollectorFromZk) {\n      LOG.debug(\"No new collector was found from Zookeeper. Will not request zookeeper for \" + zookeeperBackoffTimeMillis + \" millis\");\n      lastFailedZkRequestTime = System.currentTimeMillis();\n    }\n  }\n\n  if (allKnownLiveCollectors.size() != 0) {\n    targetCollectorHostSupplier = Suppliers.memoizeWithExpiration(\n      new Supplier<String>() {\n        @Override\n        public String get() {\n          //shardExpired flag is used to determine if the Supplier.get() is invoked through the\n          // findPreferredCollectHost method (No need to refresh collector hosts\n          // OR\n          // through Expiry (Refresh needed to pick up dead collectors that might have not become alive).\n          if (shardExpired) {\n            refreshCollectorsFromConfigured(getConfiguredCollectorHosts());\n          }\n          return metricSinkWriteShardStrategy.findCollectorShard(new ArrayList<>(allKnownLiveCollectors));\n        }\n      },  // random.nextInt(max - min + 1) + min # (60 to 75 minutes)\n      rand.nextInt(COLLECTOR_HOST_CACHE_MAX_EXPIRATION_MINUTES\n        - COLLECTOR_HOST_CACHE_MIN_EXPIRATION_MINUTES + 1)\n        + COLLECTOR_HOST_CACHE_MIN_EXPIRATION_MINUTES,\n      TimeUnit.MINUTES\n    );\n\n    String collectorHost = targetCollectorHostSupplier.get();\n    shardExpired = true;\n    return collectorHost;\n  }\n  LOG.debug(\"Couldn't find any live collectors. Returning null\");\n  shardExpired = true;\n  return null;\n}",
        "accept_response": "protected synchronized String findPreferredCollectHost() {\n  if (!isInitializedForHA) {\n    init();\n  }\n\n  shardExpired = false;\n  // Auto expire and re-calculate after 1 hour\n  if (targetCollectorHostSupplier != null) {\n    String targetCollector = targetCollectorHostSupplier.get();\n    if (targetCollector != null) {\n      return targetCollector;\n    }\n  }\n\n  // Reach out to all configured collectors before Zookeeper\n  Collection<String> collectorHosts = getConfiguredCollectorHosts();\n  refreshCollectorsFromConfigured(collectorHosts);\n\n  // Lookup Zookeeper for live hosts - max 10 seconds wait time\n  long currentTime = System.currentTimeMillis();\n  if (allKnownLiveCollectors.size() == 0 && getZookeeperQuorum() != null\n    && (currentTime - lastFailedZkRequestTime) > zookeeperBackoffTimeMillis) {\n\n    LOG.debug(\"No live collectors from configuration. Requesting zookeeper...\");\n    allKnownLiveCollectors.addAll(collectorHAHelper.findLiveCollectorHostsFromZNode());\n    boolean noNewCollectorFromZk = true;\n    for (String collectorHostFromZk : allKnownLiveCollectors) {\n      if (!collectorHosts.contains(collectorHostFromZk)) {\n        noNewCollectorFromZk = false;\n        break;\n      }\n    }\n    if (noNewCollectorFromZk) {\n      LOG.debug(\"No new collector was found from Zookeeper. Will not request zookeeper for \" + zookeeperBackoffTimeMillis + \" millis\");\n      lastFailedZkRequestTime = System.currentTimeMillis();\n    }\n  }\n\n  if (allKnownLiveCollectors.size() != 0) {\n    targetCollectorHostSupplier = Suppliers.memoizeWithExpiration(\n      new Supplier<String>() {\n        @Override\n        public String get() {\n          //shardExpired flag is used to determine if the Supplier.get() is invoked through the\n          // findPreferredCollectHost method (No need to refresh collector hosts\n          // OR\n          // through Expiry (Refresh needed to pick up dead collectors that might have not become alive).\n          if (shardExpired) {\n            refreshCollectorsFromConfigured(getConfiguredCollectorHosts());\n          }\n          return metricSinkWriteShardStrategy.findCollectorShard(new ArrayList<>(allKnownLiveCollectors));\n        }\n      },  // random.nextInt(max - min + 1) + min # (60 to 75 minutes)\n      rand.nextInt(COLLECTOR_HOST_CACHE_MAX_EXPIRATION_MINUTES\n        - COLLECTOR_HOST_CACHE_MIN_EXPIRATION_MINUTES + 1)\n        + COLLECTOR_HOST_CACHE_MIN_EXPIRATION_MINUTES,\n      TimeUnit.MINUTES\n    );\n\n    String collectorHost = targetCollectorHostSupplier.get();\n    shardExpired = true;\n    return collectorHost;\n  }\n  LOG.debug(\"Couldn't find any live collectors. Returning null\");\n  shardExpired = true;\n  return null;\n}",
        "reject_response": "protected synchronized String findPreferredCollectHost() {\n  if (!isInitializedForHA) {\n    init();\n  }\n\n  shardExpired = false;\n  // Auto expire and re-calculate after 1 hour\n  if (targetCollectorHostSupplier != null) {\n    String targetCollector = targetCollectorHostSupplier.get();\n    if (targetCollector != null) {\n      return targetCollector;\n    }\n  }\n\n  // Reach out to all configured collectors before Zookeeper\n  Collection<String> collectorHosts = getConfiguredCollectorHosts();\n  refreshCollectorsFromConfigured(collectorHosts);\n\n  // Lookup Zookeeper for live hosts - max 10 seconds wait time\n  long currentTime = System.currentTimeMillis();\n  if (allKnownLiveCollectors.size() == 0 && getZookeeperQuorum() != null\n    && (currentTime - lastFailedZkRequestTime) > zookeeperBackoffTimeMillis) {\n\n    LOG.info(\"No live collectors from configuration. Requesting zookeeper...\");\n    allKnownLiveCollectors.addAll(collectorHAHelper.findLiveCollectorHostsFromZNode());\n    boolean noNewCollectorFromZk = true;\n    for (String collectorHostFromZk : allKnownLiveCollectors) {\n      if (!collectorHosts.contains(collectorHostFromZk)) {\n        noNewCollectorFromZk = false;\n        break;\n      }\n    }\n    if (noNewCollectorFromZk) {\n      LOG.debug(\"No new collector was found from Zookeeper. Will not request zookeeper for \" + zookeeperBackoffTimeMillis + \" millis\");\n      lastFailedZkRequestTime = System.currentTimeMillis();\n    }\n  }\n\n  if (allKnownLiveCollectors.size() != 0) {\n    targetCollectorHostSupplier = Suppliers.memoizeWithExpiration(\n      new Supplier<String>() {\n        @Override\n        public String get() {\n          //shardExpired flag is used to determine if the Supplier.get() is invoked through the\n          // findPreferredCollectHost method (No need to refresh collector hosts\n          // OR\n          // through Expiry (Refresh needed to pick up dead collectors that might have not become alive).\n          if (shardExpired) {\n            refreshCollectorsFromConfigured(getConfiguredCollectorHosts());\n          }\n          return metricSinkWriteShardStrategy.findCollectorShard(new ArrayList<>(allKnownLiveCollectors));\n        }\n      },  // random.nextInt(max - min + 1) + min # (60 to 75 minutes)\n      rand.nextInt(COLLECTOR_HOST_CACHE_MAX_EXPIRATION_MINUTES\n        - COLLECTOR_HOST_CACHE_MIN_EXPIRATION_MINUTES + 1)\n        + COLLECTOR_HOST_CACHE_MIN_EXPIRATION_MINUTES,\n      TimeUnit.MINUTES\n    );\n\n    String collectorHost = targetCollectorHostSupplier.get();\n    shardExpired = true;\n    return collectorHost;\n  }\n  LOG.debug(\"Couldn't find any live collectors. Returning null\");\n  shardExpired = true;\n  return null;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2757,
        "instruction": "public static ThreadPoolExecutor getThreadPoolExecutor(int corePoolSize,\n    int maxPoolSize, long keepAliveTimeSecs, BlockingQueue<Runnable> queue,\n    String threadNamePrefix, boolean runRejectedExec) {\n  Preconditions.checkArgument(corePoolSize > 0);\n  ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(corePoolSize,\n      maxPoolSize, keepAliveTimeSecs, TimeUnit.SECONDS,\n      queue, new Daemon.DaemonFactory() {\n        private final AtomicInteger threadIndex = new AtomicInteger(0);\n\n        @Override\n        public Thread newThread(Runnable r) {\n          Thread t = super.newThread(r);\n          t.setName(threadNamePrefix + threadIndex.getAndIncrement());\n          return t;\n        }\n      });\n  if (runRejectedExec) {\n    threadPoolExecutor.setRejectedExecutionHandler(new ThreadPoolExecutor\n        .CallerRunsPolicy() {\n      @Override\n      public void rejectedExecution(Runnable runnable,\n          ThreadPoolExecutor e) {\n        // will run in the current thread\n        super.rejectedExecution(runnable, e);\n      }\n    });\n  }\n  return threadPoolExecutor;\n}",
        "accept_response": "public static ThreadPoolExecutor getThreadPoolExecutor(int corePoolSize,\n    int maxPoolSize, long keepAliveTimeSecs, BlockingQueue<Runnable> queue,\n    String threadNamePrefix, boolean runRejectedExec) {\n  Preconditions.checkArgument(corePoolSize > 0);\n  ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(corePoolSize,\n      maxPoolSize, keepAliveTimeSecs, TimeUnit.SECONDS,\n      queue, new Daemon.DaemonFactory() {\n        private final AtomicInteger threadIndex = new AtomicInteger(0);\n\n        @Override\n        public Thread newThread(Runnable r) {\n          Thread t = super.newThread(r);\n          t.setName(threadNamePrefix + threadIndex.getAndIncrement());\n          return t;\n        }\n      });\n  if (runRejectedExec) {\n    threadPoolExecutor.setRejectedExecutionHandler(new ThreadPoolExecutor\n        .CallerRunsPolicy() {\n      @Override\n      public void rejectedExecution(Runnable runnable,\n          ThreadPoolExecutor e) {\n        LOG.info(\"{} task is rejected by \" +\n            \"ThreadPoolExecutor. Executing it in current thread.\", threadNamePrefix);\n        // will run in the current thread\n        super.rejectedExecution(runnable, e);\n      }\n    });\n  }\n  return threadPoolExecutor;\n}",
        "reject_response": "public static ThreadPoolExecutor getThreadPoolExecutor(int corePoolSize,\n    int maxPoolSize, long keepAliveTimeSecs, BlockingQueue<Runnable> queue,\n    String threadNamePrefix, boolean runRejectedExec) {\n  Preconditions.checkArgument(corePoolSize > 0);\n  ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(corePoolSize,\n      maxPoolSize, keepAliveTimeSecs, TimeUnit.SECONDS,\n      queue, new Daemon.DaemonFactory() {\n        private final AtomicInteger threadIndex = new AtomicInteger(0);\n\n        @Override\n        public Thread newThread(Runnable r) {\n          Thread t = super.newThread(r);\n          t.setName(threadNamePrefix + threadIndex.getAndIncrement());\n          return t;\n        }\n      });\n  if (runRejectedExec) {\n    threadPoolExecutor.setRejectedExecutionHandler(new ThreadPoolExecutor\n        .CallerRunsPolicy() {\n      @Override\n      public void rejectedExecution(Runnable runnable,\n          ThreadPoolExecutor e) {\n        LOG.info(threadNamePrefix + \" task is rejected by \" +\n                \"ThreadPoolExecutor. Executing it in current thread.\");\n        // will run in the current thread\n        super.rejectedExecution(runnable, e);\n      }\n    });\n  }\n  return threadPoolExecutor;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3019,
        "instruction": "private synchronized boolean isLegalStateTransition(final ReefServiceProtos.State from,\n                                                    final ReefServiceProtos.State to) {\n\n  // handle diagonal elements of the transition matrix\n  if (from.equals(to)){\n    return true;\n  }\n\n  // handle non-diagonal elements\n  switch (from) {\n\n  case INIT:\n    switch (to) {\n    case RUNNING:\n    case SUSPEND:\n    case DONE:\n    case FAILED:\n    case KILLED:\n      return true;\n    default:\n      return false;\n    }\n\n  case RUNNING:\n    switch (to) {\n    case SUSPEND:\n    case DONE:\n    case FAILED:\n    case KILLED:\n      return true;\n    default:\n      return false;\n    }\n\n  case SUSPEND:\n    switch (to) {\n    case RUNNING:\n    case FAILED:\n    case KILLED:\n      return true;\n    default:\n      return false;\n    }\n\n  case DONE:\n  case FAILED:\n  case KILLED:\n    return false;\n\n  default:\n    return false;\n\n  }\n\n}",
        "accept_response": "private synchronized boolean isLegalStateTransition(final ReefServiceProtos.State from,\n                                                    final ReefServiceProtos.State to) {\n\n  // handle diagonal elements of the transition matrix\n  if (from.equals(to)){\n    LOG.finest(\"Transition from \" + from + \" state to the same state.\");\n    return true;\n  }\n\n  // handle non-diagonal elements\n  switch (from) {\n\n  case INIT:\n    switch (to) {\n    case RUNNING:\n    case SUSPEND:\n    case DONE:\n    case FAILED:\n    case KILLED:\n      return true;\n    default:\n      return false;\n    }\n\n  case RUNNING:\n    switch (to) {\n    case SUSPEND:\n    case DONE:\n    case FAILED:\n    case KILLED:\n      return true;\n    default:\n      return false;\n    }\n\n  case SUSPEND:\n    switch (to) {\n    case RUNNING:\n    case FAILED:\n    case KILLED:\n      return true;\n    default:\n      return false;\n    }\n\n  case DONE:\n  case FAILED:\n  case KILLED:\n    return false;\n\n  default:\n    return false;\n\n  }\n\n}",
        "reject_response": "private synchronized boolean isLegalStateTransition(final ReefServiceProtos.State from,\n                                                    final ReefServiceProtos.State to) {\n\n  // handle diagonal elements of the transition matrix\n  if (from.equals(to)){\n    LOG.warning(\"Transition from \" + from + \" state to the same state. This shouldn't happen.\");\n    return true;\n  }\n\n  // handle non-diagonal elements\n  switch (from) {\n\n  case INIT:\n    switch (to) {\n    case RUNNING:\n    case SUSPEND:\n    case DONE:\n    case FAILED:\n    case KILLED:\n      return true;\n    default:\n      return false;\n    }\n\n  case RUNNING:\n    switch (to) {\n    case SUSPEND:\n    case DONE:\n    case FAILED:\n    case KILLED:\n      return true;\n    default:\n      return false;\n    }\n\n  case SUSPEND:\n    switch (to) {\n    case RUNNING:\n    case FAILED:\n    case KILLED:\n      return true;\n    default:\n      return false;\n    }\n\n  case DONE:\n  case FAILED:\n  case KILLED:\n    return false;\n\n  default:\n    return false;\n\n  }\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3155,
        "instruction": "public Response toResponse( int status, E e ) {\n\n    if ( status >= 500 ) {\n        // only log real errors as errors\n\n    } else {\n        if (logger.isDebugEnabled()) {\n            logger.debug(e.getClass().getCanonicalName() + \" Uncaught Exception (\" + status + \")\", e);\n        }\n    }\n\n    ApiResponse response = new ApiResponse();\n\n\n    AuthErrorInfo authError = AuthErrorInfo.getForException( e );\n\n    if ( authError != null ) {\n        response.setError( authError.getType(), authError.getMessage(), e );\n    }\n    else {\n        response.setError( e );\n    }\n\n    String jsonResponse = mapToJsonString( response );\n\n    return toResponse( status, jsonResponse );\n}",
        "accept_response": "public Response toResponse( int status, E e ) {\n\n    if ( status >= 500 ) {\n        // only log real errors as errors\n        logger.error( e.getClass().getCanonicalName() + \" 5XX Uncaught Exception (\" + status + \")\", e );\n\n    } else {\n        if (logger.isDebugEnabled()) {\n            logger.debug(e.getClass().getCanonicalName() + \" Uncaught Exception (\" + status + \")\", e);\n        }\n    }\n\n    ApiResponse response = new ApiResponse();\n\n\n    AuthErrorInfo authError = AuthErrorInfo.getForException( e );\n\n    if ( authError != null ) {\n        response.setError( authError.getType(), authError.getMessage(), e );\n    }\n    else {\n        response.setError( e );\n    }\n\n    String jsonResponse = mapToJsonString( response );\n\n    return toResponse( status, jsonResponse );\n}",
        "reject_response": "public Response toResponse( int status, E e ) {\n\n    if ( status >= 500 ) {\n        // only log real errors as errors\n        logger.error( e.getClass().getCanonicalName() + \" Server Error (\" + status + \")\", e );\n\n    } else {\n        if (logger.isDebugEnabled()) {\n            logger.debug(e.getClass().getCanonicalName() + \" Uncaught Exception (\" + status + \")\", e);\n        }\n    }\n\n    ApiResponse response = new ApiResponse();\n\n\n    AuthErrorInfo authError = AuthErrorInfo.getForException( e );\n\n    if ( authError != null ) {\n        response.setError( authError.getType(), authError.getMessage(), e );\n    }\n    else {\n        response.setError( e );\n    }\n\n    String jsonResponse = mapToJsonString( response );\n\n    return toResponse( status, jsonResponse );\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2529,
        "instruction": "boolean acquirePermit(int logInterval)\n{\n    long logIntervalNanos = TimeUnit.MINUTES.toNanos(logInterval);\n    long timeOfLastLogging = System.nanoTime();\n    while (true)\n    {\n        if (closed)\n            return false;\n        try\n        {\n            if (fileTransferSemaphore.tryAcquire(1, TimeUnit.SECONDS))\n                return true;\n\n            // log a helpful message to operators in case they are wondering why a given session might not be making progress.\n            long now = System.nanoTime();\n            if (now - timeOfLastLogging > logIntervalNanos)\n            {\n                timeOfLastLogging = now;\n                OutgoingStreamMessage ofm = (OutgoingStreamMessage)msg;\n\n                if (logger.isInfoEnabled())\n            }\n        }\n        catch (InterruptedException ie)\n        {\n            //ignore\n        }\n    }\n}",
        "accept_response": "boolean acquirePermit(int logInterval)\n{\n    long logIntervalNanos = TimeUnit.MINUTES.toNanos(logInterval);\n    long timeOfLastLogging = System.nanoTime();\n    while (true)\n    {\n        if (closed)\n            return false;\n        try\n        {\n            if (fileTransferSemaphore.tryAcquire(1, TimeUnit.SECONDS))\n                return true;\n\n            // log a helpful message to operators in case they are wondering why a given session might not be making progress.\n            long now = System.nanoTime();\n            if (now - timeOfLastLogging > logIntervalNanos)\n            {\n                timeOfLastLogging = now;\n                OutgoingStreamMessage ofm = (OutgoingStreamMessage)msg;\n\n                if (logger.isInfoEnabled())\n                \tlogger.info(\"{} waiting to acquire a permit to begin streaming {}. This message logs every {} minutes\",\n                            createLogTag(session, null), ofm.getName(), logInterval);\n            }\n        }\n        catch (InterruptedException ie)\n        {\n            //ignore\n        }\n    }\n}",
        "reject_response": "boolean acquirePermit(int logInterval)\n{\n    long logIntervalNanos = TimeUnit.MINUTES.toNanos(logInterval);\n    long timeOfLastLogging = System.nanoTime();\n    while (true)\n    {\n        if (closed)\n            return false;\n        try\n        {\n            if (fileTransferSemaphore.tryAcquire(1, TimeUnit.SECONDS))\n                return true;\n\n            // log a helpful message to operators in case they are wondering why a given session might not be making progress.\n            long now = System.nanoTime();\n            if (now - timeOfLastLogging > logIntervalNanos)\n            {\n                timeOfLastLogging = now;\n                OutgoingStreamMessage ofm = (OutgoingStreamMessage)msg;\n\n                if (logger.isInfoEnabled())\n                logger.info(\"{} waiting to acquire a permit to begin streaming {}. This message logs every {} minutes\",\n            }\n        }\n        catch (InterruptedException ie)\n        {\n            //ignore\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2487,
        "instruction": "@BeforeClass\npublic static void startDatabase() throws Exception {\n  ServerSocket socket = new ServerSocket(0);\n  port = socket.getLocalPort();\n  socket.close();\n\n\n  System.setProperty(\"derby.stream.error.file\", \"target/derby.log\");\n\n  derbyServer = new NetworkServerControl(InetAddress.getByName(\"localhost\"), port);\n  StringWriter out = new StringWriter();\n  derbyServer.start(new PrintWriter(out));\n  boolean started = false;\n  int count = 0;\n  // Use two different methods to detect when server is started:\n  // 1) Check the server stdout for the \"started\" string\n  // 2) wait up to 15 seconds for the derby server to start based on a ping\n  // on faster machines and networks, this may return very quick, but on slower\n  // networks where the DNS lookups are slow, this may take a little time\n  while (!started && count < 30) {\n    if (out.toString().contains(\"started\")) {\n      started = true;\n    } else {\n      count++;\n      Thread.sleep(500);\n      try {\n        derbyServer.ping();\n        started = true;\n      } catch (Throwable t) {\n        //ignore, still trying to start\n      }\n    }\n  }\n\n  dataSource = new ClientDataSource();\n  dataSource.setCreateDatabase(\"create\");\n  dataSource.setDatabaseName(\"target/beam\");\n  dataSource.setServerName(\"localhost\");\n  dataSource.setPortNumber(port);\n\n  try (Connection connection = dataSource.getConnection()) {\n    try (Statement statement = connection.createStatement()) {\n      statement.executeUpdate(\"create table BEAM(id INT, name VARCHAR(500))\");\n    }\n  }\n}",
        "accept_response": "@BeforeClass\npublic static void startDatabase() throws Exception {\n  ServerSocket socket = new ServerSocket(0);\n  port = socket.getLocalPort();\n  socket.close();\n\n  LOG.info(\"Starting Derby database on {}\", port);\n\n  System.setProperty(\"derby.stream.error.file\", \"target/derby.log\");\n\n  derbyServer = new NetworkServerControl(InetAddress.getByName(\"localhost\"), port);\n  StringWriter out = new StringWriter();\n  derbyServer.start(new PrintWriter(out));\n  boolean started = false;\n  int count = 0;\n  // Use two different methods to detect when server is started:\n  // 1) Check the server stdout for the \"started\" string\n  // 2) wait up to 15 seconds for the derby server to start based on a ping\n  // on faster machines and networks, this may return very quick, but on slower\n  // networks where the DNS lookups are slow, this may take a little time\n  while (!started && count < 30) {\n    if (out.toString().contains(\"started\")) {\n      started = true;\n    } else {\n      count++;\n      Thread.sleep(500);\n      try {\n        derbyServer.ping();\n        started = true;\n      } catch (Throwable t) {\n        //ignore, still trying to start\n      }\n    }\n  }\n\n  dataSource = new ClientDataSource();\n  dataSource.setCreateDatabase(\"create\");\n  dataSource.setDatabaseName(\"target/beam\");\n  dataSource.setServerName(\"localhost\");\n  dataSource.setPortNumber(port);\n\n  try (Connection connection = dataSource.getConnection()) {\n    try (Statement statement = connection.createStatement()) {\n      statement.executeUpdate(\"create table BEAM(id INT, name VARCHAR(500))\");\n    }\n  }\n}",
        "reject_response": "@BeforeClass\npublic static void startDatabase() throws Exception {\n  ServerSocket socket = new ServerSocket(0);\n  port = socket.getLocalPort();\n  socket.close();\n\n  LOGGER.info(\"Starting Derby database on {}\", port);\n\n  System.setProperty(\"derby.stream.error.file\", \"target/derby.log\");\n\n  derbyServer = new NetworkServerControl(InetAddress.getByName(\"localhost\"), port);\n  StringWriter out = new StringWriter();\n  derbyServer.start(new PrintWriter(out));\n  boolean started = false;\n  int count = 0;\n  // Use two different methods to detect when server is started:\n  // 1) Check the server stdout for the \"started\" string\n  // 2) wait up to 15 seconds for the derby server to start based on a ping\n  // on faster machines and networks, this may return very quick, but on slower\n  // networks where the DNS lookups are slow, this may take a little time\n  while (!started && count < 30) {\n    if (out.toString().contains(\"started\")) {\n      started = true;\n    } else {\n      count++;\n      Thread.sleep(500);\n      try {\n        derbyServer.ping();\n        started = true;\n      } catch (Throwable t) {\n        //ignore, still trying to start\n      }\n    }\n  }\n\n  dataSource = new ClientDataSource();\n  dataSource.setCreateDatabase(\"create\");\n  dataSource.setDatabaseName(\"target/beam\");\n  dataSource.setServerName(\"localhost\");\n  dataSource.setPortNumber(port);\n\n  try (Connection connection = dataSource.getConnection()) {\n    try (Statement statement = connection.createStatement()) {\n      statement.executeUpdate(\"create table BEAM(id INT, name VARCHAR(500))\");\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2815,
        "instruction": "public void inject(GridDeployment dep, Class<?> taskCls, ComputeJob job, ComputeTaskSession ses,\n    GridJobContextImpl jobCtx) throws IgniteCheckedException {\n    if (log.isDebugEnabled())\n\n    // Unwrap Proxy object.\n    Object obj = unwrapTarget(job);\n\n    injectToJob(dep, taskCls, obj, ses, jobCtx);\n\n    if (obj instanceof GridInternalWrapper) {\n        Object usrObj = ((GridInternalWrapper)obj).userObject();\n\n        if (usrObj != null)\n            injectToJob(dep, taskCls, usrObj, ses, jobCtx);\n    }\n}",
        "accept_response": "public void inject(GridDeployment dep, Class<?> taskCls, ComputeJob job, ComputeTaskSession ses,\n    GridJobContextImpl jobCtx) throws IgniteCheckedException {\n    if (log.isDebugEnabled())\n        log.debug(S.toString(\"Injecting resources\", \"job\", job, true));\n\n    // Unwrap Proxy object.\n    Object obj = unwrapTarget(job);\n\n    injectToJob(dep, taskCls, obj, ses, jobCtx);\n\n    if (obj instanceof GridInternalWrapper) {\n        Object usrObj = ((GridInternalWrapper)obj).userObject();\n\n        if (usrObj != null)\n            injectToJob(dep, taskCls, usrObj, ses, jobCtx);\n    }\n}",
        "reject_response": "public void inject(GridDeployment dep, Class<?> taskCls, ComputeJob job, ComputeTaskSession ses,\n    GridJobContextImpl jobCtx) throws IgniteCheckedException {\n    if (log.isDebugEnabled())\n        log.debug(\"Injecting resources: \" + job);\n\n    // Unwrap Proxy object.\n    Object obj = unwrapTarget(job);\n\n    injectToJob(dep, taskCls, obj, ses, jobCtx);\n\n    if (obj instanceof GridInternalWrapper) {\n        Object usrObj = ((GridInternalWrapper)obj).userObject();\n\n        if (usrObj != null)\n            injectToJob(dep, taskCls, usrObj, ses, jobCtx);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2420,
        "instruction": "private void refreshCollectorsFromConfigured(Collection<String> collectorHosts) {\n\n  LOG.debug(\"Trying to find live collector host from : \" + collectorHosts);\n  if (collectorHosts != null && !collectorHosts.isEmpty()) {\n    for (String hostStr : collectorHosts) {\n      hostStr = hostStr.trim();\n      if (!hostStr.isEmpty()) {\n        try {\n          Collection<String> liveHosts = findLiveCollectorHostsFromKnownCollector(hostStr, getCollectorPort());\n          // Update live Hosts - current host will already be a part of this\n          for (String host : liveHosts) {\n            allKnownLiveCollectors.add(host);\n          }\n          break; // Found at least 1 live collector\n        } catch (MetricCollectorUnavailableException e) {\n          allKnownLiveCollectors.remove(hostStr);\n        }\n      }\n    }\n  }\n}",
        "accept_response": "private void refreshCollectorsFromConfigured(Collection<String> collectorHosts) {\n\n  LOG.debug(\"Trying to find live collector host from : \" + collectorHosts);\n  if (collectorHosts != null && !collectorHosts.isEmpty()) {\n    for (String hostStr : collectorHosts) {\n      hostStr = hostStr.trim();\n      if (!hostStr.isEmpty()) {\n        try {\n          Collection<String> liveHosts = findLiveCollectorHostsFromKnownCollector(hostStr, getCollectorPort());\n          // Update live Hosts - current host will already be a part of this\n          for (String host : liveHosts) {\n            allKnownLiveCollectors.add(host);\n          }\n          break; // Found at least 1 live collector\n        } catch (MetricCollectorUnavailableException e) {\n          LOG.debug(\"Collector \" + hostStr + \" is not longer live. Removing \" +\n            \"it from list of know live collector hosts : \" + allKnownLiveCollectors);\n          allKnownLiveCollectors.remove(hostStr);\n        }\n      }\n    }\n  }\n}",
        "reject_response": "private void refreshCollectorsFromConfigured(Collection<String> collectorHosts) {\n\n  LOG.debug(\"Trying to find live collector host from : \" + collectorHosts);\n  if (collectorHosts != null && !collectorHosts.isEmpty()) {\n    for (String hostStr : collectorHosts) {\n      hostStr = hostStr.trim();\n      if (!hostStr.isEmpty()) {\n        try {\n          Collection<String> liveHosts = findLiveCollectorHostsFromKnownCollector(hostStr, getCollectorPort());\n          // Update live Hosts - current host will already be a part of this\n          for (String host : liveHosts) {\n            allKnownLiveCollectors.add(host);\n          }\n          break; // Found at least 1 live collector\n        } catch (MetricCollectorUnavailableException e) {\n          LOG.info(\"Collector \" + hostStr + \" is not longer live. Removing \" +\n          allKnownLiveCollectors.remove(hostStr);\n        }\n      }\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2647,
        "instruction": "@Override\npublic void contextInitialized(ServletContextEvent event) {\n\n  messagesToBeLogged = messagesToBeLogged\n      .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_CONTEXT_INITIALIZED\")));\n\n  // Load Pulse version details\n  loadPulseVersionDetails();\n\n  // Load Pulse Properties\n  pulseProperties = loadProperties(PulseConstants.PULSE_PROPERTIES_FILE);\n\n  if (pulseProperties.isEmpty()) {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_NOT_FOUND\")));\n  } else {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_FOUND\")));\n\n    // set Pulse product support into the Pulse controller for access from\n    // client side\n    // to display the appropriate ui depending on which product is supported\n    // in present deployment\n    String pulseProduct =\n        pulseProperties.getProperty(PulseConstants.APPLICATION_PROPERTY_PULSE_PRODUCTSUPPORT);\n  }\n\n  pulseSecurityProperties = loadProperties(PulseConstants.PULSE_SECURITY_PROPERTIES_FILE);\n\n  // Initialize logger\n  initializeLogger();\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n  logger.info(resourceBundle.getString(\"LOG_MSG_CHECK_APP_RUNNING_MODE\"));\n\n  boolean sysIsEmbedded = Boolean.getBoolean(PulseConstants.SYSTEM_PROPERTY_PULSE_EMBEDDED);\n\n  if (sysIsEmbedded) {\n    // Application Pulse is running in Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_EMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(true);\n\n    sysPulseUseLocator = Boolean.FALSE;\n    try {\n      // Get host name of machine running pulse in embedded mode\n      sysPulseHost = InetAddress.getLocalHost().getCanonicalHostName();\n    } catch (Exception e) {\n      // Set default host name\n      sysPulseHost = PulseConstants.GEMFIRE_DEFAULT_HOST;\n    }\n    sysPulsePort = System.getProperty(PulseConstants.SYSTEM_PROPERTY_PULSE_PORT);\n    if (StringUtils.isBlank(sysPulsePort)) {\n      sysPulsePort = PulseConstants.GEMFIRE_DEFAULT_PORT;\n    }\n\n  } else {\n    // Application Pulse is running in Non-Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_NONEMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(false);\n\n    // Load JMX User Details\n    loadJMXUserDetails();\n    // Load locator and/or manager details\n    loadLocatorManagerDetails();\n\n    useGemFireCredentials = areWeUsingGemFireSecurityProfile(event);\n  }\n\n  // Set user details in repository\n  repository.setJmxUserName(jmxUserName);\n  repository.setJmxUserPassword(jmxUserPassword);\n\n  // Set locator/Manager details in repository\n  repository.setJmxUseLocator(sysPulseUseLocator);\n  repository.setJmxHost(sysPulseHost);\n  repository.setJmxPort(sysPulsePort);\n\n  // set SSL info\n  initializeSSL();\n  repository.setUseSSLLocator(sysPulseUseSSLLocator);\n  repository.setUseSSLManager(sysPulseUseSSLManager);\n\n  repository.setUseGemFireCredentials(useGemFireCredentials);\n\n}",
        "accept_response": "@Override\npublic void contextInitialized(ServletContextEvent event) {\n\n  messagesToBeLogged = messagesToBeLogged\n      .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_CONTEXT_INITIALIZED\")));\n\n  // Load Pulse version details\n  loadPulseVersionDetails();\n\n  // Load Pulse Properties\n  pulseProperties = loadProperties(PulseConstants.PULSE_PROPERTIES_FILE);\n\n  if (pulseProperties.isEmpty()) {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_NOT_FOUND\")));\n  } else {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_FOUND\")));\n\n    // set Pulse product support into the Pulse controller for access from\n    // client side\n    // to display the appropriate ui depending on which product is supported\n    // in present deployment\n    String pulseProduct =\n        pulseProperties.getProperty(PulseConstants.APPLICATION_PROPERTY_PULSE_PRODUCTSUPPORT);\n  }\n\n  pulseSecurityProperties = loadProperties(PulseConstants.PULSE_SECURITY_PROPERTIES_FILE);\n\n  // Initialize logger\n  initializeLogger();\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n  logger.info(resourceBundle.getString(\"LOG_MSG_CHECK_APP_RUNNING_MODE\"));\n\n  boolean sysIsEmbedded = Boolean.getBoolean(PulseConstants.SYSTEM_PROPERTY_PULSE_EMBEDDED);\n\n  if (sysIsEmbedded) {\n    // Application Pulse is running in Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_EMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(true);\n\n    sysPulseUseLocator = Boolean.FALSE;\n    try {\n      // Get host name of machine running pulse in embedded mode\n      sysPulseHost = InetAddress.getLocalHost().getCanonicalHostName();\n    } catch (Exception e) {\n      logger.debug(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_UNKNOWN_HOST\"), e);\n      // Set default host name\n      sysPulseHost = PulseConstants.GEMFIRE_DEFAULT_HOST;\n    }\n    sysPulsePort = System.getProperty(PulseConstants.SYSTEM_PROPERTY_PULSE_PORT);\n    if (StringUtils.isBlank(sysPulsePort)) {\n      sysPulsePort = PulseConstants.GEMFIRE_DEFAULT_PORT;\n    }\n\n  } else {\n    // Application Pulse is running in Non-Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_NONEMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(false);\n\n    // Load JMX User Details\n    loadJMXUserDetails();\n    // Load locator and/or manager details\n    loadLocatorManagerDetails();\n\n    useGemFireCredentials = areWeUsingGemFireSecurityProfile(event);\n  }\n\n  // Set user details in repository\n  repository.setJmxUserName(jmxUserName);\n  repository.setJmxUserPassword(jmxUserPassword);\n\n  // Set locator/Manager details in repository\n  repository.setJmxUseLocator(sysPulseUseLocator);\n  repository.setJmxHost(sysPulseHost);\n  repository.setJmxPort(sysPulsePort);\n\n  // set SSL info\n  initializeSSL();\n  repository.setUseSSLLocator(sysPulseUseSSLLocator);\n  repository.setUseSSLManager(sysPulseUseSSLManager);\n\n  repository.setUseGemFireCredentials(useGemFireCredentials);\n\n}",
        "reject_response": "@Override\npublic void contextInitialized(ServletContextEvent event) {\n\n  messagesToBeLogged = messagesToBeLogged\n      .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_CONTEXT_INITIALIZED\")));\n\n  // Load Pulse version details\n  loadPulseVersionDetails();\n\n  // Load Pulse Properties\n  pulseProperties = loadProperties(PulseConstants.PULSE_PROPERTIES_FILE);\n\n  if (pulseProperties.isEmpty()) {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_NOT_FOUND\")));\n  } else {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_FOUND\")));\n\n    // set Pulse product support into the Pulse controller for access from\n    // client side\n    // to display the appropriate ui depending on which product is supported\n    // in present deployment\n    String pulseProduct =\n        pulseProperties.getProperty(PulseConstants.APPLICATION_PROPERTY_PULSE_PRODUCTSUPPORT);\n  }\n\n  pulseSecurityProperties = loadProperties(PulseConstants.PULSE_SECURITY_PROPERTIES_FILE);\n\n  // Initialize logger\n  initializeLogger();\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n  logger.info(resourceBundle.getString(\"LOG_MSG_CHECK_APP_RUNNING_MODE\"));\n\n  boolean sysIsEmbedded = Boolean.getBoolean(PulseConstants.SYSTEM_PROPERTY_PULSE_EMBEDDED);\n\n  if (sysIsEmbedded) {\n    // Application Pulse is running in Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_EMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(true);\n\n    sysPulseUseLocator = Boolean.FALSE;\n    try {\n      // Get host name of machine running pulse in embedded mode\n      sysPulseHost = InetAddress.getLocalHost().getCanonicalHostName();\n    } catch (Exception e) {\n      if (LOGGER.fineEnabled()) {\n        LOGGER.fine(\n            resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_UNKNOWN_HOST\") + e.getMessage());\n      }\n      // Set default host name\n      sysPulseHost = PulseConstants.GEMFIRE_DEFAULT_HOST;\n    }\n    sysPulsePort = System.getProperty(PulseConstants.SYSTEM_PROPERTY_PULSE_PORT);\n    if (StringUtils.isBlank(sysPulsePort)) {\n      sysPulsePort = PulseConstants.GEMFIRE_DEFAULT_PORT;\n    }\n\n  } else {\n    // Application Pulse is running in Non-Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_NONEMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(false);\n\n    // Load JMX User Details\n    loadJMXUserDetails();\n    // Load locator and/or manager details\n    loadLocatorManagerDetails();\n\n    useGemFireCredentials = areWeUsingGemFireSecurityProfile(event);\n  }\n\n  // Set user details in repository\n  repository.setJmxUserName(jmxUserName);\n  repository.setJmxUserPassword(jmxUserPassword);\n\n  // Set locator/Manager details in repository\n  repository.setJmxUseLocator(sysPulseUseLocator);\n  repository.setJmxHost(sysPulseHost);\n  repository.setJmxPort(sysPulsePort);\n\n  // set SSL info\n  initializeSSL();\n  repository.setUseSSLLocator(sysPulseUseSSLLocator);\n  repository.setUseSSLManager(sysPulseUseSSLManager);\n\n  repository.setUseGemFireCredentials(useGemFireCredentials);\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3230,
        "instruction": "private boolean doAccept() {\n    boolean accepted = false;\n    SocketChannel sc = null;\n    try {\n        sc = acceptSocket.accept();\n        accepted = true;\n        InetAddress ia = sc.socket().getInetAddress();\n        int cnxncount = getClientCnxnCount(ia);\n\n        if (maxClientCnxns > 0 && cnxncount >= maxClientCnxns){\n            throw new IOException(\"Too many connections from \" + ia\n                                  + \" - max is \" + maxClientCnxns );\n        }\n\n\n        sc.configureBlocking(false);\n\n        // Round-robin assign this connection to a selector thread\n        if (!selectorIterator.hasNext()) {\n            selectorIterator = selectorThreads.iterator();\n        }\n        SelectorThread selectorThread = selectorIterator.next();\n        if (!selectorThread.addAcceptedConnection(sc)) {\n            throw new IOException(\n                \"Unable to add connection to selector queue\"\n                + (stopped ? \" (shutdown in progress)\" : \"\"));\n        }\n        acceptErrorLogger.flush();\n    } catch (IOException e) {\n        // accept, maxClientCnxns, configureBlocking\n        ServerMetrics.getMetrics().CONNECTION_REJECTED.add(1);\n        acceptErrorLogger.rateLimitLog(\n            \"Error accepting new connection: \" + e.getMessage());\n        fastCloseSock(sc);\n    }\n    return accepted;\n}",
        "accept_response": "private boolean doAccept() {\n    boolean accepted = false;\n    SocketChannel sc = null;\n    try {\n        sc = acceptSocket.accept();\n        accepted = true;\n        InetAddress ia = sc.socket().getInetAddress();\n        int cnxncount = getClientCnxnCount(ia);\n\n        if (maxClientCnxns > 0 && cnxncount >= maxClientCnxns){\n            throw new IOException(\"Too many connections from \" + ia\n                                  + \" - max is \" + maxClientCnxns );\n        }\n\n        LOG.debug(\"Accepted socket connection from {}\",\n            sc.socket().getRemoteSocketAddress());\n\n        sc.configureBlocking(false);\n\n        // Round-robin assign this connection to a selector thread\n        if (!selectorIterator.hasNext()) {\n            selectorIterator = selectorThreads.iterator();\n        }\n        SelectorThread selectorThread = selectorIterator.next();\n        if (!selectorThread.addAcceptedConnection(sc)) {\n            throw new IOException(\n                \"Unable to add connection to selector queue\"\n                + (stopped ? \" (shutdown in progress)\" : \"\"));\n        }\n        acceptErrorLogger.flush();\n    } catch (IOException e) {\n        // accept, maxClientCnxns, configureBlocking\n        ServerMetrics.getMetrics().CONNECTION_REJECTED.add(1);\n        acceptErrorLogger.rateLimitLog(\n            \"Error accepting new connection: \" + e.getMessage());\n        fastCloseSock(sc);\n    }\n    return accepted;\n}",
        "reject_response": "private boolean doAccept() {\n    boolean accepted = false;\n    SocketChannel sc = null;\n    try {\n        sc = acceptSocket.accept();\n        accepted = true;\n        InetAddress ia = sc.socket().getInetAddress();\n        int cnxncount = getClientCnxnCount(ia);\n\n        if (maxClientCnxns > 0 && cnxncount >= maxClientCnxns){\n            throw new IOException(\"Too many connections from \" + ia\n                                  + \" - max is \" + maxClientCnxns );\n        }\n\n        LOG.debug(\"Accepted socket connection from \"\n                 + sc.socket().getRemoteSocketAddress());\n\n        sc.configureBlocking(false);\n\n        // Round-robin assign this connection to a selector thread\n        if (!selectorIterator.hasNext()) {\n            selectorIterator = selectorThreads.iterator();\n        }\n        SelectorThread selectorThread = selectorIterator.next();\n        if (!selectorThread.addAcceptedConnection(sc)) {\n            throw new IOException(\n                \"Unable to add connection to selector queue\"\n                + (stopped ? \" (shutdown in progress)\" : \"\"));\n        }\n        acceptErrorLogger.flush();\n    } catch (IOException e) {\n        // accept, maxClientCnxns, configureBlocking\n        ServerMetrics.getMetrics().CONNECTION_REJECTED.add(1);\n        acceptErrorLogger.rateLimitLog(\n            \"Error accepting new connection: \" + e.getMessage());\n        fastCloseSock(sc);\n    }\n    return accepted;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2700,
        "instruction": "@GET\n@Path(\"getNodesInfo\")\n@Produces(\"application/json\")\npublic Response read() throws Exception {\n    LOG.debug(\"getNodesInfo started\");\n    StringBuilder jsonOutput = new StringBuilder(\"{\\\"regions\\\":[\");\n    try {\n        /*\n         * 1. Initialize the HADOOP client side API for a distributed file\n         * system\n         */\n        Configuration conf = new Configuration();\n        FileSystem fs = FileSystem.get(conf);\n        DistributedFileSystem dfs = (DistributedFileSystem) fs;\n\n        /*\n         * 2. Query the namenode for the datanodes info. Only live nodes are\n         * returned - in accordance with the results returned by\n         * org.apache.hadoop.hdfs.tools.DFSAdmin#report().\n         */\n        DatanodeInfo[] liveNodes = dfs.getDataNodeStats(DatanodeReportType.LIVE);\n\n        /*\n         * 3. Pack the datanodes info in a JSON text format and write it to\n         * the HTTP output stream.\n         */\n        String prefix = \"\";\n        for (DatanodeInfo node : liveNodes) {\n            verifyNode(node);\n            // write one node to the HTTP stream\n            jsonOutput.append(prefix).append(writeNode(node));\n            prefix = \",\";\n        }\n        jsonOutput.append(\"]}\");\n        LOG.debug(\"getNodesCluster output: \" + jsonOutput);\n    } catch (NodeDataException e) {\n        throw e;\n    } catch (ClientAbortException e) {\n        LOG.error(\"Remote connection closed by HAWQ\", e);\n        throw e;\n    } catch (java.io.IOException e) {\n        LOG.error(\"Unhandled exception thrown\", e);\n        throw e;\n    }\n\n    return Response.ok(jsonOutput.toString(),\n            MediaType.APPLICATION_JSON_TYPE).build();\n}",
        "accept_response": "@GET\n@Path(\"getNodesInfo\")\n@Produces(\"application/json\")\npublic Response read() throws Exception {\n    LOG.debug(\"getNodesInfo started\");\n    StringBuilder jsonOutput = new StringBuilder(\"{\\\"regions\\\":[\");\n    try {\n        /*\n         * 1. Initialize the HADOOP client side API for a distributed file\n         * system\n         */\n        Configuration conf = new Configuration();\n        FileSystem fs = FileSystem.get(conf);\n        DistributedFileSystem dfs = (DistributedFileSystem) fs;\n\n        /*\n         * 2. Query the namenode for the datanodes info. Only live nodes are\n         * returned - in accordance with the results returned by\n         * org.apache.hadoop.hdfs.tools.DFSAdmin#report().\n         */\n        DatanodeInfo[] liveNodes = dfs.getDataNodeStats(DatanodeReportType.LIVE);\n\n        /*\n         * 3. Pack the datanodes info in a JSON text format and write it to\n         * the HTTP output stream.\n         */\n        String prefix = \"\";\n        for (DatanodeInfo node : liveNodes) {\n            verifyNode(node);\n            // write one node to the HTTP stream\n            jsonOutput.append(prefix).append(writeNode(node));\n            prefix = \",\";\n        }\n        jsonOutput.append(\"]}\");\n        LOG.debug(\"getNodesCluster output: \" + jsonOutput);\n    } catch (NodeDataException e) {\n        LOG.error(\"Nodes verification failed\", e);\n        throw e;\n    } catch (ClientAbortException e) {\n        LOG.error(\"Remote connection closed by HAWQ\", e);\n        throw e;\n    } catch (java.io.IOException e) {\n        LOG.error(\"Unhandled exception thrown\", e);\n        throw e;\n    }\n\n    return Response.ok(jsonOutput.toString(),\n            MediaType.APPLICATION_JSON_TYPE).build();\n}",
        "reject_response": "@GET\n@Path(\"getNodesInfo\")\n@Produces(\"application/json\")\npublic Response read() throws Exception {\n    LOG.debug(\"getNodesInfo started\");\n    StringBuilder jsonOutput = new StringBuilder(\"{\\\"regions\\\":[\");\n    try {\n        /*\n         * 1. Initialize the HADOOP client side API for a distributed file\n         * system\n         */\n        Configuration conf = new Configuration();\n        FileSystem fs = FileSystem.get(conf);\n        DistributedFileSystem dfs = (DistributedFileSystem) fs;\n\n        /*\n         * 2. Query the namenode for the datanodes info. Only live nodes are\n         * returned - in accordance with the results returned by\n         * org.apache.hadoop.hdfs.tools.DFSAdmin#report().\n         */\n        DatanodeInfo[] liveNodes = dfs.getDataNodeStats(DatanodeReportType.LIVE);\n\n        /*\n         * 3. Pack the datanodes info in a JSON text format and write it to\n         * the HTTP output stream.\n         */\n        String prefix = \"\";\n        for (DatanodeInfo node : liveNodes) {\n            verifyNode(node);\n            // write one node to the HTTP stream\n            jsonOutput.append(prefix).append(writeNode(node));\n            prefix = \",\";\n        }\n        jsonOutput.append(\"]}\");\n        LOG.debug(\"getNodesCluster output: \" + jsonOutput);\n    } catch (NodeDataException e) {\n        Log.error(\"Nodes verification failed\", e);\n        throw e;\n    } catch (ClientAbortException e) {\n        LOG.error(\"Remote connection closed by HAWQ\", e);\n        throw e;\n    } catch (java.io.IOException e) {\n        LOG.error(\"Unhandled exception thrown\", e);\n        throw e;\n    }\n\n    return Response.ok(jsonOutput.toString(),\n            MediaType.APPLICATION_JSON_TYPE).build();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3202,
        "instruction": "private void startTimelineReaderWebApp() {\n  Configuration conf = getConfig();\n  addFilters(conf);\n\n  String hostProperty = YarnConfiguration.TIMELINE_SERVICE_READER_BIND_HOST;\n  String host = conf.getTrimmed(hostProperty);\n  if (host == null || host.isEmpty()) {\n    // if reader bind-host is not set, fall back to timeline-service.bind-host\n    // to maintain compatibility\n    hostProperty = YarnConfiguration.TIMELINE_SERVICE_BIND_HOST;\n  }\n  String bindAddress = WebAppUtils\n      .getWebAppBindURL(conf, hostProperty, webAppURLWithoutScheme);\n\n  try {\n\n    String httpScheme = WebAppUtils.getHttpSchemePrefix(conf);\n\n    HttpServer2.Builder builder = new HttpServer2.Builder()\n          .setName(\"timeline\")\n          .setConf(conf)\n          .addEndpoint(URI.create(httpScheme + bindAddress));\n\n    if (httpScheme.equals(WebAppUtils.HTTPS_PREFIX)) {\n      WebAppUtils.loadSslConfiguration(builder, conf);\n    }\n    readerWebServer = builder.build();\n    readerWebServer.addJerseyResourcePackage(\n        TimelineReaderWebServices.class.getPackage().getName() + \";\"\n            + GenericExceptionHandler.class.getPackage().getName() + \";\"\n            + YarnJacksonJaxbJsonProvider.class.getPackage().getName()+ \";\"\n            + LogWebService.class.getPackage().getName(),\n        \"/*\");\n    readerWebServer.setAttribute(TIMELINE_READER_MANAGER_ATTR,\n        timelineReaderManager);\n    readerWebServer.start();\n  } catch (Exception e) {\n    String msg = \"TimelineReaderWebApp failed to start.\";\n    LOG.error(msg, e);\n    throw new YarnRuntimeException(msg, e);\n  }\n}",
        "accept_response": "private void startTimelineReaderWebApp() {\n  Configuration conf = getConfig();\n  addFilters(conf);\n\n  String hostProperty = YarnConfiguration.TIMELINE_SERVICE_READER_BIND_HOST;\n  String host = conf.getTrimmed(hostProperty);\n  if (host == null || host.isEmpty()) {\n    // if reader bind-host is not set, fall back to timeline-service.bind-host\n    // to maintain compatibility\n    hostProperty = YarnConfiguration.TIMELINE_SERVICE_BIND_HOST;\n  }\n  String bindAddress = WebAppUtils\n      .getWebAppBindURL(conf, hostProperty, webAppURLWithoutScheme);\n\n  LOG.info(\"Instantiating TimelineReaderWebApp at {}\", bindAddress);\n  try {\n\n    String httpScheme = WebAppUtils.getHttpSchemePrefix(conf);\n\n    HttpServer2.Builder builder = new HttpServer2.Builder()\n          .setName(\"timeline\")\n          .setConf(conf)\n          .addEndpoint(URI.create(httpScheme + bindAddress));\n\n    if (httpScheme.equals(WebAppUtils.HTTPS_PREFIX)) {\n      WebAppUtils.loadSslConfiguration(builder, conf);\n    }\n    readerWebServer = builder.build();\n    readerWebServer.addJerseyResourcePackage(\n        TimelineReaderWebServices.class.getPackage().getName() + \";\"\n            + GenericExceptionHandler.class.getPackage().getName() + \";\"\n            + YarnJacksonJaxbJsonProvider.class.getPackage().getName()+ \";\"\n            + LogWebService.class.getPackage().getName(),\n        \"/*\");\n    readerWebServer.setAttribute(TIMELINE_READER_MANAGER_ATTR,\n        timelineReaderManager);\n    readerWebServer.start();\n  } catch (Exception e) {\n    String msg = \"TimelineReaderWebApp failed to start.\";\n    LOG.error(msg, e);\n    throw new YarnRuntimeException(msg, e);\n  }\n}",
        "reject_response": "private void startTimelineReaderWebApp() {\n  Configuration conf = getConfig();\n  addFilters(conf);\n\n  String hostProperty = YarnConfiguration.TIMELINE_SERVICE_READER_BIND_HOST;\n  String host = conf.getTrimmed(hostProperty);\n  if (host == null || host.isEmpty()) {\n    // if reader bind-host is not set, fall back to timeline-service.bind-host\n    // to maintain compatibility\n    hostProperty = YarnConfiguration.TIMELINE_SERVICE_BIND_HOST;\n  }\n  String bindAddress = WebAppUtils\n      .getWebAppBindURL(conf, hostProperty, webAppURLWithoutScheme);\n\n  LOG.info(\"Instantiating TimelineReaderWebApp at \" + bindAddress);\n  try {\n\n    String httpScheme = WebAppUtils.getHttpSchemePrefix(conf);\n\n    HttpServer2.Builder builder = new HttpServer2.Builder()\n          .setName(\"timeline\")\n          .setConf(conf)\n          .addEndpoint(URI.create(httpScheme + bindAddress));\n\n    if (httpScheme.equals(WebAppUtils.HTTPS_PREFIX)) {\n      WebAppUtils.loadSslConfiguration(builder, conf);\n    }\n    readerWebServer = builder.build();\n    readerWebServer.addJerseyResourcePackage(\n        TimelineReaderWebServices.class.getPackage().getName() + \";\"\n            + GenericExceptionHandler.class.getPackage().getName() + \";\"\n            + YarnJacksonJaxbJsonProvider.class.getPackage().getName()+ \";\"\n            + LogWebService.class.getPackage().getName(),\n        \"/*\");\n    readerWebServer.setAttribute(TIMELINE_READER_MANAGER_ATTR,\n        timelineReaderManager);\n    readerWebServer.start();\n  } catch (Exception e) {\n    String msg = \"TimelineReaderWebApp failed to start.\";\n    LOG.error(msg, e);\n    throw new YarnRuntimeException(msg, e);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2446,
        "instruction": "public long write(ByteBuffer buffer) throws IOException {\n  long length = buffer.remaining();\n  while (buffer.hasRemaining()) {\n    out.write(buffer);\n  }\n  currentPosition += length;\n  return length;\n}",
        "accept_response": "public long write(ByteBuffer buffer) throws IOException {\n  long length = buffer.remaining();\n  LOGGER.debug(\"Writing buffer with size: {}\", length);\n  while (buffer.hasRemaining()) {\n    out.write(buffer);\n  }\n  currentPosition += length;\n  return length;\n}",
        "reject_response": "public long write(ByteBuffer buffer) throws IOException {\n  long length = buffer.remaining();\n  LOGGER.debug(\"Writing buffer with size: \" + length);\n  while (buffer.hasRemaining()) {\n    out.write(buffer);\n  }\n  currentPosition += length;\n  return length;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2664,
        "instruction": "private ChannelInitializer<SocketChannel> createChannelInitializer(byte[] redisPasswordBytes) {\n  return new ChannelInitializer<SocketChannel>() {\n    @Override\n    public void initChannel(SocketChannel socketChannel) {\n      ChannelPipeline pipeline = socketChannel.pipeline();\n      addSSLIfEnabled(socketChannel, pipeline);\n      pipeline.addLast(ByteToCommandDecoder.class.getSimpleName(), new ByteToCommandDecoder());\n      pipeline.addLast(new WriteTimeoutHandler(10));\n      pipeline.addLast(ExecutionHandlerContext.class.getSimpleName(),\n          new ExecutionHandlerContext(socketChannel, cache, regionProvider, GeodeRedisServer.this,\n              redisPasswordBytes,\n              keyRegistrar, pubSub, hashLockService, subscriberGroup));\n    }\n  };\n}",
        "accept_response": "private ChannelInitializer<SocketChannel> createChannelInitializer(byte[] redisPasswordBytes) {\n  return new ChannelInitializer<SocketChannel>() {\n    @Override\n    public void initChannel(SocketChannel socketChannel) {\n      if (logger.isDebugEnabled()) {\n        logger.debug(\n            \"GeodeRedisServer-Connection established with \" + socketChannel.remoteAddress());\n      }\n      ChannelPipeline pipeline = socketChannel.pipeline();\n      addSSLIfEnabled(socketChannel, pipeline);\n      pipeline.addLast(ByteToCommandDecoder.class.getSimpleName(), new ByteToCommandDecoder());\n      pipeline.addLast(new WriteTimeoutHandler(10));\n      pipeline.addLast(ExecutionHandlerContext.class.getSimpleName(),\n          new ExecutionHandlerContext(socketChannel, cache, regionProvider, GeodeRedisServer.this,\n              redisPasswordBytes,\n              keyRegistrar, pubSub, hashLockService, subscriberGroup));\n    }\n  };\n}",
        "reject_response": "private ChannelInitializer<SocketChannel> createChannelInitializer(byte[] redisPasswordBytes) {\n  return new ChannelInitializer<SocketChannel>() {\n    @Override\n    public void initChannel(SocketChannel socketChannel) {\n      if (logger.isDebugEnabled()) {\n      if (logger.fineEnabled()) {\n        logger.fine(\n      }\n      ChannelPipeline pipeline = socketChannel.pipeline();\n      addSSLIfEnabled(socketChannel, pipeline);\n      pipeline.addLast(ByteToCommandDecoder.class.getSimpleName(), new ByteToCommandDecoder());\n      pipeline.addLast(new WriteTimeoutHandler(10));\n      pipeline.addLast(ExecutionHandlerContext.class.getSimpleName(),\n          new ExecutionHandlerContext(socketChannel, cache, regionProvider, GeodeRedisServer.this,\n              redisPasswordBytes,\n              keyRegistrar, pubSub, hashLockService, subscriberGroup));\n    }\n  };\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2653,
        "instruction": "@Override\npublic void contextInitialized(ServletContextEvent event) {\n\n  messagesToBeLogged = messagesToBeLogged\n      .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_CONTEXT_INITIALIZED\")));\n\n  // Load Pulse version details\n  loadPulseVersionDetails();\n\n  // Load Pulse Properties\n  pulseProperties = loadProperties(PulseConstants.PULSE_PROPERTIES_FILE);\n\n  if (pulseProperties.isEmpty()) {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_NOT_FOUND\")));\n  } else {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_FOUND\")));\n\n    // set Pulse product support into the Pulse controller for access from\n    // client side\n    // to display the appropriate ui depending on which product is supported\n    // in present deployment\n    String pulseProduct =\n        pulseProperties.getProperty(PulseConstants.APPLICATION_PROPERTY_PULSE_PRODUCTSUPPORT);\n  }\n\n  pulseSecurityProperties = loadProperties(PulseConstants.PULSE_SECURITY_PROPERTIES_FILE);\n\n  // Initialize logger\n  initializeLogger();\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n\n  boolean sysIsEmbedded = Boolean.getBoolean(PulseConstants.SYSTEM_PROPERTY_PULSE_EMBEDDED);\n\n  if (sysIsEmbedded) {\n    // Application Pulse is running in Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_EMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(true);\n\n    sysPulseUseLocator = Boolean.FALSE;\n    try {\n      // Get host name of machine running pulse in embedded mode\n      sysPulseHost = InetAddress.getLocalHost().getCanonicalHostName();\n    } catch (Exception e) {\n      logger.debug(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_UNKNOWN_HOST\"), e);\n      // Set default host name\n      sysPulseHost = PulseConstants.GEMFIRE_DEFAULT_HOST;\n    }\n    sysPulsePort = System.getProperty(PulseConstants.SYSTEM_PROPERTY_PULSE_PORT);\n    if (StringUtils.isBlank(sysPulsePort)) {\n      sysPulsePort = PulseConstants.GEMFIRE_DEFAULT_PORT;\n    }\n\n  } else {\n    // Application Pulse is running in Non-Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_NONEMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(false);\n\n    // Load JMX User Details\n    loadJMXUserDetails();\n    // Load locator and/or manager details\n    loadLocatorManagerDetails();\n\n    useGemFireCredentials = areWeUsingGemFireSecurityProfile(event);\n  }\n\n  // Set user details in repository\n  repository.setJmxUserName(jmxUserName);\n  repository.setJmxUserPassword(jmxUserPassword);\n\n  // Set locator/Manager details in repository\n  repository.setJmxUseLocator(sysPulseUseLocator);\n  repository.setJmxHost(sysPulseHost);\n  repository.setJmxPort(sysPulsePort);\n\n  // set SSL info\n  initializeSSL();\n  repository.setUseSSLLocator(sysPulseUseSSLLocator);\n  repository.setUseSSLManager(sysPulseUseSSLManager);\n\n  repository.setUseGemFireCredentials(useGemFireCredentials);\n\n}",
        "accept_response": "@Override\npublic void contextInitialized(ServletContextEvent event) {\n\n  messagesToBeLogged = messagesToBeLogged\n      .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_CONTEXT_INITIALIZED\")));\n\n  // Load Pulse version details\n  loadPulseVersionDetails();\n\n  // Load Pulse Properties\n  pulseProperties = loadProperties(PulseConstants.PULSE_PROPERTIES_FILE);\n\n  if (pulseProperties.isEmpty()) {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_NOT_FOUND\")));\n  } else {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_FOUND\")));\n\n    // set Pulse product support into the Pulse controller for access from\n    // client side\n    // to display the appropriate ui depending on which product is supported\n    // in present deployment\n    String pulseProduct =\n        pulseProperties.getProperty(PulseConstants.APPLICATION_PROPERTY_PULSE_PRODUCTSUPPORT);\n  }\n\n  pulseSecurityProperties = loadProperties(PulseConstants.PULSE_SECURITY_PROPERTIES_FILE);\n\n  // Initialize logger\n  initializeLogger();\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n  logger.info(resourceBundle.getString(\"LOG_MSG_CHECK_APP_RUNNING_MODE\"));\n\n  boolean sysIsEmbedded = Boolean.getBoolean(PulseConstants.SYSTEM_PROPERTY_PULSE_EMBEDDED);\n\n  if (sysIsEmbedded) {\n    // Application Pulse is running in Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_EMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(true);\n\n    sysPulseUseLocator = Boolean.FALSE;\n    try {\n      // Get host name of machine running pulse in embedded mode\n      sysPulseHost = InetAddress.getLocalHost().getCanonicalHostName();\n    } catch (Exception e) {\n      logger.debug(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_UNKNOWN_HOST\"), e);\n      // Set default host name\n      sysPulseHost = PulseConstants.GEMFIRE_DEFAULT_HOST;\n    }\n    sysPulsePort = System.getProperty(PulseConstants.SYSTEM_PROPERTY_PULSE_PORT);\n    if (StringUtils.isBlank(sysPulsePort)) {\n      sysPulsePort = PulseConstants.GEMFIRE_DEFAULT_PORT;\n    }\n\n  } else {\n    // Application Pulse is running in Non-Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_NONEMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(false);\n\n    // Load JMX User Details\n    loadJMXUserDetails();\n    // Load locator and/or manager details\n    loadLocatorManagerDetails();\n\n    useGemFireCredentials = areWeUsingGemFireSecurityProfile(event);\n  }\n\n  // Set user details in repository\n  repository.setJmxUserName(jmxUserName);\n  repository.setJmxUserPassword(jmxUserPassword);\n\n  // Set locator/Manager details in repository\n  repository.setJmxUseLocator(sysPulseUseLocator);\n  repository.setJmxHost(sysPulseHost);\n  repository.setJmxPort(sysPulsePort);\n\n  // set SSL info\n  initializeSSL();\n  repository.setUseSSLLocator(sysPulseUseSSLLocator);\n  repository.setUseSSLManager(sysPulseUseSSLManager);\n\n  repository.setUseGemFireCredentials(useGemFireCredentials);\n\n}",
        "reject_response": "@Override\npublic void contextInitialized(ServletContextEvent event) {\n\n  messagesToBeLogged = messagesToBeLogged\n      .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_CONTEXT_INITIALIZED\")));\n\n  // Load Pulse version details\n  loadPulseVersionDetails();\n\n  // Load Pulse Properties\n  pulseProperties = loadProperties(PulseConstants.PULSE_PROPERTIES_FILE);\n\n  if (pulseProperties.isEmpty()) {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_NOT_FOUND\")));\n  } else {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_FOUND\")));\n\n    // set Pulse product support into the Pulse controller for access from\n    // client side\n    // to display the appropriate ui depending on which product is supported\n    // in present deployment\n    String pulseProduct =\n        pulseProperties.getProperty(PulseConstants.APPLICATION_PROPERTY_PULSE_PRODUCTSUPPORT);\n  }\n\n  pulseSecurityProperties = loadProperties(PulseConstants.PULSE_SECURITY_PROPERTIES_FILE);\n\n  // Initialize logger\n  initializeLogger();\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n  if (LOGGER.infoEnabled()) {\n    LOGGER.info(resourceBundle.getString(\"LOG_MSG_CHECK_APP_RUNNING_MODE\"));\n  }\n\n  boolean sysIsEmbedded = Boolean.getBoolean(PulseConstants.SYSTEM_PROPERTY_PULSE_EMBEDDED);\n\n  if (sysIsEmbedded) {\n    // Application Pulse is running in Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_EMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(true);\n\n    sysPulseUseLocator = Boolean.FALSE;\n    try {\n      // Get host name of machine running pulse in embedded mode\n      sysPulseHost = InetAddress.getLocalHost().getCanonicalHostName();\n    } catch (Exception e) {\n      logger.debug(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_UNKNOWN_HOST\"), e);\n      // Set default host name\n      sysPulseHost = PulseConstants.GEMFIRE_DEFAULT_HOST;\n    }\n    sysPulsePort = System.getProperty(PulseConstants.SYSTEM_PROPERTY_PULSE_PORT);\n    if (StringUtils.isBlank(sysPulsePort)) {\n      sysPulsePort = PulseConstants.GEMFIRE_DEFAULT_PORT;\n    }\n\n  } else {\n    // Application Pulse is running in Non-Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_NONEMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(false);\n\n    // Load JMX User Details\n    loadJMXUserDetails();\n    // Load locator and/or manager details\n    loadLocatorManagerDetails();\n\n    useGemFireCredentials = areWeUsingGemFireSecurityProfile(event);\n  }\n\n  // Set user details in repository\n  repository.setJmxUserName(jmxUserName);\n  repository.setJmxUserPassword(jmxUserPassword);\n\n  // Set locator/Manager details in repository\n  repository.setJmxUseLocator(sysPulseUseLocator);\n  repository.setJmxHost(sysPulseHost);\n  repository.setJmxPort(sysPulsePort);\n\n  // set SSL info\n  initializeSSL();\n  repository.setUseSSLLocator(sysPulseUseSSLLocator);\n  repository.setUseSSLManager(sysPulseUseSSLManager);\n\n  repository.setUseGemFireCredentials(useGemFireCredentials);\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2684,
        "instruction": "private static Class<? extends CompressionCodec> getCodecClassByPath(String path) {\n\n    Class<? extends CompressionCodec> codecClass = null;\n    CompressionCodec codec = factory.getCodec(new Path(path));\n    if (codec != null) {\n        codecClass = codec.getClass();\n    }\n    return codecClass;\n}",
        "accept_response": "private static Class<? extends CompressionCodec> getCodecClassByPath(String path) {\n\n    Class<? extends CompressionCodec> codecClass = null;\n    CompressionCodec codec = factory.getCodec(new Path(path));\n    if (codec != null) {\n        codecClass = codec.getClass();\n    }\n    LOG.debug((codecClass == null ? \"No codec\" : \"Codec \" + codecClass)\n            + \" was found for file \" + path);\n    return codecClass;\n}",
        "reject_response": "private static Class<? extends CompressionCodec> getCodecClassByPath(String path) {\n\n    Class<? extends CompressionCodec> codecClass = null;\n    CompressionCodec codec = factory.getCodec(new Path(path));\n    if (codec != null) {\n        codecClass = codec.getClass();\n    }\n    Log.debug((codecClass == null ? \"No codec\" : \"Codec \" + codecClass)\n    return codecClass;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3225,
        "instruction": "private void sendSaslPacket(byte[] saslToken, ClientCnxn cnxn)\n  throws SaslException{\n\n    GetSASLRequest request = new GetSASLRequest();\n    request.setToken(saslToken);\n    SetSASLResponse response = new SetSASLResponse();\n    ServerSaslResponseCallback cb = new ServerSaslResponseCallback();\n\n    try {\n        cnxn.sendPacket(request,response,cb, ZooDefs.OpCode.sasl);\n    } catch (IOException e) {\n        throw new SaslException(\"Failed to send SASL packet to server.\",\n            e);\n    }\n}",
        "accept_response": "private void sendSaslPacket(byte[] saslToken, ClientCnxn cnxn)\n  throws SaslException{\n    LOG.debug(\"ClientCnxn:sendSaslPacket:length={}\", saslToken.length);\n\n    GetSASLRequest request = new GetSASLRequest();\n    request.setToken(saslToken);\n    SetSASLResponse response = new SetSASLResponse();\n    ServerSaslResponseCallback cb = new ServerSaslResponseCallback();\n\n    try {\n        cnxn.sendPacket(request,response,cb, ZooDefs.OpCode.sasl);\n    } catch (IOException e) {\n        throw new SaslException(\"Failed to send SASL packet to server.\",\n            e);\n    }\n}",
        "reject_response": "private void sendSaslPacket(byte[] saslToken, ClientCnxn cnxn)\n  throws SaslException{\n    if (LOG.isDebugEnabled()) {\n        LOG.debug(\"ClientCnxn:sendSaslPacket:length=\"+saslToken.length);\n    }\n\n    GetSASLRequest request = new GetSASLRequest();\n    request.setToken(saslToken);\n    SetSASLResponse response = new SetSASLResponse();\n    ServerSaslResponseCallback cb = new ServerSaslResponseCallback();\n\n    try {\n        cnxn.sendPacket(request,response,cb, ZooDefs.OpCode.sasl);\n    } catch (IOException e) {\n        throw new SaslException(\"Failed to send SASL packet to server.\",\n            e);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3080,
        "instruction": "public void compile() throws IOException {\n  List<String> args = new ArrayList<String>();\n\n  // ensure that the jar output dir exists.\n  String jarOutDir = options.getJarOutputDir();\n  File jarOutDirObj = new File(jarOutDir);\n  if (!jarOutDirObj.exists()) {\n    boolean mkdirSuccess = jarOutDirObj.mkdirs();\n    if (!mkdirSuccess) {\n      LOG.debug(\"Warning: Could not make directories for \" + jarOutDir);\n    }\n  } else if (LOG.isDebugEnabled()) {\n    LOG.debug(\"Found existing \" + jarOutDir);\n  }\n\n  // Make sure jarOutDir ends with a '/'.\n  if (!jarOutDir.endsWith(File.separator)) {\n    jarOutDir = jarOutDir + File.separator;\n  }\n\n  // find hadoop-*-core.jar for classpath.\n  String coreJar = findHadoopJars();\n  if (null == coreJar) {\n    // Couldn't find a core jar to insert into the CP for compilation.  If,\n    // however, we're running this from a unit test, then the path to the\n    // .class files might be set via the hadoop.alt.classpath property\n    // instead. Check there first.\n    String coreClassesPath = System.getProperty(\"hadoop.alt.classpath\");\n    if (null == coreClassesPath) {\n      // no -- we're out of options. Fail.\n      throw new IOException(\"Could not find hadoop core jar!\");\n    } else {\n      coreJar = coreClassesPath;\n    }\n  }\n\n  // find sqoop jar for compilation classpath\n  String sqoopJar = Jars.getSqoopJarPath();\n  if (null != sqoopJar) {\n    sqoopJar = File.pathSeparator + sqoopJar;\n  } else {\n    LOG.warn(\"Could not find sqoop jar; child compilation may fail\");\n    sqoopJar = \"\";\n  }\n\n  String curClasspath = System.getProperty(\"java.class.path\");\n  LOG.debug(\"Current sqoop classpath = \" + curClasspath);\n\n  args.add(\"-sourcepath\");\n  args.add(jarOutDir);\n\n  args.add(\"-d\");\n  args.add(jarOutDir);\n\n  args.add(\"-classpath\");\n  args.add(curClasspath + File.pathSeparator + coreJar + sqoopJar);\n\n  JavaCompiler compiler = ToolProvider.getSystemJavaCompiler();\n  if (null == compiler) {\n    LOG.error(\"It seems as though you are running sqoop with a JRE.\");\n    LOG.error(\"Sqoop requires a JDK that can compile Java code.\");\n    LOG.error(\"Please install a JDK and set $JAVA_HOME to use it.\");\n    throw new IOException(\"Could not start Java compiler.\");\n  }\n  StandardJavaFileManager fileManager =\n      compiler.getStandardFileManager(null, null, null);\n\n  ArrayList<String> srcFileNames = new ArrayList<String>();\n  for (String srcfile : sources) {\n    srcFileNames.add(jarOutDir + srcfile);\n    LOG.debug(\"Adding source file: \" + jarOutDir + srcfile);\n  }\n\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"Invoking javac with args:\");\n    for (String arg : args) {\n      LOG.debug(\"  \" + arg);\n    }\n  }\n\n  Iterable<? extends JavaFileObject> srcFileObjs =\n      fileManager.getJavaFileObjectsFromStrings(srcFileNames);\n  JavaCompiler.CompilationTask task = compiler.getTask(\n      null, // Write to stderr\n      fileManager,\n      null, // No special diagnostic handling\n      args,\n      null, // Compile all classes in the source compilation units\n      srcFileObjs);\n\n  boolean result = task.call();\n  if (!result) {\n    throw new IOException(\"Error returned by javac\");\n  }\n\n  // Where we should move source files after compilation.\n  String srcOutDir = new File(options.getCodeOutputDir()).getAbsolutePath();\n  if (!srcOutDir.endsWith(File.separator)) {\n    srcOutDir = srcOutDir + File.separator;\n  }\n\n  // Move these files to the srcOutDir.\n  for (String srcFileName : sources) {\n    String orig = jarOutDir + srcFileName;\n    String dest = srcOutDir + srcFileName;\n    File fOrig = new File(orig);\n    File fDest = new File(dest);\n    File fDestParent = fDest.getParentFile();\n    if (null != fDestParent && !fDestParent.exists()) {\n      if (!fDestParent.mkdirs()) {\n        LOG.error(\"Could not make directory: \" + fDestParent);\n      }\n    }\n    try {\n        FileUtils.moveFile(fOrig, fDest);\n    } catch (IOException e) {\n  \t  /*Removed the exception being thrown\n  \t   *even if the .java file can not be renamed\n  \t   *or can not be moved a \"dest\" directory for\n  \t   *any reason.*/\n    }\n  }\n}",
        "accept_response": "public void compile() throws IOException {\n  List<String> args = new ArrayList<String>();\n\n  // ensure that the jar output dir exists.\n  String jarOutDir = options.getJarOutputDir();\n  File jarOutDirObj = new File(jarOutDir);\n  if (!jarOutDirObj.exists()) {\n    boolean mkdirSuccess = jarOutDirObj.mkdirs();\n    if (!mkdirSuccess) {\n      LOG.debug(\"Warning: Could not make directories for \" + jarOutDir);\n    }\n  } else if (LOG.isDebugEnabled()) {\n    LOG.debug(\"Found existing \" + jarOutDir);\n  }\n\n  // Make sure jarOutDir ends with a '/'.\n  if (!jarOutDir.endsWith(File.separator)) {\n    jarOutDir = jarOutDir + File.separator;\n  }\n\n  // find hadoop-*-core.jar for classpath.\n  String coreJar = findHadoopJars();\n  if (null == coreJar) {\n    // Couldn't find a core jar to insert into the CP for compilation.  If,\n    // however, we're running this from a unit test, then the path to the\n    // .class files might be set via the hadoop.alt.classpath property\n    // instead. Check there first.\n    String coreClassesPath = System.getProperty(\"hadoop.alt.classpath\");\n    if (null == coreClassesPath) {\n      // no -- we're out of options. Fail.\n      throw new IOException(\"Could not find hadoop core jar!\");\n    } else {\n      coreJar = coreClassesPath;\n    }\n  }\n\n  // find sqoop jar for compilation classpath\n  String sqoopJar = Jars.getSqoopJarPath();\n  if (null != sqoopJar) {\n    sqoopJar = File.pathSeparator + sqoopJar;\n  } else {\n    LOG.warn(\"Could not find sqoop jar; child compilation may fail\");\n    sqoopJar = \"\";\n  }\n\n  String curClasspath = System.getProperty(\"java.class.path\");\n  LOG.debug(\"Current sqoop classpath = \" + curClasspath);\n\n  args.add(\"-sourcepath\");\n  args.add(jarOutDir);\n\n  args.add(\"-d\");\n  args.add(jarOutDir);\n\n  args.add(\"-classpath\");\n  args.add(curClasspath + File.pathSeparator + coreJar + sqoopJar);\n\n  JavaCompiler compiler = ToolProvider.getSystemJavaCompiler();\n  if (null == compiler) {\n    LOG.error(\"It seems as though you are running sqoop with a JRE.\");\n    LOG.error(\"Sqoop requires a JDK that can compile Java code.\");\n    LOG.error(\"Please install a JDK and set $JAVA_HOME to use it.\");\n    throw new IOException(\"Could not start Java compiler.\");\n  }\n  StandardJavaFileManager fileManager =\n      compiler.getStandardFileManager(null, null, null);\n\n  ArrayList<String> srcFileNames = new ArrayList<String>();\n  for (String srcfile : sources) {\n    srcFileNames.add(jarOutDir + srcfile);\n    LOG.debug(\"Adding source file: \" + jarOutDir + srcfile);\n  }\n\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"Invoking javac with args:\");\n    for (String arg : args) {\n      LOG.debug(\"  \" + arg);\n    }\n  }\n\n  Iterable<? extends JavaFileObject> srcFileObjs =\n      fileManager.getJavaFileObjectsFromStrings(srcFileNames);\n  JavaCompiler.CompilationTask task = compiler.getTask(\n      null, // Write to stderr\n      fileManager,\n      null, // No special diagnostic handling\n      args,\n      null, // Compile all classes in the source compilation units\n      srcFileObjs);\n\n  boolean result = task.call();\n  if (!result) {\n    throw new IOException(\"Error returned by javac\");\n  }\n\n  // Where we should move source files after compilation.\n  String srcOutDir = new File(options.getCodeOutputDir()).getAbsolutePath();\n  if (!srcOutDir.endsWith(File.separator)) {\n    srcOutDir = srcOutDir + File.separator;\n  }\n\n  // Move these files to the srcOutDir.\n  for (String srcFileName : sources) {\n    String orig = jarOutDir + srcFileName;\n    String dest = srcOutDir + srcFileName;\n    File fOrig = new File(orig);\n    File fDest = new File(dest);\n    File fDestParent = fDest.getParentFile();\n    if (null != fDestParent && !fDestParent.exists()) {\n      if (!fDestParent.mkdirs()) {\n        LOG.error(\"Could not make directory: \" + fDestParent);\n      }\n    }\n    try {\n        FileUtils.moveFile(fOrig, fDest);\n    } catch (IOException e) {\n  \t  /*Removed the exception being thrown\n  \t   *even if the .java file can not be renamed\n  \t   *or can not be moved a \"dest\" directory for\n  \t   *any reason.*/\n        LOG.debug(\"Could not rename \" + orig + \" to \" + dest);\n    }\n  }\n}",
        "reject_response": "public void compile() throws IOException {\n  List<String> args = new ArrayList<String>();\n\n  // ensure that the jar output dir exists.\n  String jarOutDir = options.getJarOutputDir();\n  File jarOutDirObj = new File(jarOutDir);\n  if (!jarOutDirObj.exists()) {\n    boolean mkdirSuccess = jarOutDirObj.mkdirs();\n    if (!mkdirSuccess) {\n      LOG.debug(\"Warning: Could not make directories for \" + jarOutDir);\n    }\n  } else if (LOG.isDebugEnabled()) {\n    LOG.debug(\"Found existing \" + jarOutDir);\n  }\n\n  // Make sure jarOutDir ends with a '/'.\n  if (!jarOutDir.endsWith(File.separator)) {\n    jarOutDir = jarOutDir + File.separator;\n  }\n\n  // find hadoop-*-core.jar for classpath.\n  String coreJar = findHadoopJars();\n  if (null == coreJar) {\n    // Couldn't find a core jar to insert into the CP for compilation.  If,\n    // however, we're running this from a unit test, then the path to the\n    // .class files might be set via the hadoop.alt.classpath property\n    // instead. Check there first.\n    String coreClassesPath = System.getProperty(\"hadoop.alt.classpath\");\n    if (null == coreClassesPath) {\n      // no -- we're out of options. Fail.\n      throw new IOException(\"Could not find hadoop core jar!\");\n    } else {\n      coreJar = coreClassesPath;\n    }\n  }\n\n  // find sqoop jar for compilation classpath\n  String sqoopJar = Jars.getSqoopJarPath();\n  if (null != sqoopJar) {\n    sqoopJar = File.pathSeparator + sqoopJar;\n  } else {\n    LOG.warn(\"Could not find sqoop jar; child compilation may fail\");\n    sqoopJar = \"\";\n  }\n\n  String curClasspath = System.getProperty(\"java.class.path\");\n  LOG.debug(\"Current sqoop classpath = \" + curClasspath);\n\n  args.add(\"-sourcepath\");\n  args.add(jarOutDir);\n\n  args.add(\"-d\");\n  args.add(jarOutDir);\n\n  args.add(\"-classpath\");\n  args.add(curClasspath + File.pathSeparator + coreJar + sqoopJar);\n\n  JavaCompiler compiler = ToolProvider.getSystemJavaCompiler();\n  if (null == compiler) {\n    LOG.error(\"It seems as though you are running sqoop with a JRE.\");\n    LOG.error(\"Sqoop requires a JDK that can compile Java code.\");\n    LOG.error(\"Please install a JDK and set $JAVA_HOME to use it.\");\n    throw new IOException(\"Could not start Java compiler.\");\n  }\n  StandardJavaFileManager fileManager =\n      compiler.getStandardFileManager(null, null, null);\n\n  ArrayList<String> srcFileNames = new ArrayList<String>();\n  for (String srcfile : sources) {\n    srcFileNames.add(jarOutDir + srcfile);\n    LOG.debug(\"Adding source file: \" + jarOutDir + srcfile);\n  }\n\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"Invoking javac with args:\");\n    for (String arg : args) {\n      LOG.debug(\"  \" + arg);\n    }\n  }\n\n  Iterable<? extends JavaFileObject> srcFileObjs =\n      fileManager.getJavaFileObjectsFromStrings(srcFileNames);\n  JavaCompiler.CompilationTask task = compiler.getTask(\n      null, // Write to stderr\n      fileManager,\n      null, // No special diagnostic handling\n      args,\n      null, // Compile all classes in the source compilation units\n      srcFileObjs);\n\n  boolean result = task.call();\n  if (!result) {\n    throw new IOException(\"Error returned by javac\");\n  }\n\n  // Where we should move source files after compilation.\n  String srcOutDir = new File(options.getCodeOutputDir()).getAbsolutePath();\n  if (!srcOutDir.endsWith(File.separator)) {\n    srcOutDir = srcOutDir + File.separator;\n  }\n\n  // Move these files to the srcOutDir.\n  for (String srcFileName : sources) {\n    String orig = jarOutDir + srcFileName;\n    String dest = srcOutDir + srcFileName;\n    File fOrig = new File(orig);\n    File fDest = new File(dest);\n    File fDestParent = fDest.getParentFile();\n    if (null != fDestParent && !fDestParent.exists()) {\n      if (!fDestParent.mkdirs()) {\n        LOG.error(\"Could not make directory: \" + fDestParent);\n      }\n    }\n    try {\n        FileUtils.moveFile(fOrig, fDest);\n    } catch (IOException e) {\n  \t  /*Removed the exception being thrown\n  \t   *even if the .java file can not be renamed\n  \t   *or can not be moved a \"dest\" directory for\n  \t   *any reason.*/\n        LOG.debug(\"Could not rename \" + orig + \" to \" + dest, e);\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2624,
        "instruction": "@ExceptionHandler(IOException.class)\n@ResponseStatus(HttpStatus.INTERNAL_SERVER_ERROR)\npublic void handleExc(IOException ext) {\n  // write errors\n  StringWriter swBuffer = new StringWriter();\n  PrintWriter prtWriter = new PrintWriter(swBuffer);\n  ext.printStackTrace(prtWriter);\n}",
        "accept_response": "@ExceptionHandler(IOException.class)\n@ResponseStatus(HttpStatus.INTERNAL_SERVER_ERROR)\npublic void handleExc(IOException ext) {\n  // write errors\n  StringWriter swBuffer = new StringWriter();\n  PrintWriter prtWriter = new PrintWriter(swBuffer);\n  ext.printStackTrace(prtWriter);\n  logger.fatal(\"IOException Details : {}\\n\", swBuffer);\n}",
        "reject_response": "@ExceptionHandler(IOException.class)\n@ResponseStatus(HttpStatus.INTERNAL_SERVER_ERROR)\npublic void handleExc(IOException ext) {\n  // write errors\n  StringWriter swBuffer = new StringWriter();\n  PrintWriter prtWriter = new PrintWriter(swBuffer);\n  ext.printStackTrace(prtWriter);\n  LOGGER.severe(\"IOException Details : \" + swBuffer.toString() + \"\\n\");\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3009,
        "instruction": "private void init(RangerPolicyEngineOptions options) {\n    RangerServiceDefHelper serviceDefHelper = new RangerServiceDefHelper(serviceDef, false);\n    options.setServiceDefHelper(serviceDefHelper);\n\n    List<RangerPolicyEvaluator> policyEvaluators = new ArrayList<>();\n    List<RangerPolicyEvaluator> dataMaskPolicyEvaluators  = new ArrayList<>();\n    List<RangerPolicyEvaluator> rowFilterPolicyEvaluators = new ArrayList<>();\n\n    for (RangerPolicy policy : policies) {\n        if (skipBuildingPolicyEvaluator(policy, options)) {\n            continue;\n        }\n\n        RangerPolicyEvaluator evaluator = buildPolicyEvaluator(policy, serviceDef, options);\n\n        if (evaluator != null) {\n            if(policy.getPolicyType() == null || policy.getPolicyType() == RangerPolicy.POLICY_TYPE_ACCESS) {\n                policyEvaluators.add(evaluator);\n            } else if(policy.getPolicyType() == RangerPolicy.POLICY_TYPE_DATAMASK) {\n                dataMaskPolicyEvaluators.add(evaluator);\n            } else if(policy.getPolicyType() == RangerPolicy.POLICY_TYPE_ROWFILTER) {\n                rowFilterPolicyEvaluators.add(evaluator);\n            } else {\n                LOG.warn(\"RangerPolicyEngine: ignoring policy id=\" + policy.getId() + \" - invalid policyType '\" + policy.getPolicyType() + \"'\");\n            }\n        }\n    }\n    LOG.info(\"This policy engine contains \" + (policyEvaluators.size()+dataMaskPolicyEvaluators.size()+rowFilterPolicyEvaluators.size()) + \" policy evaluators\");\n    RangerPolicyEvaluator.PolicyEvalOrderComparator comparator = new RangerPolicyEvaluator.PolicyEvalOrderComparator();\n    Collections.sort(policyEvaluators, comparator);\n    this.policyEvaluators = policyEvaluators;\n\n    Collections.sort(dataMaskPolicyEvaluators, comparator);\n    this.dataMaskPolicyEvaluators = dataMaskPolicyEvaluators;\n\n    Collections.sort(rowFilterPolicyEvaluators, comparator);\n    this.rowFilterPolicyEvaluators = rowFilterPolicyEvaluators;\n\n    this.policyEvaluatorsMap = createPolicyEvaluatorsMap();\n\n    if(LOG.isDebugEnabled()) {\n        LOG.debug(\"policy evaluation order: \" + this.policyEvaluators.size() + \" policies\");\n\n        int order = 0;\n        for(RangerPolicyEvaluator policyEvaluator : this.policyEvaluators) {\n            RangerPolicy policy = policyEvaluator.getPolicy();\n\n            LOG.debug(\"policy evaluation order: #\" + (++order) + \" - policy id=\" + policy.getId() + \"; name=\" + policy.getName() + \"; evalOrder=\" + policyEvaluator.getEvalOrder());\n        }\n\n        LOG.debug(\"dataMask policy evaluation order: \" + this.dataMaskPolicyEvaluators.size() + \" policies\");\n        order = 0;\n        for(RangerPolicyEvaluator policyEvaluator : this.dataMaskPolicyEvaluators) {\n            RangerPolicy policy = policyEvaluator.getPolicy();\n\n            LOG.debug(\"dataMask policy evaluation order: #\" + (++order) + \" - policy id=\" + policy.getId() + \"; name=\" + policy.getName() + \"; evalOrder=\" + policyEvaluator.getEvalOrder());\n        }\n\n        LOG.debug(\"rowFilter policy evaluation order: \" + this.rowFilterPolicyEvaluators.size() + \" policies\");\n        order = 0;\n        for(RangerPolicyEvaluator policyEvaluator : this.rowFilterPolicyEvaluators) {\n            RangerPolicy policy = policyEvaluator.getPolicy();\n\n            LOG.debug(\"rowFilter policy evaluation order: #\" + (++order) + \" - policy id=\" + policy.getId() + \"; name=\" + policy.getName() + \"; evalOrder=\" + policyEvaluator.getEvalOrder());\n        }\n\n        LOG.debug(\"audit policy evaluation order: \" + ((this.auditPolicyEvaluators != null) ? this.auditPolicyEvaluators.size() : 0  + \" policies\"));\n        if (this.auditPolicyEvaluators != null) {\n            order = 0;\n            for(RangerPolicyEvaluator policyEvaluator : this.auditPolicyEvaluators) {\n                RangerPolicy policy = policyEvaluator.getPolicy();\n\n            }\n        }\n    }\n}",
        "accept_response": "private void init(RangerPolicyEngineOptions options) {\n    RangerServiceDefHelper serviceDefHelper = new RangerServiceDefHelper(serviceDef, false);\n    options.setServiceDefHelper(serviceDefHelper);\n\n    List<RangerPolicyEvaluator> policyEvaluators = new ArrayList<>();\n    List<RangerPolicyEvaluator> dataMaskPolicyEvaluators  = new ArrayList<>();\n    List<RangerPolicyEvaluator> rowFilterPolicyEvaluators = new ArrayList<>();\n\n    for (RangerPolicy policy : policies) {\n        if (skipBuildingPolicyEvaluator(policy, options)) {\n            continue;\n        }\n\n        RangerPolicyEvaluator evaluator = buildPolicyEvaluator(policy, serviceDef, options);\n\n        if (evaluator != null) {\n            if(policy.getPolicyType() == null || policy.getPolicyType() == RangerPolicy.POLICY_TYPE_ACCESS) {\n                policyEvaluators.add(evaluator);\n            } else if(policy.getPolicyType() == RangerPolicy.POLICY_TYPE_DATAMASK) {\n                dataMaskPolicyEvaluators.add(evaluator);\n            } else if(policy.getPolicyType() == RangerPolicy.POLICY_TYPE_ROWFILTER) {\n                rowFilterPolicyEvaluators.add(evaluator);\n            } else {\n                LOG.warn(\"RangerPolicyEngine: ignoring policy id=\" + policy.getId() + \" - invalid policyType '\" + policy.getPolicyType() + \"'\");\n            }\n        }\n    }\n    LOG.info(\"This policy engine contains \" + (policyEvaluators.size()+dataMaskPolicyEvaluators.size()+rowFilterPolicyEvaluators.size()) + \" policy evaluators\");\n    RangerPolicyEvaluator.PolicyEvalOrderComparator comparator = new RangerPolicyEvaluator.PolicyEvalOrderComparator();\n    Collections.sort(policyEvaluators, comparator);\n    this.policyEvaluators = policyEvaluators;\n\n    Collections.sort(dataMaskPolicyEvaluators, comparator);\n    this.dataMaskPolicyEvaluators = dataMaskPolicyEvaluators;\n\n    Collections.sort(rowFilterPolicyEvaluators, comparator);\n    this.rowFilterPolicyEvaluators = rowFilterPolicyEvaluators;\n\n    this.policyEvaluatorsMap = createPolicyEvaluatorsMap();\n\n    if(LOG.isDebugEnabled()) {\n        LOG.debug(\"policy evaluation order: \" + this.policyEvaluators.size() + \" policies\");\n\n        int order = 0;\n        for(RangerPolicyEvaluator policyEvaluator : this.policyEvaluators) {\n            RangerPolicy policy = policyEvaluator.getPolicy();\n\n            LOG.debug(\"policy evaluation order: #\" + (++order) + \" - policy id=\" + policy.getId() + \"; name=\" + policy.getName() + \"; evalOrder=\" + policyEvaluator.getEvalOrder());\n        }\n\n        LOG.debug(\"dataMask policy evaluation order: \" + this.dataMaskPolicyEvaluators.size() + \" policies\");\n        order = 0;\n        for(RangerPolicyEvaluator policyEvaluator : this.dataMaskPolicyEvaluators) {\n            RangerPolicy policy = policyEvaluator.getPolicy();\n\n            LOG.debug(\"dataMask policy evaluation order: #\" + (++order) + \" - policy id=\" + policy.getId() + \"; name=\" + policy.getName() + \"; evalOrder=\" + policyEvaluator.getEvalOrder());\n        }\n\n        LOG.debug(\"rowFilter policy evaluation order: \" + this.rowFilterPolicyEvaluators.size() + \" policies\");\n        order = 0;\n        for(RangerPolicyEvaluator policyEvaluator : this.rowFilterPolicyEvaluators) {\n            RangerPolicy policy = policyEvaluator.getPolicy();\n\n            LOG.debug(\"rowFilter policy evaluation order: #\" + (++order) + \" - policy id=\" + policy.getId() + \"; name=\" + policy.getName() + \"; evalOrder=\" + policyEvaluator.getEvalOrder());\n        }\n\n        LOG.debug(\"audit policy evaluation order: \" + ((this.auditPolicyEvaluators != null) ? this.auditPolicyEvaluators.size() : 0  + \" policies\"));\n        if (this.auditPolicyEvaluators != null) {\n            order = 0;\n            for(RangerPolicyEvaluator policyEvaluator : this.auditPolicyEvaluators) {\n                RangerPolicy policy = policyEvaluator.getPolicy();\n\n                LOG.debug(\"audit policy evaluation order: #\" + (++order) + \" - policy id=\" + policy.getId() + \"; name=\" + policy.getName() + \"; evalOrder=\" + policyEvaluator.getEvalOrder());\n            }\n        }\n    }\n}",
        "reject_response": "private void init(RangerPolicyEngineOptions options) {\n    RangerServiceDefHelper serviceDefHelper = new RangerServiceDefHelper(serviceDef, false);\n    options.setServiceDefHelper(serviceDefHelper);\n\n    List<RangerPolicyEvaluator> policyEvaluators = new ArrayList<>();\n    List<RangerPolicyEvaluator> dataMaskPolicyEvaluators  = new ArrayList<>();\n    List<RangerPolicyEvaluator> rowFilterPolicyEvaluators = new ArrayList<>();\n\n    for (RangerPolicy policy : policies) {\n        if (skipBuildingPolicyEvaluator(policy, options)) {\n            continue;\n        }\n\n        RangerPolicyEvaluator evaluator = buildPolicyEvaluator(policy, serviceDef, options);\n\n        if (evaluator != null) {\n            if(policy.getPolicyType() == null || policy.getPolicyType() == RangerPolicy.POLICY_TYPE_ACCESS) {\n                policyEvaluators.add(evaluator);\n            } else if(policy.getPolicyType() == RangerPolicy.POLICY_TYPE_DATAMASK) {\n                dataMaskPolicyEvaluators.add(evaluator);\n            } else if(policy.getPolicyType() == RangerPolicy.POLICY_TYPE_ROWFILTER) {\n                rowFilterPolicyEvaluators.add(evaluator);\n            } else {\n                LOG.warn(\"RangerPolicyEngine: ignoring policy id=\" + policy.getId() + \" - invalid policyType '\" + policy.getPolicyType() + \"'\");\n            }\n        }\n    }\n    LOG.info(\"This policy engine contains \" + (policyEvaluators.size()+dataMaskPolicyEvaluators.size()+rowFilterPolicyEvaluators.size()) + \" policy evaluators\");\n    RangerPolicyEvaluator.PolicyEvalOrderComparator comparator = new RangerPolicyEvaluator.PolicyEvalOrderComparator();\n    Collections.sort(policyEvaluators, comparator);\n    this.policyEvaluators = policyEvaluators;\n\n    Collections.sort(dataMaskPolicyEvaluators, comparator);\n    this.dataMaskPolicyEvaluators = dataMaskPolicyEvaluators;\n\n    Collections.sort(rowFilterPolicyEvaluators, comparator);\n    this.rowFilterPolicyEvaluators = rowFilterPolicyEvaluators;\n\n    this.policyEvaluatorsMap = createPolicyEvaluatorsMap();\n\n    if(LOG.isDebugEnabled()) {\n        LOG.debug(\"policy evaluation order: \" + this.policyEvaluators.size() + \" policies\");\n\n        int order = 0;\n        for(RangerPolicyEvaluator policyEvaluator : this.policyEvaluators) {\n            RangerPolicy policy = policyEvaluator.getPolicy();\n\n            LOG.debug(\"policy evaluation order: #\" + (++order) + \" - policy id=\" + policy.getId() + \"; name=\" + policy.getName() + \"; evalOrder=\" + policyEvaluator.getEvalOrder());\n        }\n\n        LOG.debug(\"dataMask policy evaluation order: \" + this.dataMaskPolicyEvaluators.size() + \" policies\");\n        order = 0;\n        for(RangerPolicyEvaluator policyEvaluator : this.dataMaskPolicyEvaluators) {\n            RangerPolicy policy = policyEvaluator.getPolicy();\n\n            LOG.debug(\"dataMask policy evaluation order: #\" + (++order) + \" - policy id=\" + policy.getId() + \"; name=\" + policy.getName() + \"; evalOrder=\" + policyEvaluator.getEvalOrder());\n        }\n\n        LOG.debug(\"rowFilter policy evaluation order: \" + this.rowFilterPolicyEvaluators.size() + \" policies\");\n        order = 0;\n        for(RangerPolicyEvaluator policyEvaluator : this.rowFilterPolicyEvaluators) {\n            RangerPolicy policy = policyEvaluator.getPolicy();\n\n            LOG.debug(\"rowFilter policy evaluation order: #\" + (++order) + \" - policy id=\" + policy.getId() + \"; name=\" + policy.getName() + \"; evalOrder=\" + policyEvaluator.getEvalOrder());\n        }\n\n        LOG.debug(\"audit policy evaluation order: \" + ((this.auditPolicyEvaluators != null) ? this.auditPolicyEvaluators.size() : 0  + \" policies\"));\n        if (this.auditPolicyEvaluators != null) {\n            order = 0;\n            for(RangerPolicyEvaluator policyEvaluator : this.auditPolicyEvaluators) {\n                RangerPolicy policy = policyEvaluator.getPolicy();\n\n                LOG.debug(\"rowFilter policy evaluation order: #\" + (++order) + \" - policy id=\" + policy.getId() + \"; name=\" + policy.getName() + \"; evalOrder=\" + policyEvaluator.getEvalOrder());\n            }\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2396,
        "instruction": "@Override\npublic void run() {\n  try {\n    MutationSet rf = null;\n\n    synchronized (this) {\n      if (recentFailures != null && System.currentTimeMillis() - initTime > 1000) {\n        rf = recentFailures;\n        recentFailures = null;\n      }\n    }\n\n    if (rf != null) {\n      if (log.isTraceEnabled())\n      addFailedMutations(rf);\n    }\n  } catch (Throwable t) {\n    updateUnknownErrors(\"tid=\" + Thread.currentThread().getId() + \"  Failed to requeue failed mutations \" + t.getMessage(), t);\n    cancel();\n  }\n}",
        "accept_response": "@Override\npublic void run() {\n  try {\n    MutationSet rf = null;\n\n    synchronized (this) {\n      if (recentFailures != null && System.currentTimeMillis() - initTime > 1000) {\n        rf = recentFailures;\n        recentFailures = null;\n      }\n    }\n\n    if (rf != null) {\n      if (log.isTraceEnabled())\n        log.trace(\"tid={}  Requeuing {} failed mutations\", Thread.currentThread().getId(), rf.size());\n      addFailedMutations(rf);\n    }\n  } catch (Throwable t) {\n    updateUnknownErrors(\"tid=\" + Thread.currentThread().getId() + \"  Failed to requeue failed mutations \" + t.getMessage(), t);\n    cancel();\n  }\n}",
        "reject_response": "@Override\npublic void run() {\n  try {\n    MutationSet rf = null;\n\n    synchronized (this) {\n      if (recentFailures != null && System.currentTimeMillis() - initTime > 1000) {\n        rf = recentFailures;\n        recentFailures = null;\n      }\n    }\n\n    if (rf != null) {\n      if (log.isTraceEnabled())\n        log.trace(\"tid=\" + Thread.currentThread().getId() + \"  Requeuing \" + rf.size() + \" failed mutations\");\n      addFailedMutations(rf);\n    }\n  } catch (Throwable t) {\n    updateUnknownErrors(\"tid=\" + Thread.currentThread().getId() + \"  Failed to requeue failed mutations \" + t.getMessage(), t);\n    cancel();\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3186,
        "instruction": "@Override\npublic void signalToContainer(ContainerId containerId,\n    SignalContainerCommand command)\n        throws YarnException, IOException {\n  SignalContainerRequest request =\n      SignalContainerRequest.newInstance(containerId, command);\n  rmClient.signalToContainer(request);\n}",
        "accept_response": "@Override\npublic void signalToContainer(ContainerId containerId,\n    SignalContainerCommand command)\n        throws YarnException, IOException {\n  LOG.info(\"Signalling container {} with command {}\", containerId, command);\n  SignalContainerRequest request =\n      SignalContainerRequest.newInstance(containerId, command);\n  rmClient.signalToContainer(request);\n}",
        "reject_response": "@Override\npublic void signalToContainer(ContainerId containerId,\n    SignalContainerCommand command)\n        throws YarnException, IOException {\n  LOG.info(\"Signalling container \" + containerId + \" with command \" + command);\n  SignalContainerRequest request =\n      SignalContainerRequest.newInstance(containerId, command);\n  rmClient.signalToContainer(request);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2876,
        "instruction": "private static String remove(String lang) {\n    if ( rewiredAlternative != null )\n    String oldClassName = currentEntry(lang);\n    custom.remove(lang);\n    return oldClassName;\n}",
        "accept_response": "private static String remove(String lang) {\n    if ( rewiredAlternative != null )\n        Log.error(RDFWriterFImpl.class, \"Rewired RDFWriterFImpl2 - configuration changes have no effect on writing\");\n    String oldClassName = currentEntry(lang);\n    custom.remove(lang);\n    return oldClassName;\n}",
        "reject_response": "private static String remove(String lang) {\n    if ( rewiredAlternative != null )\n        Log.fatal(RDFWriterFImpl.class, \"Rewired RDFWriterFImpl2 - configuration changes have no effect on writing\");\n    String oldClassName = currentEntry(lang);\n    custom.remove(lang);\n    return oldClassName;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3131,
        "instruction": "@Override\npublic void registerRunningContainer(ContainerId containerId, int taskCommId) {\n  ContainerInfo oldInfo = registeredContainers.put(containerId, NULL_CONTAINER_INFO);\n  if (oldInfo != null) {\n    throw new TezUncheckedException(\n        \"Multiple registrations for containerId: \" + containerId);\n  }\n  NodeId nodeId = context.getAllContainers().get(containerId).getContainer().getNodeId();\n  try {\n    taskCommunicators[taskCommId].registerRunningContainer(containerId, nodeId.getHost(),\n        nodeId.getPort());\n  } catch (Exception e) {\n    String msg = \"Error in TaskCommunicator when registering running Container\"\n        + \", communicator=\" + Utils.getTaskCommIdentifierString(taskCommId, context)\n        + \", containerId=\" + containerId\n        + \", nodeId=\" + nodeId;\n    LOG.error(msg, e);\n    sendEvent(\n        new DAGAppMasterEventUserServiceFatalError(\n            DAGAppMasterEventType.TASK_COMMUNICATOR_SERVICE_FATAL_ERROR,\n            msg, e));\n  }\n}",
        "accept_response": "@Override\npublic void registerRunningContainer(ContainerId containerId, int taskCommId) {\n  LOG.debug(\"ContainerId: {} registered with TaskAttemptListener\", containerId);\n  ContainerInfo oldInfo = registeredContainers.put(containerId, NULL_CONTAINER_INFO);\n  if (oldInfo != null) {\n    throw new TezUncheckedException(\n        \"Multiple registrations for containerId: \" + containerId);\n  }\n  NodeId nodeId = context.getAllContainers().get(containerId).getContainer().getNodeId();\n  try {\n    taskCommunicators[taskCommId].registerRunningContainer(containerId, nodeId.getHost(),\n        nodeId.getPort());\n  } catch (Exception e) {\n    String msg = \"Error in TaskCommunicator when registering running Container\"\n        + \", communicator=\" + Utils.getTaskCommIdentifierString(taskCommId, context)\n        + \", containerId=\" + containerId\n        + \", nodeId=\" + nodeId;\n    LOG.error(msg, e);\n    sendEvent(\n        new DAGAppMasterEventUserServiceFatalError(\n            DAGAppMasterEventType.TASK_COMMUNICATOR_SERVICE_FATAL_ERROR,\n            msg, e));\n  }\n}",
        "reject_response": "@Override\npublic void registerRunningContainer(ContainerId containerId, int taskCommId) {\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"ContainerId: \" + containerId + \" registered with TaskAttemptListener\");\n  }\n  ContainerInfo oldInfo = registeredContainers.put(containerId, NULL_CONTAINER_INFO);\n  if (oldInfo != null) {\n    throw new TezUncheckedException(\n        \"Multiple registrations for containerId: \" + containerId);\n  }\n  NodeId nodeId = context.getAllContainers().get(containerId).getContainer().getNodeId();\n  try {\n    taskCommunicators[taskCommId].registerRunningContainer(containerId, nodeId.getHost(),\n        nodeId.getPort());\n  } catch (Exception e) {\n    String msg = \"Error in TaskCommunicator when registering running Container\"\n        + \", communicator=\" + Utils.getTaskCommIdentifierString(taskCommId, context)\n        + \", containerId=\" + containerId\n        + \", nodeId=\" + nodeId;\n    LOG.error(msg, e);\n    sendEvent(\n        new DAGAppMasterEventUserServiceFatalError(\n            DAGAppMasterEventType.TASK_COMMUNICATOR_SERVICE_FATAL_ERROR,\n            msg, e));\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2406,
        "instruction": "private void loadInputs() {\n  for (Map<String, Object> map : inputConfigList) {\n    if (map == null) {\n      continue;\n    }\n    mergeBlocks(globalConfigs, map);\n\n    String value = (String) map.get(\"source\");\n    if (StringUtils.isEmpty(value)) {\n      LOG.error(\"Input block doesn't have source element\");\n      continue;\n    }\n    Input input = (Input) AliasUtil.getClassInstance(value, AliasType.INPUT);\n    if (input == null) {\n      continue;\n    }\n    input.setType(value);\n    input.loadConfig(map);\n\n    if (input.isEnabled()) {\n      input.setOutputManager(outputManager);\n      input.setInputManager(inputManager);\n      inputManager.add(input);\n      input.logConfgs(Level.INFO);\n    } else {\n      LOG.info(\"Input is disabled. So ignoring it. \" + input.getShortDescription());\n    }\n  }\n}",
        "accept_response": "private void loadInputs() {\n  for (Map<String, Object> map : inputConfigList) {\n    if (map == null) {\n      continue;\n    }\n    mergeBlocks(globalConfigs, map);\n\n    String value = (String) map.get(\"source\");\n    if (StringUtils.isEmpty(value)) {\n      LOG.error(\"Input block doesn't have source element\");\n      continue;\n    }\n    Input input = (Input) AliasUtil.getClassInstance(value, AliasType.INPUT);\n    if (input == null) {\n      LOG.error(\"Input object could not be found\");\n      continue;\n    }\n    input.setType(value);\n    input.loadConfig(map);\n\n    if (input.isEnabled()) {\n      input.setOutputManager(outputManager);\n      input.setInputManager(inputManager);\n      inputManager.add(input);\n      input.logConfgs(Level.INFO);\n    } else {\n      LOG.info(\"Input is disabled. So ignoring it. \" + input.getShortDescription());\n    }\n  }\n}",
        "reject_response": "private void loadInputs() {\n  for (Map<String, Object> map : inputConfigList) {\n    if (map == null) {\n      continue;\n    }\n    mergeBlocks(globalConfigs, map);\n\n    String value = (String) map.get(\"source\");\n    if (StringUtils.isEmpty(value)) {\n      LOG.error(\"Input block doesn't have source element\");\n      continue;\n    }\n    Input input = (Input) AliasUtil.getClassInstance(value, AliasType.INPUT);\n    if (input == null) {\n      logger.error(\"Source Object is null\");\n      continue;\n    }\n    input.setType(value);\n    input.loadConfig(map);\n\n    if (input.isEnabled()) {\n      input.setOutputManager(outputManager);\n      input.setInputManager(inputManager);\n      inputManager.add(input);\n      input.logConfgs(Level.INFO);\n    } else {\n      LOG.info(\"Input is disabled. So ignoring it. \" + input.getShortDescription());\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2925,
        "instruction": "private void closeTopology() {\n\n    partitionGroup.clear();\n\n    // close the processors\n    // make sure close() is called for each node even when there is a RuntimeException\n    RuntimeException exception = null;\n    for (final ProcessorNode node : topology.processors()) {\n        processorContext.setCurrentNode(node);\n        try {\n            node.close();\n        } catch (final RuntimeException e) {\n            exception = e;\n        } finally {\n            processorContext.setCurrentNode(null);\n        }\n    }\n\n    if (exception != null) {\n        throw exception;\n    }\n}",
        "accept_response": "private void closeTopology() {\n    log.trace(\"{} Closing processor topology\", logPrefix);\n\n    partitionGroup.clear();\n\n    // close the processors\n    // make sure close() is called for each node even when there is a RuntimeException\n    RuntimeException exception = null;\n    for (final ProcessorNode node : topology.processors()) {\n        processorContext.setCurrentNode(node);\n        try {\n            node.close();\n        } catch (final RuntimeException e) {\n            exception = e;\n        } finally {\n            processorContext.setCurrentNode(null);\n        }\n    }\n\n    if (exception != null) {\n        throw exception;\n    }\n}",
        "reject_response": "private void closeTopology() {\n    log.debug(\"{} Closing processor topology\", logPrefix);\n\n    partitionGroup.clear();\n\n    // close the processors\n    // make sure close() is called for each node even when there is a RuntimeException\n    RuntimeException exception = null;\n    for (final ProcessorNode node : topology.processors()) {\n        processorContext.setCurrentNode(node);\n        try {\n            node.close();\n        } catch (final RuntimeException e) {\n            exception = e;\n        } finally {\n            processorContext.setCurrentNode(null);\n        }\n    }\n\n    if (exception != null) {\n        throw exception;\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2721,
        "instruction": "protected Path closeRecoveredEditsWriter(RecoveredEditsWriter editsWriter,\n    List<IOException> thrown) throws IOException {\n  try {\n    editsWriter.writer.close();\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not close recovered edits at \" + editsWriter.path;\n    LOG.error(errorMsg, ioe);\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  final String msg = \"Closed recovered edits writer path=\" + editsWriter.path + \" (wrote \"\n    + editsWriter.editsWritten + \" edits, skipped \" + editsWriter.editsSkipped + \" edits in \" + (\n    editsWriter.nanosSpent / 1000 / 1000) + \" ms)\";\n  LOG.info(msg);\n  updateStatusWithMsg(msg);\n  if (editsWriter.editsWritten == 0) {\n    // just remove the empty recovered.edits file\n    if (walSplitter.walFS.exists(editsWriter.path)\n        && !walSplitter.walFS.delete(editsWriter.path, false)) {\n      final String errorMsg = \"Failed deleting empty \" + editsWriter.path;\n      LOG.warn(errorMsg);\n      updateStatusWithMsg(errorMsg);\n      throw new IOException(\"Failed deleting empty  \" + editsWriter.path);\n    }\n    return null;\n  }\n\n  Path dst = getCompletedRecoveredEditsFilePath(editsWriter.path,\n    regionMaximumEditLogSeqNum.get(Bytes.toString(editsWriter.encodedRegionName)));\n  try {\n    if (!dst.equals(editsWriter.path) && walSplitter.walFS.exists(dst)) {\n      deleteOneWithFewerEntries(editsWriter, dst);\n    }\n    // Skip the unit tests which create a splitter that reads and\n    // writes the data without touching disk.\n    // TestHLogSplit#testThreading is an example.\n    if (walSplitter.walFS.exists(editsWriter.path)) {\n      if (!walSplitter.walFS.rename(editsWriter.path, dst)) {\n        final String errorMsg =\n          \"Failed renaming recovered edits \" + editsWriter.path + \" to \" + dst;\n        updateStatusWithMsg(errorMsg);\n        throw new IOException(errorMsg);\n      }\n      final String renameEditMsg = \"Rename recovered edits \" + editsWriter.path + \" to \" + dst;\n      LOG.info(renameEditMsg);\n      updateStatusWithMsg(renameEditMsg);\n    }\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not rename recovered edits \" + editsWriter.path\n      + \" to \" + dst;\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  return dst;\n}",
        "accept_response": "protected Path closeRecoveredEditsWriter(RecoveredEditsWriter editsWriter,\n    List<IOException> thrown) throws IOException {\n  try {\n    editsWriter.writer.close();\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not close recovered edits at \" + editsWriter.path;\n    LOG.error(errorMsg, ioe);\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  final String msg = \"Closed recovered edits writer path=\" + editsWriter.path + \" (wrote \"\n    + editsWriter.editsWritten + \" edits, skipped \" + editsWriter.editsSkipped + \" edits in \" + (\n    editsWriter.nanosSpent / 1000 / 1000) + \" ms)\";\n  LOG.info(msg);\n  updateStatusWithMsg(msg);\n  if (editsWriter.editsWritten == 0) {\n    // just remove the empty recovered.edits file\n    if (walSplitter.walFS.exists(editsWriter.path)\n        && !walSplitter.walFS.delete(editsWriter.path, false)) {\n      final String errorMsg = \"Failed deleting empty \" + editsWriter.path;\n      LOG.warn(errorMsg);\n      updateStatusWithMsg(errorMsg);\n      throw new IOException(\"Failed deleting empty  \" + editsWriter.path);\n    }\n    return null;\n  }\n\n  Path dst = getCompletedRecoveredEditsFilePath(editsWriter.path,\n    regionMaximumEditLogSeqNum.get(Bytes.toString(editsWriter.encodedRegionName)));\n  try {\n    if (!dst.equals(editsWriter.path) && walSplitter.walFS.exists(dst)) {\n      deleteOneWithFewerEntries(editsWriter, dst);\n    }\n    // Skip the unit tests which create a splitter that reads and\n    // writes the data without touching disk.\n    // TestHLogSplit#testThreading is an example.\n    if (walSplitter.walFS.exists(editsWriter.path)) {\n      if (!walSplitter.walFS.rename(editsWriter.path, dst)) {\n        final String errorMsg =\n          \"Failed renaming recovered edits \" + editsWriter.path + \" to \" + dst;\n        updateStatusWithMsg(errorMsg);\n        throw new IOException(errorMsg);\n      }\n      final String renameEditMsg = \"Rename recovered edits \" + editsWriter.path + \" to \" + dst;\n      LOG.info(renameEditMsg);\n      updateStatusWithMsg(renameEditMsg);\n    }\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not rename recovered edits \" + editsWriter.path\n      + \" to \" + dst;\n    LOG.error(errorMsg, ioe);\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  return dst;\n}",
        "reject_response": "protected Path closeRecoveredEditsWriter(RecoveredEditsWriter editsWriter,\n    List<IOException> thrown) throws IOException {\n  try {\n    editsWriter.writer.close();\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not close recovered edits at \" + editsWriter.path;\n    LOG.error(errorMsg, ioe);\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  final String msg = \"Closed recovered edits writer path=\" + editsWriter.path + \" (wrote \"\n    + editsWriter.editsWritten + \" edits, skipped \" + editsWriter.editsSkipped + \" edits in \" + (\n    editsWriter.nanosSpent / 1000 / 1000) + \" ms)\";\n  LOG.info(msg);\n  updateStatusWithMsg(msg);\n  if (editsWriter.editsWritten == 0) {\n    // just remove the empty recovered.edits file\n    if (walSplitter.walFS.exists(editsWriter.path)\n        && !walSplitter.walFS.delete(editsWriter.path, false)) {\n      final String errorMsg = \"Failed deleting empty \" + editsWriter.path;\n      LOG.warn(errorMsg);\n      updateStatusWithMsg(errorMsg);\n      throw new IOException(\"Failed deleting empty  \" + editsWriter.path);\n    }\n    return null;\n  }\n\n  Path dst = getCompletedRecoveredEditsFilePath(editsWriter.path,\n    regionMaximumEditLogSeqNum.get(Bytes.toString(editsWriter.encodedRegionName)));\n  try {\n    if (!dst.equals(editsWriter.path) && walSplitter.walFS.exists(dst)) {\n      deleteOneWithFewerEntries(editsWriter, dst);\n    }\n    // Skip the unit tests which create a splitter that reads and\n    // writes the data without touching disk.\n    // TestHLogSplit#testThreading is an example.\n    if (walSplitter.walFS.exists(editsWriter.path)) {\n      if (!walSplitter.walFS.rename(editsWriter.path, dst)) {\n        final String errorMsg =\n          \"Failed renaming recovered edits \" + editsWriter.path + \" to \" + dst;\n        updateStatusWithMsg(errorMsg);\n        throw new IOException(errorMsg);\n      }\n      final String renameEditMsg = \"Rename recovered edits \" + editsWriter.path + \" to \" + dst;\n      LOG.info(renameEditMsg);\n      updateStatusWithMsg(renameEditMsg);\n    }\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not rename recovered edits \" + editsWriter.path\n      + \" to \" + dst;\n    LOG.error(\"Could not rename recovered edits {} to {}\", editsWriter.path, dst, ioe);\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  return dst;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2931,
        "instruction": "private <K, V> void recordSendError(\n    final K key,\n    final V value,\n    final Long timestamp,\n    final String topic,\n    final Exception exception\n) {\n    String errorLogMessage = LOG_MESSAGE;\n    String errorMessage = EXCEPTION_MESSAGE;\n    if (exception instanceof RetriableException) {\n        errorLogMessage += PARAMETER_HINT;\n        errorMessage += PARAMETER_HINT;\n    }\n    log.error(errorLogMessage, topic, exception.getMessage(), exception);\n\n    // KAFKA-7510 put message key and value in TRACE level log so we don't leak data by default\n\n    sendException = new StreamsException(\n        String.format(\n            errorMessage,\n            logPrefix,\n            \"an error caught\",\n            key,\n            value,\n            timestamp,\n            topic,\n            exception.toString()\n        ),\n        exception);\n}",
        "accept_response": "private <K, V> void recordSendError(\n    final K key,\n    final V value,\n    final Long timestamp,\n    final String topic,\n    final Exception exception\n) {\n    String errorLogMessage = LOG_MESSAGE;\n    String errorMessage = EXCEPTION_MESSAGE;\n    if (exception instanceof RetriableException) {\n        errorLogMessage += PARAMETER_HINT;\n        errorMessage += PARAMETER_HINT;\n    }\n    log.error(errorLogMessage, topic, exception.getMessage(), exception);\n\n    // KAFKA-7510 put message key and value in TRACE level log so we don't leak data by default\n    log.trace(\"Failed message: key {} value {} timestamp {}\", key, value, timestamp);\n\n    sendException = new StreamsException(\n        String.format(\n            errorMessage,\n            logPrefix,\n            \"an error caught\",\n            key,\n            value,\n            timestamp,\n            topic,\n            exception.toString()\n        ),\n        exception);\n}",
        "reject_response": "private <K, V> void recordSendError(\n    final K key,\n    final V value,\n    final Long timestamp,\n    final String topic,\n    final Exception exception\n) {\n    String errorLogMessage = LOG_MESSAGE;\n    String errorMessage = EXCEPTION_MESSAGE;\n    if (exception instanceof RetriableException) {\n        errorLogMessage += PARAMETER_HINT;\n        errorMessage += PARAMETER_HINT;\n    }\n    log.error(errorLogMessage, topic, exception.getMessage(), exception);\n\n    // KAFKA-7510 put message key and value in TRACE level log so we don't leak data by default\n    log.error(errorLogMessage, key, value, timestamp, topic, exception.toString());\n\n    sendException = new StreamsException(\n        String.format(\n            errorMessage,\n            logPrefix,\n            \"an error caught\",\n            key,\n            value,\n            timestamp,\n            topic,\n            exception.toString()\n        ),\n        exception);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3118,
        "instruction": "public HashShuffleAppender getAppender(TajoConf tajoConf, ExecutionBlockId ebId, int partId,\n                            TableMeta meta, Schema outSchema) throws IOException {\n  synchronized (appenderMap) {\n    Map<Integer, PartitionAppenderMeta> partitionAppenderMap = appenderMap.get(ebId);\n\n    if (partitionAppenderMap == null) {\n      partitionAppenderMap = new ConcurrentHashMap<Integer, PartitionAppenderMeta>();\n      appenderMap.put(ebId, partitionAppenderMap);\n    }\n\n    PartitionAppenderMeta partitionAppenderMeta = partitionAppenderMap.get(partId);\n    if (partitionAppenderMeta == null) {\n      Path dataFile = getDataFile(ebId, partId);\n      FileSystem fs = dataFile.getFileSystem(systemConf);\n      if (fs.exists(dataFile)) {\n        FileStatus status = fs.getFileStatus(dataFile);\n        LOG.info(\"File \" + dataFile + \" already exists, size=\" + status.getLen());\n      }\n\n      if (!fs.exists(dataFile.getParent())) {\n        fs.mkdirs(dataFile.getParent());\n      }\n      FileAppender appender = (FileAppender)((FileStorageManager)StorageManager.getFileStorageManager(tajoConf))\n          .getAppender(meta, outSchema, dataFile);\n      appender.enableStats();\n      appender.init();\n\n      partitionAppenderMeta = new PartitionAppenderMeta();\n      partitionAppenderMeta.partId = partId;\n      partitionAppenderMeta.dataFile = dataFile;\n      partitionAppenderMeta.appender = new HashShuffleAppender(ebId, partId, pageSize, appender);\n      partitionAppenderMeta.appender.init();\n      partitionAppenderMap.put(partId, partitionAppenderMeta);\n    }\n\n    return partitionAppenderMeta.appender;\n  }\n}",
        "accept_response": "public HashShuffleAppender getAppender(TajoConf tajoConf, ExecutionBlockId ebId, int partId,\n                            TableMeta meta, Schema outSchema) throws IOException {\n  synchronized (appenderMap) {\n    Map<Integer, PartitionAppenderMeta> partitionAppenderMap = appenderMap.get(ebId);\n\n    if (partitionAppenderMap == null) {\n      partitionAppenderMap = new ConcurrentHashMap<Integer, PartitionAppenderMeta>();\n      appenderMap.put(ebId, partitionAppenderMap);\n    }\n\n    PartitionAppenderMeta partitionAppenderMeta = partitionAppenderMap.get(partId);\n    if (partitionAppenderMeta == null) {\n      Path dataFile = getDataFile(ebId, partId);\n      FileSystem fs = dataFile.getFileSystem(systemConf);\n      if (fs.exists(dataFile)) {\n        FileStatus status = fs.getFileStatus(dataFile);\n        LOG.info(\"File \" + dataFile + \" already exists, size=\" + status.getLen());\n      }\n\n      if (!fs.exists(dataFile.getParent())) {\n        fs.mkdirs(dataFile.getParent());\n      }\n      FileAppender appender = (FileAppender)((FileStorageManager)StorageManager.getFileStorageManager(tajoConf))\n          .getAppender(meta, outSchema, dataFile);\n      appender.enableStats();\n      appender.init();\n\n      partitionAppenderMeta = new PartitionAppenderMeta();\n      partitionAppenderMeta.partId = partId;\n      partitionAppenderMeta.dataFile = dataFile;\n      partitionAppenderMeta.appender = new HashShuffleAppender(ebId, partId, pageSize, appender);\n      partitionAppenderMeta.appender.init();\n      partitionAppenderMap.put(partId, partitionAppenderMeta);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Create Hash shuffle file(partId=\" + partId + \"): \" + dataFile);\n      }\n    }\n\n    return partitionAppenderMeta.appender;\n  }\n}",
        "reject_response": "public HashShuffleAppender getAppender(TajoConf tajoConf, ExecutionBlockId ebId, int partId,\n                            TableMeta meta, Schema outSchema) throws IOException {\n  synchronized (appenderMap) {\n    Map<Integer, PartitionAppenderMeta> partitionAppenderMap = appenderMap.get(ebId);\n\n    if (partitionAppenderMap == null) {\n      partitionAppenderMap = new ConcurrentHashMap<Integer, PartitionAppenderMeta>();\n      appenderMap.put(ebId, partitionAppenderMap);\n    }\n\n    PartitionAppenderMeta partitionAppenderMeta = partitionAppenderMap.get(partId);\n    if (partitionAppenderMeta == null) {\n      Path dataFile = getDataFile(ebId, partId);\n      FileSystem fs = dataFile.getFileSystem(systemConf);\n      if (fs.exists(dataFile)) {\n        FileStatus status = fs.getFileStatus(dataFile);\n        LOG.info(\"File \" + dataFile + \" already exists, size=\" + status.getLen());\n      }\n\n      if (!fs.exists(dataFile.getParent())) {\n        fs.mkdirs(dataFile.getParent());\n      }\n      FileAppender appender = (FileAppender)((FileStorageManager)StorageManager.getFileStorageManager(tajoConf))\n          .getAppender(meta, outSchema, dataFile);\n      appender.enableStats();\n      appender.init();\n\n      partitionAppenderMeta = new PartitionAppenderMeta();\n      partitionAppenderMeta.partId = partId;\n      partitionAppenderMeta.dataFile = dataFile;\n      partitionAppenderMeta.appender = new HashShuffleAppender(ebId, partId, pageSize, appender);\n      partitionAppenderMeta.appender.init();\n      partitionAppenderMap.put(partId, partitionAppenderMeta);\n\n      if (LOG.isDebugEnabled()) {\n      LOG.info(\"Create Hash shuffle file(partId=\" + partId + \"): \" + dataFile);\n      }\n    }\n\n    return partitionAppenderMeta.appender;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2390,
        "instruction": "@Override\npublic boolean hasNext() {\n  synchronized (nextLock) {\n    if (batch == LAST_BATCH)\n      return false;\n\n    if (batch != null && batchIterator.hasNext())\n      return true;\n\n    // don't have one cached, try to cache one and return success\n    try {\n      batch = null;\n      while (batch == null && fatalException == null && !queryThreadPool.isShutdown())\n        batch = resultsQueue.poll(1, TimeUnit.SECONDS);\n\n      if (fatalException != null)\n        if (fatalException instanceof RuntimeException)\n          throw (RuntimeException) fatalException;\n        else\n          throw new RuntimeException(fatalException);\n\n      if (queryThreadPool.isShutdown()) {\n        String shortMsg = \"The BatchScanner was unexpectedly closed while this Iterator was still in use.\";\n        throw new RuntimeException(shortMsg + \" Ensure proper handling of the BatchScanner.\");\n      }\n\n      batchIterator = batch.iterator();\n      return batch != LAST_BATCH;\n    } catch (InterruptedException e) {\n      throw new RuntimeException(e);\n    }\n  }\n}",
        "accept_response": "@Override\npublic boolean hasNext() {\n  synchronized (nextLock) {\n    if (batch == LAST_BATCH)\n      return false;\n\n    if (batch != null && batchIterator.hasNext())\n      return true;\n\n    // don't have one cached, try to cache one and return success\n    try {\n      batch = null;\n      while (batch == null && fatalException == null && !queryThreadPool.isShutdown())\n        batch = resultsQueue.poll(1, TimeUnit.SECONDS);\n\n      if (fatalException != null)\n        if (fatalException instanceof RuntimeException)\n          throw (RuntimeException) fatalException;\n        else\n          throw new RuntimeException(fatalException);\n\n      if (queryThreadPool.isShutdown()) {\n        String shortMsg = \"The BatchScanner was unexpectedly closed while this Iterator was still in use.\";\n        log.error(\"{} Ensure that a reference to the BatchScanner is retained so that it can be closed when this Iterator is exhausted.\"\n            + \" Not retaining a reference to the BatchScanner guarantees that you are leaking threads in your client JVM.\", shortMsg);\n        throw new RuntimeException(shortMsg + \" Ensure proper handling of the BatchScanner.\");\n      }\n\n      batchIterator = batch.iterator();\n      return batch != LAST_BATCH;\n    } catch (InterruptedException e) {\n      throw new RuntimeException(e);\n    }\n  }\n}",
        "reject_response": "@Override\npublic boolean hasNext() {\n  synchronized (nextLock) {\n    if (batch == LAST_BATCH)\n      return false;\n\n    if (batch != null && batchIterator.hasNext())\n      return true;\n\n    // don't have one cached, try to cache one and return success\n    try {\n      batch = null;\n      while (batch == null && fatalException == null && !queryThreadPool.isShutdown())\n        batch = resultsQueue.poll(1, TimeUnit.SECONDS);\n\n      if (fatalException != null)\n        if (fatalException instanceof RuntimeException)\n          throw (RuntimeException) fatalException;\n        else\n          throw new RuntimeException(fatalException);\n\n      if (queryThreadPool.isShutdown()) {\n        String shortMsg = \"The BatchScanner was unexpectedly closed while this Iterator was still in use.\";\n        log.error(shortMsg + \" Ensure that a reference to the BatchScanner is retained so that it can be closed when this Iterator is exhausted.\"\n            + \" Not retaining a reference to the BatchScanner guarantees that you are leaking threads in your client JVM.\");\n        throw new RuntimeException(shortMsg + \" Ensure proper handling of the BatchScanner.\");\n      }\n\n      batchIterator = batch.iterator();\n      return batch != LAST_BATCH;\n    } catch (InterruptedException e) {\n      throw new RuntimeException(e);\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2839,
        "instruction": "@Nullable public IgniteInternalFuture<AffinityTopologyVersion> affinityReadyFuture(AffinityTopologyVersion ver) {\n    GridDhtPartitionsExchangeFuture lastInitializedFut0 = lastInitializedFut;\n\n    if (lastInitializedFut0 != null && lastInitializedFut0.initialVersion().compareTo(ver) == 0) {\n        if (log.isTraceEnabled())\n            log.trace(\"Return lastInitializedFut for topology ready future \" +\n                \"[ver=\" + ver + \", fut=\" + lastInitializedFut0 + ']');\n\n        return lastInitializedFut0;\n    }\n\n    AffinityTopologyVersion topVer = exchFuts.readyTopVer();\n\n    if (topVer.compareTo(ver) >= 0) {\n        if (log.isTraceEnabled())\n\n        return null;\n    }\n\n    GridFutureAdapter<AffinityTopologyVersion> fut = F.addIfAbsent(readyFuts, ver,\n        new AffinityReadyFuture(ver));\n\n    if (log.isDebugEnabled())\n        log.debug(\"Created topology ready future [ver=\" + ver + \", fut=\" + fut + ']');\n\n    topVer = exchFuts.readyTopVer();\n\n    if (topVer.compareTo(ver) >= 0) {\n        if (log.isTraceEnabled())\n            log.trace(\"Completing created topology ready future \" +\n                \"[ver=\" + topVer + \", topVer=\" + topVer + \", fut=\" + fut + ']');\n\n        fut.onDone(topVer);\n    }\n    else if (stopErr != null)\n        fut.onDone(stopErr);\n\n    return fut;\n}",
        "accept_response": "@Nullable public IgniteInternalFuture<AffinityTopologyVersion> affinityReadyFuture(AffinityTopologyVersion ver) {\n    GridDhtPartitionsExchangeFuture lastInitializedFut0 = lastInitializedFut;\n\n    if (lastInitializedFut0 != null && lastInitializedFut0.initialVersion().compareTo(ver) == 0) {\n        if (log.isTraceEnabled())\n            log.trace(\"Return lastInitializedFut for topology ready future \" +\n                \"[ver=\" + ver + \", fut=\" + lastInitializedFut0 + ']');\n\n        return lastInitializedFut0;\n    }\n\n    AffinityTopologyVersion topVer = exchFuts.readyTopVer();\n\n    if (topVer.compareTo(ver) >= 0) {\n        if (log.isTraceEnabled())\n            log.trace(\"Return finished future for topology ready future [ver=\" + ver + \", topVer=\" + topVer + ']');\n\n        return null;\n    }\n\n    GridFutureAdapter<AffinityTopologyVersion> fut = F.addIfAbsent(readyFuts, ver,\n        new AffinityReadyFuture(ver));\n\n    if (log.isDebugEnabled())\n        log.debug(\"Created topology ready future [ver=\" + ver + \", fut=\" + fut + ']');\n\n    topVer = exchFuts.readyTopVer();\n\n    if (topVer.compareTo(ver) >= 0) {\n        if (log.isTraceEnabled())\n            log.trace(\"Completing created topology ready future \" +\n                \"[ver=\" + topVer + \", topVer=\" + topVer + \", fut=\" + fut + ']');\n\n        fut.onDone(topVer);\n    }\n    else if (stopErr != null)\n        fut.onDone(stopErr);\n\n    return fut;\n}",
        "reject_response": "@Nullable public IgniteInternalFuture<AffinityTopologyVersion> affinityReadyFuture(AffinityTopologyVersion ver) {\n    GridDhtPartitionsExchangeFuture lastInitializedFut0 = lastInitializedFut;\n\n    if (lastInitializedFut0 != null && lastInitializedFut0.initialVersion().compareTo(ver) == 0) {\n        if (log.isTraceEnabled())\n            log.trace(\"Return lastInitializedFut for topology ready future \" +\n                \"[ver=\" + ver + \", fut=\" + lastInitializedFut0 + ']');\n\n        return lastInitializedFut0;\n    }\n\n    AffinityTopologyVersion topVer = exchFuts.readyTopVer();\n\n    if (topVer.compareTo(ver) >= 0) {\n        if (log.isTraceEnabled())\n        if (log.isDebugEnabled())\n            log.debug(\"Return finished future for topology ready future [ver=\" + ver + \", topVer=\" + topVer + ']');\n\n        return null;\n    }\n\n    GridFutureAdapter<AffinityTopologyVersion> fut = F.addIfAbsent(readyFuts, ver,\n        new AffinityReadyFuture(ver));\n\n    if (log.isDebugEnabled())\n        log.debug(\"Created topology ready future [ver=\" + ver + \", fut=\" + fut + ']');\n\n    topVer = exchFuts.readyTopVer();\n\n    if (topVer.compareTo(ver) >= 0) {\n        if (log.isTraceEnabled())\n            log.trace(\"Completing created topology ready future \" +\n                \"[ver=\" + topVer + \", topVer=\" + topVer + \", fut=\" + fut + ']');\n\n        fut.onDone(topVer);\n    }\n    else if (stopErr != null)\n        fut.onDone(stopErr);\n\n    return fut;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2674,
        "instruction": "private void loadMap(XMLConfiguration conf) {\n    String[] profileNames = conf.getStringArray(\"profile.name\");\n    if (profileNames.length == 0) {\n        return;\n    }\n    Map<String, Map<String, String>> profileMap = new TreeMap<>(String.CASE_INSENSITIVE_ORDER);\n    for (int profileIdx = 0; profileIdx < profileNames.length; profileIdx++) {\n        String profileName = profileNames[profileIdx];\n        if (profileMap.containsKey(profileName)) {\n            LOG.warn(\"Duplicate profile definition found in \" + conf.getFileName() + \" for: \" + profileName);\n            continue;\n        }\n        Configuration profileSubset = conf.subset(\"profile(\" + profileIdx + \").plugins\");\n        profileMap.put(profileName, getProfilePluginMap(profileSubset));\n    }\n    profilesMap.putAll(profileMap);\n}",
        "accept_response": "private void loadMap(XMLConfiguration conf) {\n    String[] profileNames = conf.getStringArray(\"profile.name\");\n    if (profileNames.length == 0) {\n        LOG.warn(\"Profile file: \" + conf.getFileName() + \" is empty\");\n        return;\n    }\n    Map<String, Map<String, String>> profileMap = new TreeMap<>(String.CASE_INSENSITIVE_ORDER);\n    for (int profileIdx = 0; profileIdx < profileNames.length; profileIdx++) {\n        String profileName = profileNames[profileIdx];\n        if (profileMap.containsKey(profileName)) {\n            LOG.warn(\"Duplicate profile definition found in \" + conf.getFileName() + \" for: \" + profileName);\n            continue;\n        }\n        Configuration profileSubset = conf.subset(\"profile(\" + profileIdx + \").plugins\");\n        profileMap.put(profileName, getProfilePluginMap(profileSubset));\n    }\n    profilesMap.putAll(profileMap);\n}",
        "reject_response": "private void loadMap(XMLConfiguration conf) {\n    String[] profileNames = conf.getStringArray(\"profile.name\");\n    if (profileNames.length == 0) {\n        log.warn(\"Profile file: \" + conf.getFileName() + \" is empty\");\n        return;\n    }\n    Map<String, Map<String, String>> profileMap = new TreeMap<>(String.CASE_INSENSITIVE_ORDER);\n    for (int profileIdx = 0; profileIdx < profileNames.length; profileIdx++) {\n        String profileName = profileNames[profileIdx];\n        if (profileMap.containsKey(profileName)) {\n            LOG.warn(\"Duplicate profile definition found in \" + conf.getFileName() + \" for: \" + profileName);\n            continue;\n        }\n        Configuration profileSubset = conf.subset(\"profile(\" + profileIdx + \").plugins\");\n        profileMap.put(profileName, getProfilePluginMap(profileSubset));\n    }\n    profilesMap.putAll(profileMap);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2635,
        "instruction": "private void registerPulseUrlToManager(JMXConnector connection)\n    throws IOException, AttributeNotFoundException, InstanceNotFoundException, MBeanException,\n    ReflectionException, MalformedObjectNameException, InvalidAttributeValueException {\n  logger.info(resourceBundle.getString(\"LOG_MSG_REGISTERING_APP_URL_TO_MANAGER\"));\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n  // Register Pulse URL if not already present in the JMX Manager\n  if (connection != null) {\n    MBeanServerConnection mbsc = connection.getMBeanServerConnection();\n\n    Set<ObjectName> mbeans = mbsc.queryNames(this.MBEAN_OBJECT_NAME_MEMBER_MANAGER, null);\n\n    for (ObjectName mbeanName : mbeans) {\n      String presentUrl =\n          (String) mbsc.getAttribute(mbeanName, PulseConstants.MBEAN_MANAGER_ATTRIBUTE_PULSEURL);\n      String pulseWebAppUrl = repository.getPulseWebAppUrl();\n      if (pulseWebAppUrl != null && (presentUrl == null || !pulseWebAppUrl.equals(presentUrl))) {\n        Attribute pulseUrlAttr =\n            new Attribute(PulseConstants.MBEAN_MANAGER_ATTRIBUTE_PULSEURL, pulseWebAppUrl);\n        mbsc.setAttribute(mbeanName, pulseUrlAttr);\n      } else {\n        logger.debug(resourceBundle.getString(\"LOG_MSG_APP_URL_ALREADY_PRESENT_IN_MANAGER\"));\n      }\n    }\n  }\n}",
        "accept_response": "private void registerPulseUrlToManager(JMXConnector connection)\n    throws IOException, AttributeNotFoundException, InstanceNotFoundException, MBeanException,\n    ReflectionException, MalformedObjectNameException, InvalidAttributeValueException {\n  logger.info(resourceBundle.getString(\"LOG_MSG_REGISTERING_APP_URL_TO_MANAGER\"));\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n  // Register Pulse URL if not already present in the JMX Manager\n  if (connection != null) {\n    MBeanServerConnection mbsc = connection.getMBeanServerConnection();\n\n    Set<ObjectName> mbeans = mbsc.queryNames(this.MBEAN_OBJECT_NAME_MEMBER_MANAGER, null);\n\n    for (ObjectName mbeanName : mbeans) {\n      String presentUrl =\n          (String) mbsc.getAttribute(mbeanName, PulseConstants.MBEAN_MANAGER_ATTRIBUTE_PULSEURL);\n      String pulseWebAppUrl = repository.getPulseWebAppUrl();\n      if (pulseWebAppUrl != null && (presentUrl == null || !pulseWebAppUrl.equals(presentUrl))) {\n        logger.debug(resourceBundle.getString(\"LOG_MSG_SETTING_APP_URL_TO_MANAGER\"));\n        Attribute pulseUrlAttr =\n            new Attribute(PulseConstants.MBEAN_MANAGER_ATTRIBUTE_PULSEURL, pulseWebAppUrl);\n        mbsc.setAttribute(mbeanName, pulseUrlAttr);\n      } else {\n        logger.debug(resourceBundle.getString(\"LOG_MSG_APP_URL_ALREADY_PRESENT_IN_MANAGER\"));\n      }\n    }\n  }\n}",
        "reject_response": "private void registerPulseUrlToManager(JMXConnector connection)\n    throws IOException, AttributeNotFoundException, InstanceNotFoundException, MBeanException,\n    ReflectionException, MalformedObjectNameException, InvalidAttributeValueException {\n  logger.info(resourceBundle.getString(\"LOG_MSG_REGISTERING_APP_URL_TO_MANAGER\"));\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n  // Register Pulse URL if not already present in the JMX Manager\n  if (connection != null) {\n    MBeanServerConnection mbsc = connection.getMBeanServerConnection();\n\n    Set<ObjectName> mbeans = mbsc.queryNames(this.MBEAN_OBJECT_NAME_MEMBER_MANAGER, null);\n\n    for (ObjectName mbeanName : mbeans) {\n      String presentUrl =\n          (String) mbsc.getAttribute(mbeanName, PulseConstants.MBEAN_MANAGER_ATTRIBUTE_PULSEURL);\n      String pulseWebAppUrl = repository.getPulseWebAppUrl();\n      if (pulseWebAppUrl != null && (presentUrl == null || !pulseWebAppUrl.equals(presentUrl))) {\n        if (LOGGER.fineEnabled()) {\n          LOGGER.fine(resourceBundle.getString(\"LOG_MSG_SETTING_APP_URL_TO_MANAGER\"));\n        }\n        Attribute pulseUrlAttr =\n            new Attribute(PulseConstants.MBEAN_MANAGER_ATTRIBUTE_PULSEURL, pulseWebAppUrl);\n        mbsc.setAttribute(mbeanName, pulseUrlAttr);\n      } else {\n        logger.debug(resourceBundle.getString(\"LOG_MSG_APP_URL_ALREADY_PRESENT_IN_MANAGER\"));\n      }\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2733,
        "instruction": "private RpcSaslProto processSaslToken(RpcSaslProto saslMessage)\n    throws SaslException {\n  if (!saslMessage.hasToken()) {\n    throw new SaslException(\"Client did not send a token\");\n  }\n  byte[] saslToken = saslMessage.getToken().toByteArray();\n  saslToken = saslServer.evaluateResponse(saslToken);\n  return buildSaslResponse(\n      saslServer.isComplete() ? SaslState.SUCCESS : SaslState.CHALLENGE,\n      saslToken);\n}",
        "accept_response": "private RpcSaslProto processSaslToken(RpcSaslProto saslMessage)\n    throws SaslException {\n  if (!saslMessage.hasToken()) {\n    throw new SaslException(\"Client did not send a token\");\n  }\n  byte[] saslToken = saslMessage.getToken().toByteArray();\n  LOG.debug(\"Have read input token of size {} for processing by saslServer.evaluateResponse()\",\n      saslToken.length);\n  saslToken = saslServer.evaluateResponse(saslToken);\n  return buildSaslResponse(\n      saslServer.isComplete() ? SaslState.SUCCESS : SaslState.CHALLENGE,\n      saslToken);\n}",
        "reject_response": "private RpcSaslProto processSaslToken(RpcSaslProto saslMessage)\n    throws SaslException {\n  if (!saslMessage.hasToken()) {\n    throw new SaslException(\"Client did not send a token\");\n  }\n  byte[] saslToken = saslMessage.getToken().toByteArray();\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"Have read input token of size \" + saslToken.length\n        + \" for processing by saslServer.evaluateResponse()\");\n  }\n  saslToken = saslServer.evaluateResponse(saslToken);\n  return buildSaslResponse(\n      saslServer.isComplete() ? SaslState.SUCCESS : SaslState.CHALLENGE,\n      saslToken);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2824,
        "instruction": "public void start() {\n    if (getBoolean(IGNITE_JVM_PAUSE_DETECTOR_DISABLED, false)) {\n        if (log.isDebugEnabled())\n            log.debug(\"JVM Pause Detector is disabled.\");\n\n        return;\n    }\n\n    final Thread worker = new Thread(\"jvm-pause-detector-worker\") {\n        private long prev = System.currentTimeMillis();\n\n        @Override public void run() {\n            if (log.isDebugEnabled())\n                log.debug(getName() + \" has been started.\");\n\n            while (true) {\n                try {\n                    Thread.sleep(PRECISION);\n\n                    final long now = System.currentTimeMillis();\n                    final long pause = now - PRECISION - prev;\n\n                    prev = now;\n\n                    if (pause >= THRESHOLD) {\n                        log.warning(\"Possible too long JVM pause: \" + pause + \" milliseconds.\");\n\n                        synchronized (LongJVMPauseDetector.this) {\n                            final int next = (int)(longPausesCnt % EVT_CNT);\n\n                            longPausesCnt++;\n\n                            longPausesTotalDuration += pause;\n\n                            longPausesTimestamps[next] = now;\n\n                            longPausesDurations[next] = pause;\n                        }\n                    }\n                }\n                catch (InterruptedException e) {\n\n                    break;\n                }\n            }\n        }\n    };\n\n    if (!workerRef.compareAndSet(null, worker)) {\n        log.warning(LongJVMPauseDetector.class.getSimpleName() + \" already started!\");\n\n        return;\n    }\n\n    worker.setDaemon(true);\n    worker.start();\n\n    if (log.isDebugEnabled())\n        log.debug(\"LongJVMPauseDetector was successfully started\");\n}",
        "accept_response": "public void start() {\n    if (getBoolean(IGNITE_JVM_PAUSE_DETECTOR_DISABLED, false)) {\n        if (log.isDebugEnabled())\n            log.debug(\"JVM Pause Detector is disabled.\");\n\n        return;\n    }\n\n    final Thread worker = new Thread(\"jvm-pause-detector-worker\") {\n        private long prev = System.currentTimeMillis();\n\n        @Override public void run() {\n            if (log.isDebugEnabled())\n                log.debug(getName() + \" has been started.\");\n\n            while (true) {\n                try {\n                    Thread.sleep(PRECISION);\n\n                    final long now = System.currentTimeMillis();\n                    final long pause = now - PRECISION - prev;\n\n                    prev = now;\n\n                    if (pause >= THRESHOLD) {\n                        log.warning(\"Possible too long JVM pause: \" + pause + \" milliseconds.\");\n\n                        synchronized (LongJVMPauseDetector.this) {\n                            final int next = (int)(longPausesCnt % EVT_CNT);\n\n                            longPausesCnt++;\n\n                            longPausesTotalDuration += pause;\n\n                            longPausesTimestamps[next] = now;\n\n                            longPausesDurations[next] = pause;\n                        }\n                    }\n                }\n                catch (InterruptedException e) {\n                    log.error(getName() + \" has been interrupted\", e);\n\n                    break;\n                }\n            }\n        }\n    };\n\n    if (!workerRef.compareAndSet(null, worker)) {\n        log.warning(LongJVMPauseDetector.class.getSimpleName() + \" already started!\");\n\n        return;\n    }\n\n    worker.setDaemon(true);\n    worker.start();\n\n    if (log.isDebugEnabled())\n        log.debug(\"LongJVMPauseDetector was successfully started\");\n}",
        "reject_response": "public void start() {\n    if (getBoolean(IGNITE_JVM_PAUSE_DETECTOR_DISABLED, false)) {\n        if (log.isDebugEnabled())\n            log.debug(\"JVM Pause Detector is disabled.\");\n\n        return;\n    }\n\n    final Thread worker = new Thread(\"jvm-pause-detector-worker\") {\n        private long prev = System.currentTimeMillis();\n\n        @Override public void run() {\n            if (log.isDebugEnabled())\n                log.debug(getName() + \" has been started.\");\n\n            while (true) {\n                try {\n                    Thread.sleep(PRECISION);\n\n                    final long now = System.currentTimeMillis();\n                    final long pause = now - PRECISION - prev;\n\n                    prev = now;\n\n                    if (pause >= THRESHOLD) {\n                        log.warning(\"Possible too long JVM pause: \" + pause + \" milliseconds.\");\n\n                        synchronized (LongJVMPauseDetector.this) {\n                            final int next = (int)(longPausesCnt % EVT_CNT);\n\n                            longPausesCnt++;\n\n                            longPausesTotalDuration += pause;\n\n                            longPausesTimestamps[next] = now;\n\n                            longPausesDurations[next] = pause;\n                        }\n                    }\n                }\n                catch (InterruptedException e) {\n                    LOG.error(getName() + \" has been interrupted\", e);\n\n                    break;\n                }\n            }\n        }\n    };\n\n    if (!workerRef.compareAndSet(null, worker)) {\n        log.warning(LongJVMPauseDetector.class.getSimpleName() + \" already started!\");\n\n        return;\n    }\n\n    worker.setDaemon(true);\n    worker.start();\n\n    if (log.isDebugEnabled())\n        log.debug(\"LongJVMPauseDetector was successfully started\");\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3126,
        "instruction": "public String submitDAGToAppMaster(DAGPlan dagPlan,\n    Map<String, LocalResource> additionalResources) throws TezException {\n  if (sessionStopped.get()) {\n    throw new SessionNotRunning(\"AM unable to accept new DAG submissions.\"\n        + \" In the process of shutting down\");\n  }\n  // dag is in cleanup when dag state is completed but AM state is still RUNNING\n  synchronized (idleStateLock) {\n    while (currentDAG != null && currentDAG.isComplete() && state == DAGAppMasterState.RUNNING) {\n      try {\n        LOG.info(\"wait for previous dag cleanup\");\n        idleStateLock.wait();\n      } catch (InterruptedException e) {\n        throw new TezException(e);\n      }\n    }\n  }\n\n  synchronized (this) {\n    if (this.versionMismatch) {\n      throw new TezException(\"Unable to accept DAG submissions as the ApplicationMaster is\"\n          + \" incompatible with the client. \" + versionMismatchDiagnostics);\n    }\n    if (state.equals(DAGAppMasterState.ERROR)\n            || sessionStopped.get()) {\n      throw new SessionNotRunning(\"AM unable to accept new DAG submissions.\"\n              + \" In the process of shutting down\");\n    }\n    if (currentDAG != null\n        && !currentDAG.isComplete()) {\n      throw new TezException(\"App master already running a DAG\");\n    }\n    // RPC server runs in the context of the job user as it was started in\n    // the job user's UGI context\n    LOG.info(\"Starting DAG submitted via RPC: \" + dagPlan.getName());\n\n\n    if (!dagPlan.getName().startsWith(TezConstants.TEZ_PREWARM_DAG_NAME_PREFIX)) {\n      submittedDAGs.incrementAndGet();\n    }\n    startDAG(dagPlan, additionalResources);\n    return currentDAG.getID().toString();\n  }\n}",
        "accept_response": "public String submitDAGToAppMaster(DAGPlan dagPlan,\n    Map<String, LocalResource> additionalResources) throws TezException {\n  if (sessionStopped.get()) {\n    throw new SessionNotRunning(\"AM unable to accept new DAG submissions.\"\n        + \" In the process of shutting down\");\n  }\n  // dag is in cleanup when dag state is completed but AM state is still RUNNING\n  synchronized (idleStateLock) {\n    while (currentDAG != null && currentDAG.isComplete() && state == DAGAppMasterState.RUNNING) {\n      try {\n        LOG.info(\"wait for previous dag cleanup\");\n        idleStateLock.wait();\n      } catch (InterruptedException e) {\n        throw new TezException(e);\n      }\n    }\n  }\n\n  synchronized (this) {\n    if (this.versionMismatch) {\n      throw new TezException(\"Unable to accept DAG submissions as the ApplicationMaster is\"\n          + \" incompatible with the client. \" + versionMismatchDiagnostics);\n    }\n    if (state.equals(DAGAppMasterState.ERROR)\n            || sessionStopped.get()) {\n      throw new SessionNotRunning(\"AM unable to accept new DAG submissions.\"\n              + \" In the process of shutting down\");\n    }\n    if (currentDAG != null\n        && !currentDAG.isComplete()) {\n      throw new TezException(\"App master already running a DAG\");\n    }\n    // RPC server runs in the context of the job user as it was started in\n    // the job user's UGI context\n    LOG.info(\"Starting DAG submitted via RPC: \" + dagPlan.getName());\n\n    LOG.debug(\"Invoked with additional local resources: {}\", additionalResources);\n\n    if (!dagPlan.getName().startsWith(TezConstants.TEZ_PREWARM_DAG_NAME_PREFIX)) {\n      submittedDAGs.incrementAndGet();\n    }\n    startDAG(dagPlan, additionalResources);\n    return currentDAG.getID().toString();\n  }\n}",
        "reject_response": "public String submitDAGToAppMaster(DAGPlan dagPlan,\n    Map<String, LocalResource> additionalResources) throws TezException {\n  if (sessionStopped.get()) {\n    throw new SessionNotRunning(\"AM unable to accept new DAG submissions.\"\n        + \" In the process of shutting down\");\n  }\n  // dag is in cleanup when dag state is completed but AM state is still RUNNING\n  synchronized (idleStateLock) {\n    while (currentDAG != null && currentDAG.isComplete() && state == DAGAppMasterState.RUNNING) {\n      try {\n        LOG.info(\"wait for previous dag cleanup\");\n        idleStateLock.wait();\n      } catch (InterruptedException e) {\n        throw new TezException(e);\n      }\n    }\n  }\n\n  synchronized (this) {\n    if (this.versionMismatch) {\n      throw new TezException(\"Unable to accept DAG submissions as the ApplicationMaster is\"\n          + \" incompatible with the client. \" + versionMismatchDiagnostics);\n    }\n    if (state.equals(DAGAppMasterState.ERROR)\n            || sessionStopped.get()) {\n      throw new SessionNotRunning(\"AM unable to accept new DAG submissions.\"\n              + \" In the process of shutting down\");\n    }\n    if (currentDAG != null\n        && !currentDAG.isComplete()) {\n      throw new TezException(\"App master already running a DAG\");\n    }\n    // RPC server runs in the context of the job user as it was started in\n    // the job user's UGI context\n    LOG.info(\"Starting DAG submitted via RPC: \" + dagPlan.getName());\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Invoked with additional local resources: \" + additionalResources);\n    }\n\n    if (!dagPlan.getName().startsWith(TezConstants.TEZ_PREWARM_DAG_NAME_PREFIX)) {\n      submittedDAGs.incrementAndGet();\n    }\n    startDAG(dagPlan, additionalResources);\n    return currentDAG.getID().toString();\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2837,
        "instruction": "@Override public boolean onDone(@Nullable Boolean res, @Nullable Throwable err) {\n    if (super.onDone(res, err)) {\n        cctx.mvcc().removeFuture(futId);\n\n        if (err == null) {\n            assert res != null;\n\n            cctx.tm().finishTxOnRecovery(tx, res);\n        }\n        else {\n            if (err instanceof ClusterTopologyCheckedException && nearTxCheck) {\n                if (log.isInfoEnabled()) {\n                    log.info(\"Failed to check transaction on near node, \" +\n                            \"ignoring [err=\" + err + \", tx=\" + tx + ']');\n                }\n            }\n            else {\n\n                cctx.tm().salvageTx(tx);\n            }\n        }\n    }\n\n    return false;\n}",
        "accept_response": "@Override public boolean onDone(@Nullable Boolean res, @Nullable Throwable err) {\n    if (super.onDone(res, err)) {\n        cctx.mvcc().removeFuture(futId);\n\n        if (err == null) {\n            assert res != null;\n\n            cctx.tm().finishTxOnRecovery(tx, res);\n        }\n        else {\n            if (err instanceof ClusterTopologyCheckedException && nearTxCheck) {\n                if (log.isInfoEnabled()) {\n                    log.info(\"Failed to check transaction on near node, \" +\n                            \"ignoring [err=\" + err + \", tx=\" + tx + ']');\n                }\n            }\n            else {\n                if (log.isInfoEnabled()) {\n                    log.info(\"Failed to check prepared transactions, \" +\n                            \"invalidating transaction [err=\" + err + \", tx=\" + tx + ']');\n                }\n\n                cctx.tm().salvageTx(tx);\n            }\n        }\n    }\n\n    return false;\n}",
        "reject_response": "@Override public boolean onDone(@Nullable Boolean res, @Nullable Throwable err) {\n    if (super.onDone(res, err)) {\n        cctx.mvcc().removeFuture(futId);\n\n        if (err == null) {\n            assert res != null;\n\n            cctx.tm().finishTxOnRecovery(tx, res);\n        }\n        else {\n            if (err instanceof ClusterTopologyCheckedException && nearTxCheck) {\n                if (log.isInfoEnabled()) {\n                    log.info(\"Failed to check transaction on near node, \" +\n                            \"ignoring [err=\" + err + \", tx=\" + tx + ']');\n                }\n            }\n            else {\n                if (log.isInfoEnabled()) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Failed to check prepared transactions, \" +\n                        \"invalidating transaction [err=\" + err + \", tx=\" + tx + ']');\n                }\n\n                cctx.tm().salvageTx(tx);\n            }\n        }\n    }\n\n    return false;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2784,
        "instruction": "ClassicHttpResponse cacheAndReturnResponse(\n        final HttpHost target,\n        final HttpRequest request,\n        final ClassicHttpResponse backendResponse,\n        final ExecChain.Scope scope,\n        final Date requestSent,\n        final Date responseReceived) throws IOException {\n    LOG.debug(\"Caching backend response\");\n    final ByteArrayBuffer buf;\n    final HttpEntity entity = backendResponse.getEntity();\n    if (entity != null) {\n        buf = new ByteArrayBuffer(1024);\n        final InputStream inStream = entity.getContent();\n        final byte[] tmp = new byte[2048];\n        long total = 0;\n        int l;\n        while ((l = inStream.read(tmp)) != -1) {\n            buf.append(tmp, 0, l);\n            total += l;\n            if (total > cacheConfig.getMaxObjectSize()) {\n                backendResponse.setEntity(new CombinedEntity(entity, buf));\n                return backendResponse;\n            }\n        }\n    } else {\n        buf = null;\n    }\n    backendResponse.close();\n\n    final HttpCacheEntry cacheEntry;\n    if (cacheConfig.isFreshnessCheckEnabled()) {\n        final HttpCacheEntry existingEntry = responseCache.getCacheEntry(target, request);\n        if (DateUtils.isAfter(existingEntry, backendResponse, HttpHeaders.DATE)) {\n            LOG.debug(\"Backend already contains fresher cache entry\");\n            cacheEntry = existingEntry;\n        } else {\n            cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n            LOG.debug(\"Backend response successfully cached\");\n        }\n    } else {\n        cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n        LOG.debug(\"Backend response successfully cached (freshness check skipped)\");\n    }\n    return convert(responseGenerator.generateResponse(request, cacheEntry), scope);\n}",
        "accept_response": "ClassicHttpResponse cacheAndReturnResponse(\n        final HttpHost target,\n        final HttpRequest request,\n        final ClassicHttpResponse backendResponse,\n        final ExecChain.Scope scope,\n        final Date requestSent,\n        final Date responseReceived) throws IOException {\n    LOG.debug(\"Caching backend response\");\n    final ByteArrayBuffer buf;\n    final HttpEntity entity = backendResponse.getEntity();\n    if (entity != null) {\n        buf = new ByteArrayBuffer(1024);\n        final InputStream inStream = entity.getContent();\n        final byte[] tmp = new byte[2048];\n        long total = 0;\n        int l;\n        while ((l = inStream.read(tmp)) != -1) {\n            buf.append(tmp, 0, l);\n            total += l;\n            if (total > cacheConfig.getMaxObjectSize()) {\n                LOG.debug(\"Backend response content length exceeds maximum\");\n                backendResponse.setEntity(new CombinedEntity(entity, buf));\n                return backendResponse;\n            }\n        }\n    } else {\n        buf = null;\n    }\n    backendResponse.close();\n\n    final HttpCacheEntry cacheEntry;\n    if (cacheConfig.isFreshnessCheckEnabled()) {\n        final HttpCacheEntry existingEntry = responseCache.getCacheEntry(target, request);\n        if (DateUtils.isAfter(existingEntry, backendResponse, HttpHeaders.DATE)) {\n            LOG.debug(\"Backend already contains fresher cache entry\");\n            cacheEntry = existingEntry;\n        } else {\n            cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n            LOG.debug(\"Backend response successfully cached\");\n        }\n    } else {\n        cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n        LOG.debug(\"Backend response successfully cached (freshness check skipped)\");\n    }\n    return convert(responseGenerator.generateResponse(request, cacheEntry), scope);\n}",
        "reject_response": "ClassicHttpResponse cacheAndReturnResponse(\n        final HttpHost target,\n        final HttpRequest request,\n        final ClassicHttpResponse backendResponse,\n        final ExecChain.Scope scope,\n        final Date requestSent,\n        final Date responseReceived) throws IOException {\n    LOG.debug(\"Caching backend response\");\n    final ByteArrayBuffer buf;\n    final HttpEntity entity = backendResponse.getEntity();\n    if (entity != null) {\n        buf = new ByteArrayBuffer(1024);\n        final InputStream inStream = entity.getContent();\n        final byte[] tmp = new byte[2048];\n        long total = 0;\n        int l;\n        while ((l = inStream.read(tmp)) != -1) {\n            buf.append(tmp, 0, l);\n            total += l;\n            if (total > cacheConfig.getMaxObjectSize()) {\n                log.debug(\"Backend response content length exceeds maximum\");\n                backendResponse.setEntity(new CombinedEntity(entity, buf));\n                return backendResponse;\n            }\n        }\n    } else {\n        buf = null;\n    }\n    backendResponse.close();\n\n    final HttpCacheEntry cacheEntry;\n    if (cacheConfig.isFreshnessCheckEnabled()) {\n        final HttpCacheEntry existingEntry = responseCache.getCacheEntry(target, request);\n        if (DateUtils.isAfter(existingEntry, backendResponse, HttpHeaders.DATE)) {\n            LOG.debug(\"Backend already contains fresher cache entry\");\n            cacheEntry = existingEntry;\n        } else {\n            cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n            LOG.debug(\"Backend response successfully cached\");\n        }\n    } else {\n        cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n        LOG.debug(\"Backend response successfully cached (freshness check skipped)\");\n    }\n    return convert(responseGenerator.generateResponse(request, cacheEntry), scope);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2857,
        "instruction": "@Override\npublic void logGhostMailboxFailure(MailboxPath mailboxPath) {\n    GhostMailbox.logger()\n            .addField(GhostMailbox.MAILBOX_NAME, mailboxPath)\n            .addField(TYPE, \"readMiss\")\n}",
        "accept_response": "@Override\npublic void logGhostMailboxFailure(MailboxPath mailboxPath) {\n    GhostMailbox.logger()\n            .addField(GhostMailbox.MAILBOX_NAME, mailboxPath)\n            .addField(TYPE, \"readMiss\")\n            .log(logger -> logger.debug(\"Read mailbox missed\"));\n}",
        "reject_response": "@Override\npublic void logGhostMailboxFailure(MailboxPath mailboxPath) {\n    GhostMailbox.logger()\n            .addField(GhostMailbox.MAILBOX_NAME, mailboxPath)\n            .addField(TYPE, \"readMiss\")\n            .log(logger -> logger.info(\"Read mailbox missed\"));\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3041,
        "instruction": "private void writeEvent(PrintWriter writer, DistributionRequest distributionRequest) {\n\n    // write the event type (make sure to include the double newline)\n    writer.write(\"id: \" + System.currentTimeMillis() + \"\\n\");\n\n    // write the actual data\n    // this could be simple text or could be JSON-encoded text that the\n    // client then decodes\n    writer.write(\"data: \" + distributionRequest.getRequestType() + \" \" + Arrays.toString(distributionRequest.getPaths()) + \"\\n\\n\");\n\n    // flush the buffers to make sure the container sends the bytes\n    writer.flush();\n}",
        "accept_response": "private void writeEvent(PrintWriter writer, DistributionRequest distributionRequest) {\n\n    // write the event type (make sure to include the double newline)\n    writer.write(\"id: \" + System.currentTimeMillis() + \"\\n\");\n\n    // write the actual data\n    // this could be simple text or could be JSON-encoded text that the\n    // client then decodes\n    writer.write(\"data: \" + distributionRequest.getRequestType() + \" \" + Arrays.toString(distributionRequest.getPaths()) + \"\\n\\n\");\n\n    // flush the buffers to make sure the container sends the bytes\n    writer.flush();\n    log.debug(\"SSE event {} {}\", distributionRequest.getRequestType(), distributionRequest.getPaths());\n}",
        "reject_response": "private void writeEvent(PrintWriter writer, DistributionRequest distributionRequest) {\n\n    // write the event type (make sure to include the double newline)\n    writer.write(\"id: \" + System.currentTimeMillis() + \"\\n\");\n\n    // write the actual data\n    // this could be simple text or could be JSON-encoded text that the\n    // client then decodes\n    writer.write(\"data: \" + distributionRequest.getRequestType() + \" \" + Arrays.toString(distributionRequest.getPaths()) + \"\\n\\n\");\n\n    // flush the buffers to make sure the container sends the bytes\n    writer.flush();\n    log.debug(\"SSE event {} {}\", new Object[]{distributionRequest.getRequestType(), distributionRequest.getPaths()});\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3056,
        "instruction": "public void shutdown() {\n  log.info(\"Shutting down CoreContainer instance=\"\n      + System.identityHashCode(this));\n\n  isShutDown = true;\n\n  ExecutorUtil.shutdownAndAwaitTermination(coreContainerWorkExecutor);\n  if (metricManager != null) {\n    metricManager.closeReporters(SolrMetricManager.getRegistryName(SolrInfoBean.Group.node));\n    metricManager.closeReporters(SolrMetricManager.getRegistryName(SolrInfoBean.Group.jvm));\n    metricManager.closeReporters(SolrMetricManager.getRegistryName(SolrInfoBean.Group.jetty));\n  }\n\n  if (isZooKeeperAware()) {\n    cancelCoreRecoveries();\n    zkSys.zkController.publishNodeAsDown(zkSys.zkController.getNodeName());\n    if (metricManager != null) {\n      metricManager.closeReporters(SolrMetricManager.getRegistryName(SolrInfoBean.Group.cluster));\n    }\n  }\n\n  try {\n    if (coreAdminHandler != null) coreAdminHandler.shutdown();\n  } catch (Exception e) {\n    log.warn(\"Error shutting down CoreAdminHandler. Continuing to close CoreContainer.\", e);\n  }\n\n  try {\n    // First wake up the closer thread, it'll terminate almost immediately since it checks isShutDown.\n    synchronized (solrCores.getModifyLock()) {\n      solrCores.getModifyLock().notifyAll(); // wake up anyone waiting\n    }\n    if (backgroundCloser != null) { // Doesn't seem right, but tests get in here without initializing the core.\n      try {\n        while (true) {\n          backgroundCloser.join(15000);\n          if (backgroundCloser.isAlive()) {\n            synchronized (solrCores.getModifyLock()) {\n              solrCores.getModifyLock().notifyAll(); // there is a race we have to protect against\n            }\n          } else {\n            break;\n          }\n        }\n      } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n      }\n    }\n    // Now clear all the cores that are being operated upon.\n    solrCores.close();\n\n    // It's still possible that one of the pending dynamic load operation is waiting, so wake it up if so.\n    // Since all the pending operations queues have been drained, there should be nothing to do.\n    synchronized (solrCores.getModifyLock()) {\n      solrCores.getModifyLock().notifyAll(); // wake up the thread\n    }\n\n  } finally {\n    try {\n      if (shardHandlerFactory != null) {\n        shardHandlerFactory.close();\n      }\n    } finally {\n      try {\n        if (updateShardHandler != null) {\n          updateShardHandler.close();\n        }\n      } finally {\n        // we want to close zk stuff last\n        zkSys.close();\n      }\n    }\n  }\n\n  // It should be safe to close the authorization plugin at this point.\n  try {\n    if(authorizationPlugin != null) {\n      authorizationPlugin.plugin.close();\n    }\n  } catch (IOException e) {\n    log.warn(\"Exception while closing authorization plugin.\", e);\n  }\n\n  // It should be safe to close the authentication plugin at this point.\n  try {\n    if(authenticationPlugin != null) {\n      authenticationPlugin.plugin.close();\n      authenticationPlugin = null;\n    }\n  } catch (Exception e) {\n    log.warn(\"Exception while closing authentication plugin.\", e);\n  }\n\n  org.apache.lucene.util.IOUtils.closeWhileHandlingException(loader); // best effort\n}",
        "accept_response": "public void shutdown() {\n  log.info(\"Shutting down CoreContainer instance=\"\n      + System.identityHashCode(this));\n\n  isShutDown = true;\n\n  ExecutorUtil.shutdownAndAwaitTermination(coreContainerWorkExecutor);\n  if (metricManager != null) {\n    metricManager.closeReporters(SolrMetricManager.getRegistryName(SolrInfoBean.Group.node));\n    metricManager.closeReporters(SolrMetricManager.getRegistryName(SolrInfoBean.Group.jvm));\n    metricManager.closeReporters(SolrMetricManager.getRegistryName(SolrInfoBean.Group.jetty));\n  }\n\n  if (isZooKeeperAware()) {\n    cancelCoreRecoveries();\n    zkSys.zkController.publishNodeAsDown(zkSys.zkController.getNodeName());\n    if (metricManager != null) {\n      metricManager.closeReporters(SolrMetricManager.getRegistryName(SolrInfoBean.Group.cluster));\n    }\n  }\n\n  try {\n    if (coreAdminHandler != null) coreAdminHandler.shutdown();\n  } catch (Exception e) {\n    log.warn(\"Error shutting down CoreAdminHandler. Continuing to close CoreContainer.\", e);\n  }\n\n  try {\n    // First wake up the closer thread, it'll terminate almost immediately since it checks isShutDown.\n    synchronized (solrCores.getModifyLock()) {\n      solrCores.getModifyLock().notifyAll(); // wake up anyone waiting\n    }\n    if (backgroundCloser != null) { // Doesn't seem right, but tests get in here without initializing the core.\n      try {\n        while (true) {\n          backgroundCloser.join(15000);\n          if (backgroundCloser.isAlive()) {\n            synchronized (solrCores.getModifyLock()) {\n              solrCores.getModifyLock().notifyAll(); // there is a race we have to protect against\n            }\n          } else {\n            break;\n          }\n        }\n      } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n        log.debug(\"backgroundCloser thread was interrupted before finishing\");\n      }\n    }\n    // Now clear all the cores that are being operated upon.\n    solrCores.close();\n\n    // It's still possible that one of the pending dynamic load operation is waiting, so wake it up if so.\n    // Since all the pending operations queues have been drained, there should be nothing to do.\n    synchronized (solrCores.getModifyLock()) {\n      solrCores.getModifyLock().notifyAll(); // wake up the thread\n    }\n\n  } finally {\n    try {\n      if (shardHandlerFactory != null) {\n        shardHandlerFactory.close();\n      }\n    } finally {\n      try {\n        if (updateShardHandler != null) {\n          updateShardHandler.close();\n        }\n      } finally {\n        // we want to close zk stuff last\n        zkSys.close();\n      }\n    }\n  }\n\n  // It should be safe to close the authorization plugin at this point.\n  try {\n    if(authorizationPlugin != null) {\n      authorizationPlugin.plugin.close();\n    }\n  } catch (IOException e) {\n    log.warn(\"Exception while closing authorization plugin.\", e);\n  }\n\n  // It should be safe to close the authentication plugin at this point.\n  try {\n    if(authenticationPlugin != null) {\n      authenticationPlugin.plugin.close();\n      authenticationPlugin = null;\n    }\n  } catch (Exception e) {\n    log.warn(\"Exception while closing authentication plugin.\", e);\n  }\n\n  org.apache.lucene.util.IOUtils.closeWhileHandlingException(loader); // best effort\n}",
        "reject_response": "public void shutdown() {\n  log.info(\"Shutting down CoreContainer instance=\"\n      + System.identityHashCode(this));\n\n  isShutDown = true;\n\n  ExecutorUtil.shutdownAndAwaitTermination(coreContainerWorkExecutor);\n  if (metricManager != null) {\n    metricManager.closeReporters(SolrMetricManager.getRegistryName(SolrInfoBean.Group.node));\n    metricManager.closeReporters(SolrMetricManager.getRegistryName(SolrInfoBean.Group.jvm));\n    metricManager.closeReporters(SolrMetricManager.getRegistryName(SolrInfoBean.Group.jetty));\n  }\n\n  if (isZooKeeperAware()) {\n    cancelCoreRecoveries();\n    zkSys.zkController.publishNodeAsDown(zkSys.zkController.getNodeName());\n    if (metricManager != null) {\n      metricManager.closeReporters(SolrMetricManager.getRegistryName(SolrInfoBean.Group.cluster));\n    }\n  }\n\n  try {\n    if (coreAdminHandler != null) coreAdminHandler.shutdown();\n  } catch (Exception e) {\n    log.warn(\"Error shutting down CoreAdminHandler. Continuing to close CoreContainer.\", e);\n  }\n\n  try {\n    // First wake up the closer thread, it'll terminate almost immediately since it checks isShutDown.\n    synchronized (solrCores.getModifyLock()) {\n      solrCores.getModifyLock().notifyAll(); // wake up anyone waiting\n    }\n    if (backgroundCloser != null) { // Doesn't seem right, but tests get in here without initializing the core.\n      try {\n        while (true) {\n          backgroundCloser.join(15000);\n          if (backgroundCloser.isAlive()) {\n            synchronized (solrCores.getModifyLock()) {\n              solrCores.getModifyLock().notifyAll(); // there is a race we have to protect against\n            }\n          } else {\n            break;\n          }\n        }\n      } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n        if (log.isDebugEnabled()) {\n          log.debug(\"backgroundCloser thread was interrupted before finishing\");\n        }\n      }\n    }\n    // Now clear all the cores that are being operated upon.\n    solrCores.close();\n\n    // It's still possible that one of the pending dynamic load operation is waiting, so wake it up if so.\n    // Since all the pending operations queues have been drained, there should be nothing to do.\n    synchronized (solrCores.getModifyLock()) {\n      solrCores.getModifyLock().notifyAll(); // wake up the thread\n    }\n\n  } finally {\n    try {\n      if (shardHandlerFactory != null) {\n        shardHandlerFactory.close();\n      }\n    } finally {\n      try {\n        if (updateShardHandler != null) {\n          updateShardHandler.close();\n        }\n      } finally {\n        // we want to close zk stuff last\n        zkSys.close();\n      }\n    }\n  }\n\n  // It should be safe to close the authorization plugin at this point.\n  try {\n    if(authorizationPlugin != null) {\n      authorizationPlugin.plugin.close();\n    }\n  } catch (IOException e) {\n    log.warn(\"Exception while closing authorization plugin.\", e);\n  }\n\n  // It should be safe to close the authentication plugin at this point.\n  try {\n    if(authenticationPlugin != null) {\n      authenticationPlugin.plugin.close();\n      authenticationPlugin = null;\n    }\n  } catch (Exception e) {\n    log.warn(\"Exception while closing authentication plugin.\", e);\n  }\n\n  org.apache.lucene.util.IOUtils.closeWhileHandlingException(loader); // best effort\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2763,
        "instruction": "public boolean isResponseCacheable(final HttpRequest request, final HttpResponse response) {\n    final ProtocolVersion version = request.getVersion() != null ? request.getVersion() : HttpVersion.DEFAULT;\n    if (version.compareToVersion(HttpVersion.HTTP_1_1) > 0) {\n        return false;\n    }\n\n    final String[] uncacheableRequestDirectives = { HeaderConstants.CACHE_CONTROL_NO_STORE };\n    if (hasCacheControlParameterFrom(request,uncacheableRequestDirectives)) {\n        log.debug(\"Response is explcitily non-cacheable per cache control directive\");\n        return false;\n    }\n\n    if (request.getRequestUri().contains(\"?\")) {\n        if (neverCache1_0ResponsesWithQueryString && from1_0Origin(response)) {\n            log.debug(\"Response is not cacheable as it had a query string\");\n            return false;\n        } else if (!isExplicitlyCacheable(response)) {\n            log.debug(\"Response is not cacheable as it is missing explicit caching headers\");\n            return false;\n        }\n    }\n\n    if (expiresHeaderLessOrEqualToDateHeaderAndNoCacheControl(response)) {\n        log.debug(\"Expires header less or equal to Date header and no cache control directives\");\n        return false;\n    }\n\n    if (sharedCache) {\n        if (request.countHeaders(HeaderConstants.AUTHORIZATION) > 0\n                && !hasCacheControlParameterFrom(response, AUTH_CACHEABLE_PARAMS)) {\n            log.debug(\"Request contains private credentials\");\n            return false;\n        }\n    }\n\n    final String method = request.getMethod();\n    return isResponseCacheable(method, response);\n}",
        "accept_response": "public boolean isResponseCacheable(final HttpRequest request, final HttpResponse response) {\n    final ProtocolVersion version = request.getVersion() != null ? request.getVersion() : HttpVersion.DEFAULT;\n    if (version.compareToVersion(HttpVersion.HTTP_1_1) > 0) {\n        if (log.isDebugEnabled()) {\n            log.debug(\"Protocol version {} is non-cacheable\", version);\n        }\n        return false;\n    }\n\n    final String[] uncacheableRequestDirectives = { HeaderConstants.CACHE_CONTROL_NO_STORE };\n    if (hasCacheControlParameterFrom(request,uncacheableRequestDirectives)) {\n        log.debug(\"Response is explcitily non-cacheable per cache control directive\");\n        return false;\n    }\n\n    if (request.getRequestUri().contains(\"?\")) {\n        if (neverCache1_0ResponsesWithQueryString && from1_0Origin(response)) {\n            log.debug(\"Response is not cacheable as it had a query string\");\n            return false;\n        } else if (!isExplicitlyCacheable(response)) {\n            log.debug(\"Response is not cacheable as it is missing explicit caching headers\");\n            return false;\n        }\n    }\n\n    if (expiresHeaderLessOrEqualToDateHeaderAndNoCacheControl(response)) {\n        log.debug(\"Expires header less or equal to Date header and no cache control directives\");\n        return false;\n    }\n\n    if (sharedCache) {\n        if (request.countHeaders(HeaderConstants.AUTHORIZATION) > 0\n                && !hasCacheControlParameterFrom(response, AUTH_CACHEABLE_PARAMS)) {\n            log.debug(\"Request contains private credentials\");\n            return false;\n        }\n    }\n\n    final String method = request.getMethod();\n    return isResponseCacheable(method, response);\n}",
        "reject_response": "public boolean isResponseCacheable(final HttpRequest request, final HttpResponse response) {\n    final ProtocolVersion version = request.getVersion() != null ? request.getVersion() : HttpVersion.DEFAULT;\n    if (version.compareToVersion(HttpVersion.HTTP_1_1) > 0) {\n        if (log.isDebugEnabled()) {\n            log.debug(\"Protocol version \" + version + \" is non-cacheable\");\n        }\n        return false;\n    }\n\n    final String[] uncacheableRequestDirectives = { HeaderConstants.CACHE_CONTROL_NO_STORE };\n    if (hasCacheControlParameterFrom(request,uncacheableRequestDirectives)) {\n        log.debug(\"Response is explcitily non-cacheable per cache control directive\");\n        return false;\n    }\n\n    if (request.getRequestUri().contains(\"?\")) {\n        if (neverCache1_0ResponsesWithQueryString && from1_0Origin(response)) {\n            log.debug(\"Response is not cacheable as it had a query string\");\n            return false;\n        } else if (!isExplicitlyCacheable(response)) {\n            log.debug(\"Response is not cacheable as it is missing explicit caching headers\");\n            return false;\n        }\n    }\n\n    if (expiresHeaderLessOrEqualToDateHeaderAndNoCacheControl(response)) {\n        log.debug(\"Expires header less or equal to Date header and no cache control directives\");\n        return false;\n    }\n\n    if (sharedCache) {\n        if (request.countHeaders(HeaderConstants.AUTHORIZATION) > 0\n                && !hasCacheControlParameterFrom(response, AUTH_CACHEABLE_PARAMS)) {\n            log.debug(\"Request contains private credentials\");\n            return false;\n        }\n    }\n\n    final String method = request.getMethod();\n    return isResponseCacheable(method, response);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2862,
        "instruction": "public JcrPackage upload(InputStream in, boolean replace)\n        throws RepositoryException, IOException, PackageExistsException {\n\n    MemoryArchive archive = new MemoryArchive(true);\n    InputStreamPump pump = new InputStreamPump(in , archive);\n\n    // this will cause the input stream to be consumed and the memory archive being initialized.\n    Binary bin = session.getValueFactory().createBinary(pump);\n    if (pump.getError() != null) {\n        Exception error = pump.getError();\n        log.error(\"Error while reading from input stream.\", error);\n        bin.dispose();\n        throw new IOException(\"Error while reading from input stream\", error);\n    }\n\n    if (archive.getJcrRoot() == null) {\n        String msg = \"Stream is not a content package. Missing 'jcr_root'.\";\n        bin.dispose();\n        throw new IOException(msg);\n    }\n\n    final MetaInf inf = archive.getMetaInf();\n    PackagePropertiesImpl props = new PackagePropertiesImpl() {\n        @Override\n        protected Properties getPropertiesMap() {\n            return inf.getProperties();\n        }\n    };\n    PackageId pid = props.getId();\n\n    // invalidate pid if path is unknown\n    if (pid == null) {\n        pid = createRandomPid();\n    }\n    if (!pid.isValid()) {\n        bin.dispose();\n        throw new RepositoryException(\"Unable to create package. Illegal package name.\");\n    }\n\n    // create parent node\n    String path = getInstallationPath(pid) + \".zip\";\n    String parentPath = Text.getRelativeParent(path, 1);\n    String name = Text.getName(path);\n    Node parent = mkdir(parentPath, false);\n\n    // remember installation state properties (GRANITE-2018)\n    JcrPackageDefinitionImpl.State state = null;\n    Calendar oldCreatedDate = null;\n\n    if (parent.hasNode(name)) {\n        try (JcrPackage oldPackage = new JcrPackageImpl(this, parent.getNode(name))) {\n            JcrPackageDefinitionImpl oldDef = (JcrPackageDefinitionImpl) oldPackage.getDefinition();\n            if (oldDef != null) {\n                state = oldDef.getState();\n                oldCreatedDate = oldDef.getCreated();\n            }\n        }\n\n        if (replace) {\n            parent.getNode(name).remove();\n        } else {\n            throw new PackageExistsException(\"Package already exists: \" + pid).setId(pid);\n        }\n    }\n    JcrPackage jcrPack = null;\n    try {\n        jcrPack = createNew(parent, pid, bin, archive);\n        JcrPackageDefinitionImpl def = (JcrPackageDefinitionImpl) jcrPack.getDefinition();\n        Calendar newCreateDate = def == null ? null : def.getCreated();\n        // only transfer the old package state to the new state in case both packages have the same create date\n        if (state != null && newCreateDate != null && oldCreatedDate != null && oldCreatedDate.compareTo(newCreateDate) == 0) {\n            def.setState(state);\n        }\n        dispatch(PackageEvent.Type.UPLOAD, pid, null);\n        return jcrPack;\n    } finally {\n        bin.dispose();\n        if (jcrPack == null) {\n            session.refresh(false);\n        } else {\n            session.save();\n        }\n    }\n}",
        "accept_response": "public JcrPackage upload(InputStream in, boolean replace)\n        throws RepositoryException, IOException, PackageExistsException {\n\n    MemoryArchive archive = new MemoryArchive(true);\n    InputStreamPump pump = new InputStreamPump(in , archive);\n\n    // this will cause the input stream to be consumed and the memory archive being initialized.\n    Binary bin = session.getValueFactory().createBinary(pump);\n    if (pump.getError() != null) {\n        Exception error = pump.getError();\n        log.error(\"Error while reading from input stream.\", error);\n        bin.dispose();\n        throw new IOException(\"Error while reading from input stream\", error);\n    }\n\n    if (archive.getJcrRoot() == null) {\n        String msg = \"Stream is not a content package. Missing 'jcr_root'.\";\n        log.info(msg);\n        bin.dispose();\n        throw new IOException(msg);\n    }\n\n    final MetaInf inf = archive.getMetaInf();\n    PackagePropertiesImpl props = new PackagePropertiesImpl() {\n        @Override\n        protected Properties getPropertiesMap() {\n            return inf.getProperties();\n        }\n    };\n    PackageId pid = props.getId();\n\n    // invalidate pid if path is unknown\n    if (pid == null) {\n        pid = createRandomPid();\n    }\n    if (!pid.isValid()) {\n        bin.dispose();\n        throw new RepositoryException(\"Unable to create package. Illegal package name.\");\n    }\n\n    // create parent node\n    String path = getInstallationPath(pid) + \".zip\";\n    String parentPath = Text.getRelativeParent(path, 1);\n    String name = Text.getName(path);\n    Node parent = mkdir(parentPath, false);\n\n    // remember installation state properties (GRANITE-2018)\n    JcrPackageDefinitionImpl.State state = null;\n    Calendar oldCreatedDate = null;\n\n    if (parent.hasNode(name)) {\n        try (JcrPackage oldPackage = new JcrPackageImpl(this, parent.getNode(name))) {\n            JcrPackageDefinitionImpl oldDef = (JcrPackageDefinitionImpl) oldPackage.getDefinition();\n            if (oldDef != null) {\n                state = oldDef.getState();\n                oldCreatedDate = oldDef.getCreated();\n            }\n        }\n\n        if (replace) {\n            parent.getNode(name).remove();\n        } else {\n            throw new PackageExistsException(\"Package already exists: \" + pid).setId(pid);\n        }\n    }\n    JcrPackage jcrPack = null;\n    try {\n        jcrPack = createNew(parent, pid, bin, archive);\n        JcrPackageDefinitionImpl def = (JcrPackageDefinitionImpl) jcrPack.getDefinition();\n        Calendar newCreateDate = def == null ? null : def.getCreated();\n        // only transfer the old package state to the new state in case both packages have the same create date\n        if (state != null && newCreateDate != null && oldCreatedDate != null && oldCreatedDate.compareTo(newCreateDate) == 0) {\n            def.setState(state);\n        }\n        dispatch(PackageEvent.Type.UPLOAD, pid, null);\n        return jcrPack;\n    } finally {\n        bin.dispose();\n        if (jcrPack == null) {\n            session.refresh(false);\n        } else {\n            session.save();\n        }\n    }\n}",
        "reject_response": "public JcrPackage upload(InputStream in, boolean replace)\n        throws RepositoryException, IOException, PackageExistsException {\n\n    MemoryArchive archive = new MemoryArchive(true);\n    InputStreamPump pump = new InputStreamPump(in , archive);\n\n    // this will cause the input stream to be consumed and the memory archive being initialized.\n    Binary bin = session.getValueFactory().createBinary(pump);\n    if (pump.getError() != null) {\n        Exception error = pump.getError();\n        log.error(\"Error while reading from input stream.\", error);\n        bin.dispose();\n        throw new IOException(\"Error while reading from input stream\", error);\n    }\n\n    if (archive.getJcrRoot() == null) {\n        String msg = \"Stream is not a content package. Missing 'jcr_root'.\";\n        log.error(msg);\n        bin.dispose();\n        throw new IOException(msg);\n    }\n\n    final MetaInf inf = archive.getMetaInf();\n    PackagePropertiesImpl props = new PackagePropertiesImpl() {\n        @Override\n        protected Properties getPropertiesMap() {\n            return inf.getProperties();\n        }\n    };\n    PackageId pid = props.getId();\n\n    // invalidate pid if path is unknown\n    if (pid == null) {\n        pid = createRandomPid();\n    }\n    if (!pid.isValid()) {\n        bin.dispose();\n        throw new RepositoryException(\"Unable to create package. Illegal package name.\");\n    }\n\n    // create parent node\n    String path = getInstallationPath(pid) + \".zip\";\n    String parentPath = Text.getRelativeParent(path, 1);\n    String name = Text.getName(path);\n    Node parent = mkdir(parentPath, false);\n\n    // remember installation state properties (GRANITE-2018)\n    JcrPackageDefinitionImpl.State state = null;\n    Calendar oldCreatedDate = null;\n\n    if (parent.hasNode(name)) {\n        try (JcrPackage oldPackage = new JcrPackageImpl(this, parent.getNode(name))) {\n            JcrPackageDefinitionImpl oldDef = (JcrPackageDefinitionImpl) oldPackage.getDefinition();\n            if (oldDef != null) {\n                state = oldDef.getState();\n                oldCreatedDate = oldDef.getCreated();\n            }\n        }\n\n        if (replace) {\n            parent.getNode(name).remove();\n        } else {\n            throw new PackageExistsException(\"Package already exists: \" + pid).setId(pid);\n        }\n    }\n    JcrPackage jcrPack = null;\n    try {\n        jcrPack = createNew(parent, pid, bin, archive);\n        JcrPackageDefinitionImpl def = (JcrPackageDefinitionImpl) jcrPack.getDefinition();\n        Calendar newCreateDate = def == null ? null : def.getCreated();\n        // only transfer the old package state to the new state in case both packages have the same create date\n        if (state != null && newCreateDate != null && oldCreatedDate != null && oldCreatedDate.compareTo(newCreateDate) == 0) {\n            def.setState(state);\n        }\n        dispatch(PackageEvent.Type.UPLOAD, pid, null);\n        return jcrPack;\n    } finally {\n        bin.dispose();\n        if (jcrPack == null) {\n            session.refresh(false);\n        } else {\n            session.save();\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2967,
        "instruction": "@Override\npublic void cleanup(Context context) {\n  LOG.info(\"UpdateHostDb: feeder finished, waiting for shutdown\");\n\n  // If we're here all keys have been fed and we can issue a shut down\n  executor.shutdown();\n\n  boolean finished = false;\n\n  // Wait until all resolvers have finished\n  while (!finished) {\n    try {\n      // Wait for the executor to shut down completely\n      if (!executor.isTerminated()) {\n        Thread.sleep(1000);\n      } else {\n        // All is well, get out\n        finished = true;\n      }\n    } catch (InterruptedException e) {\n      // Huh?\n      LOG.warn(StringUtils.stringifyException(e));\n    }\n  }\n}",
        "accept_response": "@Override\npublic void cleanup(Context context) {\n  LOG.info(\"UpdateHostDb: feeder finished, waiting for shutdown\");\n\n  // If we're here all keys have been fed and we can issue a shut down\n  executor.shutdown();\n\n  boolean finished = false;\n\n  // Wait until all resolvers have finished\n  while (!finished) {\n    try {\n      // Wait for the executor to shut down completely\n      if (!executor.isTerminated()) {\n        LOG.info(\"UpdateHostDb: resolver threads waiting: {}\",\n            executor.getPoolSize());\n        Thread.sleep(1000);\n      } else {\n        // All is well, get out\n        finished = true;\n      }\n    } catch (InterruptedException e) {\n      // Huh?\n      LOG.warn(StringUtils.stringifyException(e));\n    }\n  }\n}",
        "reject_response": "@Override\npublic void cleanup(Context context) {\n  LOG.info(\"UpdateHostDb: feeder finished, waiting for shutdown\");\n\n  // If we're here all keys have been fed and we can issue a shut down\n  executor.shutdown();\n\n  boolean finished = false;\n\n  // Wait until all resolvers have finished\n  while (!finished) {\n    try {\n      // Wait for the executor to shut down completely\n      if (!executor.isTerminated()) {\n        LOG.info(\"UpdateHostDb: resolver threads waiting: \" + Integer.toString(executor.getPoolSize()));\n        Thread.sleep(1000);\n      } else {\n        // All is well, get out\n        finished = true;\n      }\n    } catch (InterruptedException e) {\n      // Huh?\n      LOG.warn(StringUtils.stringifyException(e));\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2383,
        "instruction": "private void _flush(String tableId, Text start, Text end, boolean wait) throws AccumuloException, AccumuloSecurityException, TableNotFoundException {\n\n  try {\n    long flushID;\n\n    // used to pass the table name. but the tableid associated with a table name could change between calls.\n    // so pass the tableid to both calls\n\n    while (true) {\n      MasterClientService.Iface client = null;\n      try {\n        client = MasterClient.getConnectionWithRetry(context);\n        flushID = client.initiateFlush(Tracer.traceInfo(), context.rpcCreds(), tableId);\n        break;\n      } catch (TTransportException tte) {\n        log.debug(\"Failed to call initiateFlush, retrying ... \", tte);\n        UtilWaitThread.sleep(100);\n      } finally {\n        MasterClient.close(client);\n      }\n    }\n\n    while (true) {\n      MasterClientService.Iface client = null;\n      try {\n        client = MasterClient.getConnectionWithRetry(context);\n        client.waitForFlush(Tracer.traceInfo(), context.rpcCreds(), tableId, TextUtil.getByteBuffer(start), TextUtil.getByteBuffer(end), flushID,\n            wait ? Long.MAX_VALUE : 1);\n        break;\n      } catch (TTransportException tte) {\n        log.debug(\"Failed to call initiateFlush, retrying ... \", tte);\n        UtilWaitThread.sleep(100);\n      } finally {\n        MasterClient.close(client);\n      }\n    }\n  } catch (ThriftSecurityException e) {\n    switch (e.getCode()) {\n      case TABLE_DOESNT_EXIST:\n        throw new TableNotFoundException(tableId, null, e.getMessage(), e);\n      default:\n        throw new AccumuloSecurityException(e.user, e.code, e);\n    }\n  } catch (ThriftTableOperationException e) {\n    switch (e.getType()) {\n      case NOTFOUND:\n        throw new TableNotFoundException(e);\n      default:\n        throw new AccumuloException(e.description, e);\n    }\n  } catch (Exception e) {\n    throw new AccumuloException(e);\n  }\n}",
        "accept_response": "private void _flush(String tableId, Text start, Text end, boolean wait) throws AccumuloException, AccumuloSecurityException, TableNotFoundException {\n\n  try {\n    long flushID;\n\n    // used to pass the table name. but the tableid associated with a table name could change between calls.\n    // so pass the tableid to both calls\n\n    while (true) {\n      MasterClientService.Iface client = null;\n      try {\n        client = MasterClient.getConnectionWithRetry(context);\n        flushID = client.initiateFlush(Tracer.traceInfo(), context.rpcCreds(), tableId);\n        break;\n      } catch (TTransportException tte) {\n        log.debug(\"Failed to call initiateFlush, retrying ... \", tte);\n        UtilWaitThread.sleep(100);\n      } finally {\n        MasterClient.close(client);\n      }\n    }\n\n    while (true) {\n      MasterClientService.Iface client = null;\n      try {\n        client = MasterClient.getConnectionWithRetry(context);\n        client.waitForFlush(Tracer.traceInfo(), context.rpcCreds(), tableId, TextUtil.getByteBuffer(start), TextUtil.getByteBuffer(end), flushID,\n            wait ? Long.MAX_VALUE : 1);\n        break;\n      } catch (TTransportException tte) {\n        log.debug(\"Failed to call initiateFlush, retrying ... \", tte);\n        UtilWaitThread.sleep(100);\n      } finally {\n        MasterClient.close(client);\n      }\n    }\n  } catch (ThriftSecurityException e) {\n    switch (e.getCode()) {\n      case TABLE_DOESNT_EXIST:\n        throw new TableNotFoundException(tableId, null, e.getMessage(), e);\n      default:\n        log.debug(\"flush security exception on table id {}\", tableId);\n        throw new AccumuloSecurityException(e.user, e.code, e);\n    }\n  } catch (ThriftTableOperationException e) {\n    switch (e.getType()) {\n      case NOTFOUND:\n        throw new TableNotFoundException(e);\n      default:\n        throw new AccumuloException(e.description, e);\n    }\n  } catch (Exception e) {\n    throw new AccumuloException(e);\n  }\n}",
        "reject_response": "private void _flush(String tableId, Text start, Text end, boolean wait) throws AccumuloException, AccumuloSecurityException, TableNotFoundException {\n\n  try {\n    long flushID;\n\n    // used to pass the table name. but the tableid associated with a table name could change between calls.\n    // so pass the tableid to both calls\n\n    while (true) {\n      MasterClientService.Iface client = null;\n      try {\n        client = MasterClient.getConnectionWithRetry(context);\n        flushID = client.initiateFlush(Tracer.traceInfo(), context.rpcCreds(), tableId);\n        break;\n      } catch (TTransportException tte) {\n        log.debug(\"Failed to call initiateFlush, retrying ... \", tte);\n        UtilWaitThread.sleep(100);\n      } finally {\n        MasterClient.close(client);\n      }\n    }\n\n    while (true) {\n      MasterClientService.Iface client = null;\n      try {\n        client = MasterClient.getConnectionWithRetry(context);\n        client.waitForFlush(Tracer.traceInfo(), context.rpcCreds(), tableId, TextUtil.getByteBuffer(start), TextUtil.getByteBuffer(end), flushID,\n            wait ? Long.MAX_VALUE : 1);\n        break;\n      } catch (TTransportException tte) {\n        log.debug(\"Failed to call initiateFlush, retrying ... \", tte);\n        UtilWaitThread.sleep(100);\n      } finally {\n        MasterClient.close(client);\n      }\n    }\n  } catch (ThriftSecurityException e) {\n    switch (e.getCode()) {\n      case TABLE_DOESNT_EXIST:\n        throw new TableNotFoundException(tableId, null, e.getMessage(), e);\n      default:\n        log.debug(\"flush security exception on table id \" + tableId);\n        throw new AccumuloSecurityException(e.user, e.code, e);\n    }\n  } catch (ThriftTableOperationException e) {\n    switch (e.getType()) {\n      case NOTFOUND:\n        throw new TableNotFoundException(e);\n      default:\n        throw new AccumuloException(e.description, e);\n    }\n  } catch (Exception e) {\n    throw new AccumuloException(e);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3086,
        "instruction": "private void initLogWriter(Path logFilePath) {\n    try {\n        eventLogPath = logFilePath;\n        eventLogWriter = Files.newBufferedWriter(eventLogPath, StandardCharsets.UTF_8, StandardOpenOption.CREATE,\n                                                 StandardOpenOption.WRITE, StandardOpenOption.APPEND);\n    } catch (IOException e) {\n        LOG.error(\"Error setting up FileBasedEventLogger.\", e);\n        throw new RuntimeException(e);\n    }\n}",
        "accept_response": "private void initLogWriter(Path logFilePath) {\n    try {\n        LOG.info(\"FileBasedEventLogger log path {}\", logFilePath);\n        eventLogPath = logFilePath;\n        eventLogWriter = Files.newBufferedWriter(eventLogPath, StandardCharsets.UTF_8, StandardOpenOption.CREATE,\n                                                 StandardOpenOption.WRITE, StandardOpenOption.APPEND);\n    } catch (IOException e) {\n        LOG.error(\"Error setting up FileBasedEventLogger.\", e);\n        throw new RuntimeException(e);\n    }\n}",
        "reject_response": "private void initLogWriter(Path logFilePath) {\n    try {\n        LOG.info(\"logFilePath {}\", logFilePath);\n        eventLogPath = logFilePath;\n        eventLogWriter = Files.newBufferedWriter(eventLogPath, StandardCharsets.UTF_8, StandardOpenOption.CREATE,\n                                                 StandardOpenOption.WRITE, StandardOpenOption.APPEND);\n    } catch (IOException e) {\n        LOG.error(\"Error setting up FileBasedEventLogger.\", e);\n        throw new RuntimeException(e);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2817,
        "instruction": "private void reduce(final List<ComputeJobResult> results) {\n    R reduceRes = null;\n    Throwable userE = null;\n\n    try {\n        try {\n            // Reduce results.\n            reduceRes = U.wrapThreadLoader(dep.classLoader(), new Callable<R>() {\n                @Nullable @Override public R call() {\n                    return task.reduce(results);\n                }\n            });\n        }\n        finally {\n            synchronized (mux) {\n                assert state == State.REDUCING : \"Invalid task state: \" + state;\n\n                state = State.REDUCED;\n            }\n        }\n\n        if (log.isDebugEnabled())\n\n        recordTaskEvent(EVT_TASK_REDUCED, \"Task reduced.\");\n    }\n    catch (ClusterTopologyCheckedException e) {\n        U.warn(log, \"Failed to reduce job results for task (any nodes from task topology left grid?): \" + task);\n\n        userE = e;\n    }\n    catch (IgniteCheckedException e) {\n        U.error(log, \"Failed to reduce job results for task: \" + task, e);\n\n        userE = e;\n    }\n    // Catch Throwable to protect against bad user code.\n    catch (Throwable e) {\n        String errMsg = \"Failed to reduce job results due to undeclared user exception [task=\" + task +\n            \", err=\" + e + ']';\n\n        U.error(log, errMsg, e);\n\n        userE = new ComputeUserUndeclaredException(errMsg ,e);\n\n        if (e instanceof Error)\n            throw e;\n    }\n    finally {\n        finishTask(reduceRes, userE);\n    }\n}",
        "accept_response": "private void reduce(final List<ComputeJobResult> results) {\n    R reduceRes = null;\n    Throwable userE = null;\n\n    try {\n        try {\n            // Reduce results.\n            reduceRes = U.wrapThreadLoader(dep.classLoader(), new Callable<R>() {\n                @Nullable @Override public R call() {\n                    return task.reduce(results);\n                }\n            });\n        }\n        finally {\n            synchronized (mux) {\n                assert state == State.REDUCING : \"Invalid task state: \" + state;\n\n                state = State.REDUCED;\n            }\n        }\n\n        if (log.isDebugEnabled())\n            log.debug(S.toString(\"Reduced job responses\",\n                \"reduceRes\", reduceRes, true,\n                \"ses\", ses, false));\n\n        recordTaskEvent(EVT_TASK_REDUCED, \"Task reduced.\");\n    }\n    catch (ClusterTopologyCheckedException e) {\n        U.warn(log, \"Failed to reduce job results for task (any nodes from task topology left grid?): \" + task);\n\n        userE = e;\n    }\n    catch (IgniteCheckedException e) {\n        U.error(log, \"Failed to reduce job results for task: \" + task, e);\n\n        userE = e;\n    }\n    // Catch Throwable to protect against bad user code.\n    catch (Throwable e) {\n        String errMsg = \"Failed to reduce job results due to undeclared user exception [task=\" + task +\n            \", err=\" + e + ']';\n\n        U.error(log, errMsg, e);\n\n        userE = new ComputeUserUndeclaredException(errMsg ,e);\n\n        if (e instanceof Error)\n            throw e;\n    }\n    finally {\n        finishTask(reduceRes, userE);\n    }\n}",
        "reject_response": "private void reduce(final List<ComputeJobResult> results) {\n    R reduceRes = null;\n    Throwable userE = null;\n\n    try {\n        try {\n            // Reduce results.\n            reduceRes = U.wrapThreadLoader(dep.classLoader(), new Callable<R>() {\n                @Nullable @Override public R call() {\n                    return task.reduce(results);\n                }\n            });\n        }\n        finally {\n            synchronized (mux) {\n                assert state == State.REDUCING : \"Invalid task state: \" + state;\n\n                state = State.REDUCED;\n            }\n        }\n\n        if (log.isDebugEnabled())\n            log.debug(\"Reduced job responses [reduceRes=\" + reduceRes + \", ses=\" + ses + ']');\n\n        recordTaskEvent(EVT_TASK_REDUCED, \"Task reduced.\");\n    }\n    catch (ClusterTopologyCheckedException e) {\n        U.warn(log, \"Failed to reduce job results for task (any nodes from task topology left grid?): \" + task);\n\n        userE = e;\n    }\n    catch (IgniteCheckedException e) {\n        U.error(log, \"Failed to reduce job results for task: \" + task, e);\n\n        userE = e;\n    }\n    // Catch Throwable to protect against bad user code.\n    catch (Throwable e) {\n        String errMsg = \"Failed to reduce job results due to undeclared user exception [task=\" + task +\n            \", err=\" + e + ']';\n\n        U.error(log, errMsg, e);\n\n        userE = new ComputeUserUndeclaredException(errMsg ,e);\n\n        if (e instanceof Error)\n            throw e;\n    }\n    finally {\n        finishTask(reduceRes, userE);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2504,
        "instruction": "@Override\nprotected void doStart() throws Exception {\n    super.doStart();\n\n    this.value = getAtomixEndpoint()\n        .getAtomix()\n        .getValue(\n            resourceName,\n            new DistributedValue.Config(getAtomixEndpoint().getConfiguration().getResourceOptions(resourceName)),\n            new DistributedValue.Options(getAtomixEndpoint().getConfiguration().getResourceConfig(resourceName)))\n        .join();\n\n\n    this.listeners.add(this.value.onChange(this::onEvent).join());\n}",
        "accept_response": "@Override\nprotected void doStart() throws Exception {\n    super.doStart();\n\n    this.value = getAtomixEndpoint()\n        .getAtomix()\n        .getValue(\n            resourceName,\n            new DistributedValue.Config(getAtomixEndpoint().getConfiguration().getResourceOptions(resourceName)),\n            new DistributedValue.Options(getAtomixEndpoint().getConfiguration().getResourceConfig(resourceName)))\n        .join();\n\n\n    LOGGER.debug(\"Subscribe to events for value: {}\", resourceName);\n    this.listeners.add(this.value.onChange(this::onEvent).join());\n}",
        "reject_response": "@Override\nprotected void doStart() throws Exception {\n    super.doStart();\n\n    this.value = getAtomixEndpoint()\n        .getAtomix()\n        .getValue(\n            resourceName,\n            new DistributedValue.Config(getAtomixEndpoint().getConfiguration().getResourceOptions(resourceName)),\n            new DistributedValue.Options(getAtomixEndpoint().getConfiguration().getResourceConfig(resourceName)))\n        .join();\n\n\n    LOGGER.debug(\"Subscribe to events for queue: {}\", resourceName);\n    this.listeners.add(this.value.onChange(this::onEvent).join());\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3047,
        "instruction": "@Override\n@SuppressWarnings(\"unchecked\")\npublic SolrResponse processMessage(ZkNodeProps message, String operation) {\n\n  NamedList results = new NamedList();\n  try {\n    CollectionAction action = getCollectionAction(operation);\n    Cmd command = commandMap.get(action);\n    if (command != null) {\n      command.call(zkStateReader.getClusterState(), message, results);\n    } else {\n      throw new SolrException(ErrorCode.BAD_REQUEST, \"Unknown operation:\"\n          + operation);\n    }\n  } catch (Exception e) {\n    String collName = message.getStr(\"collection\");\n    if (collName == null) collName = message.getStr(NAME);\n\n    if (collName == null) {\n      SolrException.log(log, \"Operation \" + operation + \" failed\", e);\n    } else  {\n      SolrException.log(log, \"Collection: \" + collName + \" operation: \" + operation\n          + \" failed\", e);\n    }\n\n    results.add(\"Operation \" + operation + \" caused exception:\", e);\n    SimpleOrderedMap nl = new SimpleOrderedMap();\n    nl.add(\"msg\", e.getMessage());\n    nl.add(\"rspCode\", e instanceof SolrException ? ((SolrException)e).code() : -1);\n    results.add(\"exception\", nl);\n  }\n  return new OverseerSolrResponse(results);\n}",
        "accept_response": "@Override\n@SuppressWarnings(\"unchecked\")\npublic SolrResponse processMessage(ZkNodeProps message, String operation) {\n  log.debug(\"OverseerCollectionMessageHandler.processMessage : {} , {}\", operation, message);\n\n  NamedList results = new NamedList();\n  try {\n    CollectionAction action = getCollectionAction(operation);\n    Cmd command = commandMap.get(action);\n    if (command != null) {\n      command.call(zkStateReader.getClusterState(), message, results);\n    } else {\n      throw new SolrException(ErrorCode.BAD_REQUEST, \"Unknown operation:\"\n          + operation);\n    }\n  } catch (Exception e) {\n    String collName = message.getStr(\"collection\");\n    if (collName == null) collName = message.getStr(NAME);\n\n    if (collName == null) {\n      SolrException.log(log, \"Operation \" + operation + \" failed\", e);\n    } else  {\n      SolrException.log(log, \"Collection: \" + collName + \" operation: \" + operation\n          + \" failed\", e);\n    }\n\n    results.add(\"Operation \" + operation + \" caused exception:\", e);\n    SimpleOrderedMap nl = new SimpleOrderedMap();\n    nl.add(\"msg\", e.getMessage());\n    nl.add(\"rspCode\", e instanceof SolrException ? ((SolrException)e).code() : -1);\n    results.add(\"exception\", nl);\n  }\n  return new OverseerSolrResponse(results);\n}",
        "reject_response": "@Override\n@SuppressWarnings(\"unchecked\")\npublic SolrResponse processMessage(ZkNodeProps message, String operation) {\n  log.debug(\"OverseerCollectionMessageHandler.processMessage : \"+ operation + \" , \"+ message.toString());\n\n  NamedList results = new NamedList();\n  try {\n    CollectionAction action = getCollectionAction(operation);\n    Cmd command = commandMap.get(action);\n    if (command != null) {\n      command.call(zkStateReader.getClusterState(), message, results);\n    } else {\n      throw new SolrException(ErrorCode.BAD_REQUEST, \"Unknown operation:\"\n          + operation);\n    }\n  } catch (Exception e) {\n    String collName = message.getStr(\"collection\");\n    if (collName == null) collName = message.getStr(NAME);\n\n    if (collName == null) {\n      SolrException.log(log, \"Operation \" + operation + \" failed\", e);\n    } else  {\n      SolrException.log(log, \"Collection: \" + collName + \" operation: \" + operation\n          + \" failed\", e);\n    }\n\n    results.add(\"Operation \" + operation + \" caused exception:\", e);\n    SimpleOrderedMap nl = new SimpleOrderedMap();\n    nl.add(\"msg\", e.getMessage());\n    nl.add(\"rspCode\", e instanceof SolrException ? ((SolrException)e).code() : -1);\n    results.add(\"exception\", nl);\n  }\n  return new OverseerSolrResponse(results);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2741,
        "instruction": "@Override\npublic void recoverUnfinalizedSegments() throws IOException {\n  Preconditions.checkState(!isActiveWriter, \"already active writer\");\n\n  LOG.info(\"Starting recovery process for unclosed journal segments...\");\n  Map<AsyncLogger, NewEpochResponseProto> resps = createNewUniqueEpoch();\n  LOG.info(\"Successfully started new epoch \" + loggers.getEpoch());\n\n  long mostRecentSegmentTxId = Long.MIN_VALUE;\n  for (NewEpochResponseProto r : resps.values()) {\n    if (r.hasLastSegmentTxId()) {\n      mostRecentSegmentTxId = Math.max(mostRecentSegmentTxId,\n          r.getLastSegmentTxId());\n    }\n  }\n\n  // On a completely fresh system, none of the journals have any\n  // segments, so there's nothing to recover.\n  if (mostRecentSegmentTxId != Long.MIN_VALUE) {\n    recoverUnclosedSegment(mostRecentSegmentTxId);\n  }\n  isActiveWriter = true;\n}",
        "accept_response": "@Override\npublic void recoverUnfinalizedSegments() throws IOException {\n  Preconditions.checkState(!isActiveWriter, \"already active writer\");\n\n  LOG.info(\"Starting recovery process for unclosed journal segments...\");\n  Map<AsyncLogger, NewEpochResponseProto> resps = createNewUniqueEpoch();\n  LOG.info(\"Successfully started new epoch \" + loggers.getEpoch());\n\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"newEpoch({}) responses:\\n{}\", loggers.getEpoch(), QuorumCall.mapToString(resps));\n  }\n\n  long mostRecentSegmentTxId = Long.MIN_VALUE;\n  for (NewEpochResponseProto r : resps.values()) {\n    if (r.hasLastSegmentTxId()) {\n      mostRecentSegmentTxId = Math.max(mostRecentSegmentTxId,\n          r.getLastSegmentTxId());\n    }\n  }\n\n  // On a completely fresh system, none of the journals have any\n  // segments, so there's nothing to recover.\n  if (mostRecentSegmentTxId != Long.MIN_VALUE) {\n    recoverUnclosedSegment(mostRecentSegmentTxId);\n  }\n  isActiveWriter = true;\n}",
        "reject_response": "@Override\npublic void recoverUnfinalizedSegments() throws IOException {\n  Preconditions.checkState(!isActiveWriter, \"already active writer\");\n\n  LOG.info(\"Starting recovery process for unclosed journal segments...\");\n  Map<AsyncLogger, NewEpochResponseProto> resps = createNewUniqueEpoch();\n  LOG.info(\"Successfully started new epoch \" + loggers.getEpoch());\n\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"newEpoch(\" + loggers.getEpoch() + \") responses:\\n\" +\n      QuorumCall.mapToString(resps));\n  }\n\n  long mostRecentSegmentTxId = Long.MIN_VALUE;\n  for (NewEpochResponseProto r : resps.values()) {\n    if (r.hasLastSegmentTxId()) {\n      mostRecentSegmentTxId = Math.max(mostRecentSegmentTxId,\n          r.getLastSegmentTxId());\n    }\n  }\n\n  // On a completely fresh system, none of the journals have any\n  // segments, so there's nothing to recover.\n  if (mostRecentSegmentTxId != Long.MIN_VALUE) {\n    recoverUnclosedSegment(mostRecentSegmentTxId);\n  }\n  isActiveWriter = true;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2777,
        "instruction": "@Override\npublic ClassicHttpResponse execute(\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain) throws IOException, HttpException {\n    Args.notNull(request, \"HTTP request\");\n    Args.notNull(scope, \"Scope\");\n\n    final HttpRoute route = scope.route;\n    final HttpClientContext context = scope.clientContext;\n    context.setAttribute(HttpClientContext.HTTP_ROUTE, scope.route);\n    context.setAttribute(HttpClientContext.HTTP_REQUEST, request);\n\n    final URIAuthority authority = request.getAuthority();\n    final String scheme = request.getScheme();\n    final HttpHost target = authority != null ? new HttpHost(scheme, authority) : route.getTargetHost();\n    final String via = generateViaHeader(request);\n\n    // default response context\n    setResponseStatus(context, CacheResponseStatus.CACHE_MISS);\n\n    if (clientRequestsOurOptions(request)) {\n        setResponseStatus(context, CacheResponseStatus.CACHE_MODULE_RESPONSE);\n        return new BasicClassicHttpResponse(HttpStatus.SC_NOT_IMPLEMENTED);\n    }\n\n    final SimpleHttpResponse fatalErrorResponse = getFatallyNoncompliantResponse(request, context);\n    if (fatalErrorResponse != null) {\n        return convert(fatalErrorResponse, scope);\n    }\n\n    requestCompliance.makeRequestCompliant(request);\n    request.addHeader(\"Via\",via);\n\n    if (!cacheableRequestPolicy.isServableFromCache(request)) {\n        responseCache.flushCacheEntriesInvalidatedByRequest(target, request);\n        return callBackend(target, request, scope, chain);\n    }\n\n    final HttpCacheEntry entry = responseCache.getCacheEntry(target, request);\n    if (entry == null) {\n        LOG.debug(\"Cache miss\");\n        return handleCacheMiss(target, request, scope, chain);\n    } else {\n        return handleCacheHit(target, request, scope, chain, entry);\n    }\n}",
        "accept_response": "@Override\npublic ClassicHttpResponse execute(\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain) throws IOException, HttpException {\n    Args.notNull(request, \"HTTP request\");\n    Args.notNull(scope, \"Scope\");\n\n    final HttpRoute route = scope.route;\n    final HttpClientContext context = scope.clientContext;\n    context.setAttribute(HttpClientContext.HTTP_ROUTE, scope.route);\n    context.setAttribute(HttpClientContext.HTTP_REQUEST, request);\n\n    final URIAuthority authority = request.getAuthority();\n    final String scheme = request.getScheme();\n    final HttpHost target = authority != null ? new HttpHost(scheme, authority) : route.getTargetHost();\n    final String via = generateViaHeader(request);\n\n    // default response context\n    setResponseStatus(context, CacheResponseStatus.CACHE_MISS);\n\n    if (clientRequestsOurOptions(request)) {\n        setResponseStatus(context, CacheResponseStatus.CACHE_MODULE_RESPONSE);\n        return new BasicClassicHttpResponse(HttpStatus.SC_NOT_IMPLEMENTED);\n    }\n\n    final SimpleHttpResponse fatalErrorResponse = getFatallyNoncompliantResponse(request, context);\n    if (fatalErrorResponse != null) {\n        return convert(fatalErrorResponse, scope);\n    }\n\n    requestCompliance.makeRequestCompliant(request);\n    request.addHeader(\"Via\",via);\n\n    if (!cacheableRequestPolicy.isServableFromCache(request)) {\n        LOG.debug(\"Request is not servable from cache\");\n        responseCache.flushCacheEntriesInvalidatedByRequest(target, request);\n        return callBackend(target, request, scope, chain);\n    }\n\n    final HttpCacheEntry entry = responseCache.getCacheEntry(target, request);\n    if (entry == null) {\n        LOG.debug(\"Cache miss\");\n        return handleCacheMiss(target, request, scope, chain);\n    } else {\n        return handleCacheHit(target, request, scope, chain, entry);\n    }\n}",
        "reject_response": "@Override\npublic ClassicHttpResponse execute(\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain) throws IOException, HttpException {\n    Args.notNull(request, \"HTTP request\");\n    Args.notNull(scope, \"Scope\");\n\n    final HttpRoute route = scope.route;\n    final HttpClientContext context = scope.clientContext;\n    context.setAttribute(HttpClientContext.HTTP_ROUTE, scope.route);\n    context.setAttribute(HttpClientContext.HTTP_REQUEST, request);\n\n    final URIAuthority authority = request.getAuthority();\n    final String scheme = request.getScheme();\n    final HttpHost target = authority != null ? new HttpHost(scheme, authority) : route.getTargetHost();\n    final String via = generateViaHeader(request);\n\n    // default response context\n    setResponseStatus(context, CacheResponseStatus.CACHE_MISS);\n\n    if (clientRequestsOurOptions(request)) {\n        setResponseStatus(context, CacheResponseStatus.CACHE_MODULE_RESPONSE);\n        return new BasicClassicHttpResponse(HttpStatus.SC_NOT_IMPLEMENTED);\n    }\n\n    final SimpleHttpResponse fatalErrorResponse = getFatallyNoncompliantResponse(request, context);\n    if (fatalErrorResponse != null) {\n        return convert(fatalErrorResponse, scope);\n    }\n\n    requestCompliance.makeRequestCompliant(request);\n    request.addHeader(\"Via\",via);\n\n    if (!cacheableRequestPolicy.isServableFromCache(request)) {\n        log.debug(\"Request is not servable from cache\");\n        responseCache.flushCacheEntriesInvalidatedByRequest(target, request);\n        return callBackend(target, request, scope, chain);\n    }\n\n    final HttpCacheEntry entry = responseCache.getCacheEntry(target, request);\n    if (entry == null) {\n        LOG.debug(\"Cache miss\");\n        return handleCacheMiss(target, request, scope, chain);\n    } else {\n        return handleCacheHit(target, request, scope, chain, entry);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2924,
        "instruction": "private void initTopology() {\n    // initialize the task by initializing all its processor nodes in the topology\n    for (final ProcessorNode node : topology.processors()) {\n        processorContext.setCurrentNode(node);\n        try {\n            node.init(processorContext);\n        } finally {\n            processorContext.setCurrentNode(null);\n        }\n    }\n}",
        "accept_response": "private void initTopology() {\n    // initialize the task by initializing all its processor nodes in the topology\n    log.trace(\"{} Initializing processor nodes of the topology\", logPrefix);\n    for (final ProcessorNode node : topology.processors()) {\n        processorContext.setCurrentNode(node);\n        try {\n            node.init(processorContext);\n        } finally {\n            processorContext.setCurrentNode(null);\n        }\n    }\n}",
        "reject_response": "private void initTopology() {\n    // initialize the task by initializing all its processor nodes in the topology\n    log.debug(\"{} Initializing processor nodes of the topology\", logPrefix);\n    for (final ProcessorNode node : topology.processors()) {\n        processorContext.setCurrentNode(node);\n        try {\n            node.init(processorContext);\n        } finally {\n            processorContext.setCurrentNode(null);\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2833,
        "instruction": "private SegmentArchiveResult archiveSegment(long absIdx) throws StorageException {\n    long segIdx = absIdx % dsCfg.getWalSegments();\n\n    File origFile = new File(walWorkDir, FileDescriptor.fileName(segIdx));\n\n    String name = FileDescriptor.fileName(absIdx);\n\n    File dstTmpFile = new File(walArchiveDir, name + \".tmp\");\n\n    File dstFile = new File(walArchiveDir, name);\n\n    if (log.isInfoEnabled())\n        log.info(\"Starting to copy WAL segment [absIdx=\" + absIdx + \", segIdx=\" + segIdx +\n            \", origFile=\" + origFile.getAbsolutePath() + \", dstFile=\" + dstFile.getAbsolutePath() + ']');\n\n    try {\n        Files.deleteIfExists(dstTmpFile.toPath());\n\n        Files.copy(origFile.toPath(), dstTmpFile.toPath());\n\n        Files.move(dstTmpFile.toPath(), dstFile.toPath());\n\n        if (mode != WALMode.NONE) {\n            try (FileIO f0 = ioFactory.create(dstFile, CREATE, READ, WRITE)) {\n                f0.force();\n            }\n        }\n    }\n    catch (IOException e) {\n        throw new StorageException(\"Failed to archive WAL segment [\" +\n            \"srcFile=\" + origFile.getAbsolutePath() +\n            \", dstFile=\" + dstTmpFile.getAbsolutePath() + ']', e);\n    }\n\n    if (log.isInfoEnabled())\n\n    return new SegmentArchiveResult(absIdx, origFile, dstFile);\n}",
        "accept_response": "private SegmentArchiveResult archiveSegment(long absIdx) throws StorageException {\n    long segIdx = absIdx % dsCfg.getWalSegments();\n\n    File origFile = new File(walWorkDir, FileDescriptor.fileName(segIdx));\n\n    String name = FileDescriptor.fileName(absIdx);\n\n    File dstTmpFile = new File(walArchiveDir, name + \".tmp\");\n\n    File dstFile = new File(walArchiveDir, name);\n\n    if (log.isInfoEnabled())\n        log.info(\"Starting to copy WAL segment [absIdx=\" + absIdx + \", segIdx=\" + segIdx +\n            \", origFile=\" + origFile.getAbsolutePath() + \", dstFile=\" + dstFile.getAbsolutePath() + ']');\n\n    try {\n        Files.deleteIfExists(dstTmpFile.toPath());\n\n        Files.copy(origFile.toPath(), dstTmpFile.toPath());\n\n        Files.move(dstTmpFile.toPath(), dstFile.toPath());\n\n        if (mode != WALMode.NONE) {\n            try (FileIO f0 = ioFactory.create(dstFile, CREATE, READ, WRITE)) {\n                f0.force();\n            }\n        }\n    }\n    catch (IOException e) {\n        throw new StorageException(\"Failed to archive WAL segment [\" +\n            \"srcFile=\" + origFile.getAbsolutePath() +\n            \", dstFile=\" + dstTmpFile.getAbsolutePath() + ']', e);\n    }\n\n    if (log.isInfoEnabled())\n        log.info(\"Copied file [src=\" + origFile.getAbsolutePath() +\n            \", dst=\" + dstFile.getAbsolutePath() + ']');\n\n    return new SegmentArchiveResult(absIdx, origFile, dstFile);\n}",
        "reject_response": "private SegmentArchiveResult archiveSegment(long absIdx) throws StorageException {\n    long segIdx = absIdx % dsCfg.getWalSegments();\n\n    File origFile = new File(walWorkDir, FileDescriptor.fileName(segIdx));\n\n    String name = FileDescriptor.fileName(absIdx);\n\n    File dstTmpFile = new File(walArchiveDir, name + \".tmp\");\n\n    File dstFile = new File(walArchiveDir, name);\n\n    if (log.isInfoEnabled())\n        log.info(\"Starting to copy WAL segment [absIdx=\" + absIdx + \", segIdx=\" + segIdx +\n            \", origFile=\" + origFile.getAbsolutePath() + \", dstFile=\" + dstFile.getAbsolutePath() + ']');\n\n    try {\n        Files.deleteIfExists(dstTmpFile.toPath());\n\n        Files.copy(origFile.toPath(), dstTmpFile.toPath());\n\n        Files.move(dstTmpFile.toPath(), dstFile.toPath());\n\n        if (mode != WALMode.NONE) {\n            try (FileIO f0 = ioFactory.create(dstFile, CREATE, READ, WRITE)) {\n                f0.force();\n            }\n        }\n    }\n    catch (IOException e) {\n        throw new StorageException(\"Failed to archive WAL segment [\" +\n            \"srcFile=\" + origFile.getAbsolutePath() +\n            \", dstFile=\" + dstTmpFile.getAbsolutePath() + ']', e);\n    }\n\n    if (log.isInfoEnabled())\n    if (log.isDebugEnabled())\n        log.debug(\"Copied file [src=\" + origFile.getAbsolutePath() +\n\n    return new SegmentArchiveResult(absIdx, origFile, dstFile);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2764,
        "instruction": "public boolean isServableFromCache(final HttpRequest request) {\n    final String method = request.getMethod();\n\n    final ProtocolVersion pv = request.getVersion() != null ? request.getVersion() : HttpVersion.DEFAULT;\n    if (HttpVersion.HTTP_1_1.compareToVersion(pv) != 0) {\n        log.debug(\"non-HTTP/1.1 request is not serveable from cache\");\n        return false;\n    }\n\n    if (!method.equals(HeaderConstants.GET_METHOD) && !method.equals(HeaderConstants.HEAD_METHOD)) {\n        return false;\n    }\n\n    if (request.countHeaders(HeaderConstants.PRAGMA) > 0) {\n        log.debug(\"request with Pragma header is not serveable from cache\");\n        return false;\n    }\n\n    final Iterator<HeaderElement> it = MessageSupport.iterate(request, HeaderConstants.CACHE_CONTROL);\n    while (it.hasNext()) {\n        final HeaderElement cacheControlElement = it.next();\n        if (HeaderConstants.CACHE_CONTROL_NO_STORE.equalsIgnoreCase(cacheControlElement.getName())) {\n            log.debug(\"Request with no-store is not serveable from cache\");\n            return false;\n        }\n        if (HeaderConstants.CACHE_CONTROL_NO_CACHE.equalsIgnoreCase(cacheControlElement.getName())) {\n            log.debug(\"Request with no-cache is not serveable from cache\");\n            return false;\n        }\n    }\n\n    log.debug(\"Request is serveable from cache\");\n    return true;\n}",
        "accept_response": "public boolean isServableFromCache(final HttpRequest request) {\n    final String method = request.getMethod();\n\n    final ProtocolVersion pv = request.getVersion() != null ? request.getVersion() : HttpVersion.DEFAULT;\n    if (HttpVersion.HTTP_1_1.compareToVersion(pv) != 0) {\n        log.debug(\"non-HTTP/1.1 request is not serveable from cache\");\n        return false;\n    }\n\n    if (!method.equals(HeaderConstants.GET_METHOD) && !method.equals(HeaderConstants.HEAD_METHOD)) {\n        if (log.isDebugEnabled()) {\n            log.debug(\"{} request is not serveable from cache\", method);\n        }\n        return false;\n    }\n\n    if (request.countHeaders(HeaderConstants.PRAGMA) > 0) {\n        log.debug(\"request with Pragma header is not serveable from cache\");\n        return false;\n    }\n\n    final Iterator<HeaderElement> it = MessageSupport.iterate(request, HeaderConstants.CACHE_CONTROL);\n    while (it.hasNext()) {\n        final HeaderElement cacheControlElement = it.next();\n        if (HeaderConstants.CACHE_CONTROL_NO_STORE.equalsIgnoreCase(cacheControlElement.getName())) {\n            log.debug(\"Request with no-store is not serveable from cache\");\n            return false;\n        }\n        if (HeaderConstants.CACHE_CONTROL_NO_CACHE.equalsIgnoreCase(cacheControlElement.getName())) {\n            log.debug(\"Request with no-cache is not serveable from cache\");\n            return false;\n        }\n    }\n\n    log.debug(\"Request is serveable from cache\");\n    return true;\n}",
        "reject_response": "public boolean isServableFromCache(final HttpRequest request) {\n    final String method = request.getMethod();\n\n    final ProtocolVersion pv = request.getVersion() != null ? request.getVersion() : HttpVersion.DEFAULT;\n    if (HttpVersion.HTTP_1_1.compareToVersion(pv) != 0) {\n        log.debug(\"non-HTTP/1.1 request is not serveable from cache\");\n        return false;\n    }\n\n    if (!method.equals(HeaderConstants.GET_METHOD) && !method.equals(HeaderConstants.HEAD_METHOD)) {\n        if (log.isDebugEnabled()) {\n            log.debug(method + \" request is not serveable from cache\");\n        }\n        return false;\n    }\n\n    if (request.countHeaders(HeaderConstants.PRAGMA) > 0) {\n        log.debug(\"request with Pragma header is not serveable from cache\");\n        return false;\n    }\n\n    final Iterator<HeaderElement> it = MessageSupport.iterate(request, HeaderConstants.CACHE_CONTROL);\n    while (it.hasNext()) {\n        final HeaderElement cacheControlElement = it.next();\n        if (HeaderConstants.CACHE_CONTROL_NO_STORE.equalsIgnoreCase(cacheControlElement.getName())) {\n            log.debug(\"Request with no-store is not serveable from cache\");\n            return false;\n        }\n        if (HeaderConstants.CACHE_CONTROL_NO_CACHE.equalsIgnoreCase(cacheControlElement.getName())) {\n            log.debug(\"Request with no-cache is not serveable from cache\");\n            return false;\n        }\n    }\n\n    log.debug(\"Request is serveable from cache\");\n    return true;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2873,
        "instruction": "@Override\npublic long write(ByteBuffer bb)\n{\n    log(\"W\") ;\n\n    if ( inAllocWrite )\n    inAllocWrite = false ;\n    if ( writeBuffer == null )\n    {\n        long x = rawWrite(bb) ;\n        log(\"W -> 0x%X\", x);\n        return x ;\n    }\n\n    int len = bb.limit() - bb.position() ;\n    int spaceNeeded = len + SizeOfInt ;\n\n    if ( writeBuffer.position()+spaceNeeded > writeBuffer.capacity() )\n        // No room - flush.\n        flushOutputBuffer() ;\n    if ( writeBuffer.position()+spaceNeeded > writeBuffer.capacity() )\n    {\n        long x = rawWrite(bb) ;\n        if ( logging )\n            log(\"W -> 0x%X\", x);\n        return x ;\n    }\n\n    long loc = writeBuffer.position()+filesize ;\n    writeBuffer.putInt(len) ;\n    writeBuffer.put(bb) ;\n    if ( logging )\n        log(\"W -> 0x%X\", loc);\n    return loc ;\n}",
        "accept_response": "@Override\npublic long write(ByteBuffer bb)\n{\n    log(\"W\") ;\n\n    if ( inAllocWrite )\n        Log.error(this, \"In the middle of an alloc-write\") ;\n    inAllocWrite = false ;\n    if ( writeBuffer == null )\n    {\n        long x = rawWrite(bb) ;\n        log(\"W -> 0x%X\", x);\n        return x ;\n    }\n\n    int len = bb.limit() - bb.position() ;\n    int spaceNeeded = len + SizeOfInt ;\n\n    if ( writeBuffer.position()+spaceNeeded > writeBuffer.capacity() )\n        // No room - flush.\n        flushOutputBuffer() ;\n    if ( writeBuffer.position()+spaceNeeded > writeBuffer.capacity() )\n    {\n        long x = rawWrite(bb) ;\n        if ( logging )\n            log(\"W -> 0x%X\", x);\n        return x ;\n    }\n\n    long loc = writeBuffer.position()+filesize ;\n    writeBuffer.putInt(len) ;\n    writeBuffer.put(bb) ;\n    if ( logging )\n        log(\"W -> 0x%X\", loc);\n    return loc ;\n}",
        "reject_response": "@Override\npublic long write(ByteBuffer bb)\n{\n    log(\"W\") ;\n\n    if ( inAllocWrite )\n        Log.fatal(this, \"In the middle of an alloc-write\") ;\n    inAllocWrite = false ;\n    if ( writeBuffer == null )\n    {\n        long x = rawWrite(bb) ;\n        log(\"W -> 0x%X\", x);\n        return x ;\n    }\n\n    int len = bb.limit() - bb.position() ;\n    int spaceNeeded = len + SizeOfInt ;\n\n    if ( writeBuffer.position()+spaceNeeded > writeBuffer.capacity() )\n        // No room - flush.\n        flushOutputBuffer() ;\n    if ( writeBuffer.position()+spaceNeeded > writeBuffer.capacity() )\n    {\n        long x = rawWrite(bb) ;\n        if ( logging )\n            log(\"W -> 0x%X\", x);\n        return x ;\n    }\n\n    long loc = writeBuffer.position()+filesize ;\n    writeBuffer.putInt(len) ;\n    writeBuffer.put(bb) ;\n    if ( logging )\n        log(\"W -> 0x%X\", loc);\n    return loc ;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3060,
        "instruction": "void processUpdate(SolrQueryRequest req, UpdateRequestProcessor processor, XMLStreamReader parser)\n        throws XMLStreamException, IOException, FactoryConfigurationError {\n  AddUpdateCommand addCmd = null;\n  SolrParams params = req.getParams();\n  while (true) {\n    int event = parser.next();\n    switch (event) {\n      case XMLStreamConstants.END_DOCUMENT:\n        parser.close();\n        return;\n\n      case XMLStreamConstants.START_ELEMENT:\n        String currTag = parser.getLocalName();\n        if (currTag.equals(UpdateRequestHandler.ADD)) {\n          log.trace(\"SolrCore.update(add)\");\n\n          addCmd = new AddUpdateCommand(req);\n\n          // First look for commitWithin parameter on the request, will be overwritten for individual <add>'s\n          addCmd.commitWithin = params.getInt(UpdateParams.COMMIT_WITHIN, -1);\n          addCmd.overwrite = params.getBool(UpdateParams.OVERWRITE, true);\n\n          for (int i = 0; i < parser.getAttributeCount(); i++) {\n            String attrName = parser.getAttributeLocalName(i);\n            String attrVal = parser.getAttributeValue(i);\n            if (UpdateRequestHandler.OVERWRITE.equals(attrName)) {\n              addCmd.overwrite = StrUtils.parseBoolean(attrVal);\n            } else if (UpdateRequestHandler.COMMIT_WITHIN.equals(attrName)) {\n              addCmd.commitWithin = Integer.parseInt(attrVal);\n            } else {\n              log.warn(\"XML element <add> has invalid XML attr: \" + attrName);\n            }\n          }\n\n        } else if (\"doc\".equals(currTag)) {\n          if(addCmd != null) {\n            log.trace(\"adding doc...\");\n            addCmd.clear();\n            addCmd.solrDoc = readDoc(parser);\n            processor.processAdd(addCmd);\n          } else {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unexpected <doc> tag without an <add> tag surrounding it.\");\n          }\n        } else if (UpdateRequestHandler.COMMIT.equals(currTag) || UpdateRequestHandler.OPTIMIZE.equals(currTag)) {\n\n          CommitUpdateCommand cmd = new CommitUpdateCommand(req, UpdateRequestHandler.OPTIMIZE.equals(currTag));\n          ModifiableSolrParams mp = new ModifiableSolrParams();\n\n          for (int i = 0; i < parser.getAttributeCount(); i++) {\n            String attrName = parser.getAttributeLocalName(i);\n            String attrVal = parser.getAttributeValue(i);\n            mp.set(attrName, attrVal);\n          }\n\n          RequestHandlerUtils.validateCommitParams(mp);\n          SolrParams p = SolrParams.wrapDefaults(mp, req.getParams());   // default to the normal request params for commit options\n          RequestHandlerUtils.updateCommit(cmd, p);\n\n          processor.processCommit(cmd);\n        } // end commit\n        else if (UpdateRequestHandler.ROLLBACK.equals(currTag)) {\n          log.trace(\"parsing rollback\");\n\n          RollbackUpdateCommand cmd = new RollbackUpdateCommand(req);\n\n          processor.processRollback(cmd);\n        } // end rollback\n        else if (UpdateRequestHandler.DELETE.equals(currTag)) {\n          log.trace(\"parsing delete\");\n          processDelete(req, processor, parser);\n        } // end delete\n        break;\n    }\n  }\n}",
        "accept_response": "void processUpdate(SolrQueryRequest req, UpdateRequestProcessor processor, XMLStreamReader parser)\n        throws XMLStreamException, IOException, FactoryConfigurationError {\n  AddUpdateCommand addCmd = null;\n  SolrParams params = req.getParams();\n  while (true) {\n    int event = parser.next();\n    switch (event) {\n      case XMLStreamConstants.END_DOCUMENT:\n        parser.close();\n        return;\n\n      case XMLStreamConstants.START_ELEMENT:\n        String currTag = parser.getLocalName();\n        if (currTag.equals(UpdateRequestHandler.ADD)) {\n          log.trace(\"SolrCore.update(add)\");\n\n          addCmd = new AddUpdateCommand(req);\n\n          // First look for commitWithin parameter on the request, will be overwritten for individual <add>'s\n          addCmd.commitWithin = params.getInt(UpdateParams.COMMIT_WITHIN, -1);\n          addCmd.overwrite = params.getBool(UpdateParams.OVERWRITE, true);\n\n          for (int i = 0; i < parser.getAttributeCount(); i++) {\n            String attrName = parser.getAttributeLocalName(i);\n            String attrVal = parser.getAttributeValue(i);\n            if (UpdateRequestHandler.OVERWRITE.equals(attrName)) {\n              addCmd.overwrite = StrUtils.parseBoolean(attrVal);\n            } else if (UpdateRequestHandler.COMMIT_WITHIN.equals(attrName)) {\n              addCmd.commitWithin = Integer.parseInt(attrVal);\n            } else {\n              log.warn(\"XML element <add> has invalid XML attr: \" + attrName);\n            }\n          }\n\n        } else if (\"doc\".equals(currTag)) {\n          if(addCmd != null) {\n            log.trace(\"adding doc...\");\n            addCmd.clear();\n            addCmd.solrDoc = readDoc(parser);\n            processor.processAdd(addCmd);\n          } else {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unexpected <doc> tag without an <add> tag surrounding it.\");\n          }\n        } else if (UpdateRequestHandler.COMMIT.equals(currTag) || UpdateRequestHandler.OPTIMIZE.equals(currTag)) {\n          log.trace(\"parsing {}\", currTag);\n\n          CommitUpdateCommand cmd = new CommitUpdateCommand(req, UpdateRequestHandler.OPTIMIZE.equals(currTag));\n          ModifiableSolrParams mp = new ModifiableSolrParams();\n\n          for (int i = 0; i < parser.getAttributeCount(); i++) {\n            String attrName = parser.getAttributeLocalName(i);\n            String attrVal = parser.getAttributeValue(i);\n            mp.set(attrName, attrVal);\n          }\n\n          RequestHandlerUtils.validateCommitParams(mp);\n          SolrParams p = SolrParams.wrapDefaults(mp, req.getParams());   // default to the normal request params for commit options\n          RequestHandlerUtils.updateCommit(cmd, p);\n\n          processor.processCommit(cmd);\n        } // end commit\n        else if (UpdateRequestHandler.ROLLBACK.equals(currTag)) {\n          log.trace(\"parsing rollback\");\n\n          RollbackUpdateCommand cmd = new RollbackUpdateCommand(req);\n\n          processor.processRollback(cmd);\n        } // end rollback\n        else if (UpdateRequestHandler.DELETE.equals(currTag)) {\n          log.trace(\"parsing delete\");\n          processDelete(req, processor, parser);\n        } // end delete\n        break;\n    }\n  }\n}",
        "reject_response": "void processUpdate(SolrQueryRequest req, UpdateRequestProcessor processor, XMLStreamReader parser)\n        throws XMLStreamException, IOException, FactoryConfigurationError {\n  AddUpdateCommand addCmd = null;\n  SolrParams params = req.getParams();\n  while (true) {\n    int event = parser.next();\n    switch (event) {\n      case XMLStreamConstants.END_DOCUMENT:\n        parser.close();\n        return;\n\n      case XMLStreamConstants.START_ELEMENT:\n        String currTag = parser.getLocalName();\n        if (currTag.equals(UpdateRequestHandler.ADD)) {\n          log.trace(\"SolrCore.update(add)\");\n\n          addCmd = new AddUpdateCommand(req);\n\n          // First look for commitWithin parameter on the request, will be overwritten for individual <add>'s\n          addCmd.commitWithin = params.getInt(UpdateParams.COMMIT_WITHIN, -1);\n          addCmd.overwrite = params.getBool(UpdateParams.OVERWRITE, true);\n\n          for (int i = 0; i < parser.getAttributeCount(); i++) {\n            String attrName = parser.getAttributeLocalName(i);\n            String attrVal = parser.getAttributeValue(i);\n            if (UpdateRequestHandler.OVERWRITE.equals(attrName)) {\n              addCmd.overwrite = StrUtils.parseBoolean(attrVal);\n            } else if (UpdateRequestHandler.COMMIT_WITHIN.equals(attrName)) {\n              addCmd.commitWithin = Integer.parseInt(attrVal);\n            } else {\n              log.warn(\"XML element <add> has invalid XML attr: \" + attrName);\n            }\n          }\n\n        } else if (\"doc\".equals(currTag)) {\n          if(addCmd != null) {\n            log.trace(\"adding doc...\");\n            addCmd.clear();\n            addCmd.solrDoc = readDoc(parser);\n            processor.processAdd(addCmd);\n          } else {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unexpected <doc> tag without an <add> tag surrounding it.\");\n          }\n        } else if (UpdateRequestHandler.COMMIT.equals(currTag) || UpdateRequestHandler.OPTIMIZE.equals(currTag)) {\n          log.trace(\"parsing \" + currTag);\n\n          CommitUpdateCommand cmd = new CommitUpdateCommand(req, UpdateRequestHandler.OPTIMIZE.equals(currTag));\n          ModifiableSolrParams mp = new ModifiableSolrParams();\n\n          for (int i = 0; i < parser.getAttributeCount(); i++) {\n            String attrName = parser.getAttributeLocalName(i);\n            String attrVal = parser.getAttributeValue(i);\n            mp.set(attrName, attrVal);\n          }\n\n          RequestHandlerUtils.validateCommitParams(mp);\n          SolrParams p = SolrParams.wrapDefaults(mp, req.getParams());   // default to the normal request params for commit options\n          RequestHandlerUtils.updateCommit(cmd, p);\n\n          processor.processCommit(cmd);\n        } // end commit\n        else if (UpdateRequestHandler.ROLLBACK.equals(currTag)) {\n          log.trace(\"parsing rollback\");\n\n          RollbackUpdateCommand cmd = new RollbackUpdateCommand(req);\n\n          processor.processRollback(cmd);\n        } // end rollback\n        else if (UpdateRequestHandler.DELETE.equals(currTag)) {\n          log.trace(\"parsing delete\");\n          processDelete(req, processor, parser);\n        } // end delete\n        break;\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2759,
        "instruction": "public synchronized List<ConnectionContext> removeConnections(int num) {\n  List<ConnectionContext> removed = new LinkedList<>();\n  if (this.connections.size() > this.minSize) {\n    int targetCount = Math.min(num, this.connections.size() - this.minSize);\n    // Remove and close targetCount of connections\n    List<ConnectionContext> tmpConnections = new ArrayList<>();\n    for (ConnectionContext conn : this.connections) {\n      // Only pick idle connections to close\n      if (removed.size() < targetCount && conn.isIdle()) {\n        removed.add(conn);\n      } else {\n        tmpConnections.add(conn);\n      }\n    }\n    this.connections = tmpConnections;\n  }\n  return removed;\n}",
        "accept_response": "public synchronized List<ConnectionContext> removeConnections(int num) {\n  List<ConnectionContext> removed = new LinkedList<>();\n  if (this.connections.size() > this.minSize) {\n    int targetCount = Math.min(num, this.connections.size() - this.minSize);\n    // Remove and close targetCount of connections\n    List<ConnectionContext> tmpConnections = new ArrayList<>();\n    for (ConnectionContext conn : this.connections) {\n      // Only pick idle connections to close\n      if (removed.size() < targetCount && conn.isIdle()) {\n        removed.add(conn);\n      } else {\n        tmpConnections.add(conn);\n      }\n    }\n    this.connections = tmpConnections;\n  }\n  LOG.debug(\"Expected to remove {} connection and actually removed {} connections \"\n      + \"for connectionPool: {}\", num, removed.size(), connectionPoolId);\n  return removed;\n}",
        "reject_response": "public synchronized List<ConnectionContext> removeConnections(int num) {\n  List<ConnectionContext> removed = new LinkedList<>();\n  if (this.connections.size() > this.minSize) {\n    int targetCount = Math.min(num, this.connections.size() - this.minSize);\n    // Remove and close targetCount of connections\n    List<ConnectionContext> tmpConnections = new ArrayList<>();\n    for (ConnectionContext conn : this.connections) {\n      // Only pick idle connections to close\n      if (removed.size() < targetCount && conn.isIdle()) {\n        removed.add(conn);\n      } else {\n        tmpConnections.add(conn);\n      }\n    }\n    this.connections = tmpConnections;\n  }\n  LOG.debug(\"Expected to remove {} connection and actually removed {} connections\",\n      num, removed.size());\n  return removed;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2445,
        "instruction": "public int readFully(ByteBuffer buffer) throws IOException {\n  int totalRead = 0;\n  while (buffer.remaining() != 0) {\n    int read = in.read(buffer);\n    if (read < 0) {\n      return totalRead;\n    }\n    totalRead += read;\n    if (read == 0) {\n      break;\n    }\n  }\n  this.bytesRead += totalRead;\n  return totalRead;\n}",
        "accept_response": "public int readFully(ByteBuffer buffer) throws IOException {\n  LOGGER.debug(\"Reading buffer with size: {}\", buffer.remaining());\n  int totalRead = 0;\n  while (buffer.remaining() != 0) {\n    int read = in.read(buffer);\n    if (read < 0) {\n      return totalRead;\n    }\n    totalRead += read;\n    if (read == 0) {\n      break;\n    }\n  }\n  this.bytesRead += totalRead;\n  return totalRead;\n}",
        "reject_response": "public int readFully(ByteBuffer buffer) throws IOException {\n  LOGGER.debug(\"Reading buffer with size: \" + buffer.remaining());\n  int totalRead = 0;\n  while (buffer.remaining() != 0) {\n    int read = in.read(buffer);\n    if (read < 0) {\n      return totalRead;\n    }\n    totalRead += read;\n    if (read == 0) {\n      break;\n    }\n  }\n  this.bytesRead += totalRead;\n  return totalRead;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2667,
        "instruction": "@Override\npublic synchronized State getFinalState() {\n\n  State state = new State();\n  try {\n    for (Map.Entry<GenericRecord, DataWriter<D>> entry : this.partitionWriters.asMap().entrySet()) {\n      if (entry.getValue() instanceof FinalState) {\n\n        State partitionFinalState = ((FinalState) entry.getValue()).getFinalState();\n\n        if (this.shouldPartition) {\n          for (String key : partitionFinalState.getPropertyNames()) {\n            // Prevent overwriting final state across writers\n            partitionFinalState.setProp(key + \"_\" + AvroUtils.serializeAsPath(entry.getKey(), false, true),\n                partitionFinalState.getProp(key));\n          }\n        }\n\n        state.addAll(partitionFinalState);\n      }\n    }\n    state.setProp(\"RecordsWritten\", recordsWritten());\n    state.setProp(\"BytesWritten\", bytesWritten());\n  } catch (Exception exception) {\n    // If Writer fails to return bytesWritten, it might not be implemented, or implemented incorrectly.\n    // Omit property instead of failing.\n  }\n  return state;\n}",
        "accept_response": "@Override\npublic synchronized State getFinalState() {\n\n  State state = new State();\n  try {\n    for (Map.Entry<GenericRecord, DataWriter<D>> entry : this.partitionWriters.asMap().entrySet()) {\n      if (entry.getValue() instanceof FinalState) {\n\n        State partitionFinalState = ((FinalState) entry.getValue()).getFinalState();\n\n        if (this.shouldPartition) {\n          for (String key : partitionFinalState.getPropertyNames()) {\n            // Prevent overwriting final state across writers\n            partitionFinalState.setProp(key + \"_\" + AvroUtils.serializeAsPath(entry.getKey(), false, true),\n                partitionFinalState.getProp(key));\n          }\n        }\n\n        state.addAll(partitionFinalState);\n      }\n    }\n    state.setProp(\"RecordsWritten\", recordsWritten());\n    state.setProp(\"BytesWritten\", bytesWritten());\n  } catch (Exception exception) {\n    log.warn(\"Failed to get final state.\", exception);\n    // If Writer fails to return bytesWritten, it might not be implemented, or implemented incorrectly.\n    // Omit property instead of failing.\n  }\n  return state;\n}",
        "reject_response": "@Override\npublic synchronized State getFinalState() {\n\n  State state = new State();\n  try {\n    for (Map.Entry<GenericRecord, DataWriter<D>> entry : this.partitionWriters.asMap().entrySet()) {\n      if (entry.getValue() instanceof FinalState) {\n\n        State partitionFinalState = ((FinalState) entry.getValue()).getFinalState();\n\n        if (this.shouldPartition) {\n          for (String key : partitionFinalState.getPropertyNames()) {\n            // Prevent overwriting final state across writers\n            partitionFinalState.setProp(key + \"_\" + AvroUtils.serializeAsPath(entry.getKey(), false, true),\n                partitionFinalState.getProp(key));\n          }\n        }\n\n        state.addAll(partitionFinalState);\n      }\n    }\n    state.setProp(\"RecordsWritten\", recordsWritten());\n    state.setProp(\"BytesWritten\", bytesWritten());\n  } catch (Exception exception) {\n    log.warn(\"Failed to get final state.\" + exception.getMessage());\n    // If Writer fails to return bytesWritten, it might not be implemented, or implemented incorrectly.\n    // Omit property instead of failing.\n  }\n  return state;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2488,
        "instruction": "@Override\npublic boolean tableExists(String tableId) throws IOException {\n  if (!BigtableSession.isAlpnProviderEnabled()) {\n    return true;\n  }\n\n  try (BigtableSession session = new BigtableSession(options)) {\n    GetTableRequest getTable =\n        GetTableRequest.newBuilder()\n            .setName(options.getInstanceName().toTableNameStr(tableId))\n            .build();\n    session.getTableAdminClient().getTable(getTable);\n    return true;\n  } catch (StatusRuntimeException e) {\n    if (e.getStatus().getCode() == Code.NOT_FOUND) {\n      return false;\n    }\n    String message =\n        String.format(\n            \"Error checking whether table %s (BigtableOptions %s) exists\", tableId, options);\n    LOG.error(message, e);\n    throw new IOException(message, e);\n  }\n}",
        "accept_response": "@Override\npublic boolean tableExists(String tableId) throws IOException {\n  if (!BigtableSession.isAlpnProviderEnabled()) {\n    LOG.info(\n        \"Skipping existence check for table {} (BigtableOptions {}) because ALPN is not\"\n            + \" configured.\",\n        tableId,\n        options);\n    return true;\n  }\n\n  try (BigtableSession session = new BigtableSession(options)) {\n    GetTableRequest getTable =\n        GetTableRequest.newBuilder()\n            .setName(options.getInstanceName().toTableNameStr(tableId))\n            .build();\n    session.getTableAdminClient().getTable(getTable);\n    return true;\n  } catch (StatusRuntimeException e) {\n    if (e.getStatus().getCode() == Code.NOT_FOUND) {\n      return false;\n    }\n    String message =\n        String.format(\n            \"Error checking whether table %s (BigtableOptions %s) exists\", tableId, options);\n    LOG.error(message, e);\n    throw new IOException(message, e);\n  }\n}",
        "reject_response": "@Override\npublic boolean tableExists(String tableId) throws IOException {\n  if (!BigtableSession.isAlpnProviderEnabled()) {\n    logger.info(\n    return true;\n  }\n\n  try (BigtableSession session = new BigtableSession(options)) {\n    GetTableRequest getTable =\n        GetTableRequest.newBuilder()\n            .setName(options.getInstanceName().toTableNameStr(tableId))\n            .build();\n    session.getTableAdminClient().getTable(getTable);\n    return true;\n  } catch (StatusRuntimeException e) {\n    if (e.getStatus().getCode() == Code.NOT_FOUND) {\n      return false;\n    }\n    String message =\n        String.format(\n            \"Error checking whether table %s (BigtableOptions %s) exists\", tableId, options);\n    LOG.error(message, e);\n    throw new IOException(message, e);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3038,
        "instruction": "private void clearDisposalCallbackRegistryQueue() {\n    java.lang.ref.Reference<?> ref = queue.poll();\n    while (ref != null) {\n        Disposable registry = disposalCallbacks.remove(ref);\n        registry.onDisposed();\n        ref = queue.poll();\n    }\n}",
        "accept_response": "private void clearDisposalCallbackRegistryQueue() {\n    java.lang.ref.Reference<?> ref = queue.poll();\n    while (ref != null) {\n        log.debug(\"calling disposal for {}.\", ref);\n        Disposable registry = disposalCallbacks.remove(ref);\n        registry.onDisposed();\n        ref = queue.poll();\n    }\n}",
        "reject_response": "private void clearDisposalCallbackRegistryQueue() {\n    java.lang.ref.Reference<?> ref = queue.poll();\n    while (ref != null) {\n        log.debug(\"calling disposal for {}.\", ref.toString());\n        Disposable registry = disposalCallbacks.remove(ref);\n        registry.onDisposed();\n        ref = queue.poll();\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2861,
        "instruction": "private void logReadSuccess(CassandraIdAndPath cassandraIdAndPath) {\n    GhostMailbox.logger()\n        .addField(GhostMailbox.MAILBOX_NAME, cassandraIdAndPath.getMailboxPath())\n        .addField(TYPE, \"readSuccess\")\n        .addField(GhostMailbox.MAILBOX_ID, cassandraIdAndPath.getCassandraId())\n}",
        "accept_response": "private void logReadSuccess(CassandraIdAndPath cassandraIdAndPath) {\n    GhostMailbox.logger()\n        .addField(GhostMailbox.MAILBOX_NAME, cassandraIdAndPath.getMailboxPath())\n        .addField(TYPE, \"readSuccess\")\n        .addField(GhostMailbox.MAILBOX_ID, cassandraIdAndPath.getCassandraId())\n        .log(logger -> logger.debug(\"Read mailbox succeeded\"));\n}",
        "reject_response": "private void logReadSuccess(CassandraIdAndPath cassandraIdAndPath) {\n    GhostMailbox.logger()\n        .addField(GhostMailbox.MAILBOX_NAME, cassandraIdAndPath.getMailboxPath())\n        .addField(TYPE, \"readSuccess\")\n        .addField(GhostMailbox.MAILBOX_ID, cassandraIdAndPath.getCassandraId())\n        .log(logger -> logger.info(\"Read mailbox succeeded\"));\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2985,
        "instruction": "@GET\n@Path(KMSRESTConstants.KEYS_METADATA_RESOURCE)\n@Produces(MediaType.APPLICATION_JSON)\npublic Response getKeysMetadata(@QueryParam(KMSRESTConstants.KEY)\n    List<String> keyNamesList, @Context HttpServletRequest request) throws Exception {\n  try {\n    KMSWebApp.getAdminCallsMeter().mark();\n    UserGroupInformation user = HttpUserGroupInformation.get();\n    final String[] keyNames = keyNamesList.toArray( new String[keyNamesList.size()]);\n    assertAccess(Type.GET_METADATA, user, KMSOp.GET_KEYS_METADATA, request.getRemoteAddr());\n    KeyProvider.Metadata[] keysMeta = user.doAs(new PrivilegedExceptionAction<KeyProvider.Metadata[]>() {\n      @Override\n      public KeyProvider.Metadata[] run() throws Exception {\n        return provider.getKeysMetadata(keyNames);\n      }\n    });\n    Object json = KMSServerJSONUtils.toJSON(keyNames, keysMeta);\n    kmsAudit.ok(user, KMSOp.GET_KEYS_METADATA, \"\");\n    if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Exiting getKeysMetadata method.\");\n    }\n    return Response.ok().type(MediaType.APPLICATION_JSON).entity(json).build();\n  } catch (Exception e) {\n    LOG.error(\"Exception in getKeysmetadata.\", e);\n    throw e;\n  }\n}",
        "accept_response": "@GET\n@Path(KMSRESTConstants.KEYS_METADATA_RESOURCE)\n@Produces(MediaType.APPLICATION_JSON)\npublic Response getKeysMetadata(@QueryParam(KMSRESTConstants.KEY)\n    List<String> keyNamesList, @Context HttpServletRequest request) throws Exception {\n  try {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Entering getKeysMetadata method.\");\n    }\n    KMSWebApp.getAdminCallsMeter().mark();\n    UserGroupInformation user = HttpUserGroupInformation.get();\n    final String[] keyNames = keyNamesList.toArray( new String[keyNamesList.size()]);\n    assertAccess(Type.GET_METADATA, user, KMSOp.GET_KEYS_METADATA, request.getRemoteAddr());\n    KeyProvider.Metadata[] keysMeta = user.doAs(new PrivilegedExceptionAction<KeyProvider.Metadata[]>() {\n      @Override\n      public KeyProvider.Metadata[] run() throws Exception {\n        return provider.getKeysMetadata(keyNames);\n      }\n    });\n    Object json = KMSServerJSONUtils.toJSON(keyNames, keysMeta);\n    kmsAudit.ok(user, KMSOp.GET_KEYS_METADATA, \"\");\n    if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Exiting getKeysMetadata method.\");\n    }\n    return Response.ok().type(MediaType.APPLICATION_JSON).entity(json).build();\n  } catch (Exception e) {\n    LOG.error(\"Exception in getKeysmetadata.\", e);\n    throw e;\n  }\n}",
        "reject_response": "@GET\n@Path(KMSRESTConstants.KEYS_METADATA_RESOURCE)\n@Produces(MediaType.APPLICATION_JSON)\npublic Response getKeysMetadata(@QueryParam(KMSRESTConstants.KEY)\n    List<String> keyNamesList, @Context HttpServletRequest request) throws Exception {\n  try {\n    if (LOG.isDebugEnabled()) {\n    LOG.info(\"Entering getKeysMetadata method.\");\n    }\n    KMSWebApp.getAdminCallsMeter().mark();\n    UserGroupInformation user = HttpUserGroupInformation.get();\n    final String[] keyNames = keyNamesList.toArray( new String[keyNamesList.size()]);\n    assertAccess(Type.GET_METADATA, user, KMSOp.GET_KEYS_METADATA, request.getRemoteAddr());\n    KeyProvider.Metadata[] keysMeta = user.doAs(new PrivilegedExceptionAction<KeyProvider.Metadata[]>() {\n      @Override\n      public KeyProvider.Metadata[] run() throws Exception {\n        return provider.getKeysMetadata(keyNames);\n      }\n    });\n    Object json = KMSServerJSONUtils.toJSON(keyNames, keysMeta);\n    kmsAudit.ok(user, KMSOp.GET_KEYS_METADATA, \"\");\n    if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Exiting getKeysMetadata method.\");\n    }\n    return Response.ok().type(MediaType.APPLICATION_JSON).entity(json).build();\n  } catch (Exception e) {\n    LOG.error(\"Exception in getKeysmetadata.\", e);\n    throw e;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3108,
        "instruction": "@Override\npublic BoolProto existIndexByColumn(RpcController controller, GetIndexByColumnRequest request)\n    throws ServiceException {\n\n  TableIdentifierProto identifier = request.getTableIdentifier();\n  String databaseName = identifier.getDatabaseName();\n  String tableName = identifier.getTableName();\n  String columnName = request.getColumnName();\n\n  rlock.lock();\n  try {\n    return store.existIndexByColumn(databaseName, tableName, columnName) ?\n        ProtoUtil.TRUE : ProtoUtil.FALSE;\n  } catch (Exception e) {\n    return BoolProto.newBuilder().setValue(false).build();\n  } finally {\n    rlock.unlock();\n  }\n}",
        "accept_response": "@Override\npublic BoolProto existIndexByColumn(RpcController controller, GetIndexByColumnRequest request)\n    throws ServiceException {\n\n  TableIdentifierProto identifier = request.getTableIdentifier();\n  String databaseName = identifier.getDatabaseName();\n  String tableName = identifier.getTableName();\n  String columnName = request.getColumnName();\n\n  rlock.lock();\n  try {\n    return store.existIndexByColumn(databaseName, tableName, columnName) ?\n        ProtoUtil.TRUE : ProtoUtil.FALSE;\n  } catch (Exception e) {\n    LOG.error(e, e);\n    return BoolProto.newBuilder().setValue(false).build();\n  } finally {\n    rlock.unlock();\n  }\n}",
        "reject_response": "@Override\npublic BoolProto existIndexByColumn(RpcController controller, GetIndexByColumnRequest request)\n    throws ServiceException {\n\n  TableIdentifierProto identifier = request.getTableIdentifier();\n  String databaseName = identifier.getDatabaseName();\n  String tableName = identifier.getTableName();\n  String columnName = request.getColumnName();\n\n  rlock.lock();\n  try {\n    return store.existIndexByColumn(databaseName, tableName, columnName) ?\n        ProtoUtil.TRUE : ProtoUtil.FALSE;\n  } catch (Exception e) {\n    LOG.error(e);\n    return BoolProto.newBuilder().setValue(false).build();\n  } finally {\n    rlock.unlock();\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2740,
        "instruction": "boolean close(Connection connection) {\n  boolean exists = remove(connection);\n  if (exists) {\n    // only close if actually removed to avoid double-closing due\n    // to possible races\n    connection.close();\n    // Remove authorized users only\n    if (connection.user != null && connection.connectionContextRead) {\n      decrUserConnections(connection.user.getShortUserName());\n    }\n  }\n  return exists;\n}",
        "accept_response": "boolean close(Connection connection) {\n  boolean exists = remove(connection);\n  if (exists) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"{}: disconnecting client {}. Number of active connections: {}.\",\n          Thread.currentThread().getName(), connection, size());\n    }\n    // only close if actually removed to avoid double-closing due\n    // to possible races\n    connection.close();\n    // Remove authorized users only\n    if (connection.user != null && connection.connectionContextRead) {\n      decrUserConnections(connection.user.getShortUserName());\n    }\n  }\n  return exists;\n}",
        "reject_response": "boolean close(Connection connection) {\n  boolean exists = remove(connection);\n  if (exists) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(Thread.currentThread().getName() +\n          \": disconnecting client \" + connection +\n          \". Number of active connections: \"+ size());\n    }\n    // only close if actually removed to avoid double-closing due\n    // to possible races\n    connection.close();\n    // Remove authorized users only\n    if (connection.user != null && connection.connectionContextRead) {\n      decrUserConnections(connection.user.getShortUserName());\n    }\n  }\n  return exists;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3007,
        "instruction": "@Override\n   protected void processProperties(\n    ConfigurableListableBeanFactory beanFactory, Properties props)\n    throws BeansException {\n\n   // First let's add the system properties\nSet<Object> keySet = System.getProperties().keySet();\nfor (Object key : keySet) {\n    String keyStr = key.toString();\n    propertiesMap.put(keyStr, System.getProperties().getProperty(keyStr).trim());\n}\n\n// Let's add our properties now\nkeySet = props.keySet();\nfor (Object key : keySet) {\n    String keyStr = key.toString();\n    propertiesMap.put(keyStr, props.getProperty(keyStr).trim());\n}\n\nString storeType = propertiesMap.get(\"ranger.keystore.file.type\");\n// update system trust store path with custom trust store.\nif (propertiesMap!=null && propertiesMap.containsKey(\"ranger.truststore.file\")) {\n\tif(!StringUtils.isEmpty(propertiesMap.get(\"ranger.truststore.file\"))){\n\t\tSystem.setProperty(\"javax.net.ssl.trustStore\", propertiesMap.get(\"ranger.truststore.file\"));\n\t\tSystem.setProperty(\"javax.net.ssl.trustStoreType\", KeyStore.getDefaultType());\n\t\tPath trustStoreFile = Paths.get(propertiesMap.get(\"ranger.truststore.file\"));\n\t\tif (!Files.exists(trustStoreFile) || !Files.isReadable(trustStoreFile)) {\n\t\t\tlogger.debug(\"Could not find or read truststore file '\"+propertiesMap.get(\"ranger.truststore.file\")+\"'\");\n\t\t}else{\n\t\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\")){\n\t\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\t\tString trustStoreAlias=getProperty(\"ranger.truststore.alias\",\"trustStoreAlias\");\n\t\t\t\tif(path!=null && trustStoreAlias!=null){\n\t\t\t\t\tString trustStorePassword=CredentialReader.getDecryptedString(path.trim(), trustStoreAlias.trim(), storeType);\n\t\t\t\t\tif(trustStorePassword!=null&& !trustStorePassword.trim().isEmpty() && !trustStorePassword.trim().equalsIgnoreCase(\"none\")){\n\t\t\t\t\t\tpropertiesMap.put(\"ranger.truststore.password\", trustStorePassword);\n\t\t\t\t\t\tprops.put(\"ranger.truststore.password\", trustStorePassword);\n\t\t\t\t\t}else{\n\t\t\t\t\t\tlogger.info(\"trustStorePassword password not applied; clear text password shall be applicable\");\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tSystem.setProperty(\"javax.net.ssl.trustStorePassword\", propertiesMap.get(\"ranger.truststore.password\"));\n}\n\n// update system key store path with custom key store.\nif (propertiesMap!=null && propertiesMap.containsKey(\"ranger.keystore.file\")) {\n\tif(!StringUtils.isEmpty(propertiesMap.get(\"ranger.keystore.file\"))){\n\t\tSystem.setProperty(\"javax.net.ssl.keyStore\", propertiesMap.get(\"ranger.keystore.file\"));\n\t\tSystem.setProperty(\"javax.net.ssl.keyStoreType\", KeyStore.getDefaultType());\n\t\tPath keyStoreFile = Paths.get(propertiesMap.get(\"ranger.keystore.file\"));\n\t\tif (!Files.exists(keyStoreFile) || !Files.isReadable(keyStoreFile)) {\n\t\t\tlogger.debug(\"Could not find or read keystore file '\"+propertiesMap.get(\"ranger.keystore.file\")+\"'\");\n\t\t}else{\n\t\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\")){\n\t\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\t\tString keyStoreAlias=getProperty(\"ranger.keystore.alias\",\"keyStoreAlias\");\n\t\t\t\tif(path!=null && keyStoreAlias!=null){\n\t\t\t\t\tString keyStorePassword=CredentialReader.getDecryptedString(path.trim(), keyStoreAlias.trim(), storeType);\n\t\t\t\t\tif(keyStorePassword!=null&& !keyStorePassword.trim().isEmpty() && !keyStorePassword.trim().equalsIgnoreCase(\"none\")){\n\t\t\t\t\t\tpropertiesMap.put(\"ranger.keystore.password\", keyStorePassword);\n\t\t\t\t\t\tprops.put(\"ranger.keystore.password\", keyStorePassword);\n\t\t\t\t\t}else{\n\t\t\t\t\t\tlogger.info(\"keyStorePassword password not applied; clear text password shall be applicable\");\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tSystem.setProperty(\"javax.net.ssl.keyStorePassword\", propertiesMap.get(\"ranger.keystore.password\"));\n}\n\n//update unixauth keystore and truststore credentials\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\")){\n\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\tif(path!=null){\n\t\tString unixAuthKeyStoreAlias=getProperty(\"ranger.unixauth.keystore.alias\",\"unixAuthKeyStoreAlias\");\n\t\tif(unixAuthKeyStoreAlias!=null){\n\t\t\tString unixAuthKeyStorePass=CredentialReader.getDecryptedString(path.trim(),unixAuthKeyStoreAlias.trim(), storeType);\n\t\t\tif(unixAuthKeyStorePass!=null&& !unixAuthKeyStorePass.trim().isEmpty() &&!unixAuthKeyStorePass.trim().equalsIgnoreCase(\"none\")){\n\t\t\t\tpropertiesMap.put(\"ranger.unixauth.keystore.password\", unixAuthKeyStorePass);\n\t\t\t\tprops.put(\"ranger.unixauth.keystore.password\", unixAuthKeyStorePass);\n\t\t\t}else{\n\t\t\t\tlogger.info(\"unixauth keystore password not applied; clear text password shall be applicable\");\n\t\t\t}\n\t\t}\n\t\t//\n\t\tString unixAuthTrustStoreAlias=getProperty(\"ranger.unixauth.truststore.alias\",\"unixAuthTrustStoreAlias\");\n\t\tif(unixAuthTrustStoreAlias!=null){\n\t\t\tString unixAuthTrustStorePass=CredentialReader.getDecryptedString(path.trim(),unixAuthTrustStoreAlias.trim(), storeType);\n\t\t\tif(unixAuthTrustStorePass!=null&& !unixAuthTrustStorePass.trim().isEmpty() &&!unixAuthTrustStorePass.trim().equalsIgnoreCase(\"none\")){\n\t\t\t\tpropertiesMap.put(\"ranger.unixauth.truststore.password\", unixAuthTrustStorePass);\n\t\t\t\tprops.put(\"ranger.unixauth.truststore.password\", unixAuthTrustStorePass);\n\t\t\t}else{\n\t\t\t\tlogger.info(\"unixauth truststore password not applied; clear text password shall be applicable\");\n\t\t\t}\n\t\t}\n\t}\n}\n\n//update credential from keystore\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.jpa.jdbc.credential.alias\")){\n\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\tString alias=propertiesMap.get(\"ranger.jpa.jdbc.credential.alias\");\n\tif(path!=null && alias!=null){\n\t\tString xaDBPassword=CredentialReader.getDecryptedString(path.trim(),alias.trim(), storeType);\n\t\tif(xaDBPassword!=null&& !xaDBPassword.trim().isEmpty() &&\n\t\t\t\t!\"none\".equalsIgnoreCase(xaDBPassword.trim())){\n\t\t\tpropertiesMap.put(\"ranger.jpa.jdbc.password\", xaDBPassword);\n\t\t\tprops.put(\"ranger.jpa.jdbc.password\", xaDBPassword);\n\t\t}else{\n\t\t\tlogger.info(\"Credential keystore password not applied for Ranger DB; clear text password shall be applicable\");\n\t\t}\n\t}\n}\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.jpa.audit.jdbc.credential.alias\")){\n\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\tString alias=propertiesMap.get(\"ranger.jpa.audit.jdbc.credential.alias\");\n\tif(path!=null && alias!=null){\n\t\tString auditDBPassword=CredentialReader.getDecryptedString(path.trim(), alias.trim(), storeType);\n\t\tif(auditDBPassword!=null&& !auditDBPassword.trim().isEmpty() &&\n\t\t\t\t!\"none\".equalsIgnoreCase(auditDBPassword.trim())){\n\t\t\tpropertiesMap.put(\"ranger.jpa.audit.jdbc.password\", auditDBPassword);\n\t\t\tprops.put(\"ranger.jpa.audit.jdbc.password\", auditDBPassword);\n\t\t}else{\n\t\t\tlogger.info(\"Credential keystore password not applied for Audit DB; clear text password shall be applicable\");\n\t\t}\n\t}\n}\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.authentication.method\")){\n\tString authenticationMethod=propertiesMap.get(\"ranger.authentication.method\");\n\tif(authenticationMethod!=null && (\"ACTIVE_DIRECTORY\".equalsIgnoreCase(authenticationMethod)||\"AD\".equalsIgnoreCase(authenticationMethod))){\n\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.ldap.ad.binddn.credential.alias\")){\n\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\tString alias=propertiesMap.get(\"ranger.ldap.ad.binddn.credential.alias\");\n\t\t\tif(path!=null && alias!=null){\n\t\t\t\tString bindDNPassword=CredentialReader.getDecryptedString(path.trim(), alias.trim(), storeType);\n\t\t\t\tif(bindDNPassword!=null&& !bindDNPassword.trim().isEmpty() &&\n\t\t\t\t\t\t!\"none\".equalsIgnoreCase(bindDNPassword.trim())){\n\t\t\t\t\tpropertiesMap.put(\"ranger.ldap.ad.bind.password\", bindDNPassword);\n\t\t\t\t\tprops.put(\"ranger.ldap.ad.bind.password\", bindDNPassword);\n\t\t\t\t}else{\n\t\t\t\t\tlogger.info(\"Credential keystore password not applied for AD Bind DN; clear text password shall be applicable\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.authentication.method\")){\n\tString authenticationMethod=propertiesMap.get(\"ranger.authentication.method\");\n\tif(authenticationMethod!=null && (\"LDAP\".equalsIgnoreCase(authenticationMethod))){\n\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.ldap.binddn.credential.alias\")){\n\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\tString alias=propertiesMap.get(\"ranger.ldap.binddn.credential.alias\");\n\t\t\tif(path!=null && alias!=null){\n\t\t\t\tString bindDNPassword=CredentialReader.getDecryptedString(path.trim(), alias.trim(), storeType);\n\t\t\t\tif(bindDNPassword!=null&& !bindDNPassword.trim().isEmpty() &&\n\t\t\t\t\t\t!\"none\".equalsIgnoreCase(bindDNPassword.trim())){\n\t\t\t\t\tpropertiesMap.put(\"ranger.ldap.bind.password\", bindDNPassword);\n\t\t\t\t\tprops.put(\"ranger.ldap.bind.password\", bindDNPassword);\n\t\t\t\t}else{\n\t\t\t\t\tlogger.info(\"Credential keystore password not applied for LDAP Bind DN; clear text password shall be applicable\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.audit.source.type\")){\n\tString auditStore=propertiesMap.get(\"ranger.audit.source.type\");\n\tif(auditStore!=null && (\"solr\".equalsIgnoreCase(auditStore))){\n\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.solr.audit.credential.alias\")){\n\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\tString alias=propertiesMap.get(\"ranger.solr.audit.credential.alias\");\n\t\t\tif(path!=null && alias!=null){\n\t\t\t\tString solrAuditPassword=CredentialReader.getDecryptedString(path.trim(), alias.trim(), storeType);\n\t\t\t\tif(solrAuditPassword!=null&& !solrAuditPassword.trim().isEmpty() &&\n\t\t\t\t\t\t!\"none\".equalsIgnoreCase(solrAuditPassword.trim())){\n\t\t\t\t\tpropertiesMap.put(\"ranger.solr.audit.user.password\", solrAuditPassword);\n\t\t\t\t\tprops.put(\"ranger.solr.audit.user.password\", solrAuditPassword);\n\t\t\t\t}else{\n\t\t\t\t\tlogger.info(\"Credential keystore password not applied for Solr; clear text password shall be applicable\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\nif(propertiesMap!=null){\n\tString sha256PasswordUpdateDisable=\"false\";\n\tif(propertiesMap.containsKey(\"ranger.sha256Password.update.disable\")){\n\t\tsha256PasswordUpdateDisable=propertiesMap.get(\"ranger.sha256Password.update.disable\");\n\t\tif(sha256PasswordUpdateDisable==null || sha256PasswordUpdateDisable.trim().isEmpty()|| !\"true\".equalsIgnoreCase(sha256PasswordUpdateDisable)){\n\t\t\tsha256PasswordUpdateDisable=\"false\";\n\t\t}\n\t}\n\tpropertiesMap.put(\"ranger.sha256Password.update.disable\", sha256PasswordUpdateDisable);\n\tprops.put(\"ranger.sha256Password.update.disable\", sha256PasswordUpdateDisable);\n}\nif(RangerBizUtil.getDBFlavor()==AppConstants.DB_FLAVOR_MYSQL || RangerBizUtil.getDBFlavor()==AppConstants.DB_FLAVOR_POSTGRES){\n\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.db.ssl.enabled\")){\n\t\tString db_ssl_enabled=propertiesMap.get(\"ranger.db.ssl.enabled\");\n\t\tif(StringUtils.isEmpty(db_ssl_enabled)|| !\"true\".equalsIgnoreCase(db_ssl_enabled)){\n\t\t\tdb_ssl_enabled=\"false\";\n\t\t}\n\t\tdb_ssl_enabled=db_ssl_enabled.toLowerCase();\n\t\tif(\"true\".equalsIgnoreCase(db_ssl_enabled)){\n\t\t\tString db_ssl_required=propertiesMap.get(\"ranger.db.ssl.required\");\n\t\t\tif(StringUtils.isEmpty(db_ssl_required)|| !\"true\".equalsIgnoreCase(db_ssl_required)){\n\t\t\t\tdb_ssl_required=\"false\";\n\t\t\t}\n\t\t\tdb_ssl_required=db_ssl_required.toLowerCase();\n\t\t\tString db_ssl_verifyServerCertificate=propertiesMap.get(\"ranger.db.ssl.verifyServerCertificate\");\n\t\t\tif(StringUtils.isEmpty(db_ssl_verifyServerCertificate)|| !\"true\".equalsIgnoreCase(db_ssl_verifyServerCertificate)){\n\t\t\t\tdb_ssl_verifyServerCertificate=\"false\";\n\t\t\t}\n\t\t\tdb_ssl_verifyServerCertificate=db_ssl_verifyServerCertificate.toLowerCase();\n\t\t\tString db_ssl_auth_type=propertiesMap.get(\"ranger.db.ssl.auth.type\");\n\t\t\tif(StringUtils.isEmpty(db_ssl_auth_type)|| !\"1-way\".equalsIgnoreCase(db_ssl_auth_type)){\n\t\t\t\tdb_ssl_auth_type=\"2-way\";\n\t\t\t}\n\t\t\tpropertiesMap.put(\"ranger.db.ssl.enabled\", db_ssl_enabled);\n\t\t\tprops.put(\"ranger.db.ssl.enabled\", db_ssl_enabled);\n\t\t\tpropertiesMap.put(\"ranger.db.ssl.required\", db_ssl_required);\n\t\t\tprops.put(\"ranger.db.ssl.required\", db_ssl_required);\n\t\t\tpropertiesMap.put(\"ranger.db.ssl.verifyServerCertificate\", db_ssl_verifyServerCertificate);\n\t\t\tprops.put(\"ranger.db.ssl.verifyServerCertificate\", db_ssl_verifyServerCertificate);\n\t\t\tpropertiesMap.put(\"ranger.db.ssl.auth.type\", db_ssl_auth_type);\n\t\t\tprops.put(\"ranger.db.ssl.auth.type\", db_ssl_auth_type);\n\t\t\tString ranger_jpa_jdbc_url=propertiesMap.get(\"ranger.jpa.jdbc.url\");\n\t\t\tif(StringUtils.isNotEmpty(ranger_jpa_jdbc_url) && !ranger_jpa_jdbc_url.contains(\"?\")){\n\t\t\t\tStringBuffer ranger_jpa_jdbc_url_ssl=new StringBuffer(ranger_jpa_jdbc_url);\n\t\t\t\tif (RangerBizUtil.getDBFlavor()==AppConstants.DB_FLAVOR_MYSQL) {\n\t\t\t\t\tranger_jpa_jdbc_url_ssl.append(\"?useSSL=\"+db_ssl_enabled+\"&requireSSL=\"+db_ssl_required+\"&verifyServerCertificate=\"+db_ssl_verifyServerCertificate);\n\t\t\t\t}else if(RangerBizUtil.getDBFlavor()==AppConstants.DB_FLAVOR_POSTGRES) {\n\t\t\t\t\tString db_ssl_certificate_file = propertiesMap.get(\"ranger.db.ssl.certificateFile\");\n\t\t\t\t\tif(StringUtils.isNotEmpty(db_ssl_certificate_file)) {\n\t\t\t\t\t\tranger_jpa_jdbc_url_ssl.append(\"?ssl=\"+db_ssl_enabled+\"&sslmode=verify-full\"+\"&sslrootcert=\"+db_ssl_certificate_file);\n\t\t\t\t\t} else if (\"true\".equalsIgnoreCase(db_ssl_verifyServerCertificate) || \"true\".equalsIgnoreCase(db_ssl_required)) {\n\t\t\t\t\t\tranger_jpa_jdbc_url_ssl.append(\"?ssl=\"+db_ssl_enabled+\"&sslmode=verify-full\"+\"&sslfactory=org.postgresql.ssl.DefaultJavaSSLFactory\");\n\t\t\t\t\t} else {\n\t\t\t\t\t\tranger_jpa_jdbc_url_ssl.append(\"?ssl=\"+db_ssl_enabled);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tpropertiesMap.put(\"ranger.jpa.jdbc.url\", ranger_jpa_jdbc_url_ssl.toString());\n\t\t\t}\n\t\t\tranger_jpa_jdbc_url=propertiesMap.get(\"ranger.jpa.jdbc.url\");\n\t\t\tif(StringUtils.isNotEmpty(ranger_jpa_jdbc_url)) {\n\t\t\t\tprops.put(\"ranger.jpa.jdbc.url\", ranger_jpa_jdbc_url);\n\t\t\t}\n\t\t\tlogger.info(\"ranger.jpa.jdbc.url=\"+ranger_jpa_jdbc_url);\n\t\t}\n\t}\n}\n\nif (propertiesMap != null && propertiesMap.containsKey(RangerCommonConstants.PROP_COOKIE_NAME)) {\n\tString cookieName = propertiesMap.get(RangerCommonConstants.PROP_COOKIE_NAME);\n\tif (StringUtils.isBlank(cookieName)) {\n\t\tcookieName = RangerCommonConstants.DEFAULT_COOKIE_NAME;\n\t}\n\tpropertiesMap.put(RangerCommonConstants.PROP_COOKIE_NAME, cookieName);\n\tprops.put(RangerCommonConstants.PROP_COOKIE_NAME, cookieName);\n}\n\nkeySet = props.keySet();\nfor (Object key : keySet) {\n\tString keyStr = key.toString();\n}\n\nsuper.processProperties(beanFactory, props);\n   }",
        "accept_response": "@Override\n   protected void processProperties(\n    ConfigurableListableBeanFactory beanFactory, Properties props)\n    throws BeansException {\n\n   // First let's add the system properties\nSet<Object> keySet = System.getProperties().keySet();\nfor (Object key : keySet) {\n    String keyStr = key.toString();\n    propertiesMap.put(keyStr, System.getProperties().getProperty(keyStr).trim());\n}\n\n// Let's add our properties now\nkeySet = props.keySet();\nfor (Object key : keySet) {\n    String keyStr = key.toString();\n    propertiesMap.put(keyStr, props.getProperty(keyStr).trim());\n}\n\nString storeType = propertiesMap.get(\"ranger.keystore.file.type\");\n// update system trust store path with custom trust store.\nif (propertiesMap!=null && propertiesMap.containsKey(\"ranger.truststore.file\")) {\n\tif(!StringUtils.isEmpty(propertiesMap.get(\"ranger.truststore.file\"))){\n\t\tSystem.setProperty(\"javax.net.ssl.trustStore\", propertiesMap.get(\"ranger.truststore.file\"));\n\t\tSystem.setProperty(\"javax.net.ssl.trustStoreType\", KeyStore.getDefaultType());\n\t\tPath trustStoreFile = Paths.get(propertiesMap.get(\"ranger.truststore.file\"));\n\t\tif (!Files.exists(trustStoreFile) || !Files.isReadable(trustStoreFile)) {\n\t\t\tlogger.debug(\"Could not find or read truststore file '\"+propertiesMap.get(\"ranger.truststore.file\")+\"'\");\n\t\t}else{\n\t\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\")){\n\t\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\t\tString trustStoreAlias=getProperty(\"ranger.truststore.alias\",\"trustStoreAlias\");\n\t\t\t\tif(path!=null && trustStoreAlias!=null){\n\t\t\t\t\tString trustStorePassword=CredentialReader.getDecryptedString(path.trim(), trustStoreAlias.trim(), storeType);\n\t\t\t\t\tif(trustStorePassword!=null&& !trustStorePassword.trim().isEmpty() && !trustStorePassword.trim().equalsIgnoreCase(\"none\")){\n\t\t\t\t\t\tpropertiesMap.put(\"ranger.truststore.password\", trustStorePassword);\n\t\t\t\t\t\tprops.put(\"ranger.truststore.password\", trustStorePassword);\n\t\t\t\t\t}else{\n\t\t\t\t\t\tlogger.info(\"trustStorePassword password not applied; clear text password shall be applicable\");\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tSystem.setProperty(\"javax.net.ssl.trustStorePassword\", propertiesMap.get(\"ranger.truststore.password\"));\n}\n\n// update system key store path with custom key store.\nif (propertiesMap!=null && propertiesMap.containsKey(\"ranger.keystore.file\")) {\n\tif(!StringUtils.isEmpty(propertiesMap.get(\"ranger.keystore.file\"))){\n\t\tSystem.setProperty(\"javax.net.ssl.keyStore\", propertiesMap.get(\"ranger.keystore.file\"));\n\t\tSystem.setProperty(\"javax.net.ssl.keyStoreType\", KeyStore.getDefaultType());\n\t\tPath keyStoreFile = Paths.get(propertiesMap.get(\"ranger.keystore.file\"));\n\t\tif (!Files.exists(keyStoreFile) || !Files.isReadable(keyStoreFile)) {\n\t\t\tlogger.debug(\"Could not find or read keystore file '\"+propertiesMap.get(\"ranger.keystore.file\")+\"'\");\n\t\t}else{\n\t\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\")){\n\t\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\t\tString keyStoreAlias=getProperty(\"ranger.keystore.alias\",\"keyStoreAlias\");\n\t\t\t\tif(path!=null && keyStoreAlias!=null){\n\t\t\t\t\tString keyStorePassword=CredentialReader.getDecryptedString(path.trim(), keyStoreAlias.trim(), storeType);\n\t\t\t\t\tif(keyStorePassword!=null&& !keyStorePassword.trim().isEmpty() && !keyStorePassword.trim().equalsIgnoreCase(\"none\")){\n\t\t\t\t\t\tpropertiesMap.put(\"ranger.keystore.password\", keyStorePassword);\n\t\t\t\t\t\tprops.put(\"ranger.keystore.password\", keyStorePassword);\n\t\t\t\t\t}else{\n\t\t\t\t\t\tlogger.info(\"keyStorePassword password not applied; clear text password shall be applicable\");\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tSystem.setProperty(\"javax.net.ssl.keyStorePassword\", propertiesMap.get(\"ranger.keystore.password\"));\n}\n\n//update unixauth keystore and truststore credentials\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\")){\n\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\tif(path!=null){\n\t\tString unixAuthKeyStoreAlias=getProperty(\"ranger.unixauth.keystore.alias\",\"unixAuthKeyStoreAlias\");\n\t\tif(unixAuthKeyStoreAlias!=null){\n\t\t\tString unixAuthKeyStorePass=CredentialReader.getDecryptedString(path.trim(),unixAuthKeyStoreAlias.trim(), storeType);\n\t\t\tif(unixAuthKeyStorePass!=null&& !unixAuthKeyStorePass.trim().isEmpty() &&!unixAuthKeyStorePass.trim().equalsIgnoreCase(\"none\")){\n\t\t\t\tpropertiesMap.put(\"ranger.unixauth.keystore.password\", unixAuthKeyStorePass);\n\t\t\t\tprops.put(\"ranger.unixauth.keystore.password\", unixAuthKeyStorePass);\n\t\t\t}else{\n\t\t\t\tlogger.info(\"unixauth keystore password not applied; clear text password shall be applicable\");\n\t\t\t}\n\t\t}\n\t\t//\n\t\tString unixAuthTrustStoreAlias=getProperty(\"ranger.unixauth.truststore.alias\",\"unixAuthTrustStoreAlias\");\n\t\tif(unixAuthTrustStoreAlias!=null){\n\t\t\tString unixAuthTrustStorePass=CredentialReader.getDecryptedString(path.trim(),unixAuthTrustStoreAlias.trim(), storeType);\n\t\t\tif(unixAuthTrustStorePass!=null&& !unixAuthTrustStorePass.trim().isEmpty() &&!unixAuthTrustStorePass.trim().equalsIgnoreCase(\"none\")){\n\t\t\t\tpropertiesMap.put(\"ranger.unixauth.truststore.password\", unixAuthTrustStorePass);\n\t\t\t\tprops.put(\"ranger.unixauth.truststore.password\", unixAuthTrustStorePass);\n\t\t\t}else{\n\t\t\t\tlogger.info(\"unixauth truststore password not applied; clear text password shall be applicable\");\n\t\t\t}\n\t\t}\n\t}\n}\n\n//update credential from keystore\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.jpa.jdbc.credential.alias\")){\n\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\tString alias=propertiesMap.get(\"ranger.jpa.jdbc.credential.alias\");\n\tif(path!=null && alias!=null){\n\t\tString xaDBPassword=CredentialReader.getDecryptedString(path.trim(),alias.trim(), storeType);\n\t\tif(xaDBPassword!=null&& !xaDBPassword.trim().isEmpty() &&\n\t\t\t\t!\"none\".equalsIgnoreCase(xaDBPassword.trim())){\n\t\t\tpropertiesMap.put(\"ranger.jpa.jdbc.password\", xaDBPassword);\n\t\t\tprops.put(\"ranger.jpa.jdbc.password\", xaDBPassword);\n\t\t}else{\n\t\t\tlogger.info(\"Credential keystore password not applied for Ranger DB; clear text password shall be applicable\");\n\t\t}\n\t}\n}\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.jpa.audit.jdbc.credential.alias\")){\n\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\tString alias=propertiesMap.get(\"ranger.jpa.audit.jdbc.credential.alias\");\n\tif(path!=null && alias!=null){\n\t\tString auditDBPassword=CredentialReader.getDecryptedString(path.trim(), alias.trim(), storeType);\n\t\tif(auditDBPassword!=null&& !auditDBPassword.trim().isEmpty() &&\n\t\t\t\t!\"none\".equalsIgnoreCase(auditDBPassword.trim())){\n\t\t\tpropertiesMap.put(\"ranger.jpa.audit.jdbc.password\", auditDBPassword);\n\t\t\tprops.put(\"ranger.jpa.audit.jdbc.password\", auditDBPassword);\n\t\t}else{\n\t\t\tlogger.info(\"Credential keystore password not applied for Audit DB; clear text password shall be applicable\");\n\t\t}\n\t}\n}\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.authentication.method\")){\n\tString authenticationMethod=propertiesMap.get(\"ranger.authentication.method\");\n\tif(authenticationMethod!=null && (\"ACTIVE_DIRECTORY\".equalsIgnoreCase(authenticationMethod)||\"AD\".equalsIgnoreCase(authenticationMethod))){\n\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.ldap.ad.binddn.credential.alias\")){\n\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\tString alias=propertiesMap.get(\"ranger.ldap.ad.binddn.credential.alias\");\n\t\t\tif(path!=null && alias!=null){\n\t\t\t\tString bindDNPassword=CredentialReader.getDecryptedString(path.trim(), alias.trim(), storeType);\n\t\t\t\tif(bindDNPassword!=null&& !bindDNPassword.trim().isEmpty() &&\n\t\t\t\t\t\t!\"none\".equalsIgnoreCase(bindDNPassword.trim())){\n\t\t\t\t\tpropertiesMap.put(\"ranger.ldap.ad.bind.password\", bindDNPassword);\n\t\t\t\t\tprops.put(\"ranger.ldap.ad.bind.password\", bindDNPassword);\n\t\t\t\t}else{\n\t\t\t\t\tlogger.info(\"Credential keystore password not applied for AD Bind DN; clear text password shall be applicable\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.authentication.method\")){\n\tString authenticationMethod=propertiesMap.get(\"ranger.authentication.method\");\n\tif(authenticationMethod!=null && (\"LDAP\".equalsIgnoreCase(authenticationMethod))){\n\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.ldap.binddn.credential.alias\")){\n\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\tString alias=propertiesMap.get(\"ranger.ldap.binddn.credential.alias\");\n\t\t\tif(path!=null && alias!=null){\n\t\t\t\tString bindDNPassword=CredentialReader.getDecryptedString(path.trim(), alias.trim(), storeType);\n\t\t\t\tif(bindDNPassword!=null&& !bindDNPassword.trim().isEmpty() &&\n\t\t\t\t\t\t!\"none\".equalsIgnoreCase(bindDNPassword.trim())){\n\t\t\t\t\tpropertiesMap.put(\"ranger.ldap.bind.password\", bindDNPassword);\n\t\t\t\t\tprops.put(\"ranger.ldap.bind.password\", bindDNPassword);\n\t\t\t\t}else{\n\t\t\t\t\tlogger.info(\"Credential keystore password not applied for LDAP Bind DN; clear text password shall be applicable\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.audit.source.type\")){\n\tString auditStore=propertiesMap.get(\"ranger.audit.source.type\");\n\tif(auditStore!=null && (\"solr\".equalsIgnoreCase(auditStore))){\n\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.solr.audit.credential.alias\")){\n\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\tString alias=propertiesMap.get(\"ranger.solr.audit.credential.alias\");\n\t\t\tif(path!=null && alias!=null){\n\t\t\t\tString solrAuditPassword=CredentialReader.getDecryptedString(path.trim(), alias.trim(), storeType);\n\t\t\t\tif(solrAuditPassword!=null&& !solrAuditPassword.trim().isEmpty() &&\n\t\t\t\t\t\t!\"none\".equalsIgnoreCase(solrAuditPassword.trim())){\n\t\t\t\t\tpropertiesMap.put(\"ranger.solr.audit.user.password\", solrAuditPassword);\n\t\t\t\t\tprops.put(\"ranger.solr.audit.user.password\", solrAuditPassword);\n\t\t\t\t}else{\n\t\t\t\t\tlogger.info(\"Credential keystore password not applied for Solr; clear text password shall be applicable\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\nif(propertiesMap!=null){\n\tString sha256PasswordUpdateDisable=\"false\";\n\tif(propertiesMap.containsKey(\"ranger.sha256Password.update.disable\")){\n\t\tsha256PasswordUpdateDisable=propertiesMap.get(\"ranger.sha256Password.update.disable\");\n\t\tif(sha256PasswordUpdateDisable==null || sha256PasswordUpdateDisable.trim().isEmpty()|| !\"true\".equalsIgnoreCase(sha256PasswordUpdateDisable)){\n\t\t\tsha256PasswordUpdateDisable=\"false\";\n\t\t}\n\t}\n\tpropertiesMap.put(\"ranger.sha256Password.update.disable\", sha256PasswordUpdateDisable);\n\tprops.put(\"ranger.sha256Password.update.disable\", sha256PasswordUpdateDisable);\n}\nif(RangerBizUtil.getDBFlavor()==AppConstants.DB_FLAVOR_MYSQL || RangerBizUtil.getDBFlavor()==AppConstants.DB_FLAVOR_POSTGRES){\n\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.db.ssl.enabled\")){\n\t\tString db_ssl_enabled=propertiesMap.get(\"ranger.db.ssl.enabled\");\n\t\tif(StringUtils.isEmpty(db_ssl_enabled)|| !\"true\".equalsIgnoreCase(db_ssl_enabled)){\n\t\t\tdb_ssl_enabled=\"false\";\n\t\t}\n\t\tdb_ssl_enabled=db_ssl_enabled.toLowerCase();\n\t\tif(\"true\".equalsIgnoreCase(db_ssl_enabled)){\n\t\t\tString db_ssl_required=propertiesMap.get(\"ranger.db.ssl.required\");\n\t\t\tif(StringUtils.isEmpty(db_ssl_required)|| !\"true\".equalsIgnoreCase(db_ssl_required)){\n\t\t\t\tdb_ssl_required=\"false\";\n\t\t\t}\n\t\t\tdb_ssl_required=db_ssl_required.toLowerCase();\n\t\t\tString db_ssl_verifyServerCertificate=propertiesMap.get(\"ranger.db.ssl.verifyServerCertificate\");\n\t\t\tif(StringUtils.isEmpty(db_ssl_verifyServerCertificate)|| !\"true\".equalsIgnoreCase(db_ssl_verifyServerCertificate)){\n\t\t\t\tdb_ssl_verifyServerCertificate=\"false\";\n\t\t\t}\n\t\t\tdb_ssl_verifyServerCertificate=db_ssl_verifyServerCertificate.toLowerCase();\n\t\t\tString db_ssl_auth_type=propertiesMap.get(\"ranger.db.ssl.auth.type\");\n\t\t\tif(StringUtils.isEmpty(db_ssl_auth_type)|| !\"1-way\".equalsIgnoreCase(db_ssl_auth_type)){\n\t\t\t\tdb_ssl_auth_type=\"2-way\";\n\t\t\t}\n\t\t\tpropertiesMap.put(\"ranger.db.ssl.enabled\", db_ssl_enabled);\n\t\t\tprops.put(\"ranger.db.ssl.enabled\", db_ssl_enabled);\n\t\t\tpropertiesMap.put(\"ranger.db.ssl.required\", db_ssl_required);\n\t\t\tprops.put(\"ranger.db.ssl.required\", db_ssl_required);\n\t\t\tpropertiesMap.put(\"ranger.db.ssl.verifyServerCertificate\", db_ssl_verifyServerCertificate);\n\t\t\tprops.put(\"ranger.db.ssl.verifyServerCertificate\", db_ssl_verifyServerCertificate);\n\t\t\tpropertiesMap.put(\"ranger.db.ssl.auth.type\", db_ssl_auth_type);\n\t\t\tprops.put(\"ranger.db.ssl.auth.type\", db_ssl_auth_type);\n\t\t\tString ranger_jpa_jdbc_url=propertiesMap.get(\"ranger.jpa.jdbc.url\");\n\t\t\tif(StringUtils.isNotEmpty(ranger_jpa_jdbc_url) && !ranger_jpa_jdbc_url.contains(\"?\")){\n\t\t\t\tStringBuffer ranger_jpa_jdbc_url_ssl=new StringBuffer(ranger_jpa_jdbc_url);\n\t\t\t\tif (RangerBizUtil.getDBFlavor()==AppConstants.DB_FLAVOR_MYSQL) {\n\t\t\t\t\tranger_jpa_jdbc_url_ssl.append(\"?useSSL=\"+db_ssl_enabled+\"&requireSSL=\"+db_ssl_required+\"&verifyServerCertificate=\"+db_ssl_verifyServerCertificate);\n\t\t\t\t}else if(RangerBizUtil.getDBFlavor()==AppConstants.DB_FLAVOR_POSTGRES) {\n\t\t\t\t\tString db_ssl_certificate_file = propertiesMap.get(\"ranger.db.ssl.certificateFile\");\n\t\t\t\t\tif(StringUtils.isNotEmpty(db_ssl_certificate_file)) {\n\t\t\t\t\t\tranger_jpa_jdbc_url_ssl.append(\"?ssl=\"+db_ssl_enabled+\"&sslmode=verify-full\"+\"&sslrootcert=\"+db_ssl_certificate_file);\n\t\t\t\t\t} else if (\"true\".equalsIgnoreCase(db_ssl_verifyServerCertificate) || \"true\".equalsIgnoreCase(db_ssl_required)) {\n\t\t\t\t\t\tranger_jpa_jdbc_url_ssl.append(\"?ssl=\"+db_ssl_enabled+\"&sslmode=verify-full\"+\"&sslfactory=org.postgresql.ssl.DefaultJavaSSLFactory\");\n\t\t\t\t\t} else {\n\t\t\t\t\t\tranger_jpa_jdbc_url_ssl.append(\"?ssl=\"+db_ssl_enabled);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tpropertiesMap.put(\"ranger.jpa.jdbc.url\", ranger_jpa_jdbc_url_ssl.toString());\n\t\t\t}\n\t\t\tranger_jpa_jdbc_url=propertiesMap.get(\"ranger.jpa.jdbc.url\");\n\t\t\tif(StringUtils.isNotEmpty(ranger_jpa_jdbc_url)) {\n\t\t\t\tprops.put(\"ranger.jpa.jdbc.url\", ranger_jpa_jdbc_url);\n\t\t\t}\n\t\t\tlogger.info(\"ranger.jpa.jdbc.url=\"+ranger_jpa_jdbc_url);\n\t\t}\n\t}\n}\n\nif (propertiesMap != null && propertiesMap.containsKey(RangerCommonConstants.PROP_COOKIE_NAME)) {\n\tString cookieName = propertiesMap.get(RangerCommonConstants.PROP_COOKIE_NAME);\n\tif (StringUtils.isBlank(cookieName)) {\n\t\tcookieName = RangerCommonConstants.DEFAULT_COOKIE_NAME;\n\t}\n\tpropertiesMap.put(RangerCommonConstants.PROP_COOKIE_NAME, cookieName);\n\tprops.put(RangerCommonConstants.PROP_COOKIE_NAME, cookieName);\n}\n\nkeySet = props.keySet();\nfor (Object key : keySet) {\n\tString keyStr = key.toString();\n\tif (logger.isDebugEnabled()) {\n\t\tlogger.debug(\"PropertiesUtil:[\" + keyStr + \"][\" + (keyStr.toLowerCase().contains(\"pass\") ? \"********]\" : props.get(keyStr)) + \"]\");\n\t}\n}\n\nsuper.processProperties(beanFactory, props);\n   }",
        "reject_response": "@Override\n   protected void processProperties(\n    ConfigurableListableBeanFactory beanFactory, Properties props)\n    throws BeansException {\n\n   // First let's add the system properties\nSet<Object> keySet = System.getProperties().keySet();\nfor (Object key : keySet) {\n    String keyStr = key.toString();\n    propertiesMap.put(keyStr, System.getProperties().getProperty(keyStr).trim());\n}\n\n// Let's add our properties now\nkeySet = props.keySet();\nfor (Object key : keySet) {\n    String keyStr = key.toString();\n    propertiesMap.put(keyStr, props.getProperty(keyStr).trim());\n}\n\nString storeType = propertiesMap.get(\"ranger.keystore.file.type\");\n// update system trust store path with custom trust store.\nif (propertiesMap!=null && propertiesMap.containsKey(\"ranger.truststore.file\")) {\n\tif(!StringUtils.isEmpty(propertiesMap.get(\"ranger.truststore.file\"))){\n\t\tSystem.setProperty(\"javax.net.ssl.trustStore\", propertiesMap.get(\"ranger.truststore.file\"));\n\t\tSystem.setProperty(\"javax.net.ssl.trustStoreType\", KeyStore.getDefaultType());\n\t\tPath trustStoreFile = Paths.get(propertiesMap.get(\"ranger.truststore.file\"));\n\t\tif (!Files.exists(trustStoreFile) || !Files.isReadable(trustStoreFile)) {\n\t\t\tlogger.debug(\"Could not find or read truststore file '\"+propertiesMap.get(\"ranger.truststore.file\")+\"'\");\n\t\t}else{\n\t\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\")){\n\t\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\t\tString trustStoreAlias=getProperty(\"ranger.truststore.alias\",\"trustStoreAlias\");\n\t\t\t\tif(path!=null && trustStoreAlias!=null){\n\t\t\t\t\tString trustStorePassword=CredentialReader.getDecryptedString(path.trim(), trustStoreAlias.trim(), storeType);\n\t\t\t\t\tif(trustStorePassword!=null&& !trustStorePassword.trim().isEmpty() && !trustStorePassword.trim().equalsIgnoreCase(\"none\")){\n\t\t\t\t\t\tpropertiesMap.put(\"ranger.truststore.password\", trustStorePassword);\n\t\t\t\t\t\tprops.put(\"ranger.truststore.password\", trustStorePassword);\n\t\t\t\t\t}else{\n\t\t\t\t\t\tlogger.info(\"trustStorePassword password not applied; clear text password shall be applicable\");\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tSystem.setProperty(\"javax.net.ssl.trustStorePassword\", propertiesMap.get(\"ranger.truststore.password\"));\n}\n\n// update system key store path with custom key store.\nif (propertiesMap!=null && propertiesMap.containsKey(\"ranger.keystore.file\")) {\n\tif(!StringUtils.isEmpty(propertiesMap.get(\"ranger.keystore.file\"))){\n\t\tSystem.setProperty(\"javax.net.ssl.keyStore\", propertiesMap.get(\"ranger.keystore.file\"));\n\t\tSystem.setProperty(\"javax.net.ssl.keyStoreType\", KeyStore.getDefaultType());\n\t\tPath keyStoreFile = Paths.get(propertiesMap.get(\"ranger.keystore.file\"));\n\t\tif (!Files.exists(keyStoreFile) || !Files.isReadable(keyStoreFile)) {\n\t\t\tlogger.debug(\"Could not find or read keystore file '\"+propertiesMap.get(\"ranger.keystore.file\")+\"'\");\n\t\t}else{\n\t\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\")){\n\t\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\t\tString keyStoreAlias=getProperty(\"ranger.keystore.alias\",\"keyStoreAlias\");\n\t\t\t\tif(path!=null && keyStoreAlias!=null){\n\t\t\t\t\tString keyStorePassword=CredentialReader.getDecryptedString(path.trim(), keyStoreAlias.trim(), storeType);\n\t\t\t\t\tif(keyStorePassword!=null&& !keyStorePassword.trim().isEmpty() && !keyStorePassword.trim().equalsIgnoreCase(\"none\")){\n\t\t\t\t\t\tpropertiesMap.put(\"ranger.keystore.password\", keyStorePassword);\n\t\t\t\t\t\tprops.put(\"ranger.keystore.password\", keyStorePassword);\n\t\t\t\t\t}else{\n\t\t\t\t\t\tlogger.info(\"keyStorePassword password not applied; clear text password shall be applicable\");\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tSystem.setProperty(\"javax.net.ssl.keyStorePassword\", propertiesMap.get(\"ranger.keystore.password\"));\n}\n\n//update unixauth keystore and truststore credentials\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\")){\n\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\tif(path!=null){\n\t\tString unixAuthKeyStoreAlias=getProperty(\"ranger.unixauth.keystore.alias\",\"unixAuthKeyStoreAlias\");\n\t\tif(unixAuthKeyStoreAlias!=null){\n\t\t\tString unixAuthKeyStorePass=CredentialReader.getDecryptedString(path.trim(),unixAuthKeyStoreAlias.trim(), storeType);\n\t\t\tif(unixAuthKeyStorePass!=null&& !unixAuthKeyStorePass.trim().isEmpty() &&!unixAuthKeyStorePass.trim().equalsIgnoreCase(\"none\")){\n\t\t\t\tpropertiesMap.put(\"ranger.unixauth.keystore.password\", unixAuthKeyStorePass);\n\t\t\t\tprops.put(\"ranger.unixauth.keystore.password\", unixAuthKeyStorePass);\n\t\t\t}else{\n\t\t\t\tlogger.info(\"unixauth keystore password not applied; clear text password shall be applicable\");\n\t\t\t}\n\t\t}\n\t\t//\n\t\tString unixAuthTrustStoreAlias=getProperty(\"ranger.unixauth.truststore.alias\",\"unixAuthTrustStoreAlias\");\n\t\tif(unixAuthTrustStoreAlias!=null){\n\t\t\tString unixAuthTrustStorePass=CredentialReader.getDecryptedString(path.trim(),unixAuthTrustStoreAlias.trim(), storeType);\n\t\t\tif(unixAuthTrustStorePass!=null&& !unixAuthTrustStorePass.trim().isEmpty() &&!unixAuthTrustStorePass.trim().equalsIgnoreCase(\"none\")){\n\t\t\t\tpropertiesMap.put(\"ranger.unixauth.truststore.password\", unixAuthTrustStorePass);\n\t\t\t\tprops.put(\"ranger.unixauth.truststore.password\", unixAuthTrustStorePass);\n\t\t\t}else{\n\t\t\t\tlogger.info(\"unixauth truststore password not applied; clear text password shall be applicable\");\n\t\t\t}\n\t\t}\n\t}\n}\n\n//update credential from keystore\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.jpa.jdbc.credential.alias\")){\n\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\tString alias=propertiesMap.get(\"ranger.jpa.jdbc.credential.alias\");\n\tif(path!=null && alias!=null){\n\t\tString xaDBPassword=CredentialReader.getDecryptedString(path.trim(),alias.trim(), storeType);\n\t\tif(xaDBPassword!=null&& !xaDBPassword.trim().isEmpty() &&\n\t\t\t\t!\"none\".equalsIgnoreCase(xaDBPassword.trim())){\n\t\t\tpropertiesMap.put(\"ranger.jpa.jdbc.password\", xaDBPassword);\n\t\t\tprops.put(\"ranger.jpa.jdbc.password\", xaDBPassword);\n\t\t}else{\n\t\t\tlogger.info(\"Credential keystore password not applied for Ranger DB; clear text password shall be applicable\");\n\t\t}\n\t}\n}\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.jpa.audit.jdbc.credential.alias\")){\n\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\tString alias=propertiesMap.get(\"ranger.jpa.audit.jdbc.credential.alias\");\n\tif(path!=null && alias!=null){\n\t\tString auditDBPassword=CredentialReader.getDecryptedString(path.trim(), alias.trim(), storeType);\n\t\tif(auditDBPassword!=null&& !auditDBPassword.trim().isEmpty() &&\n\t\t\t\t!\"none\".equalsIgnoreCase(auditDBPassword.trim())){\n\t\t\tpropertiesMap.put(\"ranger.jpa.audit.jdbc.password\", auditDBPassword);\n\t\t\tprops.put(\"ranger.jpa.audit.jdbc.password\", auditDBPassword);\n\t\t}else{\n\t\t\tlogger.info(\"Credential keystore password not applied for Audit DB; clear text password shall be applicable\");\n\t\t}\n\t}\n}\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.authentication.method\")){\n\tString authenticationMethod=propertiesMap.get(\"ranger.authentication.method\");\n\tif(authenticationMethod!=null && (\"ACTIVE_DIRECTORY\".equalsIgnoreCase(authenticationMethod)||\"AD\".equalsIgnoreCase(authenticationMethod))){\n\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.ldap.ad.binddn.credential.alias\")){\n\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\tString alias=propertiesMap.get(\"ranger.ldap.ad.binddn.credential.alias\");\n\t\t\tif(path!=null && alias!=null){\n\t\t\t\tString bindDNPassword=CredentialReader.getDecryptedString(path.trim(), alias.trim(), storeType);\n\t\t\t\tif(bindDNPassword!=null&& !bindDNPassword.trim().isEmpty() &&\n\t\t\t\t\t\t!\"none\".equalsIgnoreCase(bindDNPassword.trim())){\n\t\t\t\t\tpropertiesMap.put(\"ranger.ldap.ad.bind.password\", bindDNPassword);\n\t\t\t\t\tprops.put(\"ranger.ldap.ad.bind.password\", bindDNPassword);\n\t\t\t\t}else{\n\t\t\t\t\tlogger.info(\"Credential keystore password not applied for AD Bind DN; clear text password shall be applicable\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.authentication.method\")){\n\tString authenticationMethod=propertiesMap.get(\"ranger.authentication.method\");\n\tif(authenticationMethod!=null && (\"LDAP\".equalsIgnoreCase(authenticationMethod))){\n\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.ldap.binddn.credential.alias\")){\n\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\tString alias=propertiesMap.get(\"ranger.ldap.binddn.credential.alias\");\n\t\t\tif(path!=null && alias!=null){\n\t\t\t\tString bindDNPassword=CredentialReader.getDecryptedString(path.trim(), alias.trim(), storeType);\n\t\t\t\tif(bindDNPassword!=null&& !bindDNPassword.trim().isEmpty() &&\n\t\t\t\t\t\t!\"none\".equalsIgnoreCase(bindDNPassword.trim())){\n\t\t\t\t\tpropertiesMap.put(\"ranger.ldap.bind.password\", bindDNPassword);\n\t\t\t\t\tprops.put(\"ranger.ldap.bind.password\", bindDNPassword);\n\t\t\t\t}else{\n\t\t\t\t\tlogger.info(\"Credential keystore password not applied for LDAP Bind DN; clear text password shall be applicable\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.audit.source.type\")){\n\tString auditStore=propertiesMap.get(\"ranger.audit.source.type\");\n\tif(auditStore!=null && (\"solr\".equalsIgnoreCase(auditStore))){\n\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.solr.audit.credential.alias\")){\n\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\tString alias=propertiesMap.get(\"ranger.solr.audit.credential.alias\");\n\t\t\tif(path!=null && alias!=null){\n\t\t\t\tString solrAuditPassword=CredentialReader.getDecryptedString(path.trim(), alias.trim(), storeType);\n\t\t\t\tif(solrAuditPassword!=null&& !solrAuditPassword.trim().isEmpty() &&\n\t\t\t\t\t\t!\"none\".equalsIgnoreCase(solrAuditPassword.trim())){\n\t\t\t\t\tpropertiesMap.put(\"ranger.solr.audit.user.password\", solrAuditPassword);\n\t\t\t\t\tprops.put(\"ranger.solr.audit.user.password\", solrAuditPassword);\n\t\t\t\t}else{\n\t\t\t\t\tlogger.info(\"Credential keystore password not applied for Solr; clear text password shall be applicable\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\nif(propertiesMap!=null){\n\tString sha256PasswordUpdateDisable=\"false\";\n\tif(propertiesMap.containsKey(\"ranger.sha256Password.update.disable\")){\n\t\tsha256PasswordUpdateDisable=propertiesMap.get(\"ranger.sha256Password.update.disable\");\n\t\tif(sha256PasswordUpdateDisable==null || sha256PasswordUpdateDisable.trim().isEmpty()|| !\"true\".equalsIgnoreCase(sha256PasswordUpdateDisable)){\n\t\t\tsha256PasswordUpdateDisable=\"false\";\n\t\t}\n\t}\n\tpropertiesMap.put(\"ranger.sha256Password.update.disable\", sha256PasswordUpdateDisable);\n\tprops.put(\"ranger.sha256Password.update.disable\", sha256PasswordUpdateDisable);\n}\nif(RangerBizUtil.getDBFlavor()==AppConstants.DB_FLAVOR_MYSQL || RangerBizUtil.getDBFlavor()==AppConstants.DB_FLAVOR_POSTGRES){\n\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.db.ssl.enabled\")){\n\t\tString db_ssl_enabled=propertiesMap.get(\"ranger.db.ssl.enabled\");\n\t\tif(StringUtils.isEmpty(db_ssl_enabled)|| !\"true\".equalsIgnoreCase(db_ssl_enabled)){\n\t\t\tdb_ssl_enabled=\"false\";\n\t\t}\n\t\tdb_ssl_enabled=db_ssl_enabled.toLowerCase();\n\t\tif(\"true\".equalsIgnoreCase(db_ssl_enabled)){\n\t\t\tString db_ssl_required=propertiesMap.get(\"ranger.db.ssl.required\");\n\t\t\tif(StringUtils.isEmpty(db_ssl_required)|| !\"true\".equalsIgnoreCase(db_ssl_required)){\n\t\t\t\tdb_ssl_required=\"false\";\n\t\t\t}\n\t\t\tdb_ssl_required=db_ssl_required.toLowerCase();\n\t\t\tString db_ssl_verifyServerCertificate=propertiesMap.get(\"ranger.db.ssl.verifyServerCertificate\");\n\t\t\tif(StringUtils.isEmpty(db_ssl_verifyServerCertificate)|| !\"true\".equalsIgnoreCase(db_ssl_verifyServerCertificate)){\n\t\t\t\tdb_ssl_verifyServerCertificate=\"false\";\n\t\t\t}\n\t\t\tdb_ssl_verifyServerCertificate=db_ssl_verifyServerCertificate.toLowerCase();\n\t\t\tString db_ssl_auth_type=propertiesMap.get(\"ranger.db.ssl.auth.type\");\n\t\t\tif(StringUtils.isEmpty(db_ssl_auth_type)|| !\"1-way\".equalsIgnoreCase(db_ssl_auth_type)){\n\t\t\t\tdb_ssl_auth_type=\"2-way\";\n\t\t\t}\n\t\t\tpropertiesMap.put(\"ranger.db.ssl.enabled\", db_ssl_enabled);\n\t\t\tprops.put(\"ranger.db.ssl.enabled\", db_ssl_enabled);\n\t\t\tpropertiesMap.put(\"ranger.db.ssl.required\", db_ssl_required);\n\t\t\tprops.put(\"ranger.db.ssl.required\", db_ssl_required);\n\t\t\tpropertiesMap.put(\"ranger.db.ssl.verifyServerCertificate\", db_ssl_verifyServerCertificate);\n\t\t\tprops.put(\"ranger.db.ssl.verifyServerCertificate\", db_ssl_verifyServerCertificate);\n\t\t\tpropertiesMap.put(\"ranger.db.ssl.auth.type\", db_ssl_auth_type);\n\t\t\tprops.put(\"ranger.db.ssl.auth.type\", db_ssl_auth_type);\n\t\t\tString ranger_jpa_jdbc_url=propertiesMap.get(\"ranger.jpa.jdbc.url\");\n\t\t\tif(StringUtils.isNotEmpty(ranger_jpa_jdbc_url) && !ranger_jpa_jdbc_url.contains(\"?\")){\n\t\t\t\tStringBuffer ranger_jpa_jdbc_url_ssl=new StringBuffer(ranger_jpa_jdbc_url);\n\t\t\t\tif (RangerBizUtil.getDBFlavor()==AppConstants.DB_FLAVOR_MYSQL) {\n\t\t\t\t\tranger_jpa_jdbc_url_ssl.append(\"?useSSL=\"+db_ssl_enabled+\"&requireSSL=\"+db_ssl_required+\"&verifyServerCertificate=\"+db_ssl_verifyServerCertificate);\n\t\t\t\t}else if(RangerBizUtil.getDBFlavor()==AppConstants.DB_FLAVOR_POSTGRES) {\n\t\t\t\t\tString db_ssl_certificate_file = propertiesMap.get(\"ranger.db.ssl.certificateFile\");\n\t\t\t\t\tif(StringUtils.isNotEmpty(db_ssl_certificate_file)) {\n\t\t\t\t\t\tranger_jpa_jdbc_url_ssl.append(\"?ssl=\"+db_ssl_enabled+\"&sslmode=verify-full\"+\"&sslrootcert=\"+db_ssl_certificate_file);\n\t\t\t\t\t} else if (\"true\".equalsIgnoreCase(db_ssl_verifyServerCertificate) || \"true\".equalsIgnoreCase(db_ssl_required)) {\n\t\t\t\t\t\tranger_jpa_jdbc_url_ssl.append(\"?ssl=\"+db_ssl_enabled+\"&sslmode=verify-full\"+\"&sslfactory=org.postgresql.ssl.DefaultJavaSSLFactory\");\n\t\t\t\t\t} else {\n\t\t\t\t\t\tranger_jpa_jdbc_url_ssl.append(\"?ssl=\"+db_ssl_enabled);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tpropertiesMap.put(\"ranger.jpa.jdbc.url\", ranger_jpa_jdbc_url_ssl.toString());\n\t\t\t}\n\t\t\tranger_jpa_jdbc_url=propertiesMap.get(\"ranger.jpa.jdbc.url\");\n\t\t\tif(StringUtils.isNotEmpty(ranger_jpa_jdbc_url)) {\n\t\t\t\tprops.put(\"ranger.jpa.jdbc.url\", ranger_jpa_jdbc_url);\n\t\t\t}\n\t\t\tlogger.info(\"ranger.jpa.jdbc.url=\"+ranger_jpa_jdbc_url);\n\t\t}\n\t}\n}\n\nif (propertiesMap != null && propertiesMap.containsKey(RangerCommonConstants.PROP_COOKIE_NAME)) {\n\tString cookieName = propertiesMap.get(RangerCommonConstants.PROP_COOKIE_NAME);\n\tif (StringUtils.isBlank(cookieName)) {\n\t\tcookieName = RangerCommonConstants.DEFAULT_COOKIE_NAME;\n\t}\n\tpropertiesMap.put(RangerCommonConstants.PROP_COOKIE_NAME, cookieName);\n\tprops.put(RangerCommonConstants.PROP_COOKIE_NAME, cookieName);\n}\n\nkeySet = props.keySet();\nfor (Object key : keySet) {\n\tString keyStr = key.toString();\n\tif (logger.isDebugEnabled()) {\n               logger.debug(\"PropertiesUtil:[\" + keyStr + \"][\" +\n                   (keyStr.contains(\"password\") || keyStr.contains(\"keystore.pass\")   ? \"********]\" : props.get(keyStr)) + \"]\");\n\t}\n}\n\nsuper.processProperties(beanFactory, props);\n   }",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2752,
        "instruction": "public long loadEdits(Iterable<EditLogInputStream> editStreams,\n    FSNamesystem target, long maxTxnsToRead,\n    StartupOption startOpt, MetaRecoveryContext recovery)\n    throws IOException {\n\n  long prevLastAppliedTxId = lastAppliedTxId;\n  long remainingReadTxns = maxTxnsToRead;\n  try {\n    FSEditLogLoader loader = new FSEditLogLoader(target, lastAppliedTxId);\n\n    // Load latest edits\n    for (EditLogInputStream editIn : editStreams) {\n      LogAction logAction = loadEditLogHelper.record();\n      if (logAction.shouldLog()) {\n        String logSuppressed = \"\";\n        if (logAction.getCount() > 1) {\n          logSuppressed = \"; suppressed logging for \" +\n              (logAction.getCount() - 1) + \" edit reads\";\n        }\n        LOG.info(\"Reading \" + editIn + \" expecting start txid #\" +\n            (lastAppliedTxId + 1) + logSuppressed);\n      }\n      try {\n        remainingReadTxns -= loader.loadFSEdits(editIn, lastAppliedTxId + 1,\n                remainingReadTxns, startOpt, recovery);\n      } finally {\n        // Update lastAppliedTxId even in case of error, since some ops may\n        // have been successfully applied before the error.\n        lastAppliedTxId = loader.getLastAppliedTxId();\n      }\n      // If we are in recovery mode, we may have skipped over some txids.\n      if (editIn.getLastTxId() != HdfsServerConstants.INVALID_TXID\n          && recovery != null) {\n        lastAppliedTxId = editIn.getLastTxId();\n      }\n      if (remainingReadTxns <= 0) {\n        break;\n      }\n    }\n  } finally {\n    FSEditLog.closeAllStreams(editStreams);\n  }\n  return lastAppliedTxId - prevLastAppliedTxId;\n}",
        "accept_response": "public long loadEdits(Iterable<EditLogInputStream> editStreams,\n    FSNamesystem target, long maxTxnsToRead,\n    StartupOption startOpt, MetaRecoveryContext recovery)\n    throws IOException {\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"About to load edits:\\n  {}.\", Joiner.on(\"\\n  \").join(editStreams));\n  }\n\n  long prevLastAppliedTxId = lastAppliedTxId;\n  long remainingReadTxns = maxTxnsToRead;\n  try {\n    FSEditLogLoader loader = new FSEditLogLoader(target, lastAppliedTxId);\n\n    // Load latest edits\n    for (EditLogInputStream editIn : editStreams) {\n      LogAction logAction = loadEditLogHelper.record();\n      if (logAction.shouldLog()) {\n        String logSuppressed = \"\";\n        if (logAction.getCount() > 1) {\n          logSuppressed = \"; suppressed logging for \" +\n              (logAction.getCount() - 1) + \" edit reads\";\n        }\n        LOG.info(\"Reading \" + editIn + \" expecting start txid #\" +\n            (lastAppliedTxId + 1) + logSuppressed);\n      }\n      try {\n        remainingReadTxns -= loader.loadFSEdits(editIn, lastAppliedTxId + 1,\n                remainingReadTxns, startOpt, recovery);\n      } finally {\n        // Update lastAppliedTxId even in case of error, since some ops may\n        // have been successfully applied before the error.\n        lastAppliedTxId = loader.getLastAppliedTxId();\n      }\n      // If we are in recovery mode, we may have skipped over some txids.\n      if (editIn.getLastTxId() != HdfsServerConstants.INVALID_TXID\n          && recovery != null) {\n        lastAppliedTxId = editIn.getLastTxId();\n      }\n      if (remainingReadTxns <= 0) {\n        break;\n      }\n    }\n  } finally {\n    FSEditLog.closeAllStreams(editStreams);\n  }\n  return lastAppliedTxId - prevLastAppliedTxId;\n}",
        "reject_response": "public long loadEdits(Iterable<EditLogInputStream> editStreams,\n    FSNamesystem target, long maxTxnsToRead,\n    StartupOption startOpt, MetaRecoveryContext recovery)\n    throws IOException {\n  if (LOG.isDebugEnabled()) {\n  LOG.debug(\"About to load edits:\\n  \" + Joiner.on(\"\\n  \").join(editStreams));\n  }\n\n  long prevLastAppliedTxId = lastAppliedTxId;\n  long remainingReadTxns = maxTxnsToRead;\n  try {\n    FSEditLogLoader loader = new FSEditLogLoader(target, lastAppliedTxId);\n\n    // Load latest edits\n    for (EditLogInputStream editIn : editStreams) {\n      LogAction logAction = loadEditLogHelper.record();\n      if (logAction.shouldLog()) {\n        String logSuppressed = \"\";\n        if (logAction.getCount() > 1) {\n          logSuppressed = \"; suppressed logging for \" +\n              (logAction.getCount() - 1) + \" edit reads\";\n        }\n        LOG.info(\"Reading \" + editIn + \" expecting start txid #\" +\n            (lastAppliedTxId + 1) + logSuppressed);\n      }\n      try {\n        remainingReadTxns -= loader.loadFSEdits(editIn, lastAppliedTxId + 1,\n                remainingReadTxns, startOpt, recovery);\n      } finally {\n        // Update lastAppliedTxId even in case of error, since some ops may\n        // have been successfully applied before the error.\n        lastAppliedTxId = loader.getLastAppliedTxId();\n      }\n      // If we are in recovery mode, we may have skipped over some txids.\n      if (editIn.getLastTxId() != HdfsServerConstants.INVALID_TXID\n          && recovery != null) {\n        lastAppliedTxId = editIn.getLastTxId();\n      }\n      if (remainingReadTxns <= 0) {\n        break;\n      }\n    }\n  } finally {\n    FSEditLog.closeAllStreams(editStreams);\n  }\n  return lastAppliedTxId - prevLastAppliedTxId;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3203,
        "instruction": "private static void handleException(Exception e, String url, long startTime,\n    String invalidNumMsg) throws BadRequestException,\n    WebApplicationException {\n  long endTime = Time.monotonicNow();\n  if (e instanceof NumberFormatException) {\n    throw new BadRequestException(invalidNumMsg + \" is not a numeric value.\");\n  } else if (e instanceof IllegalArgumentException) {\n    throw new BadRequestException(e.getMessage() == null ?\n        \"Requested Invalid Field.\" : e.getMessage());\n  } else if (e instanceof NotFoundException) {\n    throw (NotFoundException)e;\n  } else if (e instanceof TimelineParseException) {\n    throw new BadRequestException(e.getMessage() == null ?\n        \"Filter Parsing failed.\" : e.getMessage());\n  } else if (e instanceof BadRequestException) {\n    throw (BadRequestException)e;\n  } else if (e instanceof ForbiddenException) {\n    throw (ForbiddenException) e;\n  } else {\n    LOG.error(\"Error while processing REST request\", e);\n    throw new WebApplicationException(e,\n        Response.Status.INTERNAL_SERVER_ERROR);\n  }\n}",
        "accept_response": "private static void handleException(Exception e, String url, long startTime,\n    String invalidNumMsg) throws BadRequestException,\n    WebApplicationException {\n  long endTime = Time.monotonicNow();\n  LOG.info(\"Processed URL {} but encountered exception (Took \" +\n      \"{} ms.)\", url, (endTime - startTime));\n  if (e instanceof NumberFormatException) {\n    throw new BadRequestException(invalidNumMsg + \" is not a numeric value.\");\n  } else if (e instanceof IllegalArgumentException) {\n    throw new BadRequestException(e.getMessage() == null ?\n        \"Requested Invalid Field.\" : e.getMessage());\n  } else if (e instanceof NotFoundException) {\n    throw (NotFoundException)e;\n  } else if (e instanceof TimelineParseException) {\n    throw new BadRequestException(e.getMessage() == null ?\n        \"Filter Parsing failed.\" : e.getMessage());\n  } else if (e instanceof BadRequestException) {\n    throw (BadRequestException)e;\n  } else if (e instanceof ForbiddenException) {\n    throw (ForbiddenException) e;\n  } else {\n    LOG.error(\"Error while processing REST request\", e);\n    throw new WebApplicationException(e,\n        Response.Status.INTERNAL_SERVER_ERROR);\n  }\n}",
        "reject_response": "private static void handleException(Exception e, String url, long startTime,\n    String invalidNumMsg) throws BadRequestException,\n    WebApplicationException {\n  long endTime = Time.monotonicNow();\n  LOG.info(\"Processed URL \" + url + \" but encountered exception (Took \" +\n      (endTime - startTime) + \" ms.)\");\n  if (e instanceof NumberFormatException) {\n    throw new BadRequestException(invalidNumMsg + \" is not a numeric value.\");\n  } else if (e instanceof IllegalArgumentException) {\n    throw new BadRequestException(e.getMessage() == null ?\n        \"Requested Invalid Field.\" : e.getMessage());\n  } else if (e instanceof NotFoundException) {\n    throw (NotFoundException)e;\n  } else if (e instanceof TimelineParseException) {\n    throw new BadRequestException(e.getMessage() == null ?\n        \"Filter Parsing failed.\" : e.getMessage());\n  } else if (e instanceof BadRequestException) {\n    throw (BadRequestException)e;\n  } else if (e instanceof ForbiddenException) {\n    throw (ForbiddenException) e;\n  } else {\n    LOG.error(\"Error while processing REST request\", e);\n    throw new WebApplicationException(e,\n        Response.Status.INTERNAL_SERVER_ERROR);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2852,
        "instruction": "private TSExecuteStatementResp executeUpdateStatement(String statement) {\n\n  PhysicalPlan physicalPlan;\n  try {\n    physicalPlan = processor.parseSQLToPhysicalPlan(statement, zoneIds.get());\n  } catch (QueryProcessException | MetadataException e) {\n    return getTSExecuteStatementResp(getStatus(TSStatusCode.SQL_PARSE_ERROR, e.getMessage()));\n  }\n\n  if (physicalPlan.isQuery()) {\n    return getTSExecuteStatementResp(getStatus(TSStatusCode.EXECUTE_STATEMENT_ERROR,\n        \"Statement is a query statement.\"));\n  }\n\n  return executeUpdateStatement(physicalPlan);\n}",
        "accept_response": "private TSExecuteStatementResp executeUpdateStatement(String statement) {\n\n  PhysicalPlan physicalPlan;\n  try {\n    physicalPlan = processor.parseSQLToPhysicalPlan(statement, zoneIds.get());\n  } catch (QueryProcessException | MetadataException e) {\n    logger.info(\"meet error while parsing SQL to physical plan: {}\", e.getMessage());\n    return getTSExecuteStatementResp(getStatus(TSStatusCode.SQL_PARSE_ERROR, e.getMessage()));\n  }\n\n  if (physicalPlan.isQuery()) {\n    return getTSExecuteStatementResp(getStatus(TSStatusCode.EXECUTE_STATEMENT_ERROR,\n        \"Statement is a query statement.\"));\n  }\n\n  return executeUpdateStatement(physicalPlan);\n}",
        "reject_response": "private TSExecuteStatementResp executeUpdateStatement(String statement) {\n\n  PhysicalPlan physicalPlan;\n  try {\n    physicalPlan = processor.parseSQLToPhysicalPlan(statement, zoneIds.get());\n  } catch (QueryProcessException | MetadataException e) {\n    logger.error(\"meet error while parsing SQL to physical plan!\", e);\n    return getTSExecuteStatementResp(getStatus(TSStatusCode.SQL_PARSE_ERROR, e.getMessage()));\n  }\n\n  if (physicalPlan.isQuery()) {\n    return getTSExecuteStatementResp(getStatus(TSStatusCode.EXECUTE_STATEMENT_ERROR,\n        \"Statement is a query statement.\"));\n  }\n\n  return executeUpdateStatement(physicalPlan);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2808,
        "instruction": "@Nullable private Object loadFromStore(@Nullable IgniteInternalTx tx,\n    KeyCacheObject key,\n    boolean convert)\n    throws IgniteCheckedException {\n    if (store != null) {\n        if (key.internal())\n            // Never load internal keys from store as they are never persisted.\n            return null;\n\n        Object storeKey = cctx.unwrapBinaryIfNeeded(key, !convertBinary());\n\n        if (log.isDebugEnabled())\n            log.debug(S.toString(\"Loading value from store for key\",\n                \"key\", storeKey, true));\n\n        sessionInit0(tx);\n\n        boolean threwEx = true;\n\n        Object val = null;\n\n        try {\n            val = singleThreadGate.load(storeKey);\n\n            threwEx = false;\n        }\n        catch (ClassCastException e) {\n            handleClassCastException(e);\n        }\n        catch (CacheLoaderException e) {\n            throw new IgniteCheckedException(e);\n        }\n        catch (Exception e) {\n            throw new IgniteCheckedException(new CacheLoaderException(e));\n        }\n        finally {\n            sessionEnd0(tx, threwEx);\n        }\n\n        if (log.isDebugEnabled())\n\n        if (convert) {\n            val = convert(val);\n\n            return val;\n        }\n        else\n            return val;\n    }\n\n    return null;\n}",
        "accept_response": "@Nullable private Object loadFromStore(@Nullable IgniteInternalTx tx,\n    KeyCacheObject key,\n    boolean convert)\n    throws IgniteCheckedException {\n    if (store != null) {\n        if (key.internal())\n            // Never load internal keys from store as they are never persisted.\n            return null;\n\n        Object storeKey = cctx.unwrapBinaryIfNeeded(key, !convertBinary());\n\n        if (log.isDebugEnabled())\n            log.debug(S.toString(\"Loading value from store for key\",\n                \"key\", storeKey, true));\n\n        sessionInit0(tx);\n\n        boolean threwEx = true;\n\n        Object val = null;\n\n        try {\n            val = singleThreadGate.load(storeKey);\n\n            threwEx = false;\n        }\n        catch (ClassCastException e) {\n            handleClassCastException(e);\n        }\n        catch (CacheLoaderException e) {\n            throw new IgniteCheckedException(e);\n        }\n        catch (Exception e) {\n            throw new IgniteCheckedException(new CacheLoaderException(e));\n        }\n        finally {\n            sessionEnd0(tx, threwEx);\n        }\n\n        if (log.isDebugEnabled())\n            log.debug(S.toString(\"Loaded value from store\",\n                \"key\", key, true,\n                \"val\", val, true));\n\n        if (convert) {\n            val = convert(val);\n\n            return val;\n        }\n        else\n            return val;\n    }\n\n    return null;\n}",
        "reject_response": "@Nullable private Object loadFromStore(@Nullable IgniteInternalTx tx,\n    KeyCacheObject key,\n    boolean convert)\n    throws IgniteCheckedException {\n    if (store != null) {\n        if (key.internal())\n            // Never load internal keys from store as they are never persisted.\n            return null;\n\n        Object storeKey = cctx.unwrapBinaryIfNeeded(key, !convertBinary());\n\n        if (log.isDebugEnabled())\n            log.debug(S.toString(\"Loading value from store for key\",\n                \"key\", storeKey, true));\n\n        sessionInit0(tx);\n\n        boolean threwEx = true;\n\n        Object val = null;\n\n        try {\n            val = singleThreadGate.load(storeKey);\n\n            threwEx = false;\n        }\n        catch (ClassCastException e) {\n            handleClassCastException(e);\n        }\n        catch (CacheLoaderException e) {\n            throw new IgniteCheckedException(e);\n        }\n        catch (Exception e) {\n            throw new IgniteCheckedException(new CacheLoaderException(e));\n        }\n        finally {\n            sessionEnd0(tx, threwEx);\n        }\n\n        if (log.isDebugEnabled())\n            log.debug(\"Loaded value from store [key=\" + key + \", val=\" + val + ']');\n\n        if (convert) {\n            val = convert(val);\n\n            return val;\n        }\n        else\n            return val;\n    }\n\n    return null;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2416,
        "instruction": "protected boolean emitMetrics(TimelineMetrics metrics) {\n  String collectorHost;\n  // Get cached target\n  if (targetCollectorHostSupplier != null) {\n    collectorHost = targetCollectorHostSupplier.get();\n    // Last X attempts have failed - force refresh\n    if (failedCollectorConnectionsCounter.get() > RETRY_COUNT_BEFORE_COLLECTOR_FAILOVER) {\n      allKnownLiveCollectors.remove(collectorHost);\n      targetCollectorHostSupplier = null;\n      collectorHost = findPreferredCollectHost();\n    }\n  } else {\n    collectorHost = findPreferredCollectHost();\n  }\n\n  if (collectorHost == null) {\n    if (nullCollectorCounter.getAndIncrement() == 0) {\n      LOG.info(\"No live collector to send metrics to. Metrics to be sent will be discarded. \" +\n        \"This message will be skipped for the next \" + NUMBER_OF_NULL_COLLECTOR_EXCEPTIONS + \" times.\");\n    } else {\n      nullCollectorCounter.compareAndSet(NUMBER_OF_NULL_COLLECTOR_EXCEPTIONS, 0);\n    }\n    return false;\n  } else {\n    nullCollectorCounter.set(0);\n  }\n\n  String connectUrl = getCollectorUri(collectorHost);\n  String jsonData = null;\n  LOG.debug(\"EmitMetrics connectUrl = \"  + connectUrl);\n  try {\n    jsonData = mapper.writeValueAsString(metrics);\n  } catch (IOException e) {\n    LOG.error(\"Unable to parse metrics\", e);\n  }\n  if (jsonData != null) {\n    return emitMetricsJson(connectUrl, jsonData);\n  }\n  return false;\n}",
        "accept_response": "protected boolean emitMetrics(TimelineMetrics metrics) {\n  String collectorHost;\n  // Get cached target\n  if (targetCollectorHostSupplier != null) {\n    collectorHost = targetCollectorHostSupplier.get();\n    // Last X attempts have failed - force refresh\n    if (failedCollectorConnectionsCounter.get() > RETRY_COUNT_BEFORE_COLLECTOR_FAILOVER) {\n      LOG.debug(\"Removing collector \" + collectorHost + \" from allKnownLiveCollectors.\");\n      allKnownLiveCollectors.remove(collectorHost);\n      targetCollectorHostSupplier = null;\n      collectorHost = findPreferredCollectHost();\n    }\n  } else {\n    collectorHost = findPreferredCollectHost();\n  }\n\n  if (collectorHost == null) {\n    if (nullCollectorCounter.getAndIncrement() == 0) {\n      LOG.info(\"No live collector to send metrics to. Metrics to be sent will be discarded. \" +\n        \"This message will be skipped for the next \" + NUMBER_OF_NULL_COLLECTOR_EXCEPTIONS + \" times.\");\n    } else {\n      nullCollectorCounter.compareAndSet(NUMBER_OF_NULL_COLLECTOR_EXCEPTIONS, 0);\n    }\n    return false;\n  } else {\n    nullCollectorCounter.set(0);\n  }\n\n  String connectUrl = getCollectorUri(collectorHost);\n  String jsonData = null;\n  LOG.debug(\"EmitMetrics connectUrl = \"  + connectUrl);\n  try {\n    jsonData = mapper.writeValueAsString(metrics);\n  } catch (IOException e) {\n    LOG.error(\"Unable to parse metrics\", e);\n  }\n  if (jsonData != null) {\n    return emitMetricsJson(connectUrl, jsonData);\n  }\n  return false;\n}",
        "reject_response": "protected boolean emitMetrics(TimelineMetrics metrics) {\n  String collectorHost;\n  // Get cached target\n  if (targetCollectorHostSupplier != null) {\n    collectorHost = targetCollectorHostSupplier.get();\n    // Last X attempts have failed - force refresh\n    if (failedCollectorConnectionsCounter.get() > RETRY_COUNT_BEFORE_COLLECTOR_FAILOVER) {\n      LOG.info(\"Removing collector \" + collectorHost + \" from allKnownLiveCollectors.\");\n      allKnownLiveCollectors.remove(collectorHost);\n      targetCollectorHostSupplier = null;\n      collectorHost = findPreferredCollectHost();\n    }\n  } else {\n    collectorHost = findPreferredCollectHost();\n  }\n\n  if (collectorHost == null) {\n    if (nullCollectorCounter.getAndIncrement() == 0) {\n      LOG.info(\"No live collector to send metrics to. Metrics to be sent will be discarded. \" +\n        \"This message will be skipped for the next \" + NUMBER_OF_NULL_COLLECTOR_EXCEPTIONS + \" times.\");\n    } else {\n      nullCollectorCounter.compareAndSet(NUMBER_OF_NULL_COLLECTOR_EXCEPTIONS, 0);\n    }\n    return false;\n  } else {\n    nullCollectorCounter.set(0);\n  }\n\n  String connectUrl = getCollectorUri(collectorHost);\n  String jsonData = null;\n  LOG.debug(\"EmitMetrics connectUrl = \"  + connectUrl);\n  try {\n    jsonData = mapper.writeValueAsString(metrics);\n  } catch (IOException e) {\n    LOG.error(\"Unable to parse metrics\", e);\n  }\n  if (jsonData != null) {\n    return emitMetricsJson(connectUrl, jsonData);\n  }\n  return false;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2432,
        "instruction": "@Override\npublic synchronized boolean tryAddMessageLast(MessageReference node, long wait) throws Exception {\n    if (node != null) {\n        Message msg = node.getMessage();\n        if (isStarted()) {\n            if (!msg.isPersistent()) {\n                nonPersistent.addMessageLast(node);\n            }\n        }\n        if (msg.isPersistent()) {\n            Destination dest = (Destination) msg.getRegionDestination();\n            TopicStorePrefetch tsp = topics.get(dest);\n            if (tsp != null) {\n                tsp.addMessageLast(node);\n                if (prioritizedMessages && immediatePriorityDispatch && tsp.isPaging()) {\n                    if (msg.getPriority() > tsp.getLastRecoveredPriority()) {\n                        tsp.recoverMessage(node.getMessage(), true);\n                    }\n                }\n            }\n        }\n\n    }\n    return true;\n}",
        "accept_response": "@Override\npublic synchronized boolean tryAddMessageLast(MessageReference node, long wait) throws Exception {\n    if (node != null) {\n        Message msg = node.getMessage();\n        if (isStarted()) {\n            if (!msg.isPersistent()) {\n                nonPersistent.addMessageLast(node);\n            }\n        }\n        if (msg.isPersistent()) {\n            Destination dest = (Destination) msg.getRegionDestination();\n            TopicStorePrefetch tsp = topics.get(dest);\n            if (tsp != null) {\n                tsp.addMessageLast(node);\n                if (prioritizedMessages && immediatePriorityDispatch && tsp.isPaging()) {\n                    if (msg.getPriority() > tsp.getLastRecoveredPriority()) {\n                        tsp.recoverMessage(node.getMessage(), true);\n                        LOG.trace(\"cached high priority ({}) message: {}, current paged batch priority: {}, cache size: {}\", new Object[]{ msg.getPriority(), msg.getMessageId(), tsp.getLastRecoveredPriority(), tsp.batchList.size()});\n                    }\n                }\n            }\n        }\n\n    }\n    return true;\n}",
        "reject_response": "@Override\npublic synchronized boolean tryAddMessageLast(MessageReference node, long wait) throws Exception {\n    if (node != null) {\n        Message msg = node.getMessage();\n        if (isStarted()) {\n            if (!msg.isPersistent()) {\n                nonPersistent.addMessageLast(node);\n            }\n        }\n        if (msg.isPersistent()) {\n            Destination dest = (Destination) msg.getRegionDestination();\n            TopicStorePrefetch tsp = topics.get(dest);\n            if (tsp != null) {\n                tsp.addMessageLast(node);\n                if (prioritizedMessages && immediatePriorityDispatch && tsp.isPaging()) {\n                    if (msg.getPriority() > tsp.getLastRecoveredPriority()) {\n                        tsp.recoverMessage(node.getMessage(), true);\n                        LOG.trace(\"cached high priority ({} message: {}, current paged batch priority: {}, cache size: {}\", new Object[]{ msg.getPriority(), msg.getMessageId(), tsp.getLastRecoveredPriority(), tsp.batchList.size()});\n                    }\n                }\n            }\n        }\n\n    }\n    return true;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2773,
        "instruction": "@Override\npublic void releaseEndpoint() {\n    final AsyncConnectionEndpoint endpoint = endpointRef.getAndSet(null);\n    if (endpoint != null) {\n        if (reusable) {\n            manager.release(endpoint, state, validDuration);\n        } else {\n            discardEndpoint(endpoint);\n        }\n    }\n}",
        "accept_response": "@Override\npublic void releaseEndpoint() {\n    final AsyncConnectionEndpoint endpoint = endpointRef.getAndSet(null);\n    if (endpoint != null) {\n        if (reusable) {\n            if (log.isDebugEnabled()) {\n                log.debug(\"{}: releasing valid endpoint\", ConnPoolSupport.getId(endpoint));\n            }\n            manager.release(endpoint, state, validDuration);\n        } else {\n            discardEndpoint(endpoint);\n        }\n    }\n}",
        "reject_response": "@Override\npublic void releaseEndpoint() {\n    final AsyncConnectionEndpoint endpoint = endpointRef.getAndSet(null);\n    if (endpoint != null) {\n        if (reusable) {\n            if (log.isDebugEnabled()) {\n                log.debug(ConnPoolSupport.getId(endpoint) + \": releasing valid endpoint\");\n            }\n            manager.release(endpoint, state, validDuration);\n        } else {\n            discardEndpoint(endpoint);\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2701,
        "instruction": "@GET\n@Path(\"getNodesInfo\")\n@Produces(\"application/json\")\npublic Response read() throws Exception {\n    LOG.debug(\"getNodesInfo started\");\n    StringBuilder jsonOutput = new StringBuilder(\"{\\\"regions\\\":[\");\n    try {\n        /*\n         * 1. Initialize the HADOOP client side API for a distributed file\n         * system\n         */\n        Configuration conf = new Configuration();\n        FileSystem fs = FileSystem.get(conf);\n        DistributedFileSystem dfs = (DistributedFileSystem) fs;\n\n        /*\n         * 2. Query the namenode for the datanodes info. Only live nodes are\n         * returned - in accordance with the results returned by\n         * org.apache.hadoop.hdfs.tools.DFSAdmin#report().\n         */\n        DatanodeInfo[] liveNodes = dfs.getDataNodeStats(DatanodeReportType.LIVE);\n\n        /*\n         * 3. Pack the datanodes info in a JSON text format and write it to\n         * the HTTP output stream.\n         */\n        String prefix = \"\";\n        for (DatanodeInfo node : liveNodes) {\n            verifyNode(node);\n            // write one node to the HTTP stream\n            jsonOutput.append(prefix).append(writeNode(node));\n            prefix = \",\";\n        }\n        jsonOutput.append(\"]}\");\n        LOG.debug(\"getNodesCluster output: \" + jsonOutput);\n    } catch (NodeDataException e) {\n        LOG.error(\"Nodes verification failed\", e);\n        throw e;\n    } catch (ClientAbortException e) {\n        throw e;\n    } catch (java.io.IOException e) {\n        LOG.error(\"Unhandled exception thrown\", e);\n        throw e;\n    }\n\n    return Response.ok(jsonOutput.toString(),\n            MediaType.APPLICATION_JSON_TYPE).build();\n}",
        "accept_response": "@GET\n@Path(\"getNodesInfo\")\n@Produces(\"application/json\")\npublic Response read() throws Exception {\n    LOG.debug(\"getNodesInfo started\");\n    StringBuilder jsonOutput = new StringBuilder(\"{\\\"regions\\\":[\");\n    try {\n        /*\n         * 1. Initialize the HADOOP client side API for a distributed file\n         * system\n         */\n        Configuration conf = new Configuration();\n        FileSystem fs = FileSystem.get(conf);\n        DistributedFileSystem dfs = (DistributedFileSystem) fs;\n\n        /*\n         * 2. Query the namenode for the datanodes info. Only live nodes are\n         * returned - in accordance with the results returned by\n         * org.apache.hadoop.hdfs.tools.DFSAdmin#report().\n         */\n        DatanodeInfo[] liveNodes = dfs.getDataNodeStats(DatanodeReportType.LIVE);\n\n        /*\n         * 3. Pack the datanodes info in a JSON text format and write it to\n         * the HTTP output stream.\n         */\n        String prefix = \"\";\n        for (DatanodeInfo node : liveNodes) {\n            verifyNode(node);\n            // write one node to the HTTP stream\n            jsonOutput.append(prefix).append(writeNode(node));\n            prefix = \",\";\n        }\n        jsonOutput.append(\"]}\");\n        LOG.debug(\"getNodesCluster output: \" + jsonOutput);\n    } catch (NodeDataException e) {\n        LOG.error(\"Nodes verification failed\", e);\n        throw e;\n    } catch (ClientAbortException e) {\n        LOG.error(\"Remote connection closed by HAWQ\", e);\n        throw e;\n    } catch (java.io.IOException e) {\n        LOG.error(\"Unhandled exception thrown\", e);\n        throw e;\n    }\n\n    return Response.ok(jsonOutput.toString(),\n            MediaType.APPLICATION_JSON_TYPE).build();\n}",
        "reject_response": "@GET\n@Path(\"getNodesInfo\")\n@Produces(\"application/json\")\npublic Response read() throws Exception {\n    LOG.debug(\"getNodesInfo started\");\n    StringBuilder jsonOutput = new StringBuilder(\"{\\\"regions\\\":[\");\n    try {\n        /*\n         * 1. Initialize the HADOOP client side API for a distributed file\n         * system\n         */\n        Configuration conf = new Configuration();\n        FileSystem fs = FileSystem.get(conf);\n        DistributedFileSystem dfs = (DistributedFileSystem) fs;\n\n        /*\n         * 2. Query the namenode for the datanodes info. Only live nodes are\n         * returned - in accordance with the results returned by\n         * org.apache.hadoop.hdfs.tools.DFSAdmin#report().\n         */\n        DatanodeInfo[] liveNodes = dfs.getDataNodeStats(DatanodeReportType.LIVE);\n\n        /*\n         * 3. Pack the datanodes info in a JSON text format and write it to\n         * the HTTP output stream.\n         */\n        String prefix = \"\";\n        for (DatanodeInfo node : liveNodes) {\n            verifyNode(node);\n            // write one node to the HTTP stream\n            jsonOutput.append(prefix).append(writeNode(node));\n            prefix = \",\";\n        }\n        jsonOutput.append(\"]}\");\n        LOG.debug(\"getNodesCluster output: \" + jsonOutput);\n    } catch (NodeDataException e) {\n        LOG.error(\"Nodes verification failed\", e);\n        throw e;\n    } catch (ClientAbortException e) {\n        Log.error(\"Remote connection closed by HAWQ\", e);\n        throw e;\n    } catch (java.io.IOException e) {\n        LOG.error(\"Unhandled exception thrown\", e);\n        throw e;\n    }\n\n    return Response.ok(jsonOutput.toString(),\n            MediaType.APPLICATION_JSON_TYPE).build();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2454,
        "instruction": "@Override\npublic void confirm(final Packet packet) {\n   if (resendCache != null && packet.isRequiresConfirmations()) {\n      lastConfirmedCommandID.incrementAndGet();\n\n      receivedBytes += packet.getPacketSize();\n\n      if (receivedBytes >= confWindowSize) {\n         receivedBytes = 0;\n\n         final Packet confirmed = new PacketsConfirmedMessage(lastConfirmedCommandID.get());\n\n         confirmed.setChannelID(id);\n\n         doWrite(confirmed);\n      }\n   }\n}",
        "accept_response": "@Override\npublic void confirm(final Packet packet) {\n   if (resendCache != null && packet.isRequiresConfirmations()) {\n      lastConfirmedCommandID.incrementAndGet();\n\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" ChannelImpl::confirming packet \" + packet + \" last commandID=\" + lastConfirmedCommandID);\n      }\n\n      receivedBytes += packet.getPacketSize();\n\n      if (receivedBytes >= confWindowSize) {\n         receivedBytes = 0;\n\n         final Packet confirmed = new PacketsConfirmedMessage(lastConfirmedCommandID.get());\n\n         confirmed.setChannelID(id);\n\n         doWrite(confirmed);\n      }\n   }\n}",
        "reject_response": "@Override\npublic void confirm(final Packet packet) {\n   if (resendCache != null && packet.isRequiresConfirmations()) {\n      lastConfirmedCommandID.incrementAndGet();\n\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"ChannelImpl::confirming packet \" + packet + \" last commandID=\" + lastConfirmedCommandID);\n      }\n\n      receivedBytes += packet.getPacketSize();\n\n      if (receivedBytes >= confWindowSize) {\n         receivedBytes = 0;\n\n         final Packet confirmed = new PacketsConfirmedMessage(lastConfirmedCommandID.get());\n\n         confirmed.setChannelID(id);\n\n         doWrite(confirmed);\n      }\n   }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2982,
        "instruction": "public void resetUserModulePermission(UserSessionBase userSession) {\n\n\tXXUser xUser = daoManager.getXXUser().findByUserName(userSession.getLoginId());\n\tif (xUser != null) {\n\t\tList<String> permissionList = daoManager.getXXModuleDef().findAccessibleModulesByUserId(userSession.getUserId(), xUser.getId());\n\t\tCopyOnWriteArraySet<String> userPermissions = new CopyOnWriteArraySet<String>(permissionList);\n\n\t\tUserSessionBase.RangerUserPermission rangerUserPermission = userSession.getRangerUserPermission();\n\n\t\tif (rangerUserPermission == null) {\n\t\t\trangerUserPermission = new UserSessionBase.RangerUserPermission();\n\t\t}\n\n\t\trangerUserPermission.setUserPermissions(userPermissions);\n\t\trangerUserPermission.setLastUpdatedTime(Calendar.getInstance().getTimeInMillis());\n\t\tuserSession.setRangerUserPermission(rangerUserPermission);\n\t} else {\n\t\tlogger.error(\"No XUser found with username: \" + userSession.getLoginId() + \"So Permission is not set for the user\");\n\t}\n}",
        "accept_response": "public void resetUserModulePermission(UserSessionBase userSession) {\n\n\tXXUser xUser = daoManager.getXXUser().findByUserName(userSession.getLoginId());\n\tif (xUser != null) {\n\t\tList<String> permissionList = daoManager.getXXModuleDef().findAccessibleModulesByUserId(userSession.getUserId(), xUser.getId());\n\t\tCopyOnWriteArraySet<String> userPermissions = new CopyOnWriteArraySet<String>(permissionList);\n\n\t\tUserSessionBase.RangerUserPermission rangerUserPermission = userSession.getRangerUserPermission();\n\n\t\tif (rangerUserPermission == null) {\n\t\t\trangerUserPermission = new UserSessionBase.RangerUserPermission();\n\t\t}\n\n\t\trangerUserPermission.setUserPermissions(userPermissions);\n\t\trangerUserPermission.setLastUpdatedTime(Calendar.getInstance().getTimeInMillis());\n\t\tuserSession.setRangerUserPermission(rangerUserPermission);\n\t\tif (logger.isDebugEnabled()) {\n\t\t\tlogger.debug(\"UserSession Updated to set new Permissions to User: \" + userSession.getLoginId());\n\t\t}\n\t} else {\n\t\tlogger.error(\"No XUser found with username: \" + userSession.getLoginId() + \"So Permission is not set for the user\");\n\t}\n}",
        "reject_response": "public void resetUserModulePermission(UserSessionBase userSession) {\n\n\tXXUser xUser = daoManager.getXXUser().findByUserName(userSession.getLoginId());\n\tif (xUser != null) {\n\t\tList<String> permissionList = daoManager.getXXModuleDef().findAccessibleModulesByUserId(userSession.getUserId(), xUser.getId());\n\t\tCopyOnWriteArraySet<String> userPermissions = new CopyOnWriteArraySet<String>(permissionList);\n\n\t\tUserSessionBase.RangerUserPermission rangerUserPermission = userSession.getRangerUserPermission();\n\n\t\tif (rangerUserPermission == null) {\n\t\t\trangerUserPermission = new UserSessionBase.RangerUserPermission();\n\t\t}\n\n\t\trangerUserPermission.setUserPermissions(userPermissions);\n\t\trangerUserPermission.setLastUpdatedTime(Calendar.getInstance().getTimeInMillis());\n\t\tuserSession.setRangerUserPermission(rangerUserPermission);\n\t\tif (logger.isDebugEnabled()) {\n\t\tlogger.info(\"UserSession Updated to set new Permissions to User: \" + userSession.getLoginId());\n\t\t}\n\t} else {\n\t\tlogger.error(\"No XUser found with username: \" + userSession.getLoginId() + \"So Permission is not set for the user\");\n\t}\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3116,
        "instruction": "@Override\npublic void init(Configuration conf) {\n  if (!(conf instanceof TajoConf)) {\n    throw new IllegalArgumentException(\"conf should be a TajoConf Type.\");\n  }\n  this.systemConf = (TajoConf)conf;\n\n  try {\n    // the base dir for an output dir\n    baseDirPath = getContext().createBaseDir();\n    LOG.info(\"TaskRunner basedir is created (\" + baseDirPath +\")\");\n  } catch (Throwable t) {\n    t.printStackTrace();\n  }\n  super.init(conf);\n  this.history.setState(getServiceState());\n}",
        "accept_response": "@Override\npublic void init(Configuration conf) {\n  if (!(conf instanceof TajoConf)) {\n    throw new IllegalArgumentException(\"conf should be a TajoConf Type.\");\n  }\n  this.systemConf = (TajoConf)conf;\n\n  try {\n    // the base dir for an output dir\n    baseDirPath = getContext().createBaseDir();\n    LOG.info(\"TaskRunner basedir is created (\" + baseDirPath +\")\");\n  } catch (Throwable t) {\n    t.printStackTrace();\n    LOG.error(t, t);\n  }\n  super.init(conf);\n  this.history.setState(getServiceState());\n}",
        "reject_response": "@Override\npublic void init(Configuration conf) {\n  if (!(conf instanceof TajoConf)) {\n    throw new IllegalArgumentException(\"conf should be a TajoConf Type.\");\n  }\n  this.systemConf = (TajoConf)conf;\n\n  try {\n    // the base dir for an output dir\n    baseDirPath = getContext().createBaseDir();\n    LOG.info(\"TaskRunner basedir is created (\" + baseDirPath +\")\");\n  } catch (Throwable t) {\n    t.printStackTrace();\n    LOG.error(t);\n  }\n  super.init(conf);\n  this.history.setState(getServiceState());\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2905,
        "instruction": "public DNSLookupContinuation checkSPF(SPFSession spfData) throws PermErrorException,\n        TempErrorException, NeutralException, NoneException {\n    // Get the right host.\n    String host = expandHost(spfData);\n\n    // get the ipAddress\n    try {\n        boolean validIPV4Address = Inet6Util.isValidIPV4Address(spfData.getIpAddress());\n        spfData.setAttribute(ATTRIBUTE_AMECHANISM_IPV4CHECK, Boolean.valueOf(validIPV4Address));\n        if (validIPV4Address) {\n\n            List<String> aRecords = getARecords(host);\n            if (aRecords == null) {\n                try {\n                    DNSRequest request = new DNSRequest(host, DNSRequest.A);\n                    return new DNSLookupContinuation(request, AMechanism.this);\n                } catch (NoneException e) {\n                    return onDNSResponse(new DNSResponse(aRecords), spfData);\n                }\n            } else {\n                return onDNSResponse(new DNSResponse(aRecords), spfData);\n            }\n\n        } else {\n\n            List<String> aaaaRecords = getAAAARecords(host);\n            if (aaaaRecords == null) {\n                try {\n                    DNSRequest request = new DNSRequest(host, DNSRequest.AAAA);\n                    return new DNSLookupContinuation(request, AMechanism.this);\n                } catch (NoneException e) {\n                    return onDNSResponse(new DNSResponse(aaaaRecords), spfData);\n                }\n            } else {\n                return onDNSResponse(new DNSResponse(aaaaRecords), spfData);\n            }\n\n        }\n    // PermError / TempError\n    // TODO: Should we replace this with the \"right\" Exceptions ?\n    } catch (Exception e) {\n        throw new PermErrorException(\"No valid ipAddress: \"\n                + spfData.getIpAddress());\n    }\n\n}",
        "accept_response": "public DNSLookupContinuation checkSPF(SPFSession spfData) throws PermErrorException,\n        TempErrorException, NeutralException, NoneException {\n    // Get the right host.\n    String host = expandHost(spfData);\n\n    // get the ipAddress\n    try {\n        boolean validIPV4Address = Inet6Util.isValidIPV4Address(spfData.getIpAddress());\n        spfData.setAttribute(ATTRIBUTE_AMECHANISM_IPV4CHECK, Boolean.valueOf(validIPV4Address));\n        if (validIPV4Address) {\n\n            List<String> aRecords = getARecords(host);\n            if (aRecords == null) {\n                try {\n                    DNSRequest request = new DNSRequest(host, DNSRequest.A);\n                    return new DNSLookupContinuation(request, AMechanism.this);\n                } catch (NoneException e) {\n                    return onDNSResponse(new DNSResponse(aRecords), spfData);\n                }\n            } else {\n                return onDNSResponse(new DNSResponse(aRecords), spfData);\n            }\n\n        } else {\n\n            List<String> aaaaRecords = getAAAARecords(host);\n            if (aaaaRecords == null) {\n                try {\n                    DNSRequest request = new DNSRequest(host, DNSRequest.AAAA);\n                    return new DNSLookupContinuation(request, AMechanism.this);\n                } catch (NoneException e) {\n                    return onDNSResponse(new DNSResponse(aaaaRecords), spfData);\n                }\n            } else {\n                return onDNSResponse(new DNSResponse(aaaaRecords), spfData);\n            }\n\n        }\n    // PermError / TempError\n    // TODO: Should we replace this with the \"right\" Exceptions ?\n    } catch (Exception e) {\n        LOGGER.debug(\"No valid ipAddress: \", e);\n        throw new PermErrorException(\"No valid ipAddress: \"\n                + spfData.getIpAddress());\n    }\n\n}",
        "reject_response": "public DNSLookupContinuation checkSPF(SPFSession spfData) throws PermErrorException,\n        TempErrorException, NeutralException, NoneException {\n    // Get the right host.\n    String host = expandHost(spfData);\n\n    // get the ipAddress\n    try {\n        boolean validIPV4Address = Inet6Util.isValidIPV4Address(spfData.getIpAddress());\n        spfData.setAttribute(ATTRIBUTE_AMECHANISM_IPV4CHECK, Boolean.valueOf(validIPV4Address));\n        if (validIPV4Address) {\n\n            List<String> aRecords = getARecords(host);\n            if (aRecords == null) {\n                try {\n                    DNSRequest request = new DNSRequest(host, DNSRequest.A);\n                    return new DNSLookupContinuation(request, AMechanism.this);\n                } catch (NoneException e) {\n                    return onDNSResponse(new DNSResponse(aRecords), spfData);\n                }\n            } else {\n                return onDNSResponse(new DNSResponse(aRecords), spfData);\n            }\n\n        } else {\n\n            List<String> aaaaRecords = getAAAARecords(host);\n            if (aaaaRecords == null) {\n                try {\n                    DNSRequest request = new DNSRequest(host, DNSRequest.AAAA);\n                    return new DNSLookupContinuation(request, AMechanism.this);\n                } catch (NoneException e) {\n                    return onDNSResponse(new DNSResponse(aaaaRecords), spfData);\n                }\n            } else {\n                return onDNSResponse(new DNSResponse(aaaaRecords), spfData);\n            }\n\n        }\n    // PermError / TempError\n    // TODO: Should we replace this with the \"right\" Exceptions ?\n    } catch (Exception e) {\n        log.debug(\"No valid ipAddress: \",e);\n        throw new PermErrorException(\"No valid ipAddress: \"\n                + spfData.getIpAddress());\n    }\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2958,
        "instruction": "public int run(String[] args) throws Exception {\n\n  String usage = \"Usage: Fetcher <segment> [-threads n]\";\n\n  if (args.length < 1) {\n    System.err.println(usage);\n    return -1;\n  }\n\n  Path segment = new Path(args[0]);\n\n  int threads = getConf().getInt(\"fetcher.threads.fetch\", 10);\n\n  for (int i = 1; i < args.length; i++) { // parse command line\n    if (args[i].equals(\"-threads\")) { // found -threads option\n      threads = Integer.parseInt(args[++i]);\n    }\n  }\n\n  getConf().setInt(\"fetcher.threads.fetch\", threads);\n\n  try {\n    fetch(segment, threads);\n    return 0;\n  } catch (Exception e) {\n    return -1;\n  }\n\n}",
        "accept_response": "public int run(String[] args) throws Exception {\n\n  String usage = \"Usage: Fetcher <segment> [-threads n]\";\n\n  if (args.length < 1) {\n    System.err.println(usage);\n    return -1;\n  }\n\n  Path segment = new Path(args[0]);\n\n  int threads = getConf().getInt(\"fetcher.threads.fetch\", 10);\n\n  for (int i = 1; i < args.length; i++) { // parse command line\n    if (args[i].equals(\"-threads\")) { // found -threads option\n      threads = Integer.parseInt(args[++i]);\n    }\n  }\n\n  getConf().setInt(\"fetcher.threads.fetch\", threads);\n\n  try {\n    fetch(segment, threads);\n    return 0;\n  } catch (Exception e) {\n    LOG.error(\"Fetcher: {}\", StringUtils.stringifyException(e));\n    return -1;\n  }\n\n}",
        "reject_response": "public int run(String[] args) throws Exception {\n\n  String usage = \"Usage: Fetcher <segment> [-threads n]\";\n\n  if (args.length < 1) {\n    System.err.println(usage);\n    return -1;\n  }\n\n  Path segment = new Path(args[0]);\n\n  int threads = getConf().getInt(\"fetcher.threads.fetch\", 10);\n\n  for (int i = 1; i < args.length; i++) { // parse command line\n    if (args[i].equals(\"-threads\")) { // found -threads option\n      threads = Integer.parseInt(args[++i]);\n    }\n  }\n\n  getConf().setInt(\"fetcher.threads.fetch\", threads);\n\n  try {\n    fetch(segment, threads);\n    return 0;\n  } catch (Exception e) {\n    LOG.error(\"Fetcher: \" + StringUtils.stringifyException(e));\n    return -1;\n  }\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3087,
        "instruction": "private void initLogWriter(Path logFilePath) {\n    try {\n        eventLogPath = logFilePath;\n        eventLogWriter = Files.newBufferedWriter(eventLogPath, StandardCharsets.UTF_8, StandardOpenOption.CREATE,\n                                                 StandardOpenOption.WRITE, StandardOpenOption.APPEND);\n    } catch (IOException e) {\n        LOG.error(\"Error setting up FileBasedEventLogger.\", e);\n        throw new RuntimeException(e);\n    }\n}",
        "accept_response": "private void initLogWriter(Path logFilePath) {\n    try {\n        LOG.info(\"Event log path {}\", logFilePath);\n        eventLogPath = logFilePath;\n        eventLogWriter = Files.newBufferedWriter(eventLogPath, StandardCharsets.UTF_8, StandardOpenOption.CREATE,\n                                                 StandardOpenOption.WRITE, StandardOpenOption.APPEND);\n    } catch (IOException e) {\n        LOG.error(\"Error setting up FileBasedEventLogger.\", e);\n        throw new RuntimeException(e);\n    }\n}",
        "reject_response": "private void initLogWriter(Path logFilePath) {\n    try {\n        LOG.info(\"logFilePath {}\", logFilePath);\n        eventLogPath = logFilePath;\n        eventLogWriter = Files.newBufferedWriter(eventLogPath, StandardCharsets.UTF_8, StandardOpenOption.CREATE,\n                                                 StandardOpenOption.WRITE, StandardOpenOption.APPEND);\n    } catch (IOException e) {\n        LOG.error(\"Error setting up FileBasedEventLogger.\", e);\n        throw new RuntimeException(e);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2973,
        "instruction": "public final void validate(File path, String expectedError) throws Exception\n   {\n       if (path == null)\n       {\n           return;\n       }\n       ValidationResult result = PreflightParser.validate(path);\n       assertFalse(result.isValid(), path + \" : Isartor file should be invalid (\" + path + \")\");\n       assertTrue(result.getErrorsList().size() > 0, path + \" : Should find at least one error\");\n       // could contain more than one error\n       boolean found = false;\n       if (expectedError != null)\n       {\n           for (ValidationError error : result.getErrorsList())\n           {\n               if (error.getErrorCode().equals(expectedError))\n               {\n                   found = true;\n                   if (outputResult == null)\n                   {\n                       break;\n                   }\n               }\n               if (outputResult != null)\n               {\n                   String log = path.getName().replace(\".pdf\", \"\") + \"#\" + error.getErrorCode()\n                           + \"#\" + error.getDetails() + \"\\n\";\n                   outputResult.write(log.getBytes());\n               }\n           }\n       }\n\n       if (result.getErrorsList().size() > 0)\n       {\n           if (expectedError == null)\n           {\n               LOG.info(\"File invalid as expected (no expected code) :\"\n                       + this.path.getAbsolutePath());\n           }\n           else if (!found)\n           {\n               StringBuilder message = new StringBuilder(100);\n               message.append(path).append(\" : Invalid error code returned. Expected \");\n               message.append(expectedError).append(\", found \");\n               for (ValidationError error : result.getErrorsList())\n               {\n                   message.append(error.getErrorCode()).append(\" \");\n               }\n               fail(message.toString());\n           }\n       }\n       else\n       {\n           assertEquals(path + \" : Invalid error code returned.\", expectedError,\n                   result.getErrorsList().get(0).getErrorCode());\n       }\n   }",
        "accept_response": "public final void validate(File path, String expectedError) throws Exception\n   {\n       if (path == null)\n       {\n           LOG.warn(\"This is an empty test\");\n           return;\n       }\n       ValidationResult result = PreflightParser.validate(path);\n       assertFalse(result.isValid(), path + \" : Isartor file should be invalid (\" + path + \")\");\n       assertTrue(result.getErrorsList().size() > 0, path + \" : Should find at least one error\");\n       // could contain more than one error\n       boolean found = false;\n       if (expectedError != null)\n       {\n           for (ValidationError error : result.getErrorsList())\n           {\n               if (error.getErrorCode().equals(expectedError))\n               {\n                   found = true;\n                   if (outputResult == null)\n                   {\n                       break;\n                   }\n               }\n               if (outputResult != null)\n               {\n                   String log = path.getName().replace(\".pdf\", \"\") + \"#\" + error.getErrorCode()\n                           + \"#\" + error.getDetails() + \"\\n\";\n                   outputResult.write(log.getBytes());\n               }\n           }\n       }\n\n       if (result.getErrorsList().size() > 0)\n       {\n           if (expectedError == null)\n           {\n               LOG.info(\"File invalid as expected (no expected code) :\"\n                       + this.path.getAbsolutePath());\n           }\n           else if (!found)\n           {\n               StringBuilder message = new StringBuilder(100);\n               message.append(path).append(\" : Invalid error code returned. Expected \");\n               message.append(expectedError).append(\", found \");\n               for (ValidationError error : result.getErrorsList())\n               {\n                   message.append(error.getErrorCode()).append(\" \");\n               }\n               fail(message.toString());\n           }\n       }\n       else\n       {\n           assertEquals(path + \" : Invalid error code returned.\", expectedError,\n                   result.getErrorsList().get(0).getErrorCode());\n       }\n   }",
        "reject_response": "public final void validate(File path, String expectedError) throws Exception\n   {\n       if (path == null)\n       {\n           logger.warn(\"This is an empty test\");\n           return;\n       }\n       ValidationResult result = PreflightParser.validate(path);\n       assertFalse(result.isValid(), path + \" : Isartor file should be invalid (\" + path + \")\");\n       assertTrue(result.getErrorsList().size() > 0, path + \" : Should find at least one error\");\n       // could contain more than one error\n       boolean found = false;\n       if (expectedError != null)\n       {\n           for (ValidationError error : result.getErrorsList())\n           {\n               if (error.getErrorCode().equals(expectedError))\n               {\n                   found = true;\n                   if (outputResult == null)\n                   {\n                       break;\n                   }\n               }\n               if (outputResult != null)\n               {\n                   String log = path.getName().replace(\".pdf\", \"\") + \"#\" + error.getErrorCode()\n                           + \"#\" + error.getDetails() + \"\\n\";\n                   outputResult.write(log.getBytes());\n               }\n           }\n       }\n\n       if (result.getErrorsList().size() > 0)\n       {\n           if (expectedError == null)\n           {\n               LOG.info(\"File invalid as expected (no expected code) :\"\n                       + this.path.getAbsolutePath());\n           }\n           else if (!found)\n           {\n               StringBuilder message = new StringBuilder(100);\n               message.append(path).append(\" : Invalid error code returned. Expected \");\n               message.append(expectedError).append(\", found \");\n               for (ValidationError error : result.getErrorsList())\n               {\n                   message.append(error.getErrorCode()).append(\" \");\n               }\n               fail(message.toString());\n           }\n       }\n       else\n       {\n           assertEquals(path + \" : Invalid error code returned.\", expectedError,\n                   result.getErrorsList().get(0).getErrorCode());\n       }\n   }",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3122,
        "instruction": "@Override\npublic void tryKillDAG() throws TezException, IOException {\n  try {\n    if (createAMProxyIfNeeded()) {\n      TryKillDAGRequestProto requestProto =\n          TryKillDAGRequestProto.newBuilder().setDagId(dagId).build();\n      try {\n        proxy.tryKillDAG(null, requestProto);\n      } catch (ServiceException e) {\n        resetProxy(e);\n      }\n    }\n  } catch (ApplicationNotFoundException e) {\n    throw new SessionNotRunning(\"Application already completed\");\n  }\n}",
        "accept_response": "@Override\npublic void tryKillDAG() throws TezException, IOException {\n  LOG.debug(\"TryKill for app: {} dag:{}\", appId, dagId);\n  try {\n    if (createAMProxyIfNeeded()) {\n      TryKillDAGRequestProto requestProto =\n          TryKillDAGRequestProto.newBuilder().setDagId(dagId).build();\n      try {\n        proxy.tryKillDAG(null, requestProto);\n      } catch (ServiceException e) {\n        resetProxy(e);\n      }\n    }\n  } catch (ApplicationNotFoundException e) {\n    throw new SessionNotRunning(\"Application already completed\");\n  }\n}",
        "reject_response": "@Override\npublic void tryKillDAG() throws TezException, IOException {\n  if(LOG.isDebugEnabled()) {\n    LOG.debug(\"TryKill for app: \" + appId + \" dag:\" + dagId);\n  }\n  try {\n    if (createAMProxyIfNeeded()) {\n      TryKillDAGRequestProto requestProto =\n          TryKillDAGRequestProto.newBuilder().setDagId(dagId).build();\n      try {\n        proxy.tryKillDAG(null, requestProto);\n      } catch (ServiceException e) {\n        resetProxy(e);\n      }\n    }\n  } catch (ApplicationNotFoundException e) {\n    throw new SessionNotRunning(\"Application already completed\");\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2656,
        "instruction": "private void initializeSSL() {\n\n\n  this.sysPulseUseSSLLocator = Boolean.valueOf(\n      pulseProperties.getProperty(PulseConstants.SYSTEM_PROPERTY_PULSE_USESSL_LOCATOR, \"false\"));\n\n  this.sysPulseUseSSLManager = Boolean.valueOf(\n      pulseProperties.getProperty(PulseConstants.SYSTEM_PROPERTY_PULSE_USESSL_MANAGER, \"false\"));\n\n\n  if ((sysPulseUseSSLLocator || sysPulseUseSSLManager)) {\n    Properties sslProperties = new Properties();\n    if (!pulseSecurityProperties.isEmpty()) {\n      Set entrySet = pulseSecurityProperties.entrySet();\n      for (Iterator it = entrySet.iterator(); it.hasNext();) {\n        Entry<String, String> entry = (Entry<String, String>) it.next();\n        String key = entry.getKey();\n        if (key.startsWith(\"javax.net.ssl.\")) {\n\n          String val = entry.getValue();\n          System.setProperty(key, val);\n          sslProperties.setProperty(key, val);\n        }\n      }\n    }\n    if (sslProperties.isEmpty()) {\n      logger.warn(resourceBundle.getString(\"LOG_MSG_SSL_NOT_SET\"));\n    }\n  }\n\n}",
        "accept_response": "private void initializeSSL() {\n  logger.info(resourceBundle.getString(\"LOG_MSG_GET_SSL_DETAILS\"));\n\n\n  this.sysPulseUseSSLLocator = Boolean.valueOf(\n      pulseProperties.getProperty(PulseConstants.SYSTEM_PROPERTY_PULSE_USESSL_LOCATOR, \"false\"));\n\n  this.sysPulseUseSSLManager = Boolean.valueOf(\n      pulseProperties.getProperty(PulseConstants.SYSTEM_PROPERTY_PULSE_USESSL_MANAGER, \"false\"));\n\n\n  if ((sysPulseUseSSLLocator || sysPulseUseSSLManager)) {\n    Properties sslProperties = new Properties();\n    if (!pulseSecurityProperties.isEmpty()) {\n      Set entrySet = pulseSecurityProperties.entrySet();\n      for (Iterator it = entrySet.iterator(); it.hasNext();) {\n        Entry<String, String> entry = (Entry<String, String>) it.next();\n        String key = entry.getKey();\n        if (key.startsWith(\"javax.net.ssl.\")) {\n\n          String val = entry.getValue();\n          System.setProperty(key, val);\n          sslProperties.setProperty(key, val);\n        }\n      }\n    }\n    if (sslProperties.isEmpty()) {\n      logger.warn(resourceBundle.getString(\"LOG_MSG_SSL_NOT_SET\"));\n    }\n  }\n\n}",
        "reject_response": "private void initializeSSL() {\n  if (LOGGER.infoEnabled()) {\n    LOGGER.info(resourceBundle.getString(\"LOG_MSG_GET_SSL_DETAILS\"));\n  }\n\n\n  this.sysPulseUseSSLLocator = Boolean.valueOf(\n      pulseProperties.getProperty(PulseConstants.SYSTEM_PROPERTY_PULSE_USESSL_LOCATOR, \"false\"));\n\n  this.sysPulseUseSSLManager = Boolean.valueOf(\n      pulseProperties.getProperty(PulseConstants.SYSTEM_PROPERTY_PULSE_USESSL_MANAGER, \"false\"));\n\n\n  if ((sysPulseUseSSLLocator || sysPulseUseSSLManager)) {\n    Properties sslProperties = new Properties();\n    if (!pulseSecurityProperties.isEmpty()) {\n      Set entrySet = pulseSecurityProperties.entrySet();\n      for (Iterator it = entrySet.iterator(); it.hasNext();) {\n        Entry<String, String> entry = (Entry<String, String>) it.next();\n        String key = entry.getKey();\n        if (key.startsWith(\"javax.net.ssl.\")) {\n\n          String val = entry.getValue();\n          System.setProperty(key, val);\n          sslProperties.setProperty(key, val);\n        }\n      }\n    }\n    if (sslProperties.isEmpty()) {\n      logger.warn(resourceBundle.getString(\"LOG_MSG_SSL_NOT_SET\"));\n    }\n  }\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3107,
        "instruction": "@Override\npublic BoolProto existIndexByName(RpcController controller, IndexNameProto request) throws ServiceException {\n\n  String databaseName = request.getDatabaseName();\n  String indexName = request.getIndexName();\n\n  rlock.lock();\n  try {\n    return store.existIndexByName(databaseName, indexName) ? ProtoUtil.TRUE : ProtoUtil.FALSE;\n  } catch (Exception e) {\n    return BoolProto.newBuilder().setValue(false).build();\n  } finally {\n    rlock.unlock();\n  }\n}",
        "accept_response": "@Override\npublic BoolProto existIndexByName(RpcController controller, IndexNameProto request) throws ServiceException {\n\n  String databaseName = request.getDatabaseName();\n  String indexName = request.getIndexName();\n\n  rlock.lock();\n  try {\n    return store.existIndexByName(databaseName, indexName) ? ProtoUtil.TRUE : ProtoUtil.FALSE;\n  } catch (Exception e) {\n    LOG.error(e, e);\n    return BoolProto.newBuilder().setValue(false).build();\n  } finally {\n    rlock.unlock();\n  }\n}",
        "reject_response": "@Override\npublic BoolProto existIndexByName(RpcController controller, IndexNameProto request) throws ServiceException {\n\n  String databaseName = request.getDatabaseName();\n  String indexName = request.getIndexName();\n\n  rlock.lock();\n  try {\n    return store.existIndexByName(databaseName, indexName) ? ProtoUtil.TRUE : ProtoUtil.FALSE;\n  } catch (Exception e) {\n    LOG.error(e);\n    return BoolProto.newBuilder().setValue(false).build();\n  } finally {\n    rlock.unlock();\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3048,
        "instruction": "public void addOnReconnectListener(OnReconnect listener) {\n  if (listener != null) {\n    synchronized (reconnectListeners) {\n      reconnectListeners.add(listener);\n    }\n  }\n}",
        "accept_response": "public void addOnReconnectListener(OnReconnect listener) {\n  if (listener != null) {\n    synchronized (reconnectListeners) {\n      reconnectListeners.add(listener);\n      log.debug(\"Added new OnReconnect listener {}\", listener);\n    }\n  }\n}",
        "reject_response": "public void addOnReconnectListener(OnReconnect listener) {\n  if (listener != null) {\n    synchronized (reconnectListeners) {\n      reconnectListeners.add(listener);\n      log.debug(\"Added new OnReconnect listener \"+listener);\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2421,
        "instruction": "@Override\npublic Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response) throws AuthenticationException, IOException, ServletException {\n  if (StringUtils.isEmpty(getProvidedUrl())) {\n    throw new BadCredentialsException(\"Authentication provider URL must not be null or empty.\");\n  }\n  if (StringUtils.isEmpty(getPublicKey())) {\n    throw new BadCredentialsException(\"Public key for signature validation must be provisioned.\");\n  }\n\n  try {\n    Claims claims = Jwts\n      .parser()\n      .setSigningKey(parseRSAPublicKey(getPublicKey()))\n      .parseClaimsJws(getJWTFromCookie(request))\n      .getBody();\n\n    String userName  = claims.getSubject();\n    LOG.info(\"USERNAME: \" + userName);\n    LOG.info(\"URL = \" + request.getRequestURL());\n    if (StringUtils.isNotEmpty(claims.getAudience()) && !getAudiences().contains(claims.getAudience())) {\n      throw new IllegalArgumentException(String.format(\"Audience validation failed. (Not found: %s)\", claims.getAudience()));\n    }\n    Authentication authentication = new JWTAuthenticationToken(userName, getPublicKey(), getAuthorities());\n    authentication.setAuthenticated(true);\n    SecurityContextHolder.getContext().setAuthentication(authentication);\n    return authentication;\n  } catch (ExpiredJwtException | MalformedJwtException | SignatureException | IllegalArgumentException e) {\n    LOG.info(\"URL = \" + request.getRequestURL());\n    throw new BadCredentialsException(e.getMessage(), e);\n  }\n}",
        "accept_response": "@Override\npublic Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response) throws AuthenticationException, IOException, ServletException {\n  if (StringUtils.isEmpty(getProvidedUrl())) {\n    throw new BadCredentialsException(\"Authentication provider URL must not be null or empty.\");\n  }\n  if (StringUtils.isEmpty(getPublicKey())) {\n    throw new BadCredentialsException(\"Public key for signature validation must be provisioned.\");\n  }\n\n  try {\n    Claims claims = Jwts\n      .parser()\n      .setSigningKey(parseRSAPublicKey(getPublicKey()))\n      .parseClaimsJws(getJWTFromCookie(request))\n      .getBody();\n\n    String userName  = claims.getSubject();\n    LOG.info(\"USERNAME: \" + userName);\n    LOG.info(\"URL = \" + request.getRequestURL());\n    if (StringUtils.isNotEmpty(claims.getAudience()) && !getAudiences().contains(claims.getAudience())) {\n      throw new IllegalArgumentException(String.format(\"Audience validation failed. (Not found: %s)\", claims.getAudience()));\n    }\n    Authentication authentication = new JWTAuthenticationToken(userName, getPublicKey(), getAuthorities());\n    authentication.setAuthenticated(true);\n    SecurityContextHolder.getContext().setAuthentication(authentication);\n    return authentication;\n  } catch (ExpiredJwtException | MalformedJwtException | SignatureException | IllegalArgumentException e) {\n    LOG.info(\"URL = \" + request.getRequestURL());\n    LOG.warn(\"Error during JWT authentication: {}\", e.getMessage());\n    throw new BadCredentialsException(e.getMessage(), e);\n  }\n}",
        "reject_response": "@Override\npublic Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response) throws AuthenticationException, IOException, ServletException {\n  if (StringUtils.isEmpty(getProvidedUrl())) {\n    throw new BadCredentialsException(\"Authentication provider URL must not be null or empty.\");\n  }\n  if (StringUtils.isEmpty(getPublicKey())) {\n    throw new BadCredentialsException(\"Public key for signature validation must be provisioned.\");\n  }\n\n  try {\n    Claims claims = Jwts\n      .parser()\n      .setSigningKey(parseRSAPublicKey(getPublicKey()))\n      .parseClaimsJws(getJWTFromCookie(request))\n      .getBody();\n\n    String userName  = claims.getSubject();\n    LOG.info(\"USERNAME: \" + userName);\n    LOG.info(\"URL = \" + request.getRequestURL());\n    if (StringUtils.isNotEmpty(claims.getAudience()) && !getAudiences().contains(claims.getAudience())) {\n      throw new IllegalArgumentException(String.format(\"Audience validation failed. (Not found: %s)\", claims.getAudience()));\n    }\n    Authentication authentication = new JWTAuthenticationToken(userName, getPublicKey(), getAuthorities());\n    authentication.setAuthenticated(true);\n    SecurityContextHolder.getContext().setAuthentication(authentication);\n    return authentication;\n  } catch (ExpiredJwtException | MalformedJwtException | SignatureException | IllegalArgumentException e) {\n    LOG.info(\"URL = \" + request.getRequestURL());\n    LOG.warn(\"Error during JWT authentication: \", e.getMessage());\n    throw new BadCredentialsException(e.getMessage(), e);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3160,
        "instruction": "protected final void renderComponentTag(ComponentTag tag)\n{\n\tif (needToRenderTag(tag))\n\t{\n\t\t// apply behaviors that are attached to the component tag.\n\t\tif (tag.hasBehaviors())\n\t\t{\n\t\t\tIterator<? extends Behavior> tagBehaviors = tag.getBehaviors();\n\t\t\twhile (tagBehaviors.hasNext())\n\t\t\t{\n\t\t\t\tfinal Behavior behavior = tagBehaviors.next();\n\t\t\t\tif (behavior.isEnabled(this))\n\t\t\t\t{\n\t\t\t\t\tbehavior.onComponentTag(this, tag);\n\t\t\t\t}\n\t\t\t\tbehavior.detach(this);\n\t\t\t}\n\t\t}\n\n\t\t// Apply behavior modifiers\n\t\tList<? extends Behavior> behaviors = getBehaviors();\n\t\tif ((behaviors != null) && !behaviors.isEmpty() && !tag.isClose() &&\n\t\t\t(isIgnoreAttributeModifier() == false))\n\t\t{\n\t\t\ttag = tag.mutable();\n\t\t\tfor (Behavior behavior : behaviors)\n\t\t\t{\n\t\t\t\t// Components may reject some behavior components\n\t\t\t\tif (isBehaviorAccepted(behavior))\n\t\t\t\t{\n\t\t\t\t\tbehavior.onComponentTag(this, tag);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif ((tag instanceof WicketTag) && !tag.isClose() &&\n\t\t\t!getFlag(FLAG_IGNORE_ATTRIBUTE_MODIFIER))\n\t\t{\n\t\t\tExceptionSettings.NotRenderableErrorStrategy notRenderableErrorStrategy = ExceptionSettings.NotRenderableErrorStrategy.LOG_WARNING;\n\t\t\tif (Application.exists())\n\t\t\t{\n\t\t\t\tnotRenderableErrorStrategy = getApplication().getExceptionSettings().getNotRenderableErrorStrategy();\n\t\t\t}\n\n\t\t\tString tagName = tag.getNamespace() + \":\" + tag.getName();\n\t\t\tString componentId = getId();\n\t\t\tif (getFlag(FLAG_OUTPUT_MARKUP_ID))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Markup id set on a component that is usually not rendered into markup. \" +\n\t\t\t\t                               \"Markup id: %s, component id: %s, component tag: %s.\",\n\t\t\t\t                               getMarkupId(), componentId, tagName);\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (getFlag(FLAG_PLACEHOLDER))\n\t\t\t{\n\t\t\t\tString message = String.format(\n\t\t\t\t\t\t\"Placeholder tag set on a component that is usually not rendered into markup. \" +\n\t\t\t\t\t\t\"Component id: %s, component tag: %s.\", componentId, tagName);\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t}\n\n\t\t// Write the tag\n\t\ttag.writeOutput(getResponse(), !needToRenderTag(null),\n\t\t\tgetMarkup().getMarkupResourceStream().getWicketNamespace());\n\t}\n}",
        "accept_response": "protected final void renderComponentTag(ComponentTag tag)\n{\n\tif (needToRenderTag(tag))\n\t{\n\t\t// apply behaviors that are attached to the component tag.\n\t\tif (tag.hasBehaviors())\n\t\t{\n\t\t\tIterator<? extends Behavior> tagBehaviors = tag.getBehaviors();\n\t\t\twhile (tagBehaviors.hasNext())\n\t\t\t{\n\t\t\t\tfinal Behavior behavior = tagBehaviors.next();\n\t\t\t\tif (behavior.isEnabled(this))\n\t\t\t\t{\n\t\t\t\t\tbehavior.onComponentTag(this, tag);\n\t\t\t\t}\n\t\t\t\tbehavior.detach(this);\n\t\t\t}\n\t\t}\n\n\t\t// Apply behavior modifiers\n\t\tList<? extends Behavior> behaviors = getBehaviors();\n\t\tif ((behaviors != null) && !behaviors.isEmpty() && !tag.isClose() &&\n\t\t\t(isIgnoreAttributeModifier() == false))\n\t\t{\n\t\t\ttag = tag.mutable();\n\t\t\tfor (Behavior behavior : behaviors)\n\t\t\t{\n\t\t\t\t// Components may reject some behavior components\n\t\t\t\tif (isBehaviorAccepted(behavior))\n\t\t\t\t{\n\t\t\t\t\tbehavior.onComponentTag(this, tag);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif ((tag instanceof WicketTag) && !tag.isClose() &&\n\t\t\t!getFlag(FLAG_IGNORE_ATTRIBUTE_MODIFIER))\n\t\t{\n\t\t\tExceptionSettings.NotRenderableErrorStrategy notRenderableErrorStrategy = ExceptionSettings.NotRenderableErrorStrategy.LOG_WARNING;\n\t\t\tif (Application.exists())\n\t\t\t{\n\t\t\t\tnotRenderableErrorStrategy = getApplication().getExceptionSettings().getNotRenderableErrorStrategy();\n\t\t\t}\n\n\t\t\tString tagName = tag.getNamespace() + \":\" + tag.getName();\n\t\t\tString componentId = getId();\n\t\t\tif (getFlag(FLAG_OUTPUT_MARKUP_ID))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Markup id set on a component that is usually not rendered into markup. \" +\n\t\t\t\t                               \"Markup id: %s, component id: %s, component tag: %s.\",\n\t\t\t\t                               getMarkupId(), componentId, tagName);\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t\tif (getFlag(FLAG_PLACEHOLDER))\n\t\t\t{\n\t\t\t\tString message = String.format(\n\t\t\t\t\t\t\"Placeholder tag set on a component that is usually not rendered into markup. \" +\n\t\t\t\t\t\t\"Component id: %s, component tag: %s.\", componentId, tagName);\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t}\n\n\t\t// Write the tag\n\t\ttag.writeOutput(getResponse(), !needToRenderTag(null),\n\t\t\tgetMarkup().getMarkupResourceStream().getWicketNamespace());\n\t}\n}",
        "reject_response": "protected final void renderComponentTag(ComponentTag tag)\n{\n\tif (needToRenderTag(tag))\n\t{\n\t\t// apply behaviors that are attached to the component tag.\n\t\tif (tag.hasBehaviors())\n\t\t{\n\t\t\tIterator<? extends Behavior> tagBehaviors = tag.getBehaviors();\n\t\t\twhile (tagBehaviors.hasNext())\n\t\t\t{\n\t\t\t\tfinal Behavior behavior = tagBehaviors.next();\n\t\t\t\tif (behavior.isEnabled(this))\n\t\t\t\t{\n\t\t\t\t\tbehavior.onComponentTag(this, tag);\n\t\t\t\t}\n\t\t\t\tbehavior.detach(this);\n\t\t\t}\n\t\t}\n\n\t\t// Apply behavior modifiers\n\t\tList<? extends Behavior> behaviors = getBehaviors();\n\t\tif ((behaviors != null) && !behaviors.isEmpty() && !tag.isClose() &&\n\t\t\t(isIgnoreAttributeModifier() == false))\n\t\t{\n\t\t\ttag = tag.mutable();\n\t\t\tfor (Behavior behavior : behaviors)\n\t\t\t{\n\t\t\t\t// Components may reject some behavior components\n\t\t\t\tif (isBehaviorAccepted(behavior))\n\t\t\t\t{\n\t\t\t\t\tbehavior.onComponentTag(this, tag);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif ((tag instanceof WicketTag) && !tag.isClose() &&\n\t\t\t!getFlag(FLAG_IGNORE_ATTRIBUTE_MODIFIER))\n\t\t{\n\t\t\tExceptionSettings.NotRenderableErrorStrategy notRenderableErrorStrategy = ExceptionSettings.NotRenderableErrorStrategy.LOG_WARNING;\n\t\t\tif (Application.exists())\n\t\t\t{\n\t\t\t\tnotRenderableErrorStrategy = getApplication().getExceptionSettings().getNotRenderableErrorStrategy();\n\t\t\t}\n\n\t\t\tString tagName = tag.getNamespace() + \":\" + tag.getName();\n\t\t\tString componentId = getId();\n\t\t\tif (getFlag(FLAG_OUTPUT_MARKUP_ID))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Markup id set on a component that is usually not rendered into markup. \" +\n\t\t\t\t                               \"Markup id: %s, component id: %s, component tag: %s.\",\n\t\t\t\t                               getMarkupId(), componentId, tagName);\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(String.format(\n\t\t\t\t\t\"Markup id set on a component that is usually not rendered into markup. \"\n\t\t\t\t\t\t+ \"Markup id: %s, component id: %s, component tag: %s.\", getMarkupId(),\n\t\t\t\t\tgetId(), tag.getName()));\n\t\t\t}\n\t\t\tif (getFlag(FLAG_PLACEHOLDER))\n\t\t\t{\n\t\t\t\tString message = String.format(\n\t\t\t\t\t\t\"Placeholder tag set on a component that is usually not rendered into markup. \" +\n\t\t\t\t\t\t\"Component id: %s, component tag: %s.\", componentId, tagName);\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t}\n\n\t\t// Write the tag\n\t\ttag.writeOutput(getResponse(), !needToRenderTag(null),\n\t\t\tgetMarkup().getMarkupResourceStream().getWicketNamespace());\n\t}\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2685,
        "instruction": "@Override\npublic boolean isThreadSafe() {\n    boolean result = ((Plugin) fileAccessor).isThreadSafe()\n            && ((Plugin) fieldsResolver).isThreadSafe();\n    return result;\n}",
        "accept_response": "@Override\npublic boolean isThreadSafe() {\n    boolean result = ((Plugin) fileAccessor).isThreadSafe()\n            && ((Plugin) fieldsResolver).isThreadSafe();\n    LOG.debug(\"Bridge is \" + (result ? \"\" : \"not \") + \"thread safe\");\n    return result;\n}",
        "reject_response": "@Override\npublic boolean isThreadSafe() {\n    boolean result = ((Plugin) fileAccessor).isThreadSafe()\n            && ((Plugin) fieldsResolver).isThreadSafe();\n    Log.debug(\"Bridge is \" + (result ? \"\" : \"not \") + \"thread safe\");\n    return result;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3078,
        "instruction": "@Override\n/** {@inheritDoc} */\npublic int run(SqoopOptions options) {\n\n  if (!init(options)) {\n    return 1;\n  }\n\n  codeGenerator.setManager(manager);\n\n  if (options.getUpdateKeyCol() != null) {\n    manager.configureDbOutputColumns(options);\n  }\n\n  try {\n    exportTable(options, options.getTableName());\n  } catch (IOException ioe) {\n    if (System.getProperty(Sqoop.SQOOP_RETHROW_PROPERTY) != null) {\n      throw new RuntimeException(ioe);\n    } else {\n      return 1;\n    }\n  } catch (ExportException ee) {\n    LOG.error(\"Error during export: \", ee);\n    if (System.getProperty(Sqoop.SQOOP_RETHROW_PROPERTY) != null) {\n      throw new RuntimeException(ee);\n    } else {\n      return 1;\n    }\n  } finally {\n    destroy(options);\n  }\n\n  return 0;\n}",
        "accept_response": "@Override\n/** {@inheritDoc} */\npublic int run(SqoopOptions options) {\n\n  if (!init(options)) {\n    return 1;\n  }\n\n  codeGenerator.setManager(manager);\n\n  if (options.getUpdateKeyCol() != null) {\n    manager.configureDbOutputColumns(options);\n  }\n\n  try {\n    exportTable(options, options.getTableName());\n  } catch (IOException ioe) {\n    LOG.error(\"Encountered IOException running export job: \", ioe);\n    if (System.getProperty(Sqoop.SQOOP_RETHROW_PROPERTY) != null) {\n      throw new RuntimeException(ioe);\n    } else {\n      return 1;\n    }\n  } catch (ExportException ee) {\n    LOG.error(\"Error during export: \", ee);\n    if (System.getProperty(Sqoop.SQOOP_RETHROW_PROPERTY) != null) {\n      throw new RuntimeException(ee);\n    } else {\n      return 1;\n    }\n  } finally {\n    destroy(options);\n  }\n\n  return 0;\n}",
        "reject_response": "@Override\n/** {@inheritDoc} */\npublic int run(SqoopOptions options) {\n\n  if (!init(options)) {\n    return 1;\n  }\n\n  codeGenerator.setManager(manager);\n\n  if (options.getUpdateKeyCol() != null) {\n    manager.configureDbOutputColumns(options);\n  }\n\n  try {\n    exportTable(options, options.getTableName());\n  } catch (IOException ioe) {\n    LOG.error(\"Encountered IOException running export job: \"\n        + ioe.toString());\n    if (System.getProperty(Sqoop.SQOOP_RETHROW_PROPERTY) != null) {\n      throw new RuntimeException(ioe);\n    } else {\n      return 1;\n    }\n  } catch (ExportException ee) {\n    LOG.error(\"Error during export: \", ee);\n    if (System.getProperty(Sqoop.SQOOP_RETHROW_PROPERTY) != null) {\n      throw new RuntimeException(ee);\n    } else {\n      return 1;\n    }\n  } finally {\n    destroy(options);\n  }\n\n  return 0;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2842,
        "instruction": "private void sendAllPartitions(\n    Collection<ClusterNode> nodes,\n    AffinityTopologyVersion msgTopVer\n) {\n    long time = System.currentTimeMillis();\n\n    GridDhtPartitionsFullMessage m = createPartitionsFullMessage(true, false, null, null, null, null);\n\n    m.topologyVersion(msgTopVer);\n\n    if (log.isInfoEnabled())\n        log.info(\"Full Message creating for \" + msgTopVer + \" performed in \" + (System.currentTimeMillis() - time) + \" ms.\");\n\n    if (log.isTraceEnabled())\n\n    time = System.currentTimeMillis();\n\n    for (ClusterNode node : nodes) {\n        try {\n            assert !node.equals(cctx.localNode());\n\n            cctx.io().sendNoRetry(node, m, SYSTEM_POOL);\n        }\n        catch (ClusterTopologyCheckedException ignore) {\n            if (log.isDebugEnabled())\n                log.debug(\"Failed to send partition update to node because it left grid (will ignore) [node=\" +\n                    node.id() + \", msg=\" + m + ']');\n        }\n        catch (IgniteCheckedException e) {\n            U.warn(log, \"Failed to send partitions full message [node=\" + node + \", err=\" + e + ']');\n        }\n    }\n\n    if (log.isInfoEnabled())\n        log.info(\"Sending Full Message for \" + msgTopVer + \" performed in \" + (System.currentTimeMillis() - time) + \" ms.\");\n}",
        "accept_response": "private void sendAllPartitions(\n    Collection<ClusterNode> nodes,\n    AffinityTopologyVersion msgTopVer\n) {\n    long time = System.currentTimeMillis();\n\n    GridDhtPartitionsFullMessage m = createPartitionsFullMessage(true, false, null, null, null, null);\n\n    m.topologyVersion(msgTopVer);\n\n    if (log.isInfoEnabled())\n        log.info(\"Full Message creating for \" + msgTopVer + \" performed in \" + (System.currentTimeMillis() - time) + \" ms.\");\n\n    if (log.isTraceEnabled())\n        log.trace(\"Sending all partitions [nodeIds=\" + U.nodeIds(nodes) + \", msg=\" + m + ']');\n\n    time = System.currentTimeMillis();\n\n    for (ClusterNode node : nodes) {\n        try {\n            assert !node.equals(cctx.localNode());\n\n            cctx.io().sendNoRetry(node, m, SYSTEM_POOL);\n        }\n        catch (ClusterTopologyCheckedException ignore) {\n            if (log.isDebugEnabled())\n                log.debug(\"Failed to send partition update to node because it left grid (will ignore) [node=\" +\n                    node.id() + \", msg=\" + m + ']');\n        }\n        catch (IgniteCheckedException e) {\n            U.warn(log, \"Failed to send partitions full message [node=\" + node + \", err=\" + e + ']');\n        }\n    }\n\n    if (log.isInfoEnabled())\n        log.info(\"Sending Full Message for \" + msgTopVer + \" performed in \" + (System.currentTimeMillis() - time) + \" ms.\");\n}",
        "reject_response": "private void sendAllPartitions(\n    Collection<ClusterNode> nodes,\n    AffinityTopologyVersion msgTopVer\n) {\n    long time = System.currentTimeMillis();\n\n    GridDhtPartitionsFullMessage m = createPartitionsFullMessage(true, false, null, null, null, null);\n\n    m.topologyVersion(msgTopVer);\n\n    if (log.isInfoEnabled())\n        log.info(\"Full Message creating for \" + msgTopVer + \" performed in \" + (System.currentTimeMillis() - time) + \" ms.\");\n\n    if (log.isTraceEnabled())\n    if (log.isDebugEnabled())\n        log.debug(\"Sending all partitions [nodeIds=\" + U.nodeIds(nodes) + \", msg=\" + m + ']');\n\n    time = System.currentTimeMillis();\n\n    for (ClusterNode node : nodes) {\n        try {\n            assert !node.equals(cctx.localNode());\n\n            cctx.io().sendNoRetry(node, m, SYSTEM_POOL);\n        }\n        catch (ClusterTopologyCheckedException ignore) {\n            if (log.isDebugEnabled())\n                log.debug(\"Failed to send partition update to node because it left grid (will ignore) [node=\" +\n                    node.id() + \", msg=\" + m + ']');\n        }\n        catch (IgniteCheckedException e) {\n            U.warn(log, \"Failed to send partitions full message [node=\" + node + \", err=\" + e + ']');\n        }\n    }\n\n    if (log.isInfoEnabled())\n        log.info(\"Sending Full Message for \" + msgTopVer + \" performed in \" + (System.currentTimeMillis() - time) + \" ms.\");\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3255,
        "instruction": "private List<OpResult> validatePath(Iterable<Op> ops) {\n    List<OpResult> results = new ArrayList<OpResult>();\n    boolean error = false;\n    for (Op op : ops) {\n        try {\n            op.validate();\n        } catch (IllegalArgumentException iae) {\n            LOG.error(\"Unexpected exception\", iae);\n            ErrorResult err = new ErrorResult(KeeperException.Code.BADARGUMENTS.intValue());\n            results.add(err);\n            error = true;\n            continue;\n        } catch (KeeperException ke) {\n            ErrorResult err = new ErrorResult(ke.code().intValue());\n            results.add(err);\n            error = true;\n            continue;\n        }\n        ErrorResult err = new ErrorResult(KeeperException.Code.RUNTIMEINCONSISTENCY.intValue());\n        results.add(err);\n    }\n    if (!error) {\n        results.clear();\n    }\n    return results;\n}",
        "accept_response": "private List<OpResult> validatePath(Iterable<Op> ops) {\n    List<OpResult> results = new ArrayList<OpResult>();\n    boolean error = false;\n    for (Op op : ops) {\n        try {\n            op.validate();\n        } catch (IllegalArgumentException iae) {\n            LOG.error(\"Unexpected exception\", iae);\n            ErrorResult err = new ErrorResult(KeeperException.Code.BADARGUMENTS.intValue());\n            results.add(err);\n            error = true;\n            continue;\n        } catch (KeeperException ke) {\n            LOG.error(\"Unexpected exception\", ke);\n            ErrorResult err = new ErrorResult(ke.code().intValue());\n            results.add(err);\n            error = true;\n            continue;\n        }\n        ErrorResult err = new ErrorResult(KeeperException.Code.RUNTIMEINCONSISTENCY.intValue());\n        results.add(err);\n    }\n    if (!error) {\n        results.clear();\n    }\n    return results;\n}",
        "reject_response": "private List<OpResult> validatePath(Iterable<Op> ops) {\n    List<OpResult> results = new ArrayList<OpResult>();\n    boolean error = false;\n    for (Op op : ops) {\n        try {\n            op.validate();\n        } catch (IllegalArgumentException iae) {\n            LOG.error(\"Unexpected exception\", iae);\n            ErrorResult err = new ErrorResult(KeeperException.Code.BADARGUMENTS.intValue());\n            results.add(err);\n            error = true;\n            continue;\n        } catch (KeeperException ke) {\n            LOG.error(\"KeeperException: \" + ke.getMessage());\n            ErrorResult err = new ErrorResult(ke.code().intValue());\n            results.add(err);\n            error = true;\n            continue;\n        }\n        ErrorResult err = new ErrorResult(KeeperException.Code.RUNTIMEINCONSISTENCY.intValue());\n        results.add(err);\n    }\n    if (!error) {\n        results.clear();\n    }\n    return results;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3076,
        "instruction": "protected ResultSet executeQuery(String query) throws SQLException {\n  this.statement = connection.prepareStatement(query,\n      ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);\n\n  Integer fetchSize = dbConf.getFetchSize();\n  if (fetchSize != null) {\n    LOG.debug(\"Using fetchSize for next query: \" + fetchSize);\n    statement.setFetchSize(fetchSize);\n  }\n\n  return statement.executeQuery();\n}",
        "accept_response": "protected ResultSet executeQuery(String query) throws SQLException {\n  this.statement = connection.prepareStatement(query,\n      ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);\n\n  Integer fetchSize = dbConf.getFetchSize();\n  if (fetchSize != null) {\n    LOG.debug(\"Using fetchSize for next query: \" + fetchSize);\n    statement.setFetchSize(fetchSize);\n  }\n\n  LOG.info(\"Executing query: \" + query);\n  return statement.executeQuery();\n}",
        "reject_response": "protected ResultSet executeQuery(String query) throws SQLException {\n  this.statement = connection.prepareStatement(query,\n      ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);\n\n  Integer fetchSize = dbConf.getFetchSize();\n  if (fetchSize != null) {\n    LOG.debug(\"Using fetchSize for next query: \" + fetchSize);\n    statement.setFetchSize(fetchSize);\n  }\n\n  LOG.debug(\"Executing query: \" + query);\n  return statement.executeQuery();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3174,
        "instruction": "protected void validate(Set<NodeLabel> nodeLabels)\n    throws IOException {\n  Iterator<NodeLabel> iterator = nodeLabels.iterator();\n  boolean hasInvalidLabel = false;\n  StringBuilder errorMsg = new StringBuilder();\n  while (iterator.hasNext()) {\n    try {\n      NodeLabelUtil\n          .checkAndThrowLabelName(iterator.next().getName());\n    } catch (IOException e) {\n      errorMsg.append(e.getMessage());\n      errorMsg.append(\" , \");\n      hasInvalidLabel = true;\n    }\n  }\n  if (hasInvalidLabel) {\n    throw new IOException(errorMsg.toString());\n  }\n}",
        "accept_response": "protected void validate(Set<NodeLabel> nodeLabels)\n    throws IOException {\n  Iterator<NodeLabel> iterator = nodeLabels.iterator();\n  boolean hasInvalidLabel = false;\n  StringBuilder errorMsg = new StringBuilder();\n  while (iterator.hasNext()) {\n    try {\n      NodeLabelUtil\n          .checkAndThrowLabelName(iterator.next().getName());\n    } catch (IOException e) {\n      errorMsg.append(e.getMessage());\n      errorMsg.append(\" , \");\n      hasInvalidLabel = true;\n    }\n  }\n  if (hasInvalidLabel) {\n    LOG.error(\"Invalid Node Label(s) from Provider : {}.\", errorMsg);\n    throw new IOException(errorMsg.toString());\n  }\n}",
        "reject_response": "protected void validate(Set<NodeLabel> nodeLabels)\n    throws IOException {\n  Iterator<NodeLabel> iterator = nodeLabels.iterator();\n  boolean hasInvalidLabel = false;\n  StringBuilder errorMsg = new StringBuilder();\n  while (iterator.hasNext()) {\n    try {\n      NodeLabelUtil\n          .checkAndThrowLabelName(iterator.next().getName());\n    } catch (IOException e) {\n      errorMsg.append(e.getMessage());\n      errorMsg.append(\" , \");\n      hasInvalidLabel = true;\n    }\n  }\n  if (hasInvalidLabel) {\n    LOG.error(\"Invalid Node Label(s) from Provider : \" + errorMsg);\n    throw new IOException(errorMsg.toString());\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2825,
        "instruction": "public void start() {\n    if (getBoolean(IGNITE_JVM_PAUSE_DETECTOR_DISABLED, false)) {\n        if (log.isDebugEnabled())\n            log.debug(\"JVM Pause Detector is disabled.\");\n\n        return;\n    }\n\n    final Thread worker = new Thread(\"jvm-pause-detector-worker\") {\n        private long prev = System.currentTimeMillis();\n\n        @Override public void run() {\n            if (log.isDebugEnabled())\n                log.debug(getName() + \" has been started.\");\n\n            while (true) {\n                try {\n                    Thread.sleep(PRECISION);\n\n                    final long now = System.currentTimeMillis();\n                    final long pause = now - PRECISION - prev;\n\n                    prev = now;\n\n                    if (pause >= THRESHOLD) {\n                        log.warning(\"Possible too long JVM pause: \" + pause + \" milliseconds.\");\n\n                        synchronized (LongJVMPauseDetector.this) {\n                            final int next = (int)(longPausesCnt % EVT_CNT);\n\n                            longPausesCnt++;\n\n                            longPausesTotalDuration += pause;\n\n                            longPausesTimestamps[next] = now;\n\n                            longPausesDurations[next] = pause;\n                        }\n                    }\n                }\n                catch (InterruptedException e) {\n                    log.error(getName() + \" has been interrupted\", e);\n\n                    break;\n                }\n            }\n        }\n    };\n\n    if (!workerRef.compareAndSet(null, worker)) {\n\n        return;\n    }\n\n    worker.setDaemon(true);\n    worker.start();\n\n    if (log.isDebugEnabled())\n        log.debug(\"LongJVMPauseDetector was successfully started\");\n}",
        "accept_response": "public void start() {\n    if (getBoolean(IGNITE_JVM_PAUSE_DETECTOR_DISABLED, false)) {\n        if (log.isDebugEnabled())\n            log.debug(\"JVM Pause Detector is disabled.\");\n\n        return;\n    }\n\n    final Thread worker = new Thread(\"jvm-pause-detector-worker\") {\n        private long prev = System.currentTimeMillis();\n\n        @Override public void run() {\n            if (log.isDebugEnabled())\n                log.debug(getName() + \" has been started.\");\n\n            while (true) {\n                try {\n                    Thread.sleep(PRECISION);\n\n                    final long now = System.currentTimeMillis();\n                    final long pause = now - PRECISION - prev;\n\n                    prev = now;\n\n                    if (pause >= THRESHOLD) {\n                        log.warning(\"Possible too long JVM pause: \" + pause + \" milliseconds.\");\n\n                        synchronized (LongJVMPauseDetector.this) {\n                            final int next = (int)(longPausesCnt % EVT_CNT);\n\n                            longPausesCnt++;\n\n                            longPausesTotalDuration += pause;\n\n                            longPausesTimestamps[next] = now;\n\n                            longPausesDurations[next] = pause;\n                        }\n                    }\n                }\n                catch (InterruptedException e) {\n                    log.error(getName() + \" has been interrupted\", e);\n\n                    break;\n                }\n            }\n        }\n    };\n\n    if (!workerRef.compareAndSet(null, worker)) {\n        log.warning(LongJVMPauseDetector.class.getSimpleName() + \" already started!\");\n\n        return;\n    }\n\n    worker.setDaemon(true);\n    worker.start();\n\n    if (log.isDebugEnabled())\n        log.debug(\"LongJVMPauseDetector was successfully started\");\n}",
        "reject_response": "public void start() {\n    if (getBoolean(IGNITE_JVM_PAUSE_DETECTOR_DISABLED, false)) {\n        if (log.isDebugEnabled())\n            log.debug(\"JVM Pause Detector is disabled.\");\n\n        return;\n    }\n\n    final Thread worker = new Thread(\"jvm-pause-detector-worker\") {\n        private long prev = System.currentTimeMillis();\n\n        @Override public void run() {\n            if (log.isDebugEnabled())\n                log.debug(getName() + \" has been started.\");\n\n            while (true) {\n                try {\n                    Thread.sleep(PRECISION);\n\n                    final long now = System.currentTimeMillis();\n                    final long pause = now - PRECISION - prev;\n\n                    prev = now;\n\n                    if (pause >= THRESHOLD) {\n                        log.warning(\"Possible too long JVM pause: \" + pause + \" milliseconds.\");\n\n                        synchronized (LongJVMPauseDetector.this) {\n                            final int next = (int)(longPausesCnt % EVT_CNT);\n\n                            longPausesCnt++;\n\n                            longPausesTotalDuration += pause;\n\n                            longPausesTimestamps[next] = now;\n\n                            longPausesDurations[next] = pause;\n                        }\n                    }\n                }\n                catch (InterruptedException e) {\n                    log.error(getName() + \" has been interrupted\", e);\n\n                    break;\n                }\n            }\n        }\n    };\n\n    if (!workerRef.compareAndSet(null, worker)) {\n        LOG.warning(LongJVMPauseDetector.class.getSimpleName() + \" already started!\");\n\n        return;\n    }\n\n    worker.setDaemon(true);\n    worker.start();\n\n    if (log.isDebugEnabled())\n        log.debug(\"LongJVMPauseDetector was successfully started\");\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2893,
        "instruction": "public Node parse(InputStream inputStream) throws ParseException {\n    try {\n        final SimpleNode node = new SieveParser(inputStream, \"UTF-8\")\n                .start();\n        SieveValidationVisitor visitor = new SieveValidationVisitor(\n                commandManager, testManager, comparatorManager);\n        node.jjtAccept(visitor, null);\n        return node;\n    } catch (ParseException ex) {\n        throw ex;\n    } catch (SieveException ex) {\n        LOGGER.error(\"Parse failed.\", ex);\n        throw new ParseException(ex.getMessage());\n    }\n}",
        "accept_response": "public Node parse(InputStream inputStream) throws ParseException {\n    try {\n        final SimpleNode node = new SieveParser(inputStream, \"UTF-8\")\n                .start();\n        SieveValidationVisitor visitor = new SieveValidationVisitor(\n                commandManager, testManager, comparatorManager);\n        node.jjtAccept(visitor, null);\n        return node;\n    } catch (ParseException ex) {\n        LOGGER.error(\"Parse failed.\", ex);\n        throw ex;\n    } catch (SieveException ex) {\n        LOGGER.error(\"Parse failed.\", ex);\n        throw new ParseException(ex.getMessage());\n    }\n}",
        "reject_response": "public Node parse(InputStream inputStream) throws ParseException {\n    try {\n        final SimpleNode node = new SieveParser(inputStream, \"UTF-8\")\n                .start();\n        SieveValidationVisitor visitor = new SieveValidationVisitor(\n                commandManager, testManager, comparatorManager);\n        node.jjtAccept(visitor, null);\n        return node;\n    } catch (ParseException ex) {\n        if (log.isErrorEnabled())\n            log.error(\"Parse failed. Reason: \" + ex.getMessage());\n        if (log.isDebugEnabled())\n            log.debug(\"Parse failed.\", ex);\n        throw ex;\n    } catch (SieveException ex) {\n        LOGGER.error(\"Parse failed.\", ex);\n        throw new ParseException(ex.getMessage());\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2998,
        "instruction": "public static Map<String, Object> connectionTest(String serviceName, Map<String, String> configs) throws Exception {\n  Map<String, Object> ret = null;\n\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"==> PrestoResourceManager.connectionTest() ServiceName: \" + serviceName + \" Configs: \" + configs);\n  }\n\n  try {\n    ret = PrestoClient.connectionTest(serviceName, configs);\n  } catch (Exception e) {\n    throw e;\n  }\n\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"<== PrestoResourceManager.connectionTest() Result : \" + ret);\n  }\n\n  return ret;\n}",
        "accept_response": "public static Map<String, Object> connectionTest(String serviceName, Map<String, String> configs) throws Exception {\n  Map<String, Object> ret = null;\n\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"==> PrestoResourceManager.connectionTest() ServiceName: \" + serviceName + \" Configs: \" + configs);\n  }\n\n  try {\n    ret = PrestoClient.connectionTest(serviceName, configs);\n  } catch (Exception e) {\n    LOG.error(\"<== PrestoResourceManager.connectionTest() Error: \" + e);\n    throw e;\n  }\n\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"<== PrestoResourceManager.connectionTest() Result : \" + ret);\n  }\n\n  return ret;\n}",
        "reject_response": "public static Map<String, Object> connectionTest(String serviceName, Map<String, String> configs) throws Exception {\n  Map<String, Object> ret = null;\n\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"==> PrestoResourceManager.connectionTest() ServiceName: \" + serviceName + \" Configs: \" + configs);\n  }\n\n  try {\n    ret = PrestoClient.connectionTest(serviceName, configs);\n  } catch (Exception e) {\n    LOG.error(\"<== PrestoResourceManager.connectionTest Error: \" + e);\n    throw e;\n  }\n\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"<== PrestoResourceManager.connectionTest() Result : \" + ret);\n  }\n\n  return ret;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2636,
        "instruction": "private boolean isConnected() {\n  // Reference to repository\n  Repository repository = Repository.get();\n  if (repository.getIsEmbeddedMode()) {\n    if (this.mbs == null) {\n      this.mbs = ManagementFactory.getPlatformMBeanServer();\n      cluster.setConnectedFlag(true);\n    }\n  } else {\n    try {\n      if (this.conn == null) {\n        cluster.setConnectedFlag(false);\n        cluster.setConnectionErrorMsg(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_NOT_FOUND\")\n            + \" \" + resourceBundle.getString(\"LOG_MSG_JMX_GETTING_NEW_CONNECTION\"));\n        this.conn = getJMXConnection();\n        if (this.conn != null) {\n          this.mbs = this.conn.getMBeanServerConnection();\n          cluster.setConnectedFlag(true);\n        } else {\n          logger.info(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_NOT_FOUND\"));\n          return false;\n        }\n      } else {\n        logger.debug(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_IS_AVAILABLE\"));\n        cluster.setConnectedFlag(true);\n        if (this.mbs == null) {\n          this.mbs = this.conn.getMBeanServerConnection();\n        }\n      }\n    } catch (Exception e) {\n      this.mbs = null;\n      if (this.conn != null) {\n        try {\n          this.conn.close();\n        } catch (Exception e1) {\n          logger.fatal(e);\n        }\n      }\n      this.conn = null;\n      return false;\n    }\n  }\n\n  return true;\n}",
        "accept_response": "private boolean isConnected() {\n  // Reference to repository\n  Repository repository = Repository.get();\n  if (repository.getIsEmbeddedMode()) {\n    if (this.mbs == null) {\n      this.mbs = ManagementFactory.getPlatformMBeanServer();\n      cluster.setConnectedFlag(true);\n    }\n  } else {\n    try {\n      if (this.conn == null) {\n        cluster.setConnectedFlag(false);\n        cluster.setConnectionErrorMsg(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_NOT_FOUND\")\n            + \" \" + resourceBundle.getString(\"LOG_MSG_JMX_GETTING_NEW_CONNECTION\"));\n        logger.debug(\"{} {}\", resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_NOT_FOUND\"),\n            resourceBundle.getString(\"LOG_MSG_JMX_GET_NEW_CONNECTION\"));\n        this.conn = getJMXConnection();\n        if (this.conn != null) {\n          this.mbs = this.conn.getMBeanServerConnection();\n          cluster.setConnectedFlag(true);\n        } else {\n          logger.info(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_NOT_FOUND\"));\n          return false;\n        }\n      } else {\n        logger.debug(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_IS_AVAILABLE\"));\n        cluster.setConnectedFlag(true);\n        if (this.mbs == null) {\n          this.mbs = this.conn.getMBeanServerConnection();\n        }\n      }\n    } catch (Exception e) {\n      this.mbs = null;\n      if (this.conn != null) {\n        try {\n          this.conn.close();\n        } catch (Exception e1) {\n          logger.fatal(e);\n        }\n      }\n      this.conn = null;\n      return false;\n    }\n  }\n\n  return true;\n}",
        "reject_response": "private boolean isConnected() {\n  // Reference to repository\n  Repository repository = Repository.get();\n  if (repository.getIsEmbeddedMode()) {\n    if (this.mbs == null) {\n      this.mbs = ManagementFactory.getPlatformMBeanServer();\n      cluster.setConnectedFlag(true);\n    }\n  } else {\n    try {\n      if (this.conn == null) {\n        cluster.setConnectedFlag(false);\n        cluster.setConnectionErrorMsg(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_NOT_FOUND\")\n            + \" \" + resourceBundle.getString(\"LOG_MSG_JMX_GETTING_NEW_CONNECTION\"));\n        if (LOGGER.fineEnabled()) {\n          LOGGER.fine(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_NOT_FOUND\") + \" \"\n              + resourceBundle.getString(\"LOG_MSG_JMX_GET_NEW_CONNECTION\"));\n        }\n        this.conn = getJMXConnection();\n        if (this.conn != null) {\n          this.mbs = this.conn.getMBeanServerConnection();\n          cluster.setConnectedFlag(true);\n        } else {\n          logger.info(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_NOT_FOUND\"));\n          return false;\n        }\n      } else {\n        logger.debug(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_IS_AVAILABLE\"));\n        cluster.setConnectedFlag(true);\n        if (this.mbs == null) {\n          this.mbs = this.conn.getMBeanServerConnection();\n        }\n      }\n    } catch (Exception e) {\n      this.mbs = null;\n      if (this.conn != null) {\n        try {\n          this.conn.close();\n        } catch (Exception e1) {\n          logger.fatal(e);\n        }\n      }\n      this.conn = null;\n      return false;\n    }\n  }\n\n  return true;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2666,
        "instruction": "private boolean barrierOnWorkerList(String finishedWorkerPath,\n    List<WorkerInfo> workerInfoList,\n    BspEvent event,\n    boolean ignoreDeath) {\n  try {\n    getZkExt().createOnceExt(finishedWorkerPath,\n        null,\n        Ids.OPEN_ACL_UNSAFE,\n        CreateMode.PERSISTENT,\n        true);\n  } catch (KeeperException e) {\n    throw new IllegalStateException(\n        \"barrierOnWorkerList: KeeperException - Couldn't create \" +\n            finishedWorkerPath, e);\n  } catch (InterruptedException e) {\n    throw new IllegalStateException(\n        \"barrierOnWorkerList: InterruptedException - Couldn't create \" +\n            finishedWorkerPath, e);\n  }\n  List<String> hostnameIdList =\n      new ArrayList<String>(workerInfoList.size());\n  for (WorkerInfo workerInfo : workerInfoList) {\n    hostnameIdList.add(workerInfo.getHostnameId());\n  }\n  String workerInfoHealthyPath =\n      getWorkerInfoHealthyPath(getApplicationAttempt(), getSuperstep());\n  List<String> finishedHostnameIdList = new ArrayList<>();\n  List<String> tmpFinishedHostnameIdList;\n  long nextInfoMillis = System.currentTimeMillis();\n  final int defaultTaskTimeoutMsec = 10 * 60 * 1000;  // from TaskTracker\n  final int waitBetweenLogInfoMsec = 30 * 1000;\n  final int taskTimeoutMsec = getContext().getConfiguration().getInt(\n      \"mapred.task.timeout\", defaultTaskTimeoutMsec) / 2;\n  long lastRegularRunTimeMsec = 0;\n  int eventLoopTimeout =  Math.min(taskTimeoutMsec, waitBetweenLogInfoMsec);\n  boolean logInfoOnlyRun = false;\n  List<WorkerInfo> deadWorkers = new ArrayList<>();\n  while (true) {\n    if (! logInfoOnlyRun) {\n      try {\n        tmpFinishedHostnameIdList =\n            getZkExt().getChildrenExt(finishedWorkerPath,\n                                      true,\n                                      false,\n                                      false);\n      } catch (KeeperException e) {\n        throw new IllegalStateException(\n            \"barrierOnWorkerList: KeeperException - Couldn't get \" +\n                \"children of \" + finishedWorkerPath, e);\n      } catch (InterruptedException e) {\n        throw new IllegalStateException(\n            \"barrierOnWorkerList: IllegalException - Couldn't get \" +\n                \"children of \" + finishedWorkerPath, e);\n      }\n      if (LOG.isDebugEnabled()) {\n        // Log the names of the new workers that have finished since last time\n        Set<String> newFinishedHostnames = Sets.difference(\n          Sets.newHashSet(tmpFinishedHostnameIdList),\n          Sets.newHashSet(finishedHostnameIdList));\n      }\n      finishedHostnameIdList = tmpFinishedHostnameIdList;\n    }\n\n    if (LOG.isInfoEnabled() &&\n        (System.currentTimeMillis() > nextInfoMillis)) {\n      nextInfoMillis = System.currentTimeMillis() + waitBetweenLogInfoMsec;\n      LOG.info(\"barrierOnWorkerList: \" +\n          finishedHostnameIdList.size() +\n          \" out of \" + workerInfoList.size() +\n          \" workers finished on superstep \" +\n          getSuperstep() + \" on path \" + finishedWorkerPath);\n      if (workerInfoList.size() - finishedHostnameIdList.size() <\n          MAX_PRINTABLE_REMAINING_WORKERS) {\n        Set<String> remainingWorkers = Sets.newHashSet(hostnameIdList);\n        remainingWorkers.removeAll(finishedHostnameIdList);\n        LOG.info(\"barrierOnWorkerList: Waiting on \" + remainingWorkers);\n      }\n    }\n\n    if (! logInfoOnlyRun) {\n      getContext().setStatus(getGraphTaskManager().getGraphFunctions() +\n                                 \" - \" +\n                                 finishedHostnameIdList.size() +\n                                 \" finished out of \" +\n                                 workerInfoList.size() +\n                                 \" on superstep \" + getSuperstep());\n      if (finishedHostnameIdList.containsAll(hostnameIdList)) {\n        break;\n      }\n\n      for (WorkerInfo deadWorker : deadWorkers) {\n        if (!finishedHostnameIdList.contains(deadWorker.getHostnameId())) {\n          LOG.error(\"barrierOnWorkerList: no results arived from \" +\n                        \"worker that was pronounced dead: \" + deadWorker +\n                        \" on superstep \" + getSuperstep());\n          return false;\n        }\n      }\n\n      // wall-clock time skew is ignored\n      lastRegularRunTimeMsec = System.currentTimeMillis();\n    }\n\n    // Wait for a signal or timeout\n    boolean eventTriggered = event.waitMsecs(eventLoopTimeout);\n    long elapsedTimeSinceRegularRunMsec = System.currentTimeMillis() -\n        lastRegularRunTimeMsec;\n    event.reset();\n    getContext().progress();\n\n    if (eventTriggered ||\n        taskTimeoutMsec == eventLoopTimeout ||\n        elapsedTimeSinceRegularRunMsec >= taskTimeoutMsec) {\n      logInfoOnlyRun = false;\n    } else {\n      logInfoOnlyRun = true;\n      continue;\n    }\n\n    // Did a worker die?\n    try {\n      deadWorkers.addAll(superstepChosenWorkerAlive(\n              workerInfoHealthyPath,\n              workerInfoList));\n      if (!ignoreDeath && deadWorkers.size() > 0) {\n        String errorMessage = \"******* WORKERS \" + deadWorkers +\n            \" FAILED *******\";\n        // If checkpointing is not used, we should fail the job\n        if (!getConfiguration().useCheckpointing()) {\n          setJobStateFailed(errorMessage);\n        } else {\n          LOG.error(\"barrierOnWorkerList: Missing chosen \" +\n              \"workers \" + deadWorkers +\n              \" on superstep \" + getSuperstep());\n          // Log worker failure to command line\n          getGraphTaskManager().getJobProgressTracker().logInfo(errorMessage);\n        }\n        return false;\n      }\n    } catch (KeeperException e) {\n      throw new IllegalStateException(\n          \"barrierOnWorkerList: KeeperException - \" +\n              \"Couldn't get \" + workerInfoHealthyPath, e);\n    } catch (InterruptedException e) {\n      throw new IllegalStateException(\n          \"barrierOnWorkerList: InterruptedException - \" +\n              \"Couldn't get \" + workerInfoHealthyPath, e);\n    }\n  }\n\n  return true;\n}",
        "accept_response": "private boolean barrierOnWorkerList(String finishedWorkerPath,\n    List<WorkerInfo> workerInfoList,\n    BspEvent event,\n    boolean ignoreDeath) {\n  try {\n    getZkExt().createOnceExt(finishedWorkerPath,\n        null,\n        Ids.OPEN_ACL_UNSAFE,\n        CreateMode.PERSISTENT,\n        true);\n  } catch (KeeperException e) {\n    throw new IllegalStateException(\n        \"barrierOnWorkerList: KeeperException - Couldn't create \" +\n            finishedWorkerPath, e);\n  } catch (InterruptedException e) {\n    throw new IllegalStateException(\n        \"barrierOnWorkerList: InterruptedException - Couldn't create \" +\n            finishedWorkerPath, e);\n  }\n  List<String> hostnameIdList =\n      new ArrayList<String>(workerInfoList.size());\n  for (WorkerInfo workerInfo : workerInfoList) {\n    hostnameIdList.add(workerInfo.getHostnameId());\n  }\n  String workerInfoHealthyPath =\n      getWorkerInfoHealthyPath(getApplicationAttempt(), getSuperstep());\n  List<String> finishedHostnameIdList = new ArrayList<>();\n  List<String> tmpFinishedHostnameIdList;\n  long nextInfoMillis = System.currentTimeMillis();\n  final int defaultTaskTimeoutMsec = 10 * 60 * 1000;  // from TaskTracker\n  final int waitBetweenLogInfoMsec = 30 * 1000;\n  final int taskTimeoutMsec = getContext().getConfiguration().getInt(\n      \"mapred.task.timeout\", defaultTaskTimeoutMsec) / 2;\n  long lastRegularRunTimeMsec = 0;\n  int eventLoopTimeout =  Math.min(taskTimeoutMsec, waitBetweenLogInfoMsec);\n  boolean logInfoOnlyRun = false;\n  List<WorkerInfo> deadWorkers = new ArrayList<>();\n  while (true) {\n    if (! logInfoOnlyRun) {\n      try {\n        tmpFinishedHostnameIdList =\n            getZkExt().getChildrenExt(finishedWorkerPath,\n                                      true,\n                                      false,\n                                      false);\n      } catch (KeeperException e) {\n        throw new IllegalStateException(\n            \"barrierOnWorkerList: KeeperException - Couldn't get \" +\n                \"children of \" + finishedWorkerPath, e);\n      } catch (InterruptedException e) {\n        throw new IllegalStateException(\n            \"barrierOnWorkerList: IllegalException - Couldn't get \" +\n                \"children of \" + finishedWorkerPath, e);\n      }\n      if (LOG.isDebugEnabled()) {\n        // Log the names of the new workers that have finished since last time\n        Set<String> newFinishedHostnames = Sets.difference(\n          Sets.newHashSet(tmpFinishedHostnameIdList),\n          Sets.newHashSet(finishedHostnameIdList));\n        LOG.debug(\"barrierOnWorkerList: Got new finished worker list = \" +\n                      newFinishedHostnames + \", size = \" +\n                      newFinishedHostnames.size() +\n                      \" from \" + finishedWorkerPath);\n      }\n      finishedHostnameIdList = tmpFinishedHostnameIdList;\n    }\n\n    if (LOG.isInfoEnabled() &&\n        (System.currentTimeMillis() > nextInfoMillis)) {\n      nextInfoMillis = System.currentTimeMillis() + waitBetweenLogInfoMsec;\n      LOG.info(\"barrierOnWorkerList: \" +\n          finishedHostnameIdList.size() +\n          \" out of \" + workerInfoList.size() +\n          \" workers finished on superstep \" +\n          getSuperstep() + \" on path \" + finishedWorkerPath);\n      if (workerInfoList.size() - finishedHostnameIdList.size() <\n          MAX_PRINTABLE_REMAINING_WORKERS) {\n        Set<String> remainingWorkers = Sets.newHashSet(hostnameIdList);\n        remainingWorkers.removeAll(finishedHostnameIdList);\n        LOG.info(\"barrierOnWorkerList: Waiting on \" + remainingWorkers);\n      }\n    }\n\n    if (! logInfoOnlyRun) {\n      getContext().setStatus(getGraphTaskManager().getGraphFunctions() +\n                                 \" - \" +\n                                 finishedHostnameIdList.size() +\n                                 \" finished out of \" +\n                                 workerInfoList.size() +\n                                 \" on superstep \" + getSuperstep());\n      if (finishedHostnameIdList.containsAll(hostnameIdList)) {\n        break;\n      }\n\n      for (WorkerInfo deadWorker : deadWorkers) {\n        if (!finishedHostnameIdList.contains(deadWorker.getHostnameId())) {\n          LOG.error(\"barrierOnWorkerList: no results arived from \" +\n                        \"worker that was pronounced dead: \" + deadWorker +\n                        \" on superstep \" + getSuperstep());\n          return false;\n        }\n      }\n\n      // wall-clock time skew is ignored\n      lastRegularRunTimeMsec = System.currentTimeMillis();\n    }\n\n    // Wait for a signal or timeout\n    boolean eventTriggered = event.waitMsecs(eventLoopTimeout);\n    long elapsedTimeSinceRegularRunMsec = System.currentTimeMillis() -\n        lastRegularRunTimeMsec;\n    event.reset();\n    getContext().progress();\n\n    if (eventTriggered ||\n        taskTimeoutMsec == eventLoopTimeout ||\n        elapsedTimeSinceRegularRunMsec >= taskTimeoutMsec) {\n      logInfoOnlyRun = false;\n    } else {\n      logInfoOnlyRun = true;\n      continue;\n    }\n\n    // Did a worker die?\n    try {\n      deadWorkers.addAll(superstepChosenWorkerAlive(\n              workerInfoHealthyPath,\n              workerInfoList));\n      if (!ignoreDeath && deadWorkers.size() > 0) {\n        String errorMessage = \"******* WORKERS \" + deadWorkers +\n            \" FAILED *******\";\n        // If checkpointing is not used, we should fail the job\n        if (!getConfiguration().useCheckpointing()) {\n          setJobStateFailed(errorMessage);\n        } else {\n          LOG.error(\"barrierOnWorkerList: Missing chosen \" +\n              \"workers \" + deadWorkers +\n              \" on superstep \" + getSuperstep());\n          // Log worker failure to command line\n          getGraphTaskManager().getJobProgressTracker().logInfo(errorMessage);\n        }\n        return false;\n      }\n    } catch (KeeperException e) {\n      throw new IllegalStateException(\n          \"barrierOnWorkerList: KeeperException - \" +\n              \"Couldn't get \" + workerInfoHealthyPath, e);\n    } catch (InterruptedException e) {\n      throw new IllegalStateException(\n          \"barrierOnWorkerList: InterruptedException - \" +\n              \"Couldn't get \" + workerInfoHealthyPath, e);\n    }\n  }\n\n  return true;\n}",
        "reject_response": "private boolean barrierOnWorkerList(String finishedWorkerPath,\n    List<WorkerInfo> workerInfoList,\n    BspEvent event,\n    boolean ignoreDeath) {\n  try {\n    getZkExt().createOnceExt(finishedWorkerPath,\n        null,\n        Ids.OPEN_ACL_UNSAFE,\n        CreateMode.PERSISTENT,\n        true);\n  } catch (KeeperException e) {\n    throw new IllegalStateException(\n        \"barrierOnWorkerList: KeeperException - Couldn't create \" +\n            finishedWorkerPath, e);\n  } catch (InterruptedException e) {\n    throw new IllegalStateException(\n        \"barrierOnWorkerList: InterruptedException - Couldn't create \" +\n            finishedWorkerPath, e);\n  }\n  List<String> hostnameIdList =\n      new ArrayList<String>(workerInfoList.size());\n  for (WorkerInfo workerInfo : workerInfoList) {\n    hostnameIdList.add(workerInfo.getHostnameId());\n  }\n  String workerInfoHealthyPath =\n      getWorkerInfoHealthyPath(getApplicationAttempt(), getSuperstep());\n  List<String> finishedHostnameIdList = new ArrayList<>();\n  List<String> tmpFinishedHostnameIdList;\n  long nextInfoMillis = System.currentTimeMillis();\n  final int defaultTaskTimeoutMsec = 10 * 60 * 1000;  // from TaskTracker\n  final int waitBetweenLogInfoMsec = 30 * 1000;\n  final int taskTimeoutMsec = getContext().getConfiguration().getInt(\n      \"mapred.task.timeout\", defaultTaskTimeoutMsec) / 2;\n  long lastRegularRunTimeMsec = 0;\n  int eventLoopTimeout =  Math.min(taskTimeoutMsec, waitBetweenLogInfoMsec);\n  boolean logInfoOnlyRun = false;\n  List<WorkerInfo> deadWorkers = new ArrayList<>();\n  while (true) {\n    if (! logInfoOnlyRun) {\n      try {\n        tmpFinishedHostnameIdList =\n            getZkExt().getChildrenExt(finishedWorkerPath,\n                                      true,\n                                      false,\n                                      false);\n      } catch (KeeperException e) {\n        throw new IllegalStateException(\n            \"barrierOnWorkerList: KeeperException - Couldn't get \" +\n                \"children of \" + finishedWorkerPath, e);\n      } catch (InterruptedException e) {\n        throw new IllegalStateException(\n            \"barrierOnWorkerList: IllegalException - Couldn't get \" +\n                \"children of \" + finishedWorkerPath, e);\n      }\n      if (LOG.isDebugEnabled()) {\n        // Log the names of the new workers that have finished since last time\n        Set<String> newFinishedHostnames = Sets.difference(\n          Sets.newHashSet(tmpFinishedHostnameIdList),\n          Sets.newHashSet(finishedHostnameIdList));\n        LOG.debug(\"barrierOnWorkerList: Got finished worker list = \" +\n                      finishedHostnameIdList + \", size = \" +\n                      finishedHostnameIdList.size() +\n                      \", worker list = \" +\n                      workerInfoList + \", size = \" +\n                      workerInfoList.size() +\n      }\n      finishedHostnameIdList = tmpFinishedHostnameIdList;\n    }\n\n    if (LOG.isInfoEnabled() &&\n        (System.currentTimeMillis() > nextInfoMillis)) {\n      nextInfoMillis = System.currentTimeMillis() + waitBetweenLogInfoMsec;\n      LOG.info(\"barrierOnWorkerList: \" +\n          finishedHostnameIdList.size() +\n          \" out of \" + workerInfoList.size() +\n          \" workers finished on superstep \" +\n          getSuperstep() + \" on path \" + finishedWorkerPath);\n      if (workerInfoList.size() - finishedHostnameIdList.size() <\n          MAX_PRINTABLE_REMAINING_WORKERS) {\n        Set<String> remainingWorkers = Sets.newHashSet(hostnameIdList);\n        remainingWorkers.removeAll(finishedHostnameIdList);\n        LOG.info(\"barrierOnWorkerList: Waiting on \" + remainingWorkers);\n      }\n    }\n\n    if (! logInfoOnlyRun) {\n      getContext().setStatus(getGraphTaskManager().getGraphFunctions() +\n                                 \" - \" +\n                                 finishedHostnameIdList.size() +\n                                 \" finished out of \" +\n                                 workerInfoList.size() +\n                                 \" on superstep \" + getSuperstep());\n      if (finishedHostnameIdList.containsAll(hostnameIdList)) {\n        break;\n      }\n\n      for (WorkerInfo deadWorker : deadWorkers) {\n        if (!finishedHostnameIdList.contains(deadWorker.getHostnameId())) {\n          LOG.error(\"barrierOnWorkerList: no results arived from \" +\n                        \"worker that was pronounced dead: \" + deadWorker +\n                        \" on superstep \" + getSuperstep());\n          return false;\n        }\n      }\n\n      // wall-clock time skew is ignored\n      lastRegularRunTimeMsec = System.currentTimeMillis();\n    }\n\n    // Wait for a signal or timeout\n    boolean eventTriggered = event.waitMsecs(eventLoopTimeout);\n    long elapsedTimeSinceRegularRunMsec = System.currentTimeMillis() -\n        lastRegularRunTimeMsec;\n    event.reset();\n    getContext().progress();\n\n    if (eventTriggered ||\n        taskTimeoutMsec == eventLoopTimeout ||\n        elapsedTimeSinceRegularRunMsec >= taskTimeoutMsec) {\n      logInfoOnlyRun = false;\n    } else {\n      logInfoOnlyRun = true;\n      continue;\n    }\n\n    // Did a worker die?\n    try {\n      deadWorkers.addAll(superstepChosenWorkerAlive(\n              workerInfoHealthyPath,\n              workerInfoList));\n      if (!ignoreDeath && deadWorkers.size() > 0) {\n        String errorMessage = \"******* WORKERS \" + deadWorkers +\n            \" FAILED *******\";\n        // If checkpointing is not used, we should fail the job\n        if (!getConfiguration().useCheckpointing()) {\n          setJobStateFailed(errorMessage);\n        } else {\n          LOG.error(\"barrierOnWorkerList: Missing chosen \" +\n              \"workers \" + deadWorkers +\n              \" on superstep \" + getSuperstep());\n          // Log worker failure to command line\n          getGraphTaskManager().getJobProgressTracker().logInfo(errorMessage);\n        }\n        return false;\n      }\n    } catch (KeeperException e) {\n      throw new IllegalStateException(\n          \"barrierOnWorkerList: KeeperException - \" +\n              \"Couldn't get \" + workerInfoHealthyPath, e);\n    } catch (InterruptedException e) {\n      throw new IllegalStateException(\n          \"barrierOnWorkerList: InterruptedException - \" +\n              \"Couldn't get \" + workerInfoHealthyPath, e);\n    }\n  }\n\n  return true;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2553,
        "instruction": "@Override\nprotected IterOutcome doWork() {\n  flattenMemoryManager.update();\n  flattener.setOutputCount(flattenMemoryManager.getOutputRowCount());\n\n  int incomingRecordCount = incoming.getRecordCount();\n\n  if (!doAlloc(flattenMemoryManager.getOutputRowCount())) {\n    outOfMemory = true;\n    return IterOutcome.OUT_OF_MEMORY;\n  }\n\n  // we call this in setupSchema, but we also need to call it here so we have a reference to the appropriate vector\n  // inside of the the flattener for the current batch\n  setFlattenVector();\n\n  int childCount = incomingRecordCount == 0 ? 0 : flattener.getFlattenField().getAccessor().getInnerValueCount();\n  int outputRecords = childCount == 0 ? 0: flattener.flattenRecords(incomingRecordCount, 0, monitor);\n  // TODO - change this to be based on the repeated vector length\n  if (outputRecords < childCount) {\n    setValueCount(outputRecords);\n    hasRemainder = true;\n    remainderIndex = outputRecords;\n    this.recordCount = remainderIndex;\n  } else {\n    setValueCount(outputRecords);\n    flattener.resetGroupIndex();\n    for(VectorWrapper<?> v: incoming) {\n      v.clear();\n    }\n    this.recordCount = outputRecords;\n  }\n  // In case of complex writer expression, vectors would be added to batch run-time.\n  // We have to re-build the schema.\n  if (complexWriters != null) {\n    container.buildSchema(SelectionVectorMode.NONE);\n  }\n\n  flattenMemoryManager.updateOutgoingStats(outputRecords);\n\n  // Get the final outcome based on hasRemainder since that will determine if all the incoming records were\n  // consumed in current output batch or not\n  return getFinalOutcome(hasRemainder);\n}",
        "accept_response": "@Override\nprotected IterOutcome doWork() {\n  flattenMemoryManager.update();\n  flattener.setOutputCount(flattenMemoryManager.getOutputRowCount());\n\n  int incomingRecordCount = incoming.getRecordCount();\n\n  if (!doAlloc(flattenMemoryManager.getOutputRowCount())) {\n    outOfMemory = true;\n    return IterOutcome.OUT_OF_MEMORY;\n  }\n\n  // we call this in setupSchema, but we also need to call it here so we have a reference to the appropriate vector\n  // inside of the the flattener for the current batch\n  setFlattenVector();\n\n  int childCount = incomingRecordCount == 0 ? 0 : flattener.getFlattenField().getAccessor().getInnerValueCount();\n  int outputRecords = childCount == 0 ? 0: flattener.flattenRecords(incomingRecordCount, 0, monitor);\n  // TODO - change this to be based on the repeated vector length\n  if (outputRecords < childCount) {\n    setValueCount(outputRecords);\n    hasRemainder = true;\n    remainderIndex = outputRecords;\n    this.recordCount = remainderIndex;\n  } else {\n    setValueCount(outputRecords);\n    flattener.resetGroupIndex();\n    for(VectorWrapper<?> v: incoming) {\n      v.clear();\n    }\n    this.recordCount = outputRecords;\n  }\n  // In case of complex writer expression, vectors would be added to batch run-time.\n  // We have to re-build the schema.\n  if (complexWriters != null) {\n    container.buildSchema(SelectionVectorMode.NONE);\n  }\n\n  flattenMemoryManager.updateOutgoingStats(outputRecords);\n\n  if (logger.isDebugEnabled()) {\n    logger.debug(\"BATCH_STATS, outgoing: {}\", new RecordBatchSizer(this));\n  }\n\n  // Get the final outcome based on hasRemainder since that will determine if all the incoming records were\n  // consumed in current output batch or not\n  return getFinalOutcome(hasRemainder);\n}",
        "reject_response": "@Override\nprotected IterOutcome doWork() {\n  flattenMemoryManager.update();\n  flattener.setOutputCount(flattenMemoryManager.getOutputRowCount());\n\n  int incomingRecordCount = incoming.getRecordCount();\n\n  if (!doAlloc(flattenMemoryManager.getOutputRowCount())) {\n    outOfMemory = true;\n    return IterOutcome.OUT_OF_MEMORY;\n  }\n\n  // we call this in setupSchema, but we also need to call it here so we have a reference to the appropriate vector\n  // inside of the the flattener for the current batch\n  setFlattenVector();\n\n  int childCount = incomingRecordCount == 0 ? 0 : flattener.getFlattenField().getAccessor().getInnerValueCount();\n  int outputRecords = childCount == 0 ? 0: flattener.flattenRecords(incomingRecordCount, 0, monitor);\n  // TODO - change this to be based on the repeated vector length\n  if (outputRecords < childCount) {\n    setValueCount(outputRecords);\n    hasRemainder = true;\n    remainderIndex = outputRecords;\n    this.recordCount = remainderIndex;\n  } else {\n    setValueCount(outputRecords);\n    flattener.resetGroupIndex();\n    for(VectorWrapper<?> v: incoming) {\n      v.clear();\n    }\n    this.recordCount = outputRecords;\n  }\n  // In case of complex writer expression, vectors would be added to batch run-time.\n  // We have to re-build the schema.\n  if (complexWriters != null) {\n    container.buildSchema(SelectionVectorMode.NONE);\n  }\n\n  flattenMemoryManager.updateOutgoingStats(outputRecords);\n\n  if (logger.isDebugEnabled()) {\n    logger.debug(\"BATCH_STATS, outgoing:\\n {}\", new RecordBatchSizer(this));\n  }\n\n  // Get the final outcome based on hasRemainder since that will determine if all the incoming records were\n  // consumed in current output batch or not\n  return getFinalOutcome(hasRemainder);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2412,
        "instruction": "private Command<Void> getCreateCommand(final Map<String, Object> properties, final Map<String, String> requestInfoProps) {\n  return new Command<Void>() {\n    @SuppressWarnings(\"rawtypes\")\n    @Override\n    public Void invoke() throws AmbariException {\n      String rawRequestBody = requestInfoProps.get(Request.REQUEST_INFO_BODY_PROPERTY);\n      Preconditions.checkArgument(!Strings.isNullOrEmpty(rawRequestBody), REQUEST_BODY_EMPTY_ERROR_MESSAGE);\n\n      Map<String, Object> rawBodyMap = jsonSerializer.<Map<String, Object>>fromJson(rawRequestBody, Map.class);\n      Object configurationData = rawBodyMap.get(CONFIGURATION_PROPERTY_ID);\n\n      if (configurationData != null) {\n        Preconditions.checkArgument(configurationData instanceof List, CONFIGURATION_LIST_CHECK_ERROR_MESSAGE);\n        for (Object map : (List) configurationData) {\n          Preconditions.checkArgument(map instanceof Map, CONFIGURATION_MAP_CHECK_ERROR_MESSAGE);\n          Preconditions.checkArgument(((Map) map).size() <= 1, CONFIGURATION_MAP_SIZE_CHECK_ERROR_MESSAGE);\n        }\n      }\n      SecurityConfiguration securityConfiguration = securityConfigurationFactory\n        .createSecurityConfigurationFromRequest((Map<String, Object>) rawBodyMap.get(BLUEPRINTS_PROPERTY_ID), true);\n\n      Blueprint blueprint;\n      try {\n        blueprint = blueprintFactory.createBlueprint(properties, securityConfiguration);\n      } catch (NoSuchStackException e) {\n        throw new IllegalArgumentException(\"Specified stack doesn't exist: \" + e, e);\n      }\n\n      if (blueprintDAO.findByName(blueprint.getName()) != null) {\n        throw new DuplicateResourceException(\n            \"Attempted to create a Blueprint which already exists, blueprint_name=\" +\n            blueprint.getName());\n      }\n\n      try {\n        blueprint.validateRequiredProperties();\n      } catch (InvalidTopologyException e) {\n        throw new IllegalArgumentException(\"Blueprint configuration validation failed: \" + e.getMessage(), e);\n      }\n\n      String validateTopology =  requestInfoProps.get(\"validate_topology\");\n      if (validateTopology == null || ! validateTopology.equalsIgnoreCase(\"false\")) {\n        try {\n          blueprint.validateTopology();\n        } catch (InvalidTopologyException e) {\n          throw new IllegalArgumentException(e.getMessage());\n        }\n      }\n\n      String blueprintSetting = blueprint.getSetting() == null ? \"(null)\" :\n              jsonSerializer.toJson(blueprint.getSetting().getProperties());\n      LOG.info(\"Blueprint setting=\" + blueprintSetting);\n\n      try {\n        blueprintDAO.create(blueprint.toEntity());\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      }\n      return null;\n    }\n  };\n}",
        "accept_response": "private Command<Void> getCreateCommand(final Map<String, Object> properties, final Map<String, String> requestInfoProps) {\n  return new Command<Void>() {\n    @SuppressWarnings(\"rawtypes\")\n    @Override\n    public Void invoke() throws AmbariException {\n      String rawRequestBody = requestInfoProps.get(Request.REQUEST_INFO_BODY_PROPERTY);\n      Preconditions.checkArgument(!Strings.isNullOrEmpty(rawRequestBody), REQUEST_BODY_EMPTY_ERROR_MESSAGE);\n\n      Map<String, Object> rawBodyMap = jsonSerializer.<Map<String, Object>>fromJson(rawRequestBody, Map.class);\n      Object configurationData = rawBodyMap.get(CONFIGURATION_PROPERTY_ID);\n\n      if (configurationData != null) {\n        Preconditions.checkArgument(configurationData instanceof List, CONFIGURATION_LIST_CHECK_ERROR_MESSAGE);\n        for (Object map : (List) configurationData) {\n          Preconditions.checkArgument(map instanceof Map, CONFIGURATION_MAP_CHECK_ERROR_MESSAGE);\n          Preconditions.checkArgument(((Map) map).size() <= 1, CONFIGURATION_MAP_SIZE_CHECK_ERROR_MESSAGE);\n        }\n      }\n      SecurityConfiguration securityConfiguration = securityConfigurationFactory\n        .createSecurityConfigurationFromRequest((Map<String, Object>) rawBodyMap.get(BLUEPRINTS_PROPERTY_ID), true);\n\n      Blueprint blueprint;\n      try {\n        blueprint = blueprintFactory.createBlueprint(properties, securityConfiguration);\n      } catch (NoSuchStackException e) {\n        throw new IllegalArgumentException(\"Specified stack doesn't exist: \" + e, e);\n      }\n\n      if (blueprintDAO.findByName(blueprint.getName()) != null) {\n        throw new DuplicateResourceException(\n            \"Attempted to create a Blueprint which already exists, blueprint_name=\" +\n            blueprint.getName());\n      }\n\n      try {\n        blueprint.validateRequiredProperties();\n      } catch (InvalidTopologyException e) {\n        throw new IllegalArgumentException(\"Blueprint configuration validation failed: \" + e.getMessage(), e);\n      }\n\n      String validateTopology =  requestInfoProps.get(\"validate_topology\");\n      if (validateTopology == null || ! validateTopology.equalsIgnoreCase(\"false\")) {\n        try {\n          blueprint.validateTopology();\n        } catch (InvalidTopologyException e) {\n          throw new IllegalArgumentException(e.getMessage());\n        }\n      }\n\n      LOG.info(\"Creating Blueprint, name=\" + blueprint.getName());\n      String blueprintSetting = blueprint.getSetting() == null ? \"(null)\" :\n              jsonSerializer.toJson(blueprint.getSetting().getProperties());\n      LOG.info(\"Blueprint setting=\" + blueprintSetting);\n\n      try {\n        blueprintDAO.create(blueprint.toEntity());\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      }\n      return null;\n    }\n  };\n}",
        "reject_response": "private Command<Void> getCreateCommand(final Map<String, Object> properties, final Map<String, String> requestInfoProps) {\n  return new Command<Void>() {\n    @SuppressWarnings(\"rawtypes\")\n    @Override\n    public Void invoke() throws AmbariException {\n      String rawRequestBody = requestInfoProps.get(Request.REQUEST_INFO_BODY_PROPERTY);\n      Preconditions.checkArgument(!Strings.isNullOrEmpty(rawRequestBody), REQUEST_BODY_EMPTY_ERROR_MESSAGE);\n\n      Map<String, Object> rawBodyMap = jsonSerializer.<Map<String, Object>>fromJson(rawRequestBody, Map.class);\n      Object configurationData = rawBodyMap.get(CONFIGURATION_PROPERTY_ID);\n\n      if (configurationData != null) {\n        Preconditions.checkArgument(configurationData instanceof List, CONFIGURATION_LIST_CHECK_ERROR_MESSAGE);\n        for (Object map : (List) configurationData) {\n          Preconditions.checkArgument(map instanceof Map, CONFIGURATION_MAP_CHECK_ERROR_MESSAGE);\n          Preconditions.checkArgument(((Map) map).size() <= 1, CONFIGURATION_MAP_SIZE_CHECK_ERROR_MESSAGE);\n        }\n      }\n      SecurityConfiguration securityConfiguration = securityConfigurationFactory\n        .createSecurityConfigurationFromRequest((Map<String, Object>) rawBodyMap.get(BLUEPRINTS_PROPERTY_ID), true);\n\n      Blueprint blueprint;\n      try {\n        blueprint = blueprintFactory.createBlueprint(properties, securityConfiguration);\n      } catch (NoSuchStackException e) {\n        throw new IllegalArgumentException(\"Specified stack doesn't exist: \" + e, e);\n      }\n\n      if (blueprintDAO.findByName(blueprint.getName()) != null) {\n        throw new DuplicateResourceException(\n            \"Attempted to create a Blueprint which already exists, blueprint_name=\" +\n            blueprint.getName());\n      }\n\n      try {\n        blueprint.validateRequiredProperties();\n      } catch (InvalidTopologyException e) {\n        throw new IllegalArgumentException(\"Blueprint configuration validation failed: \" + e.getMessage(), e);\n      }\n\n      String validateTopology =  requestInfoProps.get(\"validate_topology\");\n      if (validateTopology == null || ! validateTopology.equalsIgnoreCase(\"false\")) {\n        try {\n          blueprint.validateTopology();\n        } catch (InvalidTopologyException e) {\n          throw new IllegalArgumentException(e.getMessage());\n        }\n      }\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Creating Blueprint, name=\" + blueprint.getName());\n      }\n      String blueprintSetting = blueprint.getSetting() == null ? \"(null)\" :\n              jsonSerializer.toJson(blueprint.getSetting().getProperties());\n      LOG.info(\"Blueprint setting=\" + blueprintSetting);\n\n      try {\n        blueprintDAO.create(blueprint.toEntity());\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      }\n      return null;\n    }\n  };\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2948,
        "instruction": "protected void unsuccessfulAuthorization(HttpServletRequest request, HttpServletResponse response, AuthenticationException ae) throws IOException {\n    // populate the response\n    ProxiedEntitiesUtils.unsuccessfulAuthorization(request, response, ae);\n\n    // set the response status\n    response.setContentType(\"text/plain\");\n\n    // write the response message\n    PrintWriter out = response.getWriter();\n\n    // use the type of authentication exception to determine the response code\n    if (ae instanceof InvalidAuthenticationException) {\n        response.setStatus(HttpServletResponse.SC_UNAUTHORIZED);\n        out.println(ae.getMessage());\n    } else if (ae instanceof UntrustedProxyException) {\n        response.setStatus(HttpServletResponse.SC_FORBIDDEN);\n        out.println(ae.getMessage());\n    } else if (ae instanceof AuthenticationServiceException) {\n        log.error(String.format(\"Unable to authorize: %s\", ae.getMessage()), ae);\n        response.setStatus(HttpServletResponse.SC_INTERNAL_SERVER_ERROR);\n        out.println(String.format(\"Unable to authorize: %s\", ae.getMessage()));\n    } else {\n        log.error(String.format(\"Unable to authorize: %s\", ae.getMessage()), ae);\n        response.setStatus(HttpServletResponse.SC_FORBIDDEN);\n        out.println(\"Access is denied.\");\n    }\n\n    // log the failure\n\n    // optionally log the stack trace\n    if (log.isDebugEnabled()) {\n        log.debug(StringUtils.EMPTY, ae);\n    }\n}",
        "accept_response": "protected void unsuccessfulAuthorization(HttpServletRequest request, HttpServletResponse response, AuthenticationException ae) throws IOException {\n    // populate the response\n    ProxiedEntitiesUtils.unsuccessfulAuthorization(request, response, ae);\n\n    // set the response status\n    response.setContentType(\"text/plain\");\n\n    // write the response message\n    PrintWriter out = response.getWriter();\n\n    // use the type of authentication exception to determine the response code\n    if (ae instanceof InvalidAuthenticationException) {\n        response.setStatus(HttpServletResponse.SC_UNAUTHORIZED);\n        out.println(ae.getMessage());\n    } else if (ae instanceof UntrustedProxyException) {\n        response.setStatus(HttpServletResponse.SC_FORBIDDEN);\n        out.println(ae.getMessage());\n    } else if (ae instanceof AuthenticationServiceException) {\n        log.error(String.format(\"Unable to authorize: %s\", ae.getMessage()), ae);\n        response.setStatus(HttpServletResponse.SC_INTERNAL_SERVER_ERROR);\n        out.println(String.format(\"Unable to authorize: %s\", ae.getMessage()));\n    } else {\n        log.error(String.format(\"Unable to authorize: %s\", ae.getMessage()), ae);\n        response.setStatus(HttpServletResponse.SC_FORBIDDEN);\n        out.println(\"Access is denied.\");\n    }\n\n    // log the failure\n    log.warn(String.format(\"Rejecting access to web api: %s\", ae.getMessage()));\n\n    // optionally log the stack trace\n    if (log.isDebugEnabled()) {\n        log.debug(StringUtils.EMPTY, ae);\n    }\n}",
        "reject_response": "protected void unsuccessfulAuthorization(HttpServletRequest request, HttpServletResponse response, AuthenticationException ae) throws IOException {\n    // populate the response\n    ProxiedEntitiesUtils.unsuccessfulAuthorization(request, response, ae);\n\n    // set the response status\n    response.setContentType(\"text/plain\");\n\n    // write the response message\n    PrintWriter out = response.getWriter();\n\n    // use the type of authentication exception to determine the response code\n    if (ae instanceof InvalidAuthenticationException) {\n        response.setStatus(HttpServletResponse.SC_UNAUTHORIZED);\n        out.println(ae.getMessage());\n    } else if (ae instanceof UntrustedProxyException) {\n        response.setStatus(HttpServletResponse.SC_FORBIDDEN);\n        out.println(ae.getMessage());\n    } else if (ae instanceof AuthenticationServiceException) {\n        log.error(String.format(\"Unable to authorize: %s\", ae.getMessage()), ae);\n        response.setStatus(HttpServletResponse.SC_INTERNAL_SERVER_ERROR);\n        out.println(String.format(\"Unable to authorize: %s\", ae.getMessage()));\n    } else {\n        log.error(String.format(\"Unable to authorize: %s\", ae.getMessage()), ae);\n        response.setStatus(HttpServletResponse.SC_FORBIDDEN);\n        out.println(\"Access is denied.\");\n    }\n\n    // log the failure\n    log.info(String.format(\"Rejecting access to web api: %s\", ae.getMessage()));\n\n    // optionally log the stack trace\n    if (log.isDebugEnabled()) {\n        log.debug(StringUtils.EMPTY, ae);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2433,
        "instruction": "private void fireFailedForwardAdvisory(MessageDispatch messageDispatch, Throwable error) {\n    if (configuration.isAdvisoryForFailedForward()) {\n        AdvisoryBroker advisoryBroker = null;\n        try {\n            advisoryBroker = (AdvisoryBroker) brokerService.getBroker().getAdaptor(AdvisoryBroker.class);\n\n            if (advisoryBroker != null) {\n                ConnectionContext context = new ConnectionContext();\n                context.setSecurityContext(SecurityContext.BROKER_SECURITY_CONTEXT);\n                context.setBroker(brokerService.getBroker());\n\n                ActiveMQMessage advisoryMessage = new ActiveMQMessage();\n                advisoryMessage.setStringProperty(\"cause\", error.getLocalizedMessage());\n                advisoryBroker.fireAdvisory(context, AdvisorySupport.getNetworkBridgeForwardFailureAdvisoryTopic(), messageDispatch.getMessage(), null,\n                        advisoryMessage);\n\n            }\n        } catch (Exception e) {\n            LOG.debug(\"detail\", e);\n        }\n    }\n}",
        "accept_response": "private void fireFailedForwardAdvisory(MessageDispatch messageDispatch, Throwable error) {\n    if (configuration.isAdvisoryForFailedForward()) {\n        AdvisoryBroker advisoryBroker = null;\n        try {\n            advisoryBroker = (AdvisoryBroker) brokerService.getBroker().getAdaptor(AdvisoryBroker.class);\n\n            if (advisoryBroker != null) {\n                ConnectionContext context = new ConnectionContext();\n                context.setSecurityContext(SecurityContext.BROKER_SECURITY_CONTEXT);\n                context.setBroker(brokerService.getBroker());\n\n                ActiveMQMessage advisoryMessage = new ActiveMQMessage();\n                advisoryMessage.setStringProperty(\"cause\", error.getLocalizedMessage());\n                advisoryBroker.fireAdvisory(context, AdvisorySupport.getNetworkBridgeForwardFailureAdvisoryTopic(), messageDispatch.getMessage(), null,\n                        advisoryMessage);\n\n            }\n        } catch (Exception e) {\n            LOG.warn(\"failed to fire forward failure advisory, cause: {}\", (Object) e);\n            LOG.debug(\"detail\", e);\n        }\n    }\n}",
        "reject_response": "private void fireFailedForwardAdvisory(MessageDispatch messageDispatch, Throwable error) {\n    if (configuration.isAdvisoryForFailedForward()) {\n        AdvisoryBroker advisoryBroker = null;\n        try {\n            advisoryBroker = (AdvisoryBroker) brokerService.getBroker().getAdaptor(AdvisoryBroker.class);\n\n            if (advisoryBroker != null) {\n                ConnectionContext context = new ConnectionContext();\n                context.setSecurityContext(SecurityContext.BROKER_SECURITY_CONTEXT);\n                context.setBroker(brokerService.getBroker());\n\n                ActiveMQMessage advisoryMessage = new ActiveMQMessage();\n                advisoryMessage.setStringProperty(\"cause\", error.getLocalizedMessage());\n                advisoryBroker.fireAdvisory(context, AdvisorySupport.getNetworkBridgeForwardFailureAdvisoryTopic(), messageDispatch.getMessage(), null,\n                        advisoryMessage);\n\n            }\n        } catch (Exception e) {\n            LOG.warn(\"failed to fire forward failure advisory, cause: {}\", e);\n            LOG.debug(\"detail\", e);\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2961,
        "instruction": "public int run(String[] args) throws Exception {\n  String plugins = \"protocol-http|parse-tika|scoring-opic|urlfilter-regex|urlnormalizer-pass\";\n  int seeds = 1;\n  int depth = 10;\n  int threads = 10;\n  boolean delete = true;\n  long topN = Long.MAX_VALUE;\n\n  if (args.length == 0) {\n    System.err\n        .println(\"Usage: Benchmark [-seeds NN] [-depth NN] [-threads NN] [-keep] [-maxPerHost NN] [-plugins <regex>]\");\n    System.err\n        .println(\"\\t-seeds NN\\tcreate NN unique hosts in a seed list (default: 1)\");\n    System.err.println(\"\\t-depth NN\\tperform NN crawl cycles (default: 10)\");\n    System.err\n        .println(\"\\t-threads NN\\tuse NN threads per Fetcher task (default: 10)\");\n    System.err\n        .println(\"\\t-keep\\tkeep segment data (default: delete after updatedb)\");\n    System.err.println(\"\\t-plugins <regex>\\toverride 'plugin.includes'.\");\n    System.err.println(\"\\tNOTE: if not specified, this is reset to: \"\n        + plugins);\n    System.err\n        .println(\"\\tNOTE: if 'default' is specified then a value set in nutch-default/nutch-site is used.\");\n    System.err\n        .println(\"\\t-maxPerHost NN\\tmax. # of URLs per host in a fetchlist\");\n    return -1;\n  }\n  int maxPerHost = Integer.MAX_VALUE;\n  for (int i = 0; i < args.length; i++) {\n    if (args[i].equals(\"-seeds\")) {\n      seeds = Integer.parseInt(args[++i]);\n    } else if (args[i].equals(\"-threads\")) {\n      threads = Integer.parseInt(args[++i]);\n    } else if (args[i].equals(\"-depth\")) {\n      depth = Integer.parseInt(args[++i]);\n    } else if (args[i].equals(\"-keep\")) {\n      delete = false;\n    } else if (args[i].equals(\"-plugins\")) {\n      plugins = args[++i];\n    } else if (args[i].equalsIgnoreCase(\"-maxPerHost\")) {\n      maxPerHost = Integer.parseInt(args[++i]);\n    } else {\n      return -1;\n    }\n  }\n  BenchmarkResults res = benchmark(seeds, depth, threads, maxPerHost, topN,\n      delete, plugins);\n  System.out.println(res);\n  return 0;\n}",
        "accept_response": "public int run(String[] args) throws Exception {\n  String plugins = \"protocol-http|parse-tika|scoring-opic|urlfilter-regex|urlnormalizer-pass\";\n  int seeds = 1;\n  int depth = 10;\n  int threads = 10;\n  boolean delete = true;\n  long topN = Long.MAX_VALUE;\n\n  if (args.length == 0) {\n    System.err\n        .println(\"Usage: Benchmark [-seeds NN] [-depth NN] [-threads NN] [-keep] [-maxPerHost NN] [-plugins <regex>]\");\n    System.err\n        .println(\"\\t-seeds NN\\tcreate NN unique hosts in a seed list (default: 1)\");\n    System.err.println(\"\\t-depth NN\\tperform NN crawl cycles (default: 10)\");\n    System.err\n        .println(\"\\t-threads NN\\tuse NN threads per Fetcher task (default: 10)\");\n    System.err\n        .println(\"\\t-keep\\tkeep segment data (default: delete after updatedb)\");\n    System.err.println(\"\\t-plugins <regex>\\toverride 'plugin.includes'.\");\n    System.err.println(\"\\tNOTE: if not specified, this is reset to: \"\n        + plugins);\n    System.err\n        .println(\"\\tNOTE: if 'default' is specified then a value set in nutch-default/nutch-site is used.\");\n    System.err\n        .println(\"\\t-maxPerHost NN\\tmax. # of URLs per host in a fetchlist\");\n    return -1;\n  }\n  int maxPerHost = Integer.MAX_VALUE;\n  for (int i = 0; i < args.length; i++) {\n    if (args[i].equals(\"-seeds\")) {\n      seeds = Integer.parseInt(args[++i]);\n    } else if (args[i].equals(\"-threads\")) {\n      threads = Integer.parseInt(args[++i]);\n    } else if (args[i].equals(\"-depth\")) {\n      depth = Integer.parseInt(args[++i]);\n    } else if (args[i].equals(\"-keep\")) {\n      delete = false;\n    } else if (args[i].equals(\"-plugins\")) {\n      plugins = args[++i];\n    } else if (args[i].equalsIgnoreCase(\"-maxPerHost\")) {\n      maxPerHost = Integer.parseInt(args[++i]);\n    } else {\n      LOG.error(\"Invalid argument: '\" + args[i] + \"'\");\n      return -1;\n    }\n  }\n  BenchmarkResults res = benchmark(seeds, depth, threads, maxPerHost, topN,\n      delete, plugins);\n  System.out.println(res);\n  return 0;\n}",
        "reject_response": "public int run(String[] args) throws Exception {\n  String plugins = \"protocol-http|parse-tika|scoring-opic|urlfilter-regex|urlnormalizer-pass\";\n  int seeds = 1;\n  int depth = 10;\n  int threads = 10;\n  boolean delete = true;\n  long topN = Long.MAX_VALUE;\n\n  if (args.length == 0) {\n    System.err\n        .println(\"Usage: Benchmark [-seeds NN] [-depth NN] [-threads NN] [-keep] [-maxPerHost NN] [-plugins <regex>]\");\n    System.err\n        .println(\"\\t-seeds NN\\tcreate NN unique hosts in a seed list (default: 1)\");\n    System.err.println(\"\\t-depth NN\\tperform NN crawl cycles (default: 10)\");\n    System.err\n        .println(\"\\t-threads NN\\tuse NN threads per Fetcher task (default: 10)\");\n    System.err\n        .println(\"\\t-keep\\tkeep segment data (default: delete after updatedb)\");\n    System.err.println(\"\\t-plugins <regex>\\toverride 'plugin.includes'.\");\n    System.err.println(\"\\tNOTE: if not specified, this is reset to: \"\n        + plugins);\n    System.err\n        .println(\"\\tNOTE: if 'default' is specified then a value set in nutch-default/nutch-site is used.\");\n    System.err\n        .println(\"\\t-maxPerHost NN\\tmax. # of URLs per host in a fetchlist\");\n    return -1;\n  }\n  int maxPerHost = Integer.MAX_VALUE;\n  for (int i = 0; i < args.length; i++) {\n    if (args[i].equals(\"-seeds\")) {\n      seeds = Integer.parseInt(args[++i]);\n    } else if (args[i].equals(\"-threads\")) {\n      threads = Integer.parseInt(args[++i]);\n    } else if (args[i].equals(\"-depth\")) {\n      depth = Integer.parseInt(args[++i]);\n    } else if (args[i].equals(\"-keep\")) {\n      delete = false;\n    } else if (args[i].equals(\"-plugins\")) {\n      plugins = args[++i];\n    } else if (args[i].equalsIgnoreCase(\"-maxPerHost\")) {\n      maxPerHost = Integer.parseInt(args[++i]);\n    } else {\n      LOG.fatal(\"Invalid argument: '\" + args[i] + \"'\");\n      return -1;\n    }\n  }\n  BenchmarkResults res = benchmark(seeds, depth, threads, maxPerHost, topN,\n      delete, plugins);\n  System.out.println(res);\n  return 0;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2858,
        "instruction": "@Override\npublic void logGhostMailboxFailure(MailboxPath mailboxPath) {\n    GhostMailbox.logger()\n            .addField(GhostMailbox.MAILBOX_NAME, mailboxPath)\n            .addField(TYPE, \"readMiss\")\n}",
        "accept_response": "@Override\npublic void logGhostMailboxFailure(MailboxPath mailboxPath) {\n    GhostMailbox.logger()\n            .addField(GhostMailbox.MAILBOX_NAME, mailboxPath)\n            .addField(TYPE, \"readMiss\")\n            .log(logger -> logger.debug(\"Read mailbox missed\"));\n}",
        "reject_response": "@Override\npublic void logGhostMailboxFailure(MailboxPath mailboxPath) {\n    GhostMailbox.logger()\n            .addField(GhostMailbox.MAILBOX_NAME, mailboxPath)\n            .addField(TYPE, \"readMiss\")\n            .log(logger -> logger.info(\"Read mailbox missed\"));\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2404,
        "instruction": "private void loadConfigFiles() throws Exception {\n  List<String> configFiles = getConfigFiles();\n  for (String configFileName : configFiles) {\n    LOG.info(\"Going to load config file:\" + configFileName);\n    configFileName = configFileName.replace(\"\\\\ \", \"%20\");\n    File configFile = new File(configFileName);\n    if (configFile.exists() && configFile.isFile()) {\n      LOG.info(\"Config file exists in path.\" + configFile.getAbsolutePath());\n      loadConfigsUsingFile(configFile);\n    } else {\n      LOG.info(\"Trying to load config file from classloader: \" + configFileName);\n      loadConfigsUsingClassLoader(configFileName);\n    }\n  }\n}",
        "accept_response": "private void loadConfigFiles() throws Exception {\n  List<String> configFiles = getConfigFiles();\n  for (String configFileName : configFiles) {\n    LOG.info(\"Going to load config file:\" + configFileName);\n    configFileName = configFileName.replace(\"\\\\ \", \"%20\");\n    File configFile = new File(configFileName);\n    if (configFile.exists() && configFile.isFile()) {\n      LOG.info(\"Config file exists in path.\" + configFile.getAbsolutePath());\n      loadConfigsUsingFile(configFile);\n    } else {\n      LOG.info(\"Trying to load config file from classloader: \" + configFileName);\n      loadConfigsUsingClassLoader(configFileName);\n      LOG.info(\"Loaded config file from classloader: \" + configFileName);\n    }\n  }\n}",
        "reject_response": "private void loadConfigFiles() throws Exception {\n  List<String> configFiles = getConfigFiles();\n  for (String configFileName : configFiles) {\n    LOG.info(\"Going to load config file:\" + configFileName);\n    configFileName = configFileName.replace(\"\\\\ \", \"%20\");\n    File configFile = new File(configFileName);\n    if (configFile.exists() && configFile.isFile()) {\n      LOG.info(\"Config file exists in path.\" + configFile.getAbsolutePath());\n      loadConfigsUsingFile(configFile);\n    } else {\n      LOG.info(\"Trying to load config file from classloader: \" + configFileName);\n      loadConfigsUsingClassLoader(configFileName);\n      logger.info(\"Loaded config file from classloader: \"\n        + configFileName);\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2377,
        "instruction": "private void getZooLock(HostAndPort addr) throws KeeperException, InterruptedException {\n  log.info(\"Attempting to acquire Garbage Collector Lock\");\n  String path = ZooUtil.getRoot(getInstance()) + Constants.ZGC_LOCK;\n\n  LockWatcher lockWatcher = new LockWatcher() {\n    @Override\n    public void lostLock(LockLossReason reason) {\n      Halt.halt(\"GC lock in zookeeper lost (reason = \" + reason + \"), exiting!\", 1);\n    }\n\n    @Override\n    public void unableToMonitorLockNode(final Throwable e) {\n      // ACCUMULO-3651 Level changed to error and FATAL added to message for slf4j compatibility\n      Halt.halt(-1, new Runnable() {\n\n        @Override\n        public void run() {\n          log.error(\"FATAL: No longer able to monitor lock node \", e);\n        }\n      });\n\n    }\n  };\n\n  while (true) {\n    lock = new ZooLock(path);\n    if (lock.tryLock(lockWatcher, new ServerServices(addr.toString(), Service.GC_CLIENT).toString().getBytes())) {\n      return;\n    }\n    log.debug(\"Failed to acquire GC ZooKeeper lock, will retry\");\n    sleepUninterruptibly(1, TimeUnit.SECONDS);\n  }\n}",
        "accept_response": "private void getZooLock(HostAndPort addr) throws KeeperException, InterruptedException {\n  log.info(\"Attempting to acquire Garbage Collector Lock\");\n  String path = ZooUtil.getRoot(getInstance()) + Constants.ZGC_LOCK;\n\n  LockWatcher lockWatcher = new LockWatcher() {\n    @Override\n    public void lostLock(LockLossReason reason) {\n      Halt.halt(\"GC lock in zookeeper lost (reason = \" + reason + \"), exiting!\", 1);\n    }\n\n    @Override\n    public void unableToMonitorLockNode(final Throwable e) {\n      // ACCUMULO-3651 Level changed to error and FATAL added to message for slf4j compatibility\n      Halt.halt(-1, new Runnable() {\n\n        @Override\n        public void run() {\n          log.error(\"FATAL: No longer able to monitor lock node \", e);\n        }\n      });\n\n    }\n  };\n\n  while (true) {\n    lock = new ZooLock(path);\n    if (lock.tryLock(lockWatcher, new ServerServices(addr.toString(), Service.GC_CLIENT).toString().getBytes())) {\n      log.info(\"Acquired Garbage Collector Lock\");\n      return;\n    }\n    log.debug(\"Failed to acquire GC ZooKeeper lock, will retry\");\n    sleepUninterruptibly(1, TimeUnit.SECONDS);\n  }\n}",
        "reject_response": "private void getZooLock(HostAndPort addr) throws KeeperException, InterruptedException {\n  log.info(\"Attempting to acquire Garbage Collector Lock\");\n  String path = ZooUtil.getRoot(getInstance()) + Constants.ZGC_LOCK;\n\n  LockWatcher lockWatcher = new LockWatcher() {\n    @Override\n    public void lostLock(LockLossReason reason) {\n      Halt.halt(\"GC lock in zookeeper lost (reason = \" + reason + \"), exiting!\", 1);\n    }\n\n    @Override\n    public void unableToMonitorLockNode(final Throwable e) {\n      // ACCUMULO-3651 Level changed to error and FATAL added to message for slf4j compatibility\n      Halt.halt(-1, new Runnable() {\n\n        @Override\n        public void run() {\n          log.error(\"FATAL: No longer able to monitor lock node \", e);\n        }\n      });\n\n    }\n  };\n\n  while (true) {\n    lock = new ZooLock(path);\n    if (lock.tryLock(lockWatcher, new ServerServices(addr.toString(), Service.GC_CLIENT).toString().getBytes())) {\n      log.debug(\"Got GC ZooKeeper lock\");\n      return;\n    }\n    log.debug(\"Failed to acquire GC ZooKeeper lock, will retry\");\n    sleepUninterruptibly(1, TimeUnit.SECONDS);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3069,
        "instruction": "@Override\npublic void decode(ChannelHandlerContext ctx, ByteBuf in, List<Object> out) {\n  Message.Type msgType = Message.Type.decode(in);\n  Message decoded = decode(msgType, in);\n  assert decoded.type() == msgType;\n  out.add(decoded);\n}",
        "accept_response": "@Override\npublic void decode(ChannelHandlerContext ctx, ByteBuf in, List<Object> out) {\n  Message.Type msgType = Message.Type.decode(in);\n  Message decoded = decode(msgType, in);\n  assert decoded.type() == msgType;\n  logger.trace(\"Received message {}: {}\", msgType, decoded);\n  out.add(decoded);\n}",
        "reject_response": "@Override\npublic void decode(ChannelHandlerContext ctx, ByteBuf in, List<Object> out) {\n  Message.Type msgType = Message.Type.decode(in);\n  Message decoded = decode(msgType, in);\n  assert decoded.type() == msgType;\n  logger.trace(\"Received message \" + msgType + \": \" + decoded);\n  out.add(decoded);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3214,
        "instruction": "public void finishSessionInit(ServerCnxn cnxn, boolean valid) {\n    // register with JMX\n    try {\n        if (valid) {\n            if (serverCnxnFactory != null && serverCnxnFactory.cnxns.contains(cnxn)) {\n                serverCnxnFactory.registerConnection(cnxn);\n            } else if (secureServerCnxnFactory != null && secureServerCnxnFactory.cnxns.contains(cnxn)) {\n                secureServerCnxnFactory.registerConnection(cnxn);\n            }\n        }\n    } catch (Exception e) {\n        LOG.warn(\"Failed to register with JMX\", e);\n    }\n\n    try {\n        ConnectResponse rsp = new ConnectResponse(0, valid ? cnxn.getSessionTimeout()\n                : 0, valid ? cnxn.getSessionId() : 0, // send 0 if session is no\n                        // longer valid\n                        valid ? generatePasswd(cnxn.getSessionId()) : new byte[16]);\n        ByteArrayOutputStream baos = new ByteArrayOutputStream();\n        BinaryOutputArchive bos = BinaryOutputArchive.getArchive(baos);\n        bos.writeInt(-1, \"len\");\n        rsp.serialize(bos, \"connect\");\n        if (!cnxn.isOldClient) {\n            bos.writeBool(\n                    this instanceof ReadOnlyZooKeeperServer, \"readOnly\");\n        }\n        baos.close();\n        ByteBuffer bb = ByteBuffer.wrap(baos.toByteArray());\n        bb.putInt(bb.remaining() - 4).rewind();\n        cnxn.sendBuffer(bb);\n\n        if (valid) {\n            cnxn.enableRecv();\n        } else {\n\n            LOG.info(\"Invalid session 0x\"\n                    + Long.toHexString(cnxn.getSessionId())\n                    + \" for client \"\n                    + cnxn.getRemoteSocketAddress()\n                    + \", probably expired\");\n            cnxn.sendBuffer(ServerCnxnFactory.closeConn);\n        }\n\n    } catch (Exception e) {\n        LOG.warn(\"Exception while establishing session, closing\", e);\n        cnxn.close();\n    }\n}",
        "accept_response": "public void finishSessionInit(ServerCnxn cnxn, boolean valid) {\n    // register with JMX\n    try {\n        if (valid) {\n            if (serverCnxnFactory != null && serverCnxnFactory.cnxns.contains(cnxn)) {\n                serverCnxnFactory.registerConnection(cnxn);\n            } else if (secureServerCnxnFactory != null && secureServerCnxnFactory.cnxns.contains(cnxn)) {\n                secureServerCnxnFactory.registerConnection(cnxn);\n            }\n        }\n    } catch (Exception e) {\n        LOG.warn(\"Failed to register with JMX\", e);\n    }\n\n    try {\n        ConnectResponse rsp = new ConnectResponse(0, valid ? cnxn.getSessionTimeout()\n                : 0, valid ? cnxn.getSessionId() : 0, // send 0 if session is no\n                        // longer valid\n                        valid ? generatePasswd(cnxn.getSessionId()) : new byte[16]);\n        ByteArrayOutputStream baos = new ByteArrayOutputStream();\n        BinaryOutputArchive bos = BinaryOutputArchive.getArchive(baos);\n        bos.writeInt(-1, \"len\");\n        rsp.serialize(bos, \"connect\");\n        if (!cnxn.isOldClient) {\n            bos.writeBool(\n                    this instanceof ReadOnlyZooKeeperServer, \"readOnly\");\n        }\n        baos.close();\n        ByteBuffer bb = ByteBuffer.wrap(baos.toByteArray());\n        bb.putInt(bb.remaining() - 4).rewind();\n        cnxn.sendBuffer(bb);\n\n        if (valid) {\n            LOG.debug(\"Established session 0x\"\n                    + Long.toHexString(cnxn.getSessionId())\n                    + \" with negotiated timeout \" + cnxn.getSessionTimeout()\n                    + \" for client \"\n                    + cnxn.getRemoteSocketAddress());\n            cnxn.enableRecv();\n        } else {\n\n            LOG.info(\"Invalid session 0x\"\n                    + Long.toHexString(cnxn.getSessionId())\n                    + \" for client \"\n                    + cnxn.getRemoteSocketAddress()\n                    + \", probably expired\");\n            cnxn.sendBuffer(ServerCnxnFactory.closeConn);\n        }\n\n    } catch (Exception e) {\n        LOG.warn(\"Exception while establishing session, closing\", e);\n        cnxn.close();\n    }\n}",
        "reject_response": "public void finishSessionInit(ServerCnxn cnxn, boolean valid) {\n    // register with JMX\n    try {\n        if (valid) {\n            if (serverCnxnFactory != null && serverCnxnFactory.cnxns.contains(cnxn)) {\n                serverCnxnFactory.registerConnection(cnxn);\n            } else if (secureServerCnxnFactory != null && secureServerCnxnFactory.cnxns.contains(cnxn)) {\n                secureServerCnxnFactory.registerConnection(cnxn);\n            }\n        }\n    } catch (Exception e) {\n        LOG.warn(\"Failed to register with JMX\", e);\n    }\n\n    try {\n        ConnectResponse rsp = new ConnectResponse(0, valid ? cnxn.getSessionTimeout()\n                : 0, valid ? cnxn.getSessionId() : 0, // send 0 if session is no\n                        // longer valid\n                        valid ? generatePasswd(cnxn.getSessionId()) : new byte[16]);\n        ByteArrayOutputStream baos = new ByteArrayOutputStream();\n        BinaryOutputArchive bos = BinaryOutputArchive.getArchive(baos);\n        bos.writeInt(-1, \"len\");\n        rsp.serialize(bos, \"connect\");\n        if (!cnxn.isOldClient) {\n            bos.writeBool(\n                    this instanceof ReadOnlyZooKeeperServer, \"readOnly\");\n        }\n        baos.close();\n        ByteBuffer bb = ByteBuffer.wrap(baos.toByteArray());\n        bb.putInt(bb.remaining() - 4).rewind();\n        cnxn.sendBuffer(bb);\n\n        if (valid) {\n            LOG.info(\"Established session 0x\"\n            cnxn.enableRecv();\n        } else {\n\n            LOG.info(\"Invalid session 0x\"\n                    + Long.toHexString(cnxn.getSessionId())\n                    + \" for client \"\n                    + cnxn.getRemoteSocketAddress()\n                    + \", probably expired\");\n            cnxn.sendBuffer(ServerCnxnFactory.closeConn);\n        }\n\n    } catch (Exception e) {\n        LOG.warn(\"Exception while establishing session, closing\", e);\n        cnxn.close();\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2921,
        "instruction": "private void processDisconnection(List<ClientResponse> responses, String nodeId, long now) {\n    connectionStates.disconnected(nodeId, now);\n    nodeApiVersions.remove(nodeId);\n    nodesNeedingApiVersionsFetch.remove(nodeId);\n    for (InFlightRequest request : this.inFlightRequests.clearAll(nodeId)) {\n        if (request.isInternalRequest && request.header.apiKey() == ApiKeys.METADATA.id)\n            metadataUpdater.handleDisconnection(request.destination);\n        else\n            responses.add(request.disconnected(now));\n    }\n}",
        "accept_response": "private void processDisconnection(List<ClientResponse> responses, String nodeId, long now) {\n    connectionStates.disconnected(nodeId, now);\n    nodeApiVersions.remove(nodeId);\n    nodesNeedingApiVersionsFetch.remove(nodeId);\n    for (InFlightRequest request : this.inFlightRequests.clearAll(nodeId)) {\n        log.trace(\"Cancelled request {} due to node {} being disconnected\", request.request, nodeId);\n        if (request.isInternalRequest && request.header.apiKey() == ApiKeys.METADATA.id)\n            metadataUpdater.handleDisconnection(request.destination);\n        else\n            responses.add(request.disconnected(now));\n    }\n}",
        "reject_response": "private void processDisconnection(List<ClientResponse> responses, String nodeId, long now) {\n    connectionStates.disconnected(nodeId, now);\n    nodeApiVersions.remove(nodeId);\n    nodesNeedingApiVersionsFetch.remove(nodeId);\n    for (InFlightRequest request : this.inFlightRequests.clearAll(nodeId)) {\n        log.trace(\"Cancelled request {} due to node {} being disconnected\", request, nodeId);\n        if (request.isInternalRequest && request.header.apiKey() == ApiKeys.METADATA.id)\n            metadataUpdater.handleDisconnection(request.destination);\n        else\n            responses.add(request.disconnected(now));\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3110,
        "instruction": "private static void compileIfAbsent(CompilationContext context, Schema schema, EvalNode eval) {\n  Pair<Schema, EvalNode> key = new Pair<Schema, EvalNode>(schema, eval);\n  if (!context.compiledEval.containsKey(key)) {\n    try {\n      EvalNode compiled = context.compiler.compile(schema, eval);\n      context.compiledEval.put(key, compiled);\n\n    } catch (Throwable t) {\n      // If any compilation error occurs, it works in a fallback mode. This mode just uses EvalNode objects\n      // instead of a compiled EvalNode.\n      context.compiledEval.put(key, eval);\n    }\n  }\n}",
        "accept_response": "private static void compileIfAbsent(CompilationContext context, Schema schema, EvalNode eval) {\n  Pair<Schema, EvalNode> key = new Pair<Schema, EvalNode>(schema, eval);\n  if (!context.compiledEval.containsKey(key)) {\n    try {\n      EvalNode compiled = context.compiler.compile(schema, eval);\n      context.compiledEval.put(key, compiled);\n\n    } catch (Throwable t) {\n      // If any compilation error occurs, it works in a fallback mode. This mode just uses EvalNode objects\n      // instead of a compiled EvalNode.\n      context.compiledEval.put(key, eval);\n      LOG.warn(t, t);\n    }\n  }\n}",
        "reject_response": "private static void compileIfAbsent(CompilationContext context, Schema schema, EvalNode eval) {\n  Pair<Schema, EvalNode> key = new Pair<Schema, EvalNode>(schema, eval);\n  if (!context.compiledEval.containsKey(key)) {\n    try {\n      EvalNode compiled = context.compiler.compile(schema, eval);\n      context.compiledEval.put(key, compiled);\n\n    } catch (Throwable t) {\n      // If any compilation error occurs, it works in a fallback mode. This mode just uses EvalNode objects\n      // instead of a compiled EvalNode.\n      context.compiledEval.put(key, eval);\n      LOG.warn(t);\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3175,
        "instruction": "@Override\nprotected void serviceStart() throws Exception {\n  client.start();\n\n  ThreadFactory tf = new ThreadFactoryBuilder().setNameFormat(\n      this.getClass().getName() + \" #%d\").setDaemon(true).build();\n\n  // Start with a default core-pool size and change it dynamically.\n  int initSize = Math.min(INITIAL_THREAD_POOL_SIZE, maxThreadPoolSize);\n  threadPool = new ThreadPoolExecutor(initSize, Integer.MAX_VALUE, 1,\n      TimeUnit.HOURS, new LinkedBlockingQueue<Runnable>(), tf);\n\n  eventDispatcherThread = new Thread() {\n    @Override\n    public void run() {\n      ContainerEvent event = null;\n      Set<String> allNodes = new HashSet<String>();\n\n      while (!stopped.get() && !Thread.currentThread().isInterrupted()) {\n        try {\n          event = events.take();\n        } catch (InterruptedException e) {\n          if (!stopped.get()) {\n            LOG.error(\"Returning, thread interrupted\", e);\n          }\n          return;\n        }\n\n        allNodes.add(event.getNodeId().toString());\n\n        int threadPoolSize = threadPool.getCorePoolSize();\n\n        // We can increase the pool size only if haven't reached the maximum\n        // limit yet.\n        if (threadPoolSize != maxThreadPoolSize) {\n\n          // nodes where containers will run at *this* point of time. This is\n          // *not* the cluster size and doesn't need to be.\n          int nodeNum = allNodes.size();\n          int idealThreadPoolSize = Math.min(maxThreadPoolSize, nodeNum);\n\n          if (threadPoolSize < idealThreadPoolSize) {\n            // Bump up the pool size to idealThreadPoolSize +\n            // INITIAL_POOL_SIZE, the later is just a buffer so we are not\n            // always increasing the pool-size\n            int newThreadPoolSize = Math.min(maxThreadPoolSize,\n                idealThreadPoolSize + INITIAL_THREAD_POOL_SIZE);\n            threadPool.setCorePoolSize(newThreadPoolSize);\n          }\n        }\n\n        // the events from the queue are handled in parallel with a thread\n        // pool\n        threadPool.execute(getContainerEventProcessor(event));\n\n        // TODO: Group launching of multiple containers to a single\n        // NodeManager into a single connection\n      }\n    }\n  };\n  eventDispatcherThread.setName(\"Container  Event Dispatcher\");\n  eventDispatcherThread.setDaemon(false);\n  eventDispatcherThread.start();\n\n  super.serviceStart();\n}",
        "accept_response": "@Override\nprotected void serviceStart() throws Exception {\n  client.start();\n\n  ThreadFactory tf = new ThreadFactoryBuilder().setNameFormat(\n      this.getClass().getName() + \" #%d\").setDaemon(true).build();\n\n  // Start with a default core-pool size and change it dynamically.\n  int initSize = Math.min(INITIAL_THREAD_POOL_SIZE, maxThreadPoolSize);\n  threadPool = new ThreadPoolExecutor(initSize, Integer.MAX_VALUE, 1,\n      TimeUnit.HOURS, new LinkedBlockingQueue<Runnable>(), tf);\n\n  eventDispatcherThread = new Thread() {\n    @Override\n    public void run() {\n      ContainerEvent event = null;\n      Set<String> allNodes = new HashSet<String>();\n\n      while (!stopped.get() && !Thread.currentThread().isInterrupted()) {\n        try {\n          event = events.take();\n        } catch (InterruptedException e) {\n          if (!stopped.get()) {\n            LOG.error(\"Returning, thread interrupted\", e);\n          }\n          return;\n        }\n\n        allNodes.add(event.getNodeId().toString());\n\n        int threadPoolSize = threadPool.getCorePoolSize();\n\n        // We can increase the pool size only if haven't reached the maximum\n        // limit yet.\n        if (threadPoolSize != maxThreadPoolSize) {\n\n          // nodes where containers will run at *this* point of time. This is\n          // *not* the cluster size and doesn't need to be.\n          int nodeNum = allNodes.size();\n          int idealThreadPoolSize = Math.min(maxThreadPoolSize, nodeNum);\n\n          if (threadPoolSize < idealThreadPoolSize) {\n            // Bump up the pool size to idealThreadPoolSize +\n            // INITIAL_POOL_SIZE, the later is just a buffer so we are not\n            // always increasing the pool-size\n            int newThreadPoolSize = Math.min(maxThreadPoolSize,\n                idealThreadPoolSize + INITIAL_THREAD_POOL_SIZE);\n            LOG.info(\"Set NMClientAsync thread pool size to {} \" +\n                \"as the number of nodes to talk to is {}.\", newThreadPoolSize, nodeNum);\n            threadPool.setCorePoolSize(newThreadPoolSize);\n          }\n        }\n\n        // the events from the queue are handled in parallel with a thread\n        // pool\n        threadPool.execute(getContainerEventProcessor(event));\n\n        // TODO: Group launching of multiple containers to a single\n        // NodeManager into a single connection\n      }\n    }\n  };\n  eventDispatcherThread.setName(\"Container  Event Dispatcher\");\n  eventDispatcherThread.setDaemon(false);\n  eventDispatcherThread.start();\n\n  super.serviceStart();\n}",
        "reject_response": "@Override\nprotected void serviceStart() throws Exception {\n  client.start();\n\n  ThreadFactory tf = new ThreadFactoryBuilder().setNameFormat(\n      this.getClass().getName() + \" #%d\").setDaemon(true).build();\n\n  // Start with a default core-pool size and change it dynamically.\n  int initSize = Math.min(INITIAL_THREAD_POOL_SIZE, maxThreadPoolSize);\n  threadPool = new ThreadPoolExecutor(initSize, Integer.MAX_VALUE, 1,\n      TimeUnit.HOURS, new LinkedBlockingQueue<Runnable>(), tf);\n\n  eventDispatcherThread = new Thread() {\n    @Override\n    public void run() {\n      ContainerEvent event = null;\n      Set<String> allNodes = new HashSet<String>();\n\n      while (!stopped.get() && !Thread.currentThread().isInterrupted()) {\n        try {\n          event = events.take();\n        } catch (InterruptedException e) {\n          if (!stopped.get()) {\n            LOG.error(\"Returning, thread interrupted\", e);\n          }\n          return;\n        }\n\n        allNodes.add(event.getNodeId().toString());\n\n        int threadPoolSize = threadPool.getCorePoolSize();\n\n        // We can increase the pool size only if haven't reached the maximum\n        // limit yet.\n        if (threadPoolSize != maxThreadPoolSize) {\n\n          // nodes where containers will run at *this* point of time. This is\n          // *not* the cluster size and doesn't need to be.\n          int nodeNum = allNodes.size();\n          int idealThreadPoolSize = Math.min(maxThreadPoolSize, nodeNum);\n\n          if (threadPoolSize < idealThreadPoolSize) {\n            // Bump up the pool size to idealThreadPoolSize +\n            // INITIAL_POOL_SIZE, the later is just a buffer so we are not\n            // always increasing the pool-size\n            int newThreadPoolSize = Math.min(maxThreadPoolSize,\n                idealThreadPoolSize + INITIAL_THREAD_POOL_SIZE);\n            LOG.info(\"Set NMClientAsync thread pool size to \" +\n                newThreadPoolSize + \" as the number of nodes to talk to is \"\n                + nodeNum);\n            threadPool.setCorePoolSize(newThreadPoolSize);\n          }\n        }\n\n        // the events from the queue are handled in parallel with a thread\n        // pool\n        threadPool.execute(getContainerEventProcessor(event));\n\n        // TODO: Group launching of multiple containers to a single\n        // NodeManager into a single connection\n      }\n    }\n  };\n  eventDispatcherThread.setName(\"Container  Event Dispatcher\");\n  eventDispatcherThread.setDaemon(false);\n  eventDispatcherThread.start();\n\n  super.serviceStart();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2592,
        "instruction": "private void beginNewAlignment(long checkpointId, int channelIndex) throws IOException {\n\tcurrentCheckpointId = checkpointId;\n\tonBarrier(channelIndex);\n\n\tstartOfAlignmentTimestamp = System.nanoTime();\n}",
        "accept_response": "private void beginNewAlignment(long checkpointId, int channelIndex) throws IOException {\n\tcurrentCheckpointId = checkpointId;\n\tonBarrier(channelIndex);\n\n\tstartOfAlignmentTimestamp = System.nanoTime();\n\n\tif (LOG.isDebugEnabled()) {\n\t\tLOG.debug(\"{}: Starting stream alignment for checkpoint {}.\",\n\t\t\tinputGate.getOwningTaskName(),\n\t\t\tcheckpointId);\n\t}\n}",
        "reject_response": "private void beginNewAlignment(long checkpointId, int channelIndex) throws IOException {\n\tcurrentCheckpointId = checkpointId;\n\tonBarrier(channelIndex);\n\n\tstartOfAlignmentTimestamp = System.nanoTime();\n\n\tif (LOG.isDebugEnabled()) {\n\t\tLOG.debug(\"Starting stream alignment for checkpoint \" + checkpointId + '.');\n\t}\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2747,
        "instruction": "public List<DatanodeDescriptor> getDatanodeListForReport(\n    final DatanodeReportType type) {\n  final boolean listLiveNodes =\n      type == DatanodeReportType.ALL ||\n      type == DatanodeReportType.LIVE;\n  final boolean listDeadNodes =\n      type == DatanodeReportType.ALL ||\n      type == DatanodeReportType.DEAD;\n  final boolean listDecommissioningNodes =\n      type == DatanodeReportType.ALL ||\n      type == DatanodeReportType.DECOMMISSIONING;\n  final boolean listEnteringMaintenanceNodes =\n      type == DatanodeReportType.ALL ||\n      type == DatanodeReportType.ENTERING_MAINTENANCE;\n  final boolean listInMaintenanceNodes =\n      type == DatanodeReportType.ALL ||\n      type == DatanodeReportType.IN_MAINTENANCE;\n\n  ArrayList<DatanodeDescriptor> nodes;\n  final HostSet foundNodes = new HostSet();\n  final Iterable<InetSocketAddress> includedNodes =\n      hostConfigManager.getIncludes();\n\n  synchronized(this) {\n    nodes = new ArrayList<>(datanodeMap.size());\n    for (DatanodeDescriptor dn : datanodeMap.values()) {\n      final boolean isDead = isDatanodeDead(dn);\n      final boolean isDecommissioning = dn.isDecommissionInProgress();\n      final boolean isEnteringMaintenance = dn.isEnteringMaintenance();\n      final boolean isInMaintenance = dn.isInMaintenance();\n\n      if (((listLiveNodes && !isDead) ||\n          (listDeadNodes && isDead) ||\n          (listDecommissioningNodes && isDecommissioning) ||\n          (listEnteringMaintenanceNodes && isEnteringMaintenance) ||\n          (listInMaintenanceNodes && isInMaintenance)) &&\n          hostConfigManager.isIncluded(dn)) {\n        nodes.add(dn);\n      }\n\n      foundNodes.add(dn.getResolvedAddress());\n    }\n  }\n  Collections.sort(nodes);\n\n  if (listDeadNodes) {\n    for (InetSocketAddress addr : includedNodes) {\n      if (foundNodes.matchedBy(addr)) {\n        continue;\n      }\n      // The remaining nodes are ones that are referenced by the hosts\n      // files but that we do not know about, ie that we have never\n      // head from. Eg. an entry that is no longer part of the cluster\n      // or a bogus entry was given in the hosts files\n      //\n      // If the host file entry specified the xferPort, we use that.\n      // Otherwise, we guess that it is the default xfer port.\n      // We can't ask the DataNode what it had configured, because it's\n      // dead.\n      DatanodeDescriptor dn = new DatanodeDescriptor(new DatanodeID(addr\n              .getAddress().getHostAddress(), addr.getHostName(), \"\",\n              addr.getPort() == 0 ? defaultXferPort : addr.getPort(),\n              defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n      setDatanodeDead(dn);\n      if (hostConfigManager.isExcluded(dn)) {\n        dn.setDecommissioned();\n      }\n      nodes.add(dn);\n    }\n  }\n  return nodes;\n}",
        "accept_response": "public List<DatanodeDescriptor> getDatanodeListForReport(\n    final DatanodeReportType type) {\n  final boolean listLiveNodes =\n      type == DatanodeReportType.ALL ||\n      type == DatanodeReportType.LIVE;\n  final boolean listDeadNodes =\n      type == DatanodeReportType.ALL ||\n      type == DatanodeReportType.DEAD;\n  final boolean listDecommissioningNodes =\n      type == DatanodeReportType.ALL ||\n      type == DatanodeReportType.DECOMMISSIONING;\n  final boolean listEnteringMaintenanceNodes =\n      type == DatanodeReportType.ALL ||\n      type == DatanodeReportType.ENTERING_MAINTENANCE;\n  final boolean listInMaintenanceNodes =\n      type == DatanodeReportType.ALL ||\n      type == DatanodeReportType.IN_MAINTENANCE;\n\n  ArrayList<DatanodeDescriptor> nodes;\n  final HostSet foundNodes = new HostSet();\n  final Iterable<InetSocketAddress> includedNodes =\n      hostConfigManager.getIncludes();\n\n  synchronized(this) {\n    nodes = new ArrayList<>(datanodeMap.size());\n    for (DatanodeDescriptor dn : datanodeMap.values()) {\n      final boolean isDead = isDatanodeDead(dn);\n      final boolean isDecommissioning = dn.isDecommissionInProgress();\n      final boolean isEnteringMaintenance = dn.isEnteringMaintenance();\n      final boolean isInMaintenance = dn.isInMaintenance();\n\n      if (((listLiveNodes && !isDead) ||\n          (listDeadNodes && isDead) ||\n          (listDecommissioningNodes && isDecommissioning) ||\n          (listEnteringMaintenanceNodes && isEnteringMaintenance) ||\n          (listInMaintenanceNodes && isInMaintenance)) &&\n          hostConfigManager.isIncluded(dn)) {\n        nodes.add(dn);\n      }\n\n      foundNodes.add(dn.getResolvedAddress());\n    }\n  }\n  Collections.sort(nodes);\n\n  if (listDeadNodes) {\n    for (InetSocketAddress addr : includedNodes) {\n      if (foundNodes.matchedBy(addr)) {\n        continue;\n      }\n      // The remaining nodes are ones that are referenced by the hosts\n      // files but that we do not know about, ie that we have never\n      // head from. Eg. an entry that is no longer part of the cluster\n      // or a bogus entry was given in the hosts files\n      //\n      // If the host file entry specified the xferPort, we use that.\n      // Otherwise, we guess that it is the default xfer port.\n      // We can't ask the DataNode what it had configured, because it's\n      // dead.\n      DatanodeDescriptor dn = new DatanodeDescriptor(new DatanodeID(addr\n              .getAddress().getHostAddress(), addr.getHostName(), \"\",\n              addr.getPort() == 0 ? defaultXferPort : addr.getPort(),\n              defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n      setDatanodeDead(dn);\n      if (hostConfigManager.isExcluded(dn)) {\n        dn.setDecommissioned();\n      }\n      nodes.add(dn);\n    }\n  }\n\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"getDatanodeListForReport with includedNodes = {}, excludedNodes = {}\"\n            + \", foundNodes = {}, nodes = {}.\", hostConfigManager.getIncludes(),\n        hostConfigManager.getExcludes(), foundNodes, nodes);\n  }\n  return nodes;\n}",
        "reject_response": "public List<DatanodeDescriptor> getDatanodeListForReport(\n    final DatanodeReportType type) {\n  final boolean listLiveNodes =\n      type == DatanodeReportType.ALL ||\n      type == DatanodeReportType.LIVE;\n  final boolean listDeadNodes =\n      type == DatanodeReportType.ALL ||\n      type == DatanodeReportType.DEAD;\n  final boolean listDecommissioningNodes =\n      type == DatanodeReportType.ALL ||\n      type == DatanodeReportType.DECOMMISSIONING;\n  final boolean listEnteringMaintenanceNodes =\n      type == DatanodeReportType.ALL ||\n      type == DatanodeReportType.ENTERING_MAINTENANCE;\n  final boolean listInMaintenanceNodes =\n      type == DatanodeReportType.ALL ||\n      type == DatanodeReportType.IN_MAINTENANCE;\n\n  ArrayList<DatanodeDescriptor> nodes;\n  final HostSet foundNodes = new HostSet();\n  final Iterable<InetSocketAddress> includedNodes =\n      hostConfigManager.getIncludes();\n\n  synchronized(this) {\n    nodes = new ArrayList<>(datanodeMap.size());\n    for (DatanodeDescriptor dn : datanodeMap.values()) {\n      final boolean isDead = isDatanodeDead(dn);\n      final boolean isDecommissioning = dn.isDecommissionInProgress();\n      final boolean isEnteringMaintenance = dn.isEnteringMaintenance();\n      final boolean isInMaintenance = dn.isInMaintenance();\n\n      if (((listLiveNodes && !isDead) ||\n          (listDeadNodes && isDead) ||\n          (listDecommissioningNodes && isDecommissioning) ||\n          (listEnteringMaintenanceNodes && isEnteringMaintenance) ||\n          (listInMaintenanceNodes && isInMaintenance)) &&\n          hostConfigManager.isIncluded(dn)) {\n        nodes.add(dn);\n      }\n\n      foundNodes.add(dn.getResolvedAddress());\n    }\n  }\n  Collections.sort(nodes);\n\n  if (listDeadNodes) {\n    for (InetSocketAddress addr : includedNodes) {\n      if (foundNodes.matchedBy(addr)) {\n        continue;\n      }\n      // The remaining nodes are ones that are referenced by the hosts\n      // files but that we do not know about, ie that we have never\n      // head from. Eg. an entry that is no longer part of the cluster\n      // or a bogus entry was given in the hosts files\n      //\n      // If the host file entry specified the xferPort, we use that.\n      // Otherwise, we guess that it is the default xfer port.\n      // We can't ask the DataNode what it had configured, because it's\n      // dead.\n      DatanodeDescriptor dn = new DatanodeDescriptor(new DatanodeID(addr\n              .getAddress().getHostAddress(), addr.getHostName(), \"\",\n              addr.getPort() == 0 ? defaultXferPort : addr.getPort(),\n              defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n      setDatanodeDead(dn);\n      if (hostConfigManager.isExcluded(dn)) {\n        dn.setDecommissioned();\n      }\n      nodes.add(dn);\n    }\n  }\n\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"getDatanodeListForReport with \" +\n        \"includedNodes = \" + hostConfigManager.getIncludes() +\n        \", excludedNodes = \" + hostConfigManager.getExcludes() +\n        \", foundNodes = \" + foundNodes +\n        \", nodes = \" + nodes);\n  }\n  return nodes;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2470,
        "instruction": "public void deleteEntities(List<AtlasVertex> instanceVertices) throws AtlasException {\n   RequestContext requestContext = RequestContext.get();\n\n   Set<AtlasVertex> deletionCandidateVertices = new HashSet<>();\n\n   for (AtlasVertex instanceVertex : instanceVertices) {\n        String guid = GraphHelper.getGuid(instanceVertex);\n        Id.EntityState state = GraphHelper.getState(instanceVertex);\n        if (requestContext.getDeletedEntityIds().contains(guid) || state == Id.EntityState.DELETED) {\n\n            continue;\n        }\n\n       // Get GUIDs and vertices for all deletion candidates.\n       Set<VertexInfo> compositeVertices = graphHelper.getCompositeVertices(instanceVertex);\n\n       // Record all deletion candidate GUIDs in RequestContext\n       // and gather deletion candidate vertices.\n       for (VertexInfo vertexInfo : compositeVertices) {\n           requestContext.recordEntityDelete(vertexInfo.getGuid(), vertexInfo.getTypeName());\n           deletionCandidateVertices.add(vertexInfo.getVertex());\n       }\n   }\n\n   // Delete traits and vertices.\n   for (AtlasVertex deletionCandidateVertex : deletionCandidateVertices) {\n       deleteAllTraits(deletionCandidateVertex);\n       deleteTypeVertex(deletionCandidateVertex, false);\n   }\n}",
        "accept_response": "public void deleteEntities(List<AtlasVertex> instanceVertices) throws AtlasException {\n   RequestContext requestContext = RequestContext.get();\n\n   Set<AtlasVertex> deletionCandidateVertices = new HashSet<>();\n\n   for (AtlasVertex instanceVertex : instanceVertices) {\n        String guid = GraphHelper.getGuid(instanceVertex);\n        Id.EntityState state = GraphHelper.getState(instanceVertex);\n        if (requestContext.getDeletedEntityIds().contains(guid) || state == Id.EntityState.DELETED) {\n            if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Skipping deletion of {} as it is already deleted\", guid);\n            }\n\n            continue;\n        }\n\n       // Get GUIDs and vertices for all deletion candidates.\n       Set<VertexInfo> compositeVertices = graphHelper.getCompositeVertices(instanceVertex);\n\n       // Record all deletion candidate GUIDs in RequestContext\n       // and gather deletion candidate vertices.\n       for (VertexInfo vertexInfo : compositeVertices) {\n           requestContext.recordEntityDelete(vertexInfo.getGuid(), vertexInfo.getTypeName());\n           deletionCandidateVertices.add(vertexInfo.getVertex());\n       }\n   }\n\n   // Delete traits and vertices.\n   for (AtlasVertex deletionCandidateVertex : deletionCandidateVertices) {\n       deleteAllTraits(deletionCandidateVertex);\n       deleteTypeVertex(deletionCandidateVertex, false);\n   }\n}",
        "reject_response": "public void deleteEntities(List<AtlasVertex> instanceVertices) throws AtlasException {\n   RequestContext requestContext = RequestContext.get();\n\n   Set<AtlasVertex> deletionCandidateVertices = new HashSet<>();\n\n   for (AtlasVertex instanceVertex : instanceVertices) {\n        String guid = GraphHelper.getGuid(instanceVertex);\n        Id.EntityState state = GraphHelper.getState(instanceVertex);\n        if (requestContext.getDeletedEntityIds().contains(guid) || state == Id.EntityState.DELETED) {\n            if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Skipping deletion of {} as it is already deleted\", guid);\n               continue;\n            }\n\n            continue;\n        }\n\n       // Get GUIDs and vertices for all deletion candidates.\n       Set<VertexInfo> compositeVertices = graphHelper.getCompositeVertices(instanceVertex);\n\n       // Record all deletion candidate GUIDs in RequestContext\n       // and gather deletion candidate vertices.\n       for (VertexInfo vertexInfo : compositeVertices) {\n           requestContext.recordEntityDelete(vertexInfo.getGuid(), vertexInfo.getTypeName());\n           deletionCandidateVertices.add(vertexInfo.getVertex());\n       }\n   }\n\n   // Delete traits and vertices.\n   for (AtlasVertex deletionCandidateVertex : deletionCandidateVertices) {\n       deleteAllTraits(deletionCandidateVertex);\n       deleteTypeVertex(deletionCandidateVertex, false);\n   }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3150,
        "instruction": "private void maybeFlush(RecoveryStream recoveryStream) throws IOException {\n  long currentTime = appContext.getClock().getTime();\n  boolean doFlush = false;\n  if (maxUnflushedEvents >=0\n      && unflushedEventsCount >= maxUnflushedEvents) {\n    doFlush = true;\n  } else if (flushInterval >= 0\n      && ((currentTime - lastFlushTime) >= (flushInterval*1000))) {\n    LOG.debug(\"Flush interval time period elapsed. Flushing recovery data\"\n        + \", lastTimeSinceFLush=\" + lastFlushTime\n        + \", timeSinceLastFlush=\" + (currentTime - lastFlushTime));\n    doFlush = true;\n  }\n  if (!doFlush) {\n    return;\n  }\n  doFlush(recoveryStream, currentTime);\n}",
        "accept_response": "private void maybeFlush(RecoveryStream recoveryStream) throws IOException {\n  long currentTime = appContext.getClock().getTime();\n  boolean doFlush = false;\n  if (maxUnflushedEvents >=0\n      && unflushedEventsCount >= maxUnflushedEvents) {\n      LOG.debug(\"Max unflushed events count reached. Flushing recovery data, \"\n          + \"unflushedEventsCount={}, maxUnflushedEvents={}\", unflushedEventsCount,\n          maxUnflushedEvents);\n    doFlush = true;\n  } else if (flushInterval >= 0\n      && ((currentTime - lastFlushTime) >= (flushInterval*1000))) {\n    LOG.debug(\"Flush interval time period elapsed. Flushing recovery data\"\n        + \", lastTimeSinceFLush=\" + lastFlushTime\n        + \", timeSinceLastFlush=\" + (currentTime - lastFlushTime));\n    doFlush = true;\n  }\n  if (!doFlush) {\n    return;\n  }\n  doFlush(recoveryStream, currentTime);\n}",
        "reject_response": "private void maybeFlush(RecoveryStream recoveryStream) throws IOException {\n  long currentTime = appContext.getClock().getTime();\n  boolean doFlush = false;\n  if (maxUnflushedEvents >=0\n      && unflushedEventsCount >= maxUnflushedEvents) {\n    if  (LOG.isDebugEnabled()) {\n      LOG.debug(\"Max unflushed events count reached. Flushing recovery data\"\n          + \", unflushedEventsCount=\" + unflushedEventsCount\n          + \", maxUnflushedEvents=\" + maxUnflushedEvents);\n    }\n    doFlush = true;\n  } else if (flushInterval >= 0\n      && ((currentTime - lastFlushTime) >= (flushInterval*1000))) {\n    LOG.debug(\"Flush interval time period elapsed. Flushing recovery data\"\n        + \", lastTimeSinceFLush=\" + lastFlushTime\n        + \", timeSinceLastFlush=\" + (currentTime - lastFlushTime));\n    doFlush = true;\n  }\n  if (!doFlush) {\n    return;\n  }\n  doFlush(recoveryStream, currentTime);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3057,
        "instruction": "private long downloadIndexFiles(boolean downloadCompleteIndex, Directory indexDir, Directory tmpIndexDir, long latestGeneration)\n    throws Exception {\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"Download files to dir: \" + Arrays.asList(indexDir.listAll()));\n  }\n  long bytesDownloaded = 0;\n  for (Map<String,Object> file : filesToDownload) {\n    String filename = (String) file.get(NAME);\n    long size = (Long) file.get(SIZE);\n    Long checksum = (Long) file.get(CHECKSUM);\n    CompareResult compareResult = compareFile(indexDir, filename, size, checksum);\n    boolean alwaysDownload = filesToAlwaysDownloadIfNoChecksums(filename, size, compareResult);\n    if (!compareResult.equal || downloadCompleteIndex || alwaysDownload) {\n      dirFileFetcher = new DirectoryFileFetcher(tmpIndexDir, file,\n          (String) file.get(NAME), FILE, latestGeneration);\n      currentFile = file;\n      dirFileFetcher.fetchFile();\n      bytesDownloaded += dirFileFetcher.getBytesDownloaded();\n      filesDownloaded.add(new HashMap<>(file));\n    } else {\n      LOG.info(\"Skipping download for \" + file.get(NAME)\n          + \" because it already exists\");\n    }\n  }\n  return bytesDownloaded;\n}",
        "accept_response": "private long downloadIndexFiles(boolean downloadCompleteIndex, Directory indexDir, Directory tmpIndexDir, long latestGeneration)\n    throws Exception {\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"Download files to dir: \" + Arrays.asList(indexDir.listAll()));\n  }\n  long bytesDownloaded = 0;\n  for (Map<String,Object> file : filesToDownload) {\n    String filename = (String) file.get(NAME);\n    long size = (Long) file.get(SIZE);\n    Long checksum = (Long) file.get(CHECKSUM);\n    CompareResult compareResult = compareFile(indexDir, filename, size, checksum);\n    boolean alwaysDownload = filesToAlwaysDownloadIfNoChecksums(filename, size, compareResult);\n    LOG.debug(\"Downloading file={} size={} checksum={} alwaysDownload={}\", filename, size, checksum, alwaysDownload);\n    if (!compareResult.equal || downloadCompleteIndex || alwaysDownload) {\n      dirFileFetcher = new DirectoryFileFetcher(tmpIndexDir, file,\n          (String) file.get(NAME), FILE, latestGeneration);\n      currentFile = file;\n      dirFileFetcher.fetchFile();\n      bytesDownloaded += dirFileFetcher.getBytesDownloaded();\n      filesDownloaded.add(new HashMap<>(file));\n    } else {\n      LOG.info(\"Skipping download for \" + file.get(NAME)\n          + \" because it already exists\");\n    }\n  }\n  return bytesDownloaded;\n}",
        "reject_response": "private long downloadIndexFiles(boolean downloadCompleteIndex, Directory indexDir, Directory tmpIndexDir, long latestGeneration)\n    throws Exception {\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"Download files to dir: \" + Arrays.asList(indexDir.listAll()));\n  }\n  long bytesDownloaded = 0;\n  for (Map<String,Object> file : filesToDownload) {\n    String filename = (String) file.get(NAME);\n    long size = (Long) file.get(SIZE);\n    Long checksum = (Long) file.get(CHECKSUM);\n    CompareResult compareResult = compareFile(indexDir, filename, size, checksum);\n    boolean alwaysDownload = filesToAlwaysDownloadIfNoChecksums(filename, size, compareResult);\n    LOG.debug(\"Downloading file={} size={} checksum={} alwaysDownload={}\", filename, size, file.get(CHECKSUM), alwaysDownload);\n    if (!compareResult.equal || downloadCompleteIndex || alwaysDownload) {\n      dirFileFetcher = new DirectoryFileFetcher(tmpIndexDir, file,\n          (String) file.get(NAME), FILE, latestGeneration);\n      currentFile = file;\n      dirFileFetcher.fetchFile();\n      bytesDownloaded += dirFileFetcher.getBytesDownloaded();\n      filesDownloaded.add(new HashMap<>(file));\n    } else {\n      LOG.info(\"Skipping download for \" + file.get(NAME)\n          + \" because it already exists\");\n    }\n  }\n  return bytesDownloaded;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2943,
        "instruction": "public static boolean validateIfIssueManagementComplete( MavenProject project, String issueManagementSystem,\n                                                         String mojoResult, Log log )\n{\n    if ( project.getIssueManagement() == null )\n    {\n        log.error( \"No Issue Management set. No \" + mojoResult + \" will be generated.\" );\n\n        return false;\n    }\n    else if ( ( project.getIssueManagement().getUrl() == null )\n        || ( project.getIssueManagement().getUrl().trim().equals( \"\" ) ) )\n    {\n        log.error( \"No URL set in Issue Management. No \" + mojoResult + \" will be generated.\" );\n\n        return false;\n    }\n    else if ( ( project.getIssueManagement().getSystem() == null )\n        || ( project.getIssueManagement().getSystem().trim().equals(\"\") ) )\n    {\n\n        return false;\n    }\n    else if ( !( project.getIssueManagement().getSystem().equalsIgnoreCase( issueManagementSystem ) ) )\n    {\n        log.debug( \"The \" + mojoResult + \" only supports \" + issueManagementSystem + \".  No \" + mojoResult\n            + \" will be generated.\" );\n\n        return false;\n    }\n    return true;\n}",
        "accept_response": "public static boolean validateIfIssueManagementComplete( MavenProject project, String issueManagementSystem,\n                                                         String mojoResult, Log log )\n{\n    if ( project.getIssueManagement() == null )\n    {\n        log.error( \"No Issue Management set. No \" + mojoResult + \" will be generated.\" );\n\n        return false;\n    }\n    else if ( ( project.getIssueManagement().getUrl() == null )\n        || ( project.getIssueManagement().getUrl().trim().equals( \"\" ) ) )\n    {\n        log.error( \"No URL set in Issue Management. No \" + mojoResult + \" will be generated.\" );\n\n        return false;\n    }\n    else if ( ( project.getIssueManagement().getSystem() == null )\n        || ( project.getIssueManagement().getSystem().trim().equals(\"\") ) )\n    {\n        log.error( \"No System set in Issue Management. No \" + mojoResult + \" will be generated.\" );\n\n        return false;\n    }\n    else if ( !( project.getIssueManagement().getSystem().equalsIgnoreCase( issueManagementSystem ) ) )\n    {\n        log.debug( \"The \" + mojoResult + \" only supports \" + issueManagementSystem + \".  No \" + mojoResult\n            + \" will be generated.\" );\n\n        return false;\n    }\n    return true;\n}",
        "reject_response": "public static boolean validateIfIssueManagementComplete( MavenProject project, String issueManagementSystem,\n                                                         String mojoResult, Log log )\n{\n    if ( project.getIssueManagement() == null )\n    {\n        log.error( \"No Issue Management set. No \" + mojoResult + \" will be generated.\" );\n\n        return false;\n    }\n    else if ( ( project.getIssueManagement().getUrl() == null )\n        || ( project.getIssueManagement().getUrl().trim().equals( \"\" ) ) )\n    {\n        log.error( \"No URL set in Issue Management. No \" + mojoResult + \" will be generated.\" );\n\n        return false;\n    }\n    else if ( ( project.getIssueManagement().getSystem() == null )\n        || ( project.getIssueManagement().getSystem().trim().equals(\"\") ) )\n    {\n        log.error( \"The \" + mojoResult + \" only supports \" + issueManagementSystem + \".  No \" + mojoResult\n\n        return false;\n    }\n    else if ( !( project.getIssueManagement().getSystem().equalsIgnoreCase( issueManagementSystem ) ) )\n    {\n        log.debug( \"The \" + mojoResult + \" only supports \" + issueManagementSystem + \".  No \" + mojoResult\n            + \" will be generated.\" );\n\n        return false;\n    }\n    return true;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2596,
        "instruction": "@Override\npublic LocatableInputSplit getNextInputSplit(String host) {\n\t// for a null host, we return an arbitrary split\n\tif (host == null) {\n\n\t\tsynchronized (this.unassigned) {\n\t\t\tIterator<LocatableInputSplit> iter = this.unassigned.iterator();\n\t\t\tif (iter.hasNext()) {\n\t\t\t\tLocatableInputSplit next = iter.next();\n\t\t\t\titer.remove();\n\n\t\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\t\tLOG.info(\"Assigning split to null host (random assignment).\");\n\t\t\t\t}\n\n\t\t\t\tremoteAssignments++;\n\t\t\t\treturn next;\n\t\t\t} else {\n\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\tLOG.debug(\"No more unassigned input splits remaining.\");\n\t\t\t\t}\n\t\t\t\treturn null;\n\t\t\t}\n\t\t}\n\t}\n\n\thost = host.toLowerCase(Locale.US);\n\n\t// for any non-null host, we take the list of non-null splits\n\tList<LocatableInputSplit> localSplits = this.localPerHost.get(host);\n\n\t// if we have no list for this host yet, create one\n\tif (localSplits == null) {\n\t\tlocalSplits = new ArrayList<LocatableInputSplit>(16);\n\n\t\t// lock the list, to be sure that others have to wait for that host's local list\n\t\tsynchronized (localSplits) {\n\t\t\tList<LocatableInputSplit> prior = this.localPerHost.putIfAbsent(host, localSplits);\n\n\t\t\t// if someone else beat us in the case to create this list, then we do not populate this one, but\n\t\t\t// simply work with that other list\n\t\t\tif (prior == null) {\n\t\t\t\t// we are the first, we populate\n\n\t\t\t\t// first, copy the remaining splits to release the lock on the set early\n\t\t\t\t// because that is shared among threads\n\t\t\t\tLocatableInputSplit[] remaining;\n\t\t\t\tsynchronized (this.unassigned) {\n\t\t\t\t\tremaining = (LocatableInputSplit[]) this.unassigned.toArray(new LocatableInputSplit[this.unassigned.size()]);\n\t\t\t\t}\n\n\t\t\t\tfor (LocatableInputSplit is : remaining) {\n\t\t\t\t\tif (isLocal(host, is.getHostnames())) {\n\t\t\t\t\t\tlocalSplits.add(is);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\telse {\n\t\t\t\t// someone else was faster\n\t\t\t\tlocalSplits = prior;\n\t\t\t}\n\t\t}\n\t}\n\n\n\t// at this point, we have a list of local splits (possibly empty)\n\t// we need to make sure no one else operates in the current list (that protects against\n\t// list creation races) and that the unassigned set is consistent\n\t// NOTE: we need to obtain the locks in this order, strictly!!!\n\tsynchronized (localSplits) {\n\t\tint size = localSplits.size();\n\t\tif (size > 0) {\n\t\t\tsynchronized (this.unassigned) {\n\t\t\t\tdo {\n\t\t\t\t\t--size;\n\t\t\t\t\tLocatableInputSplit split = localSplits.remove(size);\n\t\t\t\t\tif (this.unassigned.remove(split)) {\n\n\t\t\t\t\t\tlocalAssignments++;\n\t\t\t\t\t\treturn split;\n\t\t\t\t\t}\n\t\t\t\t} while (size > 0);\n\t\t\t}\n\t\t}\n\t}\n\n\t// we did not find a local split, return any\n\tsynchronized (this.unassigned) {\n\t\tIterator<LocatableInputSplit> iter = this.unassigned.iterator();\n\t\tif (iter.hasNext()) {\n\t\t\tLocatableInputSplit next = iter.next();\n\t\t\titer.remove();\n\n\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\tLOG.info(\"Assigning remote split to host \" + host);\n\t\t\t}\n\n\t\t\tremoteAssignments++;\n\t\t\treturn next;\n\t\t} else {\n\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n\t\t\t}\n\t\t\treturn null;\n\t\t}\n\t}\n}",
        "accept_response": "@Override\npublic LocatableInputSplit getNextInputSplit(String host) {\n\t// for a null host, we return an arbitrary split\n\tif (host == null) {\n\n\t\tsynchronized (this.unassigned) {\n\t\t\tIterator<LocatableInputSplit> iter = this.unassigned.iterator();\n\t\t\tif (iter.hasNext()) {\n\t\t\t\tLocatableInputSplit next = iter.next();\n\t\t\t\titer.remove();\n\n\t\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\t\tLOG.info(\"Assigning split to null host (random assignment).\");\n\t\t\t\t}\n\n\t\t\t\tremoteAssignments++;\n\t\t\t\treturn next;\n\t\t\t} else {\n\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\tLOG.debug(\"No more unassigned input splits remaining.\");\n\t\t\t\t}\n\t\t\t\treturn null;\n\t\t\t}\n\t\t}\n\t}\n\n\thost = host.toLowerCase(Locale.US);\n\n\t// for any non-null host, we take the list of non-null splits\n\tList<LocatableInputSplit> localSplits = this.localPerHost.get(host);\n\n\t// if we have no list for this host yet, create one\n\tif (localSplits == null) {\n\t\tlocalSplits = new ArrayList<LocatableInputSplit>(16);\n\n\t\t// lock the list, to be sure that others have to wait for that host's local list\n\t\tsynchronized (localSplits) {\n\t\t\tList<LocatableInputSplit> prior = this.localPerHost.putIfAbsent(host, localSplits);\n\n\t\t\t// if someone else beat us in the case to create this list, then we do not populate this one, but\n\t\t\t// simply work with that other list\n\t\t\tif (prior == null) {\n\t\t\t\t// we are the first, we populate\n\n\t\t\t\t// first, copy the remaining splits to release the lock on the set early\n\t\t\t\t// because that is shared among threads\n\t\t\t\tLocatableInputSplit[] remaining;\n\t\t\t\tsynchronized (this.unassigned) {\n\t\t\t\t\tremaining = (LocatableInputSplit[]) this.unassigned.toArray(new LocatableInputSplit[this.unassigned.size()]);\n\t\t\t\t}\n\n\t\t\t\tfor (LocatableInputSplit is : remaining) {\n\t\t\t\t\tif (isLocal(host, is.getHostnames())) {\n\t\t\t\t\t\tlocalSplits.add(is);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\telse {\n\t\t\t\t// someone else was faster\n\t\t\t\tlocalSplits = prior;\n\t\t\t}\n\t\t}\n\t}\n\n\n\t// at this point, we have a list of local splits (possibly empty)\n\t// we need to make sure no one else operates in the current list (that protects against\n\t// list creation races) and that the unassigned set is consistent\n\t// NOTE: we need to obtain the locks in this order, strictly!!!\n\tsynchronized (localSplits) {\n\t\tint size = localSplits.size();\n\t\tif (size > 0) {\n\t\t\tsynchronized (this.unassigned) {\n\t\t\t\tdo {\n\t\t\t\t\t--size;\n\t\t\t\t\tLocatableInputSplit split = localSplits.remove(size);\n\t\t\t\t\tif (this.unassigned.remove(split)) {\n\n\t\t\t\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\t\t\t\tLOG.info(\"Assigning local split to host \" + host);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tlocalAssignments++;\n\t\t\t\t\t\treturn split;\n\t\t\t\t\t}\n\t\t\t\t} while (size > 0);\n\t\t\t}\n\t\t}\n\t}\n\n\t// we did not find a local split, return any\n\tsynchronized (this.unassigned) {\n\t\tIterator<LocatableInputSplit> iter = this.unassigned.iterator();\n\t\tif (iter.hasNext()) {\n\t\t\tLocatableInputSplit next = iter.next();\n\t\t\titer.remove();\n\n\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\tLOG.info(\"Assigning remote split to host \" + host);\n\t\t\t}\n\n\t\t\tremoteAssignments++;\n\t\t\treturn next;\n\t\t} else {\n\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n\t\t\t}\n\t\t\treturn null;\n\t\t}\n\t}\n}",
        "reject_response": "@Override\npublic LocatableInputSplit getNextInputSplit(String host) {\n\t// for a null host, we return an arbitrary split\n\tif (host == null) {\n\n\t\tsynchronized (this.unassigned) {\n\t\t\tIterator<LocatableInputSplit> iter = this.unassigned.iterator();\n\t\t\tif (iter.hasNext()) {\n\t\t\t\tLocatableInputSplit next = iter.next();\n\t\t\t\titer.remove();\n\n\t\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\t\tLOG.info(\"Assigning split to null host (random assignment).\");\n\t\t\t\t}\n\n\t\t\t\tremoteAssignments++;\n\t\t\t\treturn next;\n\t\t\t} else {\n\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\tLOG.debug(\"No more unassigned input splits remaining.\");\n\t\t\t\t}\n\t\t\t\treturn null;\n\t\t\t}\n\t\t}\n\t}\n\n\thost = host.toLowerCase(Locale.US);\n\n\t// for any non-null host, we take the list of non-null splits\n\tList<LocatableInputSplit> localSplits = this.localPerHost.get(host);\n\n\t// if we have no list for this host yet, create one\n\tif (localSplits == null) {\n\t\tlocalSplits = new ArrayList<LocatableInputSplit>(16);\n\n\t\t// lock the list, to be sure that others have to wait for that host's local list\n\t\tsynchronized (localSplits) {\n\t\t\tList<LocatableInputSplit> prior = this.localPerHost.putIfAbsent(host, localSplits);\n\n\t\t\t// if someone else beat us in the case to create this list, then we do not populate this one, but\n\t\t\t// simply work with that other list\n\t\t\tif (prior == null) {\n\t\t\t\t// we are the first, we populate\n\n\t\t\t\t// first, copy the remaining splits to release the lock on the set early\n\t\t\t\t// because that is shared among threads\n\t\t\t\tLocatableInputSplit[] remaining;\n\t\t\t\tsynchronized (this.unassigned) {\n\t\t\t\t\tremaining = (LocatableInputSplit[]) this.unassigned.toArray(new LocatableInputSplit[this.unassigned.size()]);\n\t\t\t\t}\n\n\t\t\t\tfor (LocatableInputSplit is : remaining) {\n\t\t\t\t\tif (isLocal(host, is.getHostnames())) {\n\t\t\t\t\t\tlocalSplits.add(is);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\telse {\n\t\t\t\t// someone else was faster\n\t\t\t\tlocalSplits = prior;\n\t\t\t}\n\t\t}\n\t}\n\n\n\t// at this point, we have a list of local splits (possibly empty)\n\t// we need to make sure no one else operates in the current list (that protects against\n\t// list creation races) and that the unassigned set is consistent\n\t// NOTE: we need to obtain the locks in this order, strictly!!!\n\tsynchronized (localSplits) {\n\t\tint size = localSplits.size();\n\t\tif (size > 0) {\n\t\t\tsynchronized (this.unassigned) {\n\t\t\t\tdo {\n\t\t\t\t\t--size;\n\t\t\t\t\tLocatableInputSplit split = localSplits.remove(size);\n\t\t\t\t\tif (this.unassigned.remove(split)) {\n\n\t\t\t\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\t\tLOG.debug(\"Assigning local split to host \" + host);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tlocalAssignments++;\n\t\t\t\t\t\treturn split;\n\t\t\t\t\t}\n\t\t\t\t} while (size > 0);\n\t\t\t}\n\t\t}\n\t}\n\n\t// we did not find a local split, return any\n\tsynchronized (this.unassigned) {\n\t\tIterator<LocatableInputSplit> iter = this.unassigned.iterator();\n\t\tif (iter.hasNext()) {\n\t\t\tLocatableInputSplit next = iter.next();\n\t\t\titer.remove();\n\n\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\tLOG.info(\"Assigning remote split to host \" + host);\n\t\t\t}\n\n\t\t\tremoteAssignments++;\n\t\t\treturn next;\n\t\t} else {\n\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n\t\t\t}\n\t\t\treturn null;\n\t\t}\n\t}\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2476,
        "instruction": "public double estimateFractionForKey(ByteKey key) {\n  checkNotNull(key, \"key\");\n  checkArgument(!key.isEmpty(), \"Cannot compute fraction for an empty key\");\n  checkArgument(\n      key.compareTo(startKey) >= 0, \"Expected key %s >= range start key %s\", key, startKey);\n\n  if (key.equals(endKey)) {\n    return 1.0;\n  }\n  checkArgument(containsKey(key), \"Cannot compute fraction for %s outside this %s\", key, this);\n\n  byte[] startBytes = startKey.getBytes();\n  byte[] endBytes = endKey.getBytes();\n  byte[] keyBytes = key.getBytes();\n  // If the endKey is unspecified, add a leading 1 byte to it and a leading 0 byte to all other\n  // keys, to get a concrete least upper bound for the desired range.\n  if (endKey.isEmpty()) {\n    startBytes = addHeadByte(startBytes, (byte) 0);\n    endBytes = addHeadByte(endBytes, (byte) 1);\n    keyBytes = addHeadByte(keyBytes, (byte) 0);\n  }\n\n  // Pad to the longest of all 3 keys.\n  int paddedKeyLength = Math.max(Math.max(startBytes.length, endBytes.length), keyBytes.length);\n  BigInteger rangeStartInt = paddedPositiveInt(startBytes, paddedKeyLength);\n  BigInteger rangeEndInt = paddedPositiveInt(endBytes, paddedKeyLength);\n  BigInteger keyInt = paddedPositiveInt(keyBytes, paddedKeyLength);\n\n  // Keys are equal subject to padding by 0.\n  BigInteger range = rangeEndInt.subtract(rangeStartInt);\n  if (range.equals(BigInteger.ZERO)) {\n    return 0.0;\n  }\n\n  // Compute the progress (key-start)/(end-start) scaling by 2^64, dividing (which rounds),\n  // and then scaling down after the division. This gives ample precision when converted to\n  // double.\n  BigInteger progressScaled = keyInt.subtract(rangeStartInt).shiftLeft(64);\n  return progressScaled.divide(range).doubleValue() / Math.pow(2, 64);\n}",
        "accept_response": "public double estimateFractionForKey(ByteKey key) {\n  checkNotNull(key, \"key\");\n  checkArgument(!key.isEmpty(), \"Cannot compute fraction for an empty key\");\n  checkArgument(\n      key.compareTo(startKey) >= 0, \"Expected key %s >= range start key %s\", key, startKey);\n\n  if (key.equals(endKey)) {\n    return 1.0;\n  }\n  checkArgument(containsKey(key), \"Cannot compute fraction for %s outside this %s\", key, this);\n\n  byte[] startBytes = startKey.getBytes();\n  byte[] endBytes = endKey.getBytes();\n  byte[] keyBytes = key.getBytes();\n  // If the endKey is unspecified, add a leading 1 byte to it and a leading 0 byte to all other\n  // keys, to get a concrete least upper bound for the desired range.\n  if (endKey.isEmpty()) {\n    startBytes = addHeadByte(startBytes, (byte) 0);\n    endBytes = addHeadByte(endBytes, (byte) 1);\n    keyBytes = addHeadByte(keyBytes, (byte) 0);\n  }\n\n  // Pad to the longest of all 3 keys.\n  int paddedKeyLength = Math.max(Math.max(startBytes.length, endBytes.length), keyBytes.length);\n  BigInteger rangeStartInt = paddedPositiveInt(startBytes, paddedKeyLength);\n  BigInteger rangeEndInt = paddedPositiveInt(endBytes, paddedKeyLength);\n  BigInteger keyInt = paddedPositiveInt(keyBytes, paddedKeyLength);\n\n  // Keys are equal subject to padding by 0.\n  BigInteger range = rangeEndInt.subtract(rangeStartInt);\n  if (range.equals(BigInteger.ZERO)) {\n    LOG.warn(\n        \"Using 0.0 as the default fraction for this near-empty range {} where start and end keys\"\n            + \" differ only by trailing zeros.\",\n        this);\n    return 0.0;\n  }\n\n  // Compute the progress (key-start)/(end-start) scaling by 2^64, dividing (which rounds),\n  // and then scaling down after the division. This gives ample precision when converted to\n  // double.\n  BigInteger progressScaled = keyInt.subtract(rangeStartInt).shiftLeft(64);\n  return progressScaled.divide(range).doubleValue() / Math.pow(2, 64);\n}",
        "reject_response": "public double estimateFractionForKey(ByteKey key) {\n  checkNotNull(key, \"key\");\n  checkArgument(!key.isEmpty(), \"Cannot compute fraction for an empty key\");\n  checkArgument(\n      key.compareTo(startKey) >= 0, \"Expected key %s >= range start key %s\", key, startKey);\n\n  if (key.equals(endKey)) {\n    return 1.0;\n  }\n  checkArgument(containsKey(key), \"Cannot compute fraction for %s outside this %s\", key, this);\n\n  byte[] startBytes = startKey.getBytes();\n  byte[] endBytes = endKey.getBytes();\n  byte[] keyBytes = key.getBytes();\n  // If the endKey is unspecified, add a leading 1 byte to it and a leading 0 byte to all other\n  // keys, to get a concrete least upper bound for the desired range.\n  if (endKey.isEmpty()) {\n    startBytes = addHeadByte(startBytes, (byte) 0);\n    endBytes = addHeadByte(endBytes, (byte) 1);\n    keyBytes = addHeadByte(keyBytes, (byte) 0);\n  }\n\n  // Pad to the longest of all 3 keys.\n  int paddedKeyLength = Math.max(Math.max(startBytes.length, endBytes.length), keyBytes.length);\n  BigInteger rangeStartInt = paddedPositiveInt(startBytes, paddedKeyLength);\n  BigInteger rangeEndInt = paddedPositiveInt(endBytes, paddedKeyLength);\n  BigInteger keyInt = paddedPositiveInt(keyBytes, paddedKeyLength);\n\n  // Keys are equal subject to padding by 0.\n  BigInteger range = rangeEndInt.subtract(rangeStartInt);\n  if (range.equals(BigInteger.ZERO)) {\n    logger.warn(\n    return 0.0;\n  }\n\n  // Compute the progress (key-start)/(end-start) scaling by 2^64, dividing (which rounds),\n  // and then scaling down after the division. This gives ample precision when converted to\n  // double.\n  BigInteger progressScaled = keyInt.subtract(rangeStartInt).shiftLeft(64);\n  return progressScaled.divide(range).doubleValue() / Math.pow(2, 64);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2840,
        "instruction": "@Nullable public IgniteInternalFuture<AffinityTopologyVersion> affinityReadyFuture(AffinityTopologyVersion ver) {\n    GridDhtPartitionsExchangeFuture lastInitializedFut0 = lastInitializedFut;\n\n    if (lastInitializedFut0 != null && lastInitializedFut0.initialVersion().compareTo(ver) == 0) {\n        if (log.isTraceEnabled())\n            log.trace(\"Return lastInitializedFut for topology ready future \" +\n                \"[ver=\" + ver + \", fut=\" + lastInitializedFut0 + ']');\n\n        return lastInitializedFut0;\n    }\n\n    AffinityTopologyVersion topVer = exchFuts.readyTopVer();\n\n    if (topVer.compareTo(ver) >= 0) {\n        if (log.isTraceEnabled())\n            log.trace(\"Return finished future for topology ready future [ver=\" + ver + \", topVer=\" + topVer + ']');\n\n        return null;\n    }\n\n    GridFutureAdapter<AffinityTopologyVersion> fut = F.addIfAbsent(readyFuts, ver,\n        new AffinityReadyFuture(ver));\n\n    if (log.isDebugEnabled())\n        log.debug(\"Created topology ready future [ver=\" + ver + \", fut=\" + fut + ']');\n\n    topVer = exchFuts.readyTopVer();\n\n    if (topVer.compareTo(ver) >= 0) {\n        if (log.isTraceEnabled())\n\n        fut.onDone(topVer);\n    }\n    else if (stopErr != null)\n        fut.onDone(stopErr);\n\n    return fut;\n}",
        "accept_response": "@Nullable public IgniteInternalFuture<AffinityTopologyVersion> affinityReadyFuture(AffinityTopologyVersion ver) {\n    GridDhtPartitionsExchangeFuture lastInitializedFut0 = lastInitializedFut;\n\n    if (lastInitializedFut0 != null && lastInitializedFut0.initialVersion().compareTo(ver) == 0) {\n        if (log.isTraceEnabled())\n            log.trace(\"Return lastInitializedFut for topology ready future \" +\n                \"[ver=\" + ver + \", fut=\" + lastInitializedFut0 + ']');\n\n        return lastInitializedFut0;\n    }\n\n    AffinityTopologyVersion topVer = exchFuts.readyTopVer();\n\n    if (topVer.compareTo(ver) >= 0) {\n        if (log.isTraceEnabled())\n            log.trace(\"Return finished future for topology ready future [ver=\" + ver + \", topVer=\" + topVer + ']');\n\n        return null;\n    }\n\n    GridFutureAdapter<AffinityTopologyVersion> fut = F.addIfAbsent(readyFuts, ver,\n        new AffinityReadyFuture(ver));\n\n    if (log.isDebugEnabled())\n        log.debug(\"Created topology ready future [ver=\" + ver + \", fut=\" + fut + ']');\n\n    topVer = exchFuts.readyTopVer();\n\n    if (topVer.compareTo(ver) >= 0) {\n        if (log.isTraceEnabled())\n            log.trace(\"Completing created topology ready future \" +\n                \"[ver=\" + topVer + \", topVer=\" + topVer + \", fut=\" + fut + ']');\n\n        fut.onDone(topVer);\n    }\n    else if (stopErr != null)\n        fut.onDone(stopErr);\n\n    return fut;\n}",
        "reject_response": "@Nullable public IgniteInternalFuture<AffinityTopologyVersion> affinityReadyFuture(AffinityTopologyVersion ver) {\n    GridDhtPartitionsExchangeFuture lastInitializedFut0 = lastInitializedFut;\n\n    if (lastInitializedFut0 != null && lastInitializedFut0.initialVersion().compareTo(ver) == 0) {\n        if (log.isTraceEnabled())\n            log.trace(\"Return lastInitializedFut for topology ready future \" +\n                \"[ver=\" + ver + \", fut=\" + lastInitializedFut0 + ']');\n\n        return lastInitializedFut0;\n    }\n\n    AffinityTopologyVersion topVer = exchFuts.readyTopVer();\n\n    if (topVer.compareTo(ver) >= 0) {\n        if (log.isTraceEnabled())\n            log.trace(\"Return finished future for topology ready future [ver=\" + ver + \", topVer=\" + topVer + ']');\n\n        return null;\n    }\n\n    GridFutureAdapter<AffinityTopologyVersion> fut = F.addIfAbsent(readyFuts, ver,\n        new AffinityReadyFuture(ver));\n\n    if (log.isDebugEnabled())\n        log.debug(\"Created topology ready future [ver=\" + ver + \", fut=\" + fut + ']');\n\n    topVer = exchFuts.readyTopVer();\n\n    if (topVer.compareTo(ver) >= 0) {\n        if (log.isTraceEnabled())\n        if (log.isDebugEnabled())\n            log.debug(\"Completing created topology ready future \" +\n\n        fut.onDone(topVer);\n    }\n    else if (stopErr != null)\n        fut.onDone(stopErr);\n\n    return fut;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3231,
        "instruction": "@Override\npublic void processRequest(Request request) {\n    if (stopped) {\n        return;\n    }\n    request.commitProcQueueStartTime = Time.currentElapsedTime();\n    queuedRequests.add(request);\n    // If the request will block, add it to the queue of blocking requests\n    if (needCommit(request)) {\n        queuedWriteRequests.add(request);\n        numWriteQueuedRequests.incrementAndGet();\n    } else {\n        numReadQueuedRequests.incrementAndGet();\n    }\n    wakeup();\n}",
        "accept_response": "@Override\npublic void processRequest(Request request) {\n    if (stopped) {\n        return;\n    }\n    LOG.debug(\"Processing request:: {}\", request);\n    request.commitProcQueueStartTime = Time.currentElapsedTime();\n    queuedRequests.add(request);\n    // If the request will block, add it to the queue of blocking requests\n    if (needCommit(request)) {\n        queuedWriteRequests.add(request);\n        numWriteQueuedRequests.incrementAndGet();\n    } else {\n        numReadQueuedRequests.incrementAndGet();\n    }\n    wakeup();\n}",
        "reject_response": "@Override\npublic void processRequest(Request request) {\n    if (stopped) {\n        return;\n    }\n    if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Processing request:: \" + request);\n    }\n    request.commitProcQueueStartTime = Time.currentElapsedTime();\n    queuedRequests.add(request);\n    // If the request will block, add it to the queue of blocking requests\n    if (needCommit(request)) {\n        queuedWriteRequests.add(request);\n        numWriteQueuedRequests.incrementAndGet();\n    } else {\n        numReadQueuedRequests.incrementAndGet();\n    }\n    wakeup();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2867,
        "instruction": "public static Model loadAsModel(Model model, String filenameOrURI, ResultsFormat format) {\n    if (model == null)\n        model = GraphFactory.makeDefaultModel();\n\n    if (format == null)\n        format = ResultsFormat.guessSyntax(filenameOrURI);\n\n    if (format == null) {\n        Log.warn(ResultSet.class, \"Null format - defaulting to XML\");\n        format = ResultsFormat.FMT_RS_XML;\n    }\n\n    if (format.equals(ResultsFormat.FMT_TEXT)) {\n        throw new ResultSetException(\"Can't read a text result set\");\n    }\n\n    if (format.equals(ResultsFormat.FMT_RS_XML) || format.equals(ResultsFormat.FMT_RS_JSON)) {\n        InputStream in = null;\n        try {\n            in = FileManager.get().open(filenameOrURI);\n            if (in == null)\n                throw new NotFoundException(filenameOrURI);\n        } catch (NotFoundException ex) {\n            throw new NotFoundException(\"File not found: \" + filenameOrURI);\n        }\n\n        SPARQLResult x = null;\n\n        if (format.equals(ResultsFormat.FMT_RS_JSON))\n            x = JSONInput.make(in, GraphFactory.makeDefaultModel());\n        else\n            x = XMLInput.make(in, GraphFactory.makeDefaultModel());\n\n        if (x.isResultSet())\n            RDFOutput.encodeAsRDF(model, x.getResultSet());\n        else\n            RDFOutput.encodeAsRDF(model, x.getBooleanResult());\n\n        return model;\n    }\n\n    if (ResultsFormat.isRDFGraphSyntax(format))\n        return FileManager.get().readModel(model, filenameOrURI);\n\n    Log.error(ResultSet.class, \"Unknown result set syntax: \" + format);\n    return null;\n}",
        "accept_response": "public static Model loadAsModel(Model model, String filenameOrURI, ResultsFormat format) {\n    if (model == null)\n        model = GraphFactory.makeDefaultModel();\n\n    if (format == null)\n        format = ResultsFormat.guessSyntax(filenameOrURI);\n\n    if (format == null) {\n        Log.warn(ResultSet.class, \"Null format - defaulting to XML\");\n        format = ResultsFormat.FMT_RS_XML;\n    }\n\n    if (format.equals(ResultsFormat.FMT_TEXT)) {\n        Log.error(ResultSet.class, \"Can't read a text result set\");\n        throw new ResultSetException(\"Can't read a text result set\");\n    }\n\n    if (format.equals(ResultsFormat.FMT_RS_XML) || format.equals(ResultsFormat.FMT_RS_JSON)) {\n        InputStream in = null;\n        try {\n            in = FileManager.get().open(filenameOrURI);\n            if (in == null)\n                throw new NotFoundException(filenameOrURI);\n        } catch (NotFoundException ex) {\n            throw new NotFoundException(\"File not found: \" + filenameOrURI);\n        }\n\n        SPARQLResult x = null;\n\n        if (format.equals(ResultsFormat.FMT_RS_JSON))\n            x = JSONInput.make(in, GraphFactory.makeDefaultModel());\n        else\n            x = XMLInput.make(in, GraphFactory.makeDefaultModel());\n\n        if (x.isResultSet())\n            RDFOutput.encodeAsRDF(model, x.getResultSet());\n        else\n            RDFOutput.encodeAsRDF(model, x.getBooleanResult());\n\n        return model;\n    }\n\n    if (ResultsFormat.isRDFGraphSyntax(format))\n        return FileManager.get().readModel(model, filenameOrURI);\n\n    Log.error(ResultSet.class, \"Unknown result set syntax: \" + format);\n    return null;\n}",
        "reject_response": "public static Model loadAsModel(Model model, String filenameOrURI, ResultsFormat format) {\n    if (model == null)\n        model = GraphFactory.makeDefaultModel();\n\n    if (format == null)\n        format = ResultsFormat.guessSyntax(filenameOrURI);\n\n    if (format == null) {\n        Log.warn(ResultSet.class, \"Null format - defaulting to XML\");\n        format = ResultsFormat.FMT_RS_XML;\n    }\n\n    if (format.equals(ResultsFormat.FMT_TEXT)) {\n        Log.fatal(ResultSet.class, \"Can't read a text result set\");\n        throw new ResultSetException(\"Can't read a text result set\");\n    }\n\n    if (format.equals(ResultsFormat.FMT_RS_XML) || format.equals(ResultsFormat.FMT_RS_JSON)) {\n        InputStream in = null;\n        try {\n            in = FileManager.get().open(filenameOrURI);\n            if (in == null)\n                throw new NotFoundException(filenameOrURI);\n        } catch (NotFoundException ex) {\n            throw new NotFoundException(\"File not found: \" + filenameOrURI);\n        }\n\n        SPARQLResult x = null;\n\n        if (format.equals(ResultsFormat.FMT_RS_JSON))\n            x = JSONInput.make(in, GraphFactory.makeDefaultModel());\n        else\n            x = XMLInput.make(in, GraphFactory.makeDefaultModel());\n\n        if (x.isResultSet())\n            RDFOutput.encodeAsRDF(model, x.getResultSet());\n        else\n            RDFOutput.encodeAsRDF(model, x.getBooleanResult());\n\n        return model;\n    }\n\n    if (ResultsFormat.isRDFGraphSyntax(format))\n        return FileManager.get().readModel(model, filenameOrURI);\n\n    Log.error(ResultSet.class, \"Unknown result set syntax: \" + format);\n    return null;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2996,
        "instruction": "public List<String> getColumnList(String needle, List<String> catalogs, List<String> schemas, List<String> tables, List<String> columns) throws HadoopException {\n  final String ndl = needle;\n  final List<String> cats = catalogs;\n  final List<String> shms = schemas;\n  final List<String> tbls = tables;\n  final List<String> cols = columns;\n\n  List<String> columnList = Subject.doAs(getLoginSubject(), new PrivilegedAction<List<String>>() {\n    @Override\n    public List<String> run() {\n      List<String> ret = null;\n      try {\n        ret = getColumns(ndl, cats, shms, tbls, cols);\n      } catch (HadoopException he) {\n        throw he;\n      }\n      return ret;\n    }\n  });\n  return columnList;\n}",
        "accept_response": "public List<String> getColumnList(String needle, List<String> catalogs, List<String> schemas, List<String> tables, List<String> columns) throws HadoopException {\n  final String ndl = needle;\n  final List<String> cats = catalogs;\n  final List<String> shms = schemas;\n  final List<String> tbls = tables;\n  final List<String> cols = columns;\n\n  List<String> columnList = Subject.doAs(getLoginSubject(), new PrivilegedAction<List<String>>() {\n    @Override\n    public List<String> run() {\n      List<String> ret = null;\n      try {\n        ret = getColumns(ndl, cats, shms, tbls, cols);\n      } catch (HadoopException he) {\n        LOG.error(\"<== PrestoClient.getColumnList() :Unable to get the Column List\", he);\n        throw he;\n      }\n      return ret;\n    }\n  });\n  return columnList;\n}",
        "reject_response": "public List<String> getColumnList(String needle, List<String> catalogs, List<String> schemas, List<String> tables, List<String> columns) throws HadoopException {\n  final String ndl = needle;\n  final List<String> cats = catalogs;\n  final List<String> shms = schemas;\n  final List<String> tbls = tables;\n  final List<String> cols = columns;\n\n  List<String> columnList = Subject.doAs(getLoginSubject(), new PrivilegedAction<List<String>>() {\n    @Override\n    public List<String> run() {\n      List<String> ret = null;\n      try {\n        ret = getColumns(ndl, cats, shms, tbls, cols);\n      } catch (HadoopException he) {\n        LOG.error(\"<== PrestoClient getColumnList() :Unable to get the Column List\", he);\n        throw he;\n      }\n      return ret;\n    }\n  });\n  return columnList;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2677,
        "instruction": "private MRClientProtocol getProxy() throws IOException {\n  if (realProxy != null) {\n    return realProxy;\n  }\n\n  // Possibly allow nulls through the PB tunnel, otherwise deal with an exception\n  // and redirect to the history server.\n  ApplicationReport application = null;\n  try {\n    application = rm.getApplicationReport(appId);\n  } catch (ApplicationNotFoundException e) {\n    application = null;\n  } catch (YarnException e2) {\n    throw new IOException(e2);\n  }\n  if (application != null) {\n    trackingUrl = application.getTrackingUrl();\n  }\n  InetSocketAddress serviceAddr = null;\n  while (application == null\n      || YarnApplicationState.RUNNING == application\n          .getYarnApplicationState()) {\n    if (application == null) {\n      LOG.info(\"Could not get Job info from RM for job \" + jobId\n          + \". Redirecting to job history server.\");\n      return checkAndGetHSProxy(null, JobState.NEW);\n    }\n    try {\n      if (application.getHost() == null || \"\".equals(application.getHost())) {\n        LOG.debug(\"AM not assigned to Job. Waiting to get the AM ...\");\n        Thread.sleep(2000);\n\n        LOG.debug(\"Application state is \" + application.getYarnApplicationState());\n        application = rm.getApplicationReport(appId);\n        continue;\n      } else if (UNAVAILABLE.equals(application.getHost())) {\n        if (!amAclDisabledStatusLogged) {\n          LOG.info(\"Job \" + jobId + \" is running, but the host is unknown.\"\n              + \" Verify user has VIEW_JOB access.\");\n          amAclDisabledStatusLogged = true;\n        }\n        return getNotRunningJob(application, JobState.RUNNING);\n      }\n      if(!conf.getBoolean(MRJobConfig.JOB_AM_ACCESS_DISABLED, false)) {\n        UserGroupInformation newUgi = UserGroupInformation.createRemoteUser(\n            UserGroupInformation.getCurrentUser().getUserName());\n        serviceAddr = NetUtils.createSocketAddrForHost(\n            application.getHost(), application.getRpcPort());\n        if (UserGroupInformation.isSecurityEnabled()) {\n          org.apache.hadoop.yarn.api.records.Token clientToAMToken =\n              application.getClientToAMToken();\n          Token<ClientToAMTokenIdentifier> token =\n              ConverterUtils.convertFromYarn(clientToAMToken, serviceAddr);\n          newUgi.addToken(token);\n        }\n        LOG.debug(\"Connecting to \" + serviceAddr);\n        final InetSocketAddress finalServiceAddr = serviceAddr;\n        realProxy = newUgi.doAs(new PrivilegedExceptionAction<MRClientProtocol>() {\n          @Override\n          public MRClientProtocol run() throws IOException {\n            return instantiateAMProxy(finalServiceAddr);\n          }\n        });\n      } else {\n        if (!amAclDisabledStatusLogged) {\n          LOG.info(\"Network ACL closed to AM for job \" + jobId\n              + \". Not going to try to reach the AM.\");\n          amAclDisabledStatusLogged = true;\n        }\n        return getNotRunningJob(null, JobState.RUNNING);\n      }\n      return realProxy;\n    } catch (IOException e) {\n      //possibly the AM has crashed\n      //there may be some time before AM is restarted\n      //keep retrying by getting the address from RM\n      LOG.info(\"Could not connect to \" + serviceAddr +\n      \". Waiting for getting the latest AM address...\");\n      try {\n        Thread.sleep(2000);\n      } catch (InterruptedException e1) {\n        throw new YarnRuntimeException(e1);\n      }\n      try {\n        application = rm.getApplicationReport(appId);\n      } catch (YarnException e1) {\n        throw new IOException(e1);\n      }\n      if (application == null) {\n        LOG.info(\"Could not get Job info from RM for job \" + jobId\n            + \". Redirecting to job history server.\");\n        return checkAndGetHSProxy(null, JobState.RUNNING);\n      }\n    } catch (InterruptedException e) {\n      LOG.warn(\"getProxy() call interrupted\", e);\n      throw new YarnRuntimeException(e);\n    } catch (YarnException e) {\n      throw new IOException(e);\n    }\n  }\n\n  /** we just want to return if its allocating, so that we don't\n   * block on it. This is to be able to return job status\n   * on an allocating Application.\n   */\n  String user = application.getUser();\n  if (user == null) {\n    throw new IOException(\"User is not set in the application report\");\n  }\n  if (application.getYarnApplicationState() == YarnApplicationState.NEW\n      || application.getYarnApplicationState() ==\n          YarnApplicationState.NEW_SAVING\n      || application.getYarnApplicationState() == YarnApplicationState.SUBMITTED\n      || application.getYarnApplicationState() == YarnApplicationState.ACCEPTED) {\n    realProxy = null;\n    return getNotRunningJob(application, JobState.NEW);\n  }\n\n  if (application.getYarnApplicationState() == YarnApplicationState.FAILED) {\n    realProxy = null;\n    return getNotRunningJob(application, JobState.FAILED);\n  }\n\n  if (application.getYarnApplicationState() == YarnApplicationState.KILLED) {\n    realProxy = null;\n    return getNotRunningJob(application, JobState.KILLED);\n  }\n\n  //History server can serve a job only if application\n  //succeeded.\n  if (application.getYarnApplicationState() == YarnApplicationState.FINISHED) {\n    LOG.info(\"Application state is completed. FinalApplicationStatus=\"\n        + application.getFinalApplicationStatus().toString()\n        + \". Redirecting to job history server\");\n    realProxy = checkAndGetHSProxy(application, JobState.SUCCEEDED);\n  }\n  return realProxy;\n}",
        "accept_response": "private MRClientProtocol getProxy() throws IOException {\n  if (realProxy != null) {\n    return realProxy;\n  }\n\n  // Possibly allow nulls through the PB tunnel, otherwise deal with an exception\n  // and redirect to the history server.\n  ApplicationReport application = null;\n  try {\n    application = rm.getApplicationReport(appId);\n  } catch (ApplicationNotFoundException e) {\n    application = null;\n  } catch (YarnException e2) {\n    throw new IOException(e2);\n  }\n  if (application != null) {\n    trackingUrl = application.getTrackingUrl();\n  }\n  InetSocketAddress serviceAddr = null;\n  while (application == null\n      || YarnApplicationState.RUNNING == application\n          .getYarnApplicationState()) {\n    if (application == null) {\n      LOG.info(\"Could not get Job info from RM for job \" + jobId\n          + \". Redirecting to job history server.\");\n      return checkAndGetHSProxy(null, JobState.NEW);\n    }\n    try {\n      if (application.getHost() == null || \"\".equals(application.getHost())) {\n        LOG.debug(\"AM not assigned to Job. Waiting to get the AM ...\");\n        Thread.sleep(2000);\n\n        LOG.debug(\"Application state is \" + application.getYarnApplicationState());\n        application = rm.getApplicationReport(appId);\n        continue;\n      } else if (UNAVAILABLE.equals(application.getHost())) {\n        if (!amAclDisabledStatusLogged) {\n          LOG.info(\"Job \" + jobId + \" is running, but the host is unknown.\"\n              + \" Verify user has VIEW_JOB access.\");\n          amAclDisabledStatusLogged = true;\n        }\n        return getNotRunningJob(application, JobState.RUNNING);\n      }\n      if(!conf.getBoolean(MRJobConfig.JOB_AM_ACCESS_DISABLED, false)) {\n        UserGroupInformation newUgi = UserGroupInformation.createRemoteUser(\n            UserGroupInformation.getCurrentUser().getUserName());\n        serviceAddr = NetUtils.createSocketAddrForHost(\n            application.getHost(), application.getRpcPort());\n        if (UserGroupInformation.isSecurityEnabled()) {\n          org.apache.hadoop.yarn.api.records.Token clientToAMToken =\n              application.getClientToAMToken();\n          Token<ClientToAMTokenIdentifier> token =\n              ConverterUtils.convertFromYarn(clientToAMToken, serviceAddr);\n          newUgi.addToken(token);\n        }\n        LOG.debug(\"Connecting to \" + serviceAddr);\n        final InetSocketAddress finalServiceAddr = serviceAddr;\n        realProxy = newUgi.doAs(new PrivilegedExceptionAction<MRClientProtocol>() {\n          @Override\n          public MRClientProtocol run() throws IOException {\n            return instantiateAMProxy(finalServiceAddr);\n          }\n        });\n      } else {\n        if (!amAclDisabledStatusLogged) {\n          LOG.info(\"Network ACL closed to AM for job \" + jobId\n              + \". Not going to try to reach the AM.\");\n          amAclDisabledStatusLogged = true;\n        }\n        return getNotRunningJob(null, JobState.RUNNING);\n      }\n      return realProxy;\n    } catch (IOException e) {\n      //possibly the AM has crashed\n      //there may be some time before AM is restarted\n      //keep retrying by getting the address from RM\n      LOG.info(\"Could not connect to \" + serviceAddr +\n      \". Waiting for getting the latest AM address...\");\n      try {\n        Thread.sleep(2000);\n      } catch (InterruptedException e1) {\n        LOG.warn(\"getProxy() call interrupted\", e1);\n        throw new YarnRuntimeException(e1);\n      }\n      try {\n        application = rm.getApplicationReport(appId);\n      } catch (YarnException e1) {\n        throw new IOException(e1);\n      }\n      if (application == null) {\n        LOG.info(\"Could not get Job info from RM for job \" + jobId\n            + \". Redirecting to job history server.\");\n        return checkAndGetHSProxy(null, JobState.RUNNING);\n      }\n    } catch (InterruptedException e) {\n      LOG.warn(\"getProxy() call interrupted\", e);\n      throw new YarnRuntimeException(e);\n    } catch (YarnException e) {\n      throw new IOException(e);\n    }\n  }\n\n  /** we just want to return if its allocating, so that we don't\n   * block on it. This is to be able to return job status\n   * on an allocating Application.\n   */\n  String user = application.getUser();\n  if (user == null) {\n    throw new IOException(\"User is not set in the application report\");\n  }\n  if (application.getYarnApplicationState() == YarnApplicationState.NEW\n      || application.getYarnApplicationState() ==\n          YarnApplicationState.NEW_SAVING\n      || application.getYarnApplicationState() == YarnApplicationState.SUBMITTED\n      || application.getYarnApplicationState() == YarnApplicationState.ACCEPTED) {\n    realProxy = null;\n    return getNotRunningJob(application, JobState.NEW);\n  }\n\n  if (application.getYarnApplicationState() == YarnApplicationState.FAILED) {\n    realProxy = null;\n    return getNotRunningJob(application, JobState.FAILED);\n  }\n\n  if (application.getYarnApplicationState() == YarnApplicationState.KILLED) {\n    realProxy = null;\n    return getNotRunningJob(application, JobState.KILLED);\n  }\n\n  //History server can serve a job only if application\n  //succeeded.\n  if (application.getYarnApplicationState() == YarnApplicationState.FINISHED) {\n    LOG.info(\"Application state is completed. FinalApplicationStatus=\"\n        + application.getFinalApplicationStatus().toString()\n        + \". Redirecting to job history server\");\n    realProxy = checkAndGetHSProxy(application, JobState.SUCCEEDED);\n  }\n  return realProxy;\n}",
        "reject_response": "private MRClientProtocol getProxy() throws IOException {\n  if (realProxy != null) {\n    return realProxy;\n  }\n\n  // Possibly allow nulls through the PB tunnel, otherwise deal with an exception\n  // and redirect to the history server.\n  ApplicationReport application = null;\n  try {\n    application = rm.getApplicationReport(appId);\n  } catch (ApplicationNotFoundException e) {\n    application = null;\n  } catch (YarnException e2) {\n    throw new IOException(e2);\n  }\n  if (application != null) {\n    trackingUrl = application.getTrackingUrl();\n  }\n  InetSocketAddress serviceAddr = null;\n  while (application == null\n      || YarnApplicationState.RUNNING == application\n          .getYarnApplicationState()) {\n    if (application == null) {\n      LOG.info(\"Could not get Job info from RM for job \" + jobId\n          + \". Redirecting to job history server.\");\n      return checkAndGetHSProxy(null, JobState.NEW);\n    }\n    try {\n      if (application.getHost() == null || \"\".equals(application.getHost())) {\n        LOG.debug(\"AM not assigned to Job. Waiting to get the AM ...\");\n        Thread.sleep(2000);\n\n        LOG.debug(\"Application state is \" + application.getYarnApplicationState());\n        application = rm.getApplicationReport(appId);\n        continue;\n      } else if (UNAVAILABLE.equals(application.getHost())) {\n        if (!amAclDisabledStatusLogged) {\n          LOG.info(\"Job \" + jobId + \" is running, but the host is unknown.\"\n              + \" Verify user has VIEW_JOB access.\");\n          amAclDisabledStatusLogged = true;\n        }\n        return getNotRunningJob(application, JobState.RUNNING);\n      }\n      if(!conf.getBoolean(MRJobConfig.JOB_AM_ACCESS_DISABLED, false)) {\n        UserGroupInformation newUgi = UserGroupInformation.createRemoteUser(\n            UserGroupInformation.getCurrentUser().getUserName());\n        serviceAddr = NetUtils.createSocketAddrForHost(\n            application.getHost(), application.getRpcPort());\n        if (UserGroupInformation.isSecurityEnabled()) {\n          org.apache.hadoop.yarn.api.records.Token clientToAMToken =\n              application.getClientToAMToken();\n          Token<ClientToAMTokenIdentifier> token =\n              ConverterUtils.convertFromYarn(clientToAMToken, serviceAddr);\n          newUgi.addToken(token);\n        }\n        LOG.debug(\"Connecting to \" + serviceAddr);\n        final InetSocketAddress finalServiceAddr = serviceAddr;\n        realProxy = newUgi.doAs(new PrivilegedExceptionAction<MRClientProtocol>() {\n          @Override\n          public MRClientProtocol run() throws IOException {\n            return instantiateAMProxy(finalServiceAddr);\n          }\n        });\n      } else {\n        if (!amAclDisabledStatusLogged) {\n          LOG.info(\"Network ACL closed to AM for job \" + jobId\n              + \". Not going to try to reach the AM.\");\n          amAclDisabledStatusLogged = true;\n        }\n        return getNotRunningJob(null, JobState.RUNNING);\n      }\n      return realProxy;\n    } catch (IOException e) {\n      //possibly the AM has crashed\n      //there may be some time before AM is restarted\n      //keep retrying by getting the address from RM\n      LOG.info(\"Could not connect to \" + serviceAddr +\n      \". Waiting for getting the latest AM address...\");\n      try {\n        Thread.sleep(2000);\n      } catch (InterruptedException e1) {\n        LOG.warn(\"getProxy() call interruped\", e1);\n        throw new YarnRuntimeException(e1);\n      }\n      try {\n        application = rm.getApplicationReport(appId);\n      } catch (YarnException e1) {\n        throw new IOException(e1);\n      }\n      if (application == null) {\n        LOG.info(\"Could not get Job info from RM for job \" + jobId\n            + \". Redirecting to job history server.\");\n        return checkAndGetHSProxy(null, JobState.RUNNING);\n      }\n    } catch (InterruptedException e) {\n      LOG.warn(\"getProxy() call interrupted\", e);\n      throw new YarnRuntimeException(e);\n    } catch (YarnException e) {\n      throw new IOException(e);\n    }\n  }\n\n  /** we just want to return if its allocating, so that we don't\n   * block on it. This is to be able to return job status\n   * on an allocating Application.\n   */\n  String user = application.getUser();\n  if (user == null) {\n    throw new IOException(\"User is not set in the application report\");\n  }\n  if (application.getYarnApplicationState() == YarnApplicationState.NEW\n      || application.getYarnApplicationState() ==\n          YarnApplicationState.NEW_SAVING\n      || application.getYarnApplicationState() == YarnApplicationState.SUBMITTED\n      || application.getYarnApplicationState() == YarnApplicationState.ACCEPTED) {\n    realProxy = null;\n    return getNotRunningJob(application, JobState.NEW);\n  }\n\n  if (application.getYarnApplicationState() == YarnApplicationState.FAILED) {\n    realProxy = null;\n    return getNotRunningJob(application, JobState.FAILED);\n  }\n\n  if (application.getYarnApplicationState() == YarnApplicationState.KILLED) {\n    realProxy = null;\n    return getNotRunningJob(application, JobState.KILLED);\n  }\n\n  //History server can serve a job only if application\n  //succeeded.\n  if (application.getYarnApplicationState() == YarnApplicationState.FINISHED) {\n    LOG.info(\"Application state is completed. FinalApplicationStatus=\"\n        + application.getFinalApplicationStatus().toString()\n        + \". Redirecting to job history server\");\n    realProxy = checkAndGetHSProxy(application, JobState.SUCCEEDED);\n  }\n  return realProxy;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3169,
        "instruction": "protected final void renderComponentTag(ComponentTag tag)\n{\n\tif (needToRenderTag(tag))\n\t{\n\t\t// apply behaviors that are attached to the component tag.\n\t\tif (tag.hasBehaviors())\n\t\t{\n\t\t\tIterator<? extends Behavior> tagBehaviors = tag.getBehaviors();\n\t\t\twhile (tagBehaviors.hasNext())\n\t\t\t{\n\t\t\t\tfinal Behavior behavior = tagBehaviors.next();\n\t\t\t\tif (behavior.isEnabled(this))\n\t\t\t\t{\n\t\t\t\t\tbehavior.onComponentTag(this, tag);\n\t\t\t\t}\n\t\t\t\tbehavior.detach(this);\n\t\t\t}\n\t\t}\n\n\t\t// Apply behavior modifiers\n\t\tList<? extends Behavior> behaviors = getBehaviors();\n\t\tif ((behaviors != null) && !behaviors.isEmpty() && !tag.isClose() &&\n\t\t\t(isIgnoreAttributeModifier() == false))\n\t\t{\n\t\t\ttag = tag.mutable();\n\t\t\tfor (Behavior behavior : behaviors)\n\t\t\t{\n\t\t\t\t// Components may reject some behavior components\n\t\t\t\tif (isBehaviorAccepted(behavior))\n\t\t\t\t{\n\t\t\t\t\tbehavior.onComponentTag(this, tag);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif ((tag instanceof WicketTag) && !tag.isClose() &&\n\t\t\t!getFlag(FLAG_IGNORE_ATTRIBUTE_MODIFIER))\n\t\t{\n\t\t\tExceptionSettings.NotRenderableErrorStrategy notRenderableErrorStrategy = ExceptionSettings.NotRenderableErrorStrategy.LOG_WARNING;\n\t\t\tif (Application.exists())\n\t\t\t{\n\t\t\t\tnotRenderableErrorStrategy = getApplication().getExceptionSettings().getNotRenderableErrorStrategy();\n\t\t\t}\n\n\t\t\tString tagName = tag.getNamespace() + \":\" + tag.getName();\n\t\t\tString componentId = getId();\n\t\t\tif (getFlag(FLAG_OUTPUT_MARKUP_ID))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Markup id set on a component that is usually not rendered into markup. \" +\n\t\t\t\t                               \"Markup id: %s, component id: %s, component tag: %s.\",\n\t\t\t\t                               getMarkupId(), componentId, tagName);\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (getFlag(FLAG_PLACEHOLDER))\n\t\t\t{\n\t\t\t\tString message = String.format(\n\t\t\t\t\t\t\"Placeholder tag set on a component that is usually not rendered into markup. \" +\n\t\t\t\t\t\t\"Component id: %s, component tag: %s.\", componentId, tagName);\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t}\n\n\t\t// Write the tag\n\t\ttag.writeOutput(getResponse(), !needToRenderTag(null),\n\t\t\tgetMarkup().getMarkupResourceStream().getWicketNamespace());\n\t}\n}",
        "accept_response": "protected final void renderComponentTag(ComponentTag tag)\n{\n\tif (needToRenderTag(tag))\n\t{\n\t\t// apply behaviors that are attached to the component tag.\n\t\tif (tag.hasBehaviors())\n\t\t{\n\t\t\tIterator<? extends Behavior> tagBehaviors = tag.getBehaviors();\n\t\t\twhile (tagBehaviors.hasNext())\n\t\t\t{\n\t\t\t\tfinal Behavior behavior = tagBehaviors.next();\n\t\t\t\tif (behavior.isEnabled(this))\n\t\t\t\t{\n\t\t\t\t\tbehavior.onComponentTag(this, tag);\n\t\t\t\t}\n\t\t\t\tbehavior.detach(this);\n\t\t\t}\n\t\t}\n\n\t\t// Apply behavior modifiers\n\t\tList<? extends Behavior> behaviors = getBehaviors();\n\t\tif ((behaviors != null) && !behaviors.isEmpty() && !tag.isClose() &&\n\t\t\t(isIgnoreAttributeModifier() == false))\n\t\t{\n\t\t\ttag = tag.mutable();\n\t\t\tfor (Behavior behavior : behaviors)\n\t\t\t{\n\t\t\t\t// Components may reject some behavior components\n\t\t\t\tif (isBehaviorAccepted(behavior))\n\t\t\t\t{\n\t\t\t\t\tbehavior.onComponentTag(this, tag);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif ((tag instanceof WicketTag) && !tag.isClose() &&\n\t\t\t!getFlag(FLAG_IGNORE_ATTRIBUTE_MODIFIER))\n\t\t{\n\t\t\tExceptionSettings.NotRenderableErrorStrategy notRenderableErrorStrategy = ExceptionSettings.NotRenderableErrorStrategy.LOG_WARNING;\n\t\t\tif (Application.exists())\n\t\t\t{\n\t\t\t\tnotRenderableErrorStrategy = getApplication().getExceptionSettings().getNotRenderableErrorStrategy();\n\t\t\t}\n\n\t\t\tString tagName = tag.getNamespace() + \":\" + tag.getName();\n\t\t\tString componentId = getId();\n\t\t\tif (getFlag(FLAG_OUTPUT_MARKUP_ID))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Markup id set on a component that is usually not rendered into markup. \" +\n\t\t\t\t                               \"Markup id: %s, component id: %s, component tag: %s.\",\n\t\t\t\t                               getMarkupId(), componentId, tagName);\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t\tif (getFlag(FLAG_PLACEHOLDER))\n\t\t\t{\n\t\t\t\tString message = String.format(\n\t\t\t\t\t\t\"Placeholder tag set on a component that is usually not rendered into markup. \" +\n\t\t\t\t\t\t\"Component id: %s, component tag: %s.\", componentId, tagName);\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t}\n\n\t\t// Write the tag\n\t\ttag.writeOutput(getResponse(), !needToRenderTag(null),\n\t\t\tgetMarkup().getMarkupResourceStream().getWicketNamespace());\n\t}\n}",
        "reject_response": "protected final void renderComponentTag(ComponentTag tag)\n{\n\tif (needToRenderTag(tag))\n\t{\n\t\t// apply behaviors that are attached to the component tag.\n\t\tif (tag.hasBehaviors())\n\t\t{\n\t\t\tIterator<? extends Behavior> tagBehaviors = tag.getBehaviors();\n\t\t\twhile (tagBehaviors.hasNext())\n\t\t\t{\n\t\t\t\tfinal Behavior behavior = tagBehaviors.next();\n\t\t\t\tif (behavior.isEnabled(this))\n\t\t\t\t{\n\t\t\t\t\tbehavior.onComponentTag(this, tag);\n\t\t\t\t}\n\t\t\t\tbehavior.detach(this);\n\t\t\t}\n\t\t}\n\n\t\t// Apply behavior modifiers\n\t\tList<? extends Behavior> behaviors = getBehaviors();\n\t\tif ((behaviors != null) && !behaviors.isEmpty() && !tag.isClose() &&\n\t\t\t(isIgnoreAttributeModifier() == false))\n\t\t{\n\t\t\ttag = tag.mutable();\n\t\t\tfor (Behavior behavior : behaviors)\n\t\t\t{\n\t\t\t\t// Components may reject some behavior components\n\t\t\t\tif (isBehaviorAccepted(behavior))\n\t\t\t\t{\n\t\t\t\t\tbehavior.onComponentTag(this, tag);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif ((tag instanceof WicketTag) && !tag.isClose() &&\n\t\t\t!getFlag(FLAG_IGNORE_ATTRIBUTE_MODIFIER))\n\t\t{\n\t\t\tExceptionSettings.NotRenderableErrorStrategy notRenderableErrorStrategy = ExceptionSettings.NotRenderableErrorStrategy.LOG_WARNING;\n\t\t\tif (Application.exists())\n\t\t\t{\n\t\t\t\tnotRenderableErrorStrategy = getApplication().getExceptionSettings().getNotRenderableErrorStrategy();\n\t\t\t}\n\n\t\t\tString tagName = tag.getNamespace() + \":\" + tag.getName();\n\t\t\tString componentId = getId();\n\t\t\tif (getFlag(FLAG_OUTPUT_MARKUP_ID))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Markup id set on a component that is usually not rendered into markup. \" +\n\t\t\t\t                               \"Markup id: %s, component id: %s, component tag: %s.\",\n\t\t\t\t                               getMarkupId(), componentId, tagName);\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(String.format(\n\t\t\t\t\t\"Markup id set on a component that is usually not rendered into markup. \"\n\t\t\t\t\t\t+ \"Markup id: %s, component id: %s, component tag: %s.\", getMarkupId(),\n\t\t\t\t\tgetId(), tag.getName()));\n\t\t\t}\n\t\t\tif (getFlag(FLAG_PLACEHOLDER))\n\t\t\t{\n\t\t\t\tString message = String.format(\n\t\t\t\t\t\t\"Placeholder tag set on a component that is usually not rendered into markup. \" +\n\t\t\t\t\t\t\"Component id: %s, component tag: %s.\", componentId, tagName);\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t}\n\n\t\t// Write the tag\n\t\ttag.writeOutput(getResponse(), !needToRenderTag(null),\n\t\t\tgetMarkup().getMarkupResourceStream().getWicketNamespace());\n\t}\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3127,
        "instruction": "void stopServices() {\n  Exception firstException = null;\n  // stop in reverse order of start\n  if (currentDAG != null) {\n    stopVertexServices(currentDAG);\n  }\n  List<Service> serviceList = new ArrayList<Service>(services.size());\n  for (ServiceWithDependency sd : services.values()) {\n    serviceList.add(sd.service);\n  }\n\n  for (int i = services.size() - 1; i >= 0; i--) {\n    Service service = serviceList.get(i);\n    Exception ex = ServiceOperations.stopQuietly(service);\n    if (ex != null && firstException == null) {\n      LOG.warn(\"Failed to stop service, name=\" + service.getName(), ex);\n      firstException = ex;\n    }\n  }\n  //after stopping all services, rethrow the first exception raised\n  if (firstException != null) {\n    throw ServiceStateException.convert(firstException);\n  }\n}",
        "accept_response": "void stopServices() {\n  Exception firstException = null;\n  // stop in reverse order of start\n  if (currentDAG != null) {\n    stopVertexServices(currentDAG);\n  }\n  List<Service> serviceList = new ArrayList<Service>(services.size());\n  for (ServiceWithDependency sd : services.values()) {\n    serviceList.add(sd.service);\n  }\n\n  for (int i = services.size() - 1; i >= 0; i--) {\n    Service service = serviceList.get(i);\n    LOG.debug(\"Stopping service : {}\", service);\n    Exception ex = ServiceOperations.stopQuietly(service);\n    if (ex != null && firstException == null) {\n      LOG.warn(\"Failed to stop service, name=\" + service.getName(), ex);\n      firstException = ex;\n    }\n  }\n  //after stopping all services, rethrow the first exception raised\n  if (firstException != null) {\n    throw ServiceStateException.convert(firstException);\n  }\n}",
        "reject_response": "void stopServices() {\n  Exception firstException = null;\n  // stop in reverse order of start\n  if (currentDAG != null) {\n    stopVertexServices(currentDAG);\n  }\n  List<Service> serviceList = new ArrayList<Service>(services.size());\n  for (ServiceWithDependency sd : services.values()) {\n    serviceList.add(sd.service);\n  }\n\n  for (int i = services.size() - 1; i >= 0; i--) {\n    Service service = serviceList.get(i);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Stopping service : \" + service);\n    }\n    Exception ex = ServiceOperations.stopQuietly(service);\n    if (ex != null && firstException == null) {\n      LOG.warn(\"Failed to stop service, name=\" + service.getName(), ex);\n      firstException = ex;\n    }\n  }\n  //after stopping all services, rethrow the first exception raised\n  if (firstException != null) {\n    throw ServiceStateException.convert(firstException);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2841,
        "instruction": "private void sendAllPartitions(\n    Collection<ClusterNode> nodes,\n    AffinityTopologyVersion msgTopVer\n) {\n    long time = System.currentTimeMillis();\n\n    GridDhtPartitionsFullMessage m = createPartitionsFullMessage(true, false, null, null, null, null);\n\n    m.topologyVersion(msgTopVer);\n\n    if (log.isInfoEnabled())\n\n    if (log.isTraceEnabled())\n        log.trace(\"Sending all partitions [nodeIds=\" + U.nodeIds(nodes) + \", msg=\" + m + ']');\n\n    time = System.currentTimeMillis();\n\n    for (ClusterNode node : nodes) {\n        try {\n            assert !node.equals(cctx.localNode());\n\n            cctx.io().sendNoRetry(node, m, SYSTEM_POOL);\n        }\n        catch (ClusterTopologyCheckedException ignore) {\n            if (log.isDebugEnabled())\n                log.debug(\"Failed to send partition update to node because it left grid (will ignore) [node=\" +\n                    node.id() + \", msg=\" + m + ']');\n        }\n        catch (IgniteCheckedException e) {\n            U.warn(log, \"Failed to send partitions full message [node=\" + node + \", err=\" + e + ']');\n        }\n    }\n\n    if (log.isInfoEnabled())\n        log.info(\"Sending Full Message for \" + msgTopVer + \" performed in \" + (System.currentTimeMillis() - time) + \" ms.\");\n}",
        "accept_response": "private void sendAllPartitions(\n    Collection<ClusterNode> nodes,\n    AffinityTopologyVersion msgTopVer\n) {\n    long time = System.currentTimeMillis();\n\n    GridDhtPartitionsFullMessage m = createPartitionsFullMessage(true, false, null, null, null, null);\n\n    m.topologyVersion(msgTopVer);\n\n    if (log.isInfoEnabled())\n        log.info(\"Full Message creating for \" + msgTopVer + \" performed in \" + (System.currentTimeMillis() - time) + \" ms.\");\n\n    if (log.isTraceEnabled())\n        log.trace(\"Sending all partitions [nodeIds=\" + U.nodeIds(nodes) + \", msg=\" + m + ']');\n\n    time = System.currentTimeMillis();\n\n    for (ClusterNode node : nodes) {\n        try {\n            assert !node.equals(cctx.localNode());\n\n            cctx.io().sendNoRetry(node, m, SYSTEM_POOL);\n        }\n        catch (ClusterTopologyCheckedException ignore) {\n            if (log.isDebugEnabled())\n                log.debug(\"Failed to send partition update to node because it left grid (will ignore) [node=\" +\n                    node.id() + \", msg=\" + m + ']');\n        }\n        catch (IgniteCheckedException e) {\n            U.warn(log, \"Failed to send partitions full message [node=\" + node + \", err=\" + e + ']');\n        }\n    }\n\n    if (log.isInfoEnabled())\n        log.info(\"Sending Full Message for \" + msgTopVer + \" performed in \" + (System.currentTimeMillis() - time) + \" ms.\");\n}",
        "reject_response": "private void sendAllPartitions(\n    Collection<ClusterNode> nodes,\n    AffinityTopologyVersion msgTopVer\n) {\n    long time = System.currentTimeMillis();\n\n    GridDhtPartitionsFullMessage m = createPartitionsFullMessage(true, false, null, null, null, null);\n\n    m.topologyVersion(msgTopVer);\n\n    if (log.isInfoEnabled())\n    if (log.isDebugEnabled())\n        log.debug(\"Sending all partitions [nodeIds=\" + U.nodeIds(nodes) + \", msg=\" + m + ']');\n\n    if (log.isTraceEnabled())\n        log.trace(\"Sending all partitions [nodeIds=\" + U.nodeIds(nodes) + \", msg=\" + m + ']');\n\n    time = System.currentTimeMillis();\n\n    for (ClusterNode node : nodes) {\n        try {\n            assert !node.equals(cctx.localNode());\n\n            cctx.io().sendNoRetry(node, m, SYSTEM_POOL);\n        }\n        catch (ClusterTopologyCheckedException ignore) {\n            if (log.isDebugEnabled())\n                log.debug(\"Failed to send partition update to node because it left grid (will ignore) [node=\" +\n                    node.id() + \", msg=\" + m + ']');\n        }\n        catch (IgniteCheckedException e) {\n            U.warn(log, \"Failed to send partitions full message [node=\" + node + \", err=\" + e + ']');\n        }\n    }\n\n    if (log.isInfoEnabled())\n        log.info(\"Sending Full Message for \" + msgTopVer + \" performed in \" + (System.currentTimeMillis() - time) + \" ms.\");\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2576,
        "instruction": "private void handleDataIntegrityIssues(final DataIntegrityViolationException dve) {\n\n    final Throwable realCause = dve.getMostSpecificCause();\n    throw new PlatformDataIntegrityException(\"error.msg.globalConfiguration.unknown.data.integrity.issue\",\n            \"Unknown data integrity issue with resource: \" + realCause.getMessage());\n}",
        "accept_response": "private void handleDataIntegrityIssues(final DataIntegrityViolationException dve) {\n\n    final Throwable realCause = dve.getMostSpecificCause();\n    logger.error(\"Error occured.\", dve);\n    throw new PlatformDataIntegrityException(\"error.msg.globalConfiguration.unknown.data.integrity.issue\",\n            \"Unknown data integrity issue with resource: \" + realCause.getMessage());\n}",
        "reject_response": "private void handleDataIntegrityIssues(final DataIntegrityViolationException dve) {\n\n    final Throwable realCause = dve.getMostSpecificCause();\n    logger.error(dve.getMessage(), dve);\n    throw new PlatformDataIntegrityException(\"error.msg.globalConfiguration.unknown.data.integrity.issue\",\n            \"Unknown data integrity issue with resource: \" + realCause.getMessage());\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2887,
        "instruction": "@SuppressWarnings(\"unused\")\npublic int size() {\n\n  LOG.debug(\"Counting n-grams in ARPA file\");\n  int count=0;\n\n  for (ArpaNgram ngram : this) {\n    count++;\n  }\n\n  return count;\n}",
        "accept_response": "@SuppressWarnings(\"unused\")\npublic int size() {\n\n  LOG.debug(\"Counting n-grams in ARPA file\");\n  int count=0;\n\n  for (ArpaNgram ngram : this) {\n    count++;\n  }\n  LOG.debug(\"Done counting n-grams in ARPA file\");\n\n  return count;\n}",
        "reject_response": "@SuppressWarnings(\"unused\")\npublic int size() {\n\n  LOG.debug(\"Counting n-grams in ARPA file\");\n  int count=0;\n\n  for (ArpaNgram ngram : this) {\n    count++;\n  }\n  logger.fine(\"Done counting n-grams in ARPA file\");\n\n  return count;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2590,
        "instruction": "private void checkSizeLimit() throws Exception {\n\tif (maxBufferedBytes > 0 && (numQueuedBytes + bufferBlocker.getBytesBlocked()) > maxBufferedBytes) {\n\t\t// exceeded our limit - abort this checkpoint\n\n\t\treleaseBlocksAndResetBarriers();\n\t\tnotifyAbort(currentCheckpointId, new AlignmentLimitExceededException(maxBufferedBytes));\n\t}\n}",
        "accept_response": "private void checkSizeLimit() throws Exception {\n\tif (maxBufferedBytes > 0 && (numQueuedBytes + bufferBlocker.getBytesBlocked()) > maxBufferedBytes) {\n\t\t// exceeded our limit - abort this checkpoint\n\t\tLOG.info(\"{}: Checkpoint {} aborted because alignment volume limit ({} bytes) exceeded.\",\n\t\t\tinputGate.getOwningTaskName(),\n\t\t\tcurrentCheckpointId,\n\t\t\tmaxBufferedBytes);\n\n\t\treleaseBlocksAndResetBarriers();\n\t\tnotifyAbort(currentCheckpointId, new AlignmentLimitExceededException(maxBufferedBytes));\n\t}\n}",
        "reject_response": "private void checkSizeLimit() throws Exception {\n\tif (maxBufferedBytes > 0 && (numQueuedBytes + bufferBlocker.getBytesBlocked()) > maxBufferedBytes) {\n\t\t// exceeded our limit - abort this checkpoint\n\t\tLOG.info(\"Checkpoint {} aborted because alignment volume limit ({} bytes) exceeded\",\n\t\t\t\tcurrentCheckpointId, maxBufferedBytes);\n\n\t\treleaseBlocksAndResetBarriers();\n\t\tnotifyAbort(currentCheckpointId, new AlignmentLimitExceededException(maxBufferedBytes));\n\t}\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2762,
        "instruction": "@Override\npublic InetAddress[] resolve(final String host) throws UnknownHostException {\n    final InetAddress[] resolvedAddresses = dnsMap.get(host);\n    if(resolvedAddresses == null){\n        throw new UnknownHostException(host + \" cannot be resolved\");\n    }\n    return resolvedAddresses;\n}",
        "accept_response": "@Override\npublic InetAddress[] resolve(final String host) throws UnknownHostException {\n    final InetAddress[] resolvedAddresses = dnsMap.get(host);\n    if (log.isInfoEnabled()) {\n        log.info(\"Resolving {} to {}\", host, Arrays.deepToString(resolvedAddresses));\n    }\n    if(resolvedAddresses == null){\n        throw new UnknownHostException(host + \" cannot be resolved\");\n    }\n    return resolvedAddresses;\n}",
        "reject_response": "@Override\npublic InetAddress[] resolve(final String host) throws UnknownHostException {\n    final InetAddress[] resolvedAddresses = dnsMap.get(host);\n    if (log.isInfoEnabled()) {\n        log.info(\"Resolving \" + host + \" to \" + Arrays.deepToString(resolvedAddresses));\n    }\n    if(resolvedAddresses == null){\n        throw new UnknownHostException(host + \" cannot be resolved\");\n    }\n    return resolvedAddresses;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3005,
        "instruction": "private void buildAndUploadServiceTags() throws Exception {\n\tif (LOG.isDebugEnabled()) {\n\t\tLOG.debug(\"==> buildAndUploadServiceTags()\");\n\t}\n\n\tcommitToKafka();\n\n\tMap<String, ServiceTags> serviceTagsMap = AtlasNotificationMapper.processAtlasEntities(atlasEntitiesWithTags);\n\n\tif (MapUtils.isNotEmpty(serviceTagsMap)) {\n\t\tif (serviceTagsMap.size() != 1) {\n\t\t\tLOG.warn(\"Unexpected!! Notifications for more than one service received by AtlasTagSource.. Service-Names:[\" + serviceTagsMap.keySet() + \"]\");\n\t\t}\n\t\tfor (Map.Entry<String, ServiceTags> entry : serviceTagsMap.entrySet()) {\n\t\t\tif (isHandlingDeleteOps) {\n\t\t\t\tentry.getValue().setOp(ServiceTags.OP_DELETE);\n\t\t\t\tentry.getValue().setTagDefinitions(Collections.EMPTY_MAP);\n\t\t\t\tentry.getValue().setTags(Collections.EMPTY_MAP);\n\t\t\t} else {\n\t\t\t\tentry.getValue().setOp(ServiceTags.OP_ADD_OR_UPDATE);\n\t\t\t}\n\n\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\tGson gsonBuilder = new GsonBuilder().setDateFormat(\"yyyyMMdd-HH:mm:ss.SSS-Z\").setPrettyPrinting().create();\n\t\t\t\tString serviceTagsString = gsonBuilder.toJson(entry.getValue());\n\n\t\t\t\tLOG.debug(\"serviceTags=\" + serviceTagsString);\n\t\t\t}\n\t\t\tupdateSink(entry.getValue());\n\t\t}\n\t\toffsetOfLastMessageDeliveredToRanger = messages.get(messages.size()-1).getOffset();\n\n\t\tcommitToKafka();\n\t}\n\n\tatlasEntitiesWithTags.clear();\n\tmessages.clear();\n\n\tif (LOG.isDebugEnabled()) {\n\t\tLOG.debug(\"<== buildAndUploadServiceTags()\");\n\t}\n}",
        "accept_response": "private void buildAndUploadServiceTags() throws Exception {\n\tif (LOG.isDebugEnabled()) {\n\t\tLOG.debug(\"==> buildAndUploadServiceTags()\");\n\t}\n\n\tcommitToKafka();\n\n\tMap<String, ServiceTags> serviceTagsMap = AtlasNotificationMapper.processAtlasEntities(atlasEntitiesWithTags);\n\n\tif (MapUtils.isNotEmpty(serviceTagsMap)) {\n\t\tif (serviceTagsMap.size() != 1) {\n\t\t\tLOG.warn(\"Unexpected!! Notifications for more than one service received by AtlasTagSource.. Service-Names:[\" + serviceTagsMap.keySet() + \"]\");\n\t\t}\n\t\tfor (Map.Entry<String, ServiceTags> entry : serviceTagsMap.entrySet()) {\n\t\t\tif (isHandlingDeleteOps) {\n\t\t\t\tentry.getValue().setOp(ServiceTags.OP_DELETE);\n\t\t\t\tentry.getValue().setTagDefinitions(Collections.EMPTY_MAP);\n\t\t\t\tentry.getValue().setTags(Collections.EMPTY_MAP);\n\t\t\t} else {\n\t\t\t\tentry.getValue().setOp(ServiceTags.OP_ADD_OR_UPDATE);\n\t\t\t}\n\n\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\tGson gsonBuilder = new GsonBuilder().setDateFormat(\"yyyyMMdd-HH:mm:ss.SSS-Z\").setPrettyPrinting().create();\n\t\t\t\tString serviceTagsString = gsonBuilder.toJson(entry.getValue());\n\n\t\t\t\tLOG.debug(\"serviceTags=\" + serviceTagsString);\n\t\t\t}\n\t\t\tupdateSink(entry.getValue());\n\t\t}\n\t\toffsetOfLastMessageDeliveredToRanger = messages.get(messages.size()-1).getOffset();\n\t\tif (LOG.isDebugEnabled()) {\n\t\t\tLOG.debug(\"Completed processing batch of messages of size:[\" + messages.size() + \"] received from NotificationConsumer\");\n\t\t}\n\n\t\tcommitToKafka();\n\t}\n\n\tatlasEntitiesWithTags.clear();\n\tmessages.clear();\n\n\tif (LOG.isDebugEnabled()) {\n\t\tLOG.debug(\"<== buildAndUploadServiceTags()\");\n\t}\n}",
        "reject_response": "private void buildAndUploadServiceTags() throws Exception {\n\tif (LOG.isDebugEnabled()) {\n\t\tLOG.debug(\"==> buildAndUploadServiceTags()\");\n\t}\n\n\tcommitToKafka();\n\n\tMap<String, ServiceTags> serviceTagsMap = AtlasNotificationMapper.processAtlasEntities(atlasEntitiesWithTags);\n\n\tif (MapUtils.isNotEmpty(serviceTagsMap)) {\n\t\tif (serviceTagsMap.size() != 1) {\n\t\t\tLOG.warn(\"Unexpected!! Notifications for more than one service received by AtlasTagSource.. Service-Names:[\" + serviceTagsMap.keySet() + \"]\");\n\t\t}\n\t\tfor (Map.Entry<String, ServiceTags> entry : serviceTagsMap.entrySet()) {\n\t\t\tif (isHandlingDeleteOps) {\n\t\t\t\tentry.getValue().setOp(ServiceTags.OP_DELETE);\n\t\t\t\tentry.getValue().setTagDefinitions(Collections.EMPTY_MAP);\n\t\t\t\tentry.getValue().setTags(Collections.EMPTY_MAP);\n\t\t\t} else {\n\t\t\t\tentry.getValue().setOp(ServiceTags.OP_ADD_OR_UPDATE);\n\t\t\t}\n\n\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\tGson gsonBuilder = new GsonBuilder().setDateFormat(\"yyyyMMdd-HH:mm:ss.SSS-Z\").setPrettyPrinting().create();\n\t\t\t\tString serviceTagsString = gsonBuilder.toJson(entry.getValue());\n\n\t\t\t\tLOG.debug(\"serviceTags=\" + serviceTagsString);\n\t\t\t}\n\t\t\tupdateSink(entry.getValue());\n\t\t}\n\t\toffsetOfLastMessageDeliveredToRanger = messages.get(messages.size()-1).getOffset();\n\t\tif (LOG.isDebugEnabled()) {\n\t\tLOG.info(\"Completed processing batch of messages of size:[\" + messages.size() + \"] received from NotificationConsumer\");\n\t\t}\n\n\t\tcommitToKafka();\n\t}\n\n\tatlasEntitiesWithTags.clear();\n\tmessages.clear();\n\n\tif (LOG.isDebugEnabled()) {\n\t\tLOG.debug(\"<== buildAndUploadServiceTags()\");\n\t}\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2557,
        "instruction": "@Override\npublic void init() throws FalconException {\n    String uri = StartupProperties.get().getProperty(\"entity.sla.service.store.uri\");\n    storePath = new Path(uri);\n    filePath = new Path(storePath, \"entitySLAMonitoringService\");\n    fileSystem = initializeFileSystem();\n\n    String freq = StartupProperties.get().getProperty(\"entity.sla.statusCheck.frequency.seconds\", \"600\");\n    statusCheckFrequencySeconds = Integer.parseInt(freq);\n\n    freq = StartupProperties.get().getProperty(\"entity.sla.lookAheadWindow.millis\", \"900000\");\n    lookAheadWindowMillis = Integer.parseInt(freq);\n    ScheduledThreadPoolExecutor executor = new ScheduledThreadPoolExecutor(1);\n    executor.scheduleWithFixedDelay(new Monitor(), 0, statusCheckFrequencySeconds, TimeUnit.SECONDS);\n}",
        "accept_response": "@Override\npublic void init() throws FalconException {\n    String uri = StartupProperties.get().getProperty(\"entity.sla.service.store.uri\");\n    storePath = new Path(uri);\n    filePath = new Path(storePath, \"entitySLAMonitoringService\");\n    fileSystem = initializeFileSystem();\n\n    String freq = StartupProperties.get().getProperty(\"entity.sla.statusCheck.frequency.seconds\", \"600\");\n    statusCheckFrequencySeconds = Integer.parseInt(freq);\n\n    freq = StartupProperties.get().getProperty(\"entity.sla.lookAheadWindow.millis\", \"900000\");\n    lookAheadWindowMillis = Integer.parseInt(freq);\n    LOG.info(\"Initializing EntitySLAMonitoringService from \", filePath.toString());\n    ScheduledThreadPoolExecutor executor = new ScheduledThreadPoolExecutor(1);\n    executor.scheduleWithFixedDelay(new Monitor(), 0, statusCheckFrequencySeconds, TimeUnit.SECONDS);\n}",
        "reject_response": "@Override\npublic void init() throws FalconException {\n    String uri = StartupProperties.get().getProperty(\"entity.sla.service.store.uri\");\n    storePath = new Path(uri);\n    filePath = new Path(storePath, \"entitySLAMonitoringService\");\n    fileSystem = initializeFileSystem();\n\n    String freq = StartupProperties.get().getProperty(\"entity.sla.statusCheck.frequency.seconds\", \"600\");\n    statusCheckFrequencySeconds = Integer.parseInt(freq);\n\n    freq = StartupProperties.get().getProperty(\"entity.sla.lookAheadWindow.millis\", \"900000\");\n    lookAheadWindowMillis = Integer.parseInt(freq);\n    LOG.debug(\"No old state exists at: {}, Initializing a clean state.\", filePath.toString());\n    ScheduledThreadPoolExecutor executor = new ScheduledThreadPoolExecutor(1);\n    executor.scheduleWithFixedDelay(new Monitor(), 0, statusCheckFrequencySeconds, TimeUnit.SECONDS);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2722,
        "instruction": "boolean finishWriterThreads() throws IOException {\n  LOG.debug(\"Waiting for split writer threads to finish\");\n  boolean progressFailed = false;\n  for (WriterThread t : writerThreads) {\n    t.finish();\n  }\n\n  for (WriterThread t : writerThreads) {\n    if (!progressFailed && reporter != null && !reporter.progress()) {\n      progressFailed = true;\n    }\n    try {\n      t.join();\n    } catch (InterruptedException ie) {\n      IOException iie = new InterruptedIOException();\n      iie.initCause(ie);\n      throw iie;\n    }\n  }\n  controller.checkForErrors();\n  final String msg = this.writerThreads.size() + \" split writer threads finished\";\n  updateStatusWithMsg(msg);\n  return (!progressFailed);\n}",
        "accept_response": "boolean finishWriterThreads() throws IOException {\n  LOG.debug(\"Waiting for split writer threads to finish\");\n  boolean progressFailed = false;\n  for (WriterThread t : writerThreads) {\n    t.finish();\n  }\n\n  for (WriterThread t : writerThreads) {\n    if (!progressFailed && reporter != null && !reporter.progress()) {\n      progressFailed = true;\n    }\n    try {\n      t.join();\n    } catch (InterruptedException ie) {\n      IOException iie = new InterruptedIOException();\n      iie.initCause(ie);\n      throw iie;\n    }\n  }\n  controller.checkForErrors();\n  final String msg = this.writerThreads.size() + \" split writer threads finished\";\n  LOG.info(msg);\n  updateStatusWithMsg(msg);\n  return (!progressFailed);\n}",
        "reject_response": "boolean finishWriterThreads() throws IOException {\n  LOG.debug(\"Waiting for split writer threads to finish\");\n  boolean progressFailed = false;\n  for (WriterThread t : writerThreads) {\n    t.finish();\n  }\n\n  for (WriterThread t : writerThreads) {\n    if (!progressFailed && reporter != null && !reporter.progress()) {\n      progressFailed = true;\n    }\n    try {\n      t.join();\n    } catch (InterruptedException ie) {\n      IOException iie = new InterruptedIOException();\n      iie.initCause(ie);\n      throw iie;\n    }\n  }\n  controller.checkForErrors();\n  final String msg = this.writerThreads.size() + \" split writer threads finished\";\n  LOG.info(\"{} split writer threads finished\", this.writerThreads.size());\n  updateStatusWithMsg(msg);\n  return (!progressFailed);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3052,
        "instruction": "@Override\npublic InputStream openResource(String resource) throws IOException {\n  InputStream is;\n  String file = (\".\".equals(resource)) ? configSetZkPath : configSetZkPath + \"/\" + resource;\n  int maxTries = 10;\n  Exception exception = null;\n  while (maxTries -- > 0) {\n    try {\n      if (zkController.pathExists(file)) {\n        Stat stat = new Stat();\n        byte[] bytes = zkController.getZkClient().getData(file, null, stat, true);\n        return new ZkByteArrayInputStream(bytes, stat);\n      } else {\n        //Path does not exists. We only retry for session expired exceptions.\n        break;\n      }\n    } catch (KeeperException.SessionExpiredException e) {\n      exception = e;\n      if (!zkController.getCoreContainer().isShutDown()) {\n        // Retry in case of session expiry\n        try {\n          Thread.sleep(1000);\n        } catch (InterruptedException ie) {\n          Thread.currentThread().interrupt();\n          throw new IOException(\"Could not load resource=\" + resource, ie);\n        }\n      }\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new IOException(\"Error opening \" + file, e);\n    } catch (Exception e) {\n      throw new IOException(\"Error opening \" + file, e);\n    }\n  }\n\n  if (exception != null) {\n    throw new IOException(\"We re-tried 10 times but was still unable to fetch resource=\" + resource + \" from ZK\", exception);\n  }\n\n  try {\n    // delegate to the class loader (looking into $INSTANCE_DIR/lib jars)\n    is = classLoader.getResourceAsStream(resource.replace(File.separatorChar, '/'));\n  } catch (Exception e) {\n    throw new IOException(\"Error opening \" + resource, e);\n  }\n  if (is == null) {\n    throw new SolrResourceNotFoundException(\"Can't find resource '\" + resource\n        + \"' in classpath or '\" + configSetZkPath + \"', cwd=\"\n        + System.getProperty(\"user.dir\"));\n  }\n  return is;\n}",
        "accept_response": "@Override\npublic InputStream openResource(String resource) throws IOException {\n  InputStream is;\n  String file = (\".\".equals(resource)) ? configSetZkPath : configSetZkPath + \"/\" + resource;\n  int maxTries = 10;\n  Exception exception = null;\n  while (maxTries -- > 0) {\n    try {\n      if (zkController.pathExists(file)) {\n        Stat stat = new Stat();\n        byte[] bytes = zkController.getZkClient().getData(file, null, stat, true);\n        return new ZkByteArrayInputStream(bytes, stat);\n      } else {\n        //Path does not exists. We only retry for session expired exceptions.\n        break;\n      }\n    } catch (KeeperException.SessionExpiredException e) {\n      exception = e;\n      if (!zkController.getCoreContainer().isShutDown()) {\n        // Retry in case of session expiry\n        try {\n          Thread.sleep(1000);\n          log.debug(\"Sleeping for 1s before retrying fetching resource={}\", resource);\n        } catch (InterruptedException ie) {\n          Thread.currentThread().interrupt();\n          throw new IOException(\"Could not load resource=\" + resource, ie);\n        }\n      }\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new IOException(\"Error opening \" + file, e);\n    } catch (Exception e) {\n      throw new IOException(\"Error opening \" + file, e);\n    }\n  }\n\n  if (exception != null) {\n    throw new IOException(\"We re-tried 10 times but was still unable to fetch resource=\" + resource + \" from ZK\", exception);\n  }\n\n  try {\n    // delegate to the class loader (looking into $INSTANCE_DIR/lib jars)\n    is = classLoader.getResourceAsStream(resource.replace(File.separatorChar, '/'));\n  } catch (Exception e) {\n    throw new IOException(\"Error opening \" + resource, e);\n  }\n  if (is == null) {\n    throw new SolrResourceNotFoundException(\"Can't find resource '\" + resource\n        + \"' in classpath or '\" + configSetZkPath + \"', cwd=\"\n        + System.getProperty(\"user.dir\"));\n  }\n  return is;\n}",
        "reject_response": "@Override\npublic InputStream openResource(String resource) throws IOException {\n  InputStream is;\n  String file = (\".\".equals(resource)) ? configSetZkPath : configSetZkPath + \"/\" + resource;\n  int maxTries = 10;\n  Exception exception = null;\n  while (maxTries -- > 0) {\n    try {\n      if (zkController.pathExists(file)) {\n        Stat stat = new Stat();\n        byte[] bytes = zkController.getZkClient().getData(file, null, stat, true);\n        return new ZkByteArrayInputStream(bytes, stat);\n      } else {\n        //Path does not exists. We only retry for session expired exceptions.\n        break;\n      }\n    } catch (KeeperException.SessionExpiredException e) {\n      exception = e;\n      if (!zkController.getCoreContainer().isShutDown()) {\n        // Retry in case of session expiry\n        try {\n          Thread.sleep(1000);\n          log.debug(\"Sleeping for 1s before retrying fetching resource=\" + resource);\n        } catch (InterruptedException ie) {\n          Thread.currentThread().interrupt();\n          throw new IOException(\"Could not load resource=\" + resource, ie);\n        }\n      }\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new IOException(\"Error opening \" + file, e);\n    } catch (Exception e) {\n      throw new IOException(\"Error opening \" + file, e);\n    }\n  }\n\n  if (exception != null) {\n    throw new IOException(\"We re-tried 10 times but was still unable to fetch resource=\" + resource + \" from ZK\", exception);\n  }\n\n  try {\n    // delegate to the class loader (looking into $INSTANCE_DIR/lib jars)\n    is = classLoader.getResourceAsStream(resource.replace(File.separatorChar, '/'));\n  } catch (Exception e) {\n    throw new IOException(\"Error opening \" + resource, e);\n  }\n  if (is == null) {\n    throw new SolrResourceNotFoundException(\"Can't find resource '\" + resource\n        + \"' in classpath or '\" + configSetZkPath + \"', cwd=\"\n        + System.getProperty(\"user.dir\"));\n  }\n  return is;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3106,
        "instruction": "@Override\npublic final FunctionDesc getFunction(final String signature, FunctionType funcType, DataType... paramTypes) {\n  final GetFunctionMetaRequest.Builder builder = GetFunctionMetaRequest.newBuilder();\n  builder.setSignature(signature);\n  if (funcType != null) {\n    builder.setFunctionType(funcType);\n  }\n  for (DataType type : paramTypes) {\n    builder.addParameterTypes(type);\n  }\n\n  FunctionDescProto descProto = null;\n  try {\n    descProto = new ServerCallable<FunctionDescProto>(this.pool, getCatalogServerAddr(), CatalogProtocol.class, false) {\n      public FunctionDescProto call(NettyClientBase client) throws ServiceException {\n        try {\n          CatalogProtocolService.BlockingInterface stub = getStub(client);\n          return stub.getFunctionMeta(null, builder.build());\n        } catch (NoSuchFunctionException e) {\n          abort();\n          throw e;\n        }\n      }\n    }.withRetries();\n  } catch(ServiceException e) {\n    // this is not good. we need to define user massage exception\n    if(e.getCause() instanceof NoSuchFunctionException){\n      LOG.debug(e.getMessage());\n    } else {\n      LOG.error(e.getMessage(), e);\n    }\n  }\n\n  if (descProto == null) {\n    throw new NoSuchFunctionException(signature, paramTypes);\n  }\n\n  try {\n    return new FunctionDesc(descProto);\n  } catch (ClassNotFoundException e) {\n    throw new NoSuchFunctionException(signature, paramTypes);\n  }\n}",
        "accept_response": "@Override\npublic final FunctionDesc getFunction(final String signature, FunctionType funcType, DataType... paramTypes) {\n  final GetFunctionMetaRequest.Builder builder = GetFunctionMetaRequest.newBuilder();\n  builder.setSignature(signature);\n  if (funcType != null) {\n    builder.setFunctionType(funcType);\n  }\n  for (DataType type : paramTypes) {\n    builder.addParameterTypes(type);\n  }\n\n  FunctionDescProto descProto = null;\n  try {\n    descProto = new ServerCallable<FunctionDescProto>(this.pool, getCatalogServerAddr(), CatalogProtocol.class, false) {\n      public FunctionDescProto call(NettyClientBase client) throws ServiceException {\n        try {\n          CatalogProtocolService.BlockingInterface stub = getStub(client);\n          return stub.getFunctionMeta(null, builder.build());\n        } catch (NoSuchFunctionException e) {\n          abort();\n          throw e;\n        }\n      }\n    }.withRetries();\n  } catch(ServiceException e) {\n    // this is not good. we need to define user massage exception\n    if(e.getCause() instanceof NoSuchFunctionException){\n      LOG.debug(e.getMessage());\n    } else {\n      LOG.error(e.getMessage(), e);\n    }\n  }\n\n  if (descProto == null) {\n    throw new NoSuchFunctionException(signature, paramTypes);\n  }\n\n  try {\n    return new FunctionDesc(descProto);\n  } catch (ClassNotFoundException e) {\n    LOG.error(e, e);\n    throw new NoSuchFunctionException(signature, paramTypes);\n  }\n}",
        "reject_response": "@Override\npublic final FunctionDesc getFunction(final String signature, FunctionType funcType, DataType... paramTypes) {\n  final GetFunctionMetaRequest.Builder builder = GetFunctionMetaRequest.newBuilder();\n  builder.setSignature(signature);\n  if (funcType != null) {\n    builder.setFunctionType(funcType);\n  }\n  for (DataType type : paramTypes) {\n    builder.addParameterTypes(type);\n  }\n\n  FunctionDescProto descProto = null;\n  try {\n    descProto = new ServerCallable<FunctionDescProto>(this.pool, getCatalogServerAddr(), CatalogProtocol.class, false) {\n      public FunctionDescProto call(NettyClientBase client) throws ServiceException {\n        try {\n          CatalogProtocolService.BlockingInterface stub = getStub(client);\n          return stub.getFunctionMeta(null, builder.build());\n        } catch (NoSuchFunctionException e) {\n          abort();\n          throw e;\n        }\n      }\n    }.withRetries();\n  } catch(ServiceException e) {\n    // this is not good. we need to define user massage exception\n    if(e.getCause() instanceof NoSuchFunctionException){\n      LOG.debug(e.getMessage());\n    } else {\n      LOG.error(e.getMessage(), e);\n    }\n  }\n\n  if (descProto == null) {\n    throw new NoSuchFunctionException(signature, paramTypes);\n  }\n\n  try {\n    return new FunctionDesc(descProto);\n  } catch (ClassNotFoundException e) {\n    LOG.error(e);\n    throw new NoSuchFunctionException(signature, paramTypes);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2571,
        "instruction": "@Override\n@CronTarget(jobName = JobName.UPDATE_EMAIL_OUTBOUND_WITH_CAMPAIGN_MESSAGE)\npublic void storeTemplateMessageIntoEmailOutBoundTable() throws JobExecutionException {\n    final Collection<EmailCampaignData> emailCampaignDataCollection = this.emailCampaignReadPlatformService\n            .retrieveAllScheduleActiveCampaign();\n    if (emailCampaignDataCollection != null) {\n        for (EmailCampaignData emailCampaignData : emailCampaignDataCollection) {\n            LocalDateTime tenantDateNow = tenantDateTime();\n            LocalDateTime nextTriggerDate = emailCampaignData.getNextTriggerDate().toLocalDateTime();\n\n            if (nextTriggerDate.isBefore(tenantDateNow)) {\n                insertDirectCampaignIntoEmailOutboundTable(emailCampaignData.getParamValue(), emailCampaignData.getEmailSubject(),\n                        emailCampaignData.getMessage(), emailCampaignData.getCampaignName(), emailCampaignData.getId());\n                this.updateTriggerDates(emailCampaignData.getId());\n            }\n        }\n    }\n}",
        "accept_response": "@Override\n@CronTarget(jobName = JobName.UPDATE_EMAIL_OUTBOUND_WITH_CAMPAIGN_MESSAGE)\npublic void storeTemplateMessageIntoEmailOutBoundTable() throws JobExecutionException {\n    final Collection<EmailCampaignData> emailCampaignDataCollection = this.emailCampaignReadPlatformService\n            .retrieveAllScheduleActiveCampaign();\n    if (emailCampaignDataCollection != null) {\n        for (EmailCampaignData emailCampaignData : emailCampaignDataCollection) {\n            LocalDateTime tenantDateNow = tenantDateTime();\n            LocalDateTime nextTriggerDate = emailCampaignData.getNextTriggerDate().toLocalDateTime();\n\n            logger.info(\"tenant time {} trigger time {}\", tenantDateNow, nextTriggerDate);\n            if (nextTriggerDate.isBefore(tenantDateNow)) {\n                insertDirectCampaignIntoEmailOutboundTable(emailCampaignData.getParamValue(), emailCampaignData.getEmailSubject(),\n                        emailCampaignData.getMessage(), emailCampaignData.getCampaignName(), emailCampaignData.getId());\n                this.updateTriggerDates(emailCampaignData.getId());\n            }\n        }\n    }\n}",
        "reject_response": "@Override\n@CronTarget(jobName = JobName.UPDATE_EMAIL_OUTBOUND_WITH_CAMPAIGN_MESSAGE)\npublic void storeTemplateMessageIntoEmailOutBoundTable() throws JobExecutionException {\n    final Collection<EmailCampaignData> emailCampaignDataCollection = this.emailCampaignReadPlatformService\n            .retrieveAllScheduleActiveCampaign();\n    if (emailCampaignDataCollection != null) {\n        for (EmailCampaignData emailCampaignData : emailCampaignDataCollection) {\n            LocalDateTime tenantDateNow = tenantDateTime();\n            LocalDateTime nextTriggerDate = emailCampaignData.getNextTriggerDate().toLocalDateTime();\n\n            logger.info(\"tenant time \" + tenantDateNow.toString() + \" trigger time \" + nextTriggerDate.toString());\n            if (nextTriggerDate.isBefore(tenantDateNow)) {\n                insertDirectCampaignIntoEmailOutboundTable(emailCampaignData.getParamValue(), emailCampaignData.getEmailSubject(),\n                        emailCampaignData.getMessage(), emailCampaignData.getCampaignName(), emailCampaignData.getId());\n                this.updateTriggerDates(emailCampaignData.getId());\n            }\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3036,
        "instruction": "@Override\npublic String getValidXML(String xml, String defaultXml) {\n    if (xml == null) {\n        return getValidXML(defaultXml, \"\");\n    }\n    xml = xml.trim();\n    if (\"\".equals(xml)) {\n        return \"\";\n    }\n\n    try {\n        SAXParser parser = factory.newSAXParser();\n        XMLReader reader = parser.getXMLReader();\n        reader.parse(new InputSource(new StringReader(xml)));\n        return xml;\n    } catch (Exception e) {\n        LOGGER.warn(\"Unable to get valid XML from the input.\", e);\n    }\n    return getValidXML(defaultXml, \"\");\n}",
        "accept_response": "@Override\npublic String getValidXML(String xml, String defaultXml) {\n    if (xml == null) {\n        return getValidXML(defaultXml, \"\");\n    }\n    xml = xml.trim();\n    if (\"\".equals(xml)) {\n        return \"\";\n    }\n\n    try {\n        SAXParser parser = factory.newSAXParser();\n        XMLReader reader = parser.getXMLReader();\n        reader.parse(new InputSource(new StringReader(xml)));\n        return xml;\n    } catch (Exception e) {\n        LOGGER.warn(\"Unable to get valid XML from the input.\", e);\n        LOGGER.debug(\"XML input:\\n{}\", xml);\n    }\n    return getValidXML(defaultXml, \"\");\n}",
        "reject_response": "@Override\npublic String getValidXML(String xml, String defaultXml) {\n    if (xml == null) {\n        return getValidXML(defaultXml, \"\");\n    }\n    xml = xml.trim();\n    if (\"\".equals(xml)) {\n        return \"\";\n    }\n\n    try {\n        SAXParser parser = factory.newSAXParser();\n        XMLReader reader = parser.getXMLReader();\n        reader.parse(new InputSource(new StringReader(xml)));\n        return xml;\n    } catch (Exception e) {\n        LOGGER.warn(\"Unable to get valid XML from the input.\", e);\n        LOGGER.debug(\"XML validation failed: \" + e.getMessage(), e);\n    }\n    return getValidXML(defaultXml, \"\");\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2804,
        "instruction": "public boolean removeCheckpoint(GridTaskSessionInternal ses, String key) {\n    if (!enabled())\n        return false;\n\n    assert ses != null;\n    assert key != null;\n\n    Set<String> keys = keyMap.get(ses.getId());\n\n    boolean rmv = false;\n\n    // Note: Check that keys exists because session may be invalidated during removing\n    // checkpoint from GridFuture.\n    if (keys != null) {\n        keys.remove(key);\n\n        rmv = getSpi(ses.getCheckpointSpi()).removeCheckpoint(key);\n    }\n    else if (log.isDebugEnabled())\n\n    return rmv;\n}",
        "accept_response": "public boolean removeCheckpoint(GridTaskSessionInternal ses, String key) {\n    if (!enabled())\n        return false;\n\n    assert ses != null;\n    assert key != null;\n\n    Set<String> keys = keyMap.get(ses.getId());\n\n    boolean rmv = false;\n\n    // Note: Check that keys exists because session may be invalidated during removing\n    // checkpoint from GridFuture.\n    if (keys != null) {\n        keys.remove(key);\n\n        rmv = getSpi(ses.getCheckpointSpi()).removeCheckpoint(key);\n    }\n    else if (log.isDebugEnabled())\n        log.debug(S.toString(\"Checkpoint will not be removed (key map not found)\",\n            \"key\", key, true, \"ses\", ses, false));\n\n    return rmv;\n}",
        "reject_response": "public boolean removeCheckpoint(GridTaskSessionInternal ses, String key) {\n    if (!enabled())\n        return false;\n\n    assert ses != null;\n    assert key != null;\n\n    Set<String> keys = keyMap.get(ses.getId());\n\n    boolean rmv = false;\n\n    // Note: Check that keys exists because session may be invalidated during removing\n    // checkpoint from GridFuture.\n    if (keys != null) {\n        keys.remove(key);\n\n        rmv = getSpi(ses.getCheckpointSpi()).removeCheckpoint(key);\n    }\n    else if (log.isDebugEnabled())\n        log.debug(\"Checkpoint will not be removed (key map not found) [key=\" + key + \", ses=\" + ses + ']');\n\n    return rmv;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2458,
        "instruction": "private void sendShouldBlockProducer(final ProducerInfo producerInfo,\n                                     final Message messageSend,\n                                     final boolean sendProducerAck,\n                                     final PagingStore store,\n                                     final ActiveMQDestination dest,\n                                     final AtomicInteger count,\n                                     final org.apache.activemq.artemis.api.core.Message coreMsg,\n                                     final SimpleString address) throws ResourceAllocationException {\n   final Runnable task = () -> {\n      Exception exceptionToSend = null;\n\n      try {\n         getCoreSession().send(coreMsg, false, dest.isTemporary());\n      } catch (Exception e) {\n         exceptionToSend = e;\n      }\n      connection.enableTtl();\n      if (count == null || count.decrementAndGet() == 0) {\n         if (exceptionToSend != null) {\n            this.connection.getContext().setDontSendReponse(false);\n            connection.sendException(exceptionToSend);\n         } else {\n            server.getStorageManager().afterCompleteOperations(new IOCallback() {\n               @Override\n               public void done() {\n                  if (sendProducerAck) {\n                     try {\n                        ProducerAck ack = new ProducerAck(producerInfo.getProducerId(), messageSend.getSize());\n                        connection.dispatchAsync(ack);\n                     } catch (Exception e) {\n                        connection.getContext().setDontSendReponse(false);\n                        ActiveMQServerLogger.LOGGER.warn(e.getMessage(), e);\n                        connection.sendException(e);\n                     }\n                  } else {\n                     connection.getContext().setDontSendReponse(false);\n                     try {\n                        Response response = new Response();\n                        response.setCorrelationId(messageSend.getCommandId());\n                        connection.dispatchAsync(response);\n                     } catch (Exception e) {\n                        ActiveMQServerLogger.LOGGER.warn(e.getMessage(), e);\n                        connection.sendException(e);\n                     }\n                  }\n               }\n\n               @Override\n               public void onError(int errorCode, String errorMessage) {\n                  try {\n                     final IOException e = new IOException(errorMessage);\n                     ActiveMQServerLogger.LOGGER.warn(errorMessage);\n                     connection.serviceException(e);\n                  } catch (Exception ex) {\n                     ActiveMQServerLogger.LOGGER.debug(ex);\n                  }\n               }\n            });\n         }\n      }\n   };\n   if (store != null) {\n      if (!store.checkMemory(false, task)) {\n         this.connection.getContext().setDontSendReponse(false);\n         connection.enableTtl();\n         throw new ResourceAllocationException(\"Queue is full \" + address);\n      }\n   } else {\n      task.run();\n   }\n}",
        "accept_response": "private void sendShouldBlockProducer(final ProducerInfo producerInfo,\n                                     final Message messageSend,\n                                     final boolean sendProducerAck,\n                                     final PagingStore store,\n                                     final ActiveMQDestination dest,\n                                     final AtomicInteger count,\n                                     final org.apache.activemq.artemis.api.core.Message coreMsg,\n                                     final SimpleString address) throws ResourceAllocationException {\n   final Runnable task = () -> {\n      Exception exceptionToSend = null;\n\n      try {\n         getCoreSession().send(coreMsg, false, dest.isTemporary());\n      } catch (Exception e) {\n         logger.debug(\"Sending exception to the client\", e);\n         exceptionToSend = e;\n      }\n      connection.enableTtl();\n      if (count == null || count.decrementAndGet() == 0) {\n         if (exceptionToSend != null) {\n            this.connection.getContext().setDontSendReponse(false);\n            connection.sendException(exceptionToSend);\n         } else {\n            server.getStorageManager().afterCompleteOperations(new IOCallback() {\n               @Override\n               public void done() {\n                  if (sendProducerAck) {\n                     try {\n                        ProducerAck ack = new ProducerAck(producerInfo.getProducerId(), messageSend.getSize());\n                        connection.dispatchAsync(ack);\n                     } catch (Exception e) {\n                        connection.getContext().setDontSendReponse(false);\n                        ActiveMQServerLogger.LOGGER.warn(e.getMessage(), e);\n                        connection.sendException(e);\n                     }\n                  } else {\n                     connection.getContext().setDontSendReponse(false);\n                     try {\n                        Response response = new Response();\n                        response.setCorrelationId(messageSend.getCommandId());\n                        connection.dispatchAsync(response);\n                     } catch (Exception e) {\n                        ActiveMQServerLogger.LOGGER.warn(e.getMessage(), e);\n                        connection.sendException(e);\n                     }\n                  }\n               }\n\n               @Override\n               public void onError(int errorCode, String errorMessage) {\n                  try {\n                     final IOException e = new IOException(errorMessage);\n                     ActiveMQServerLogger.LOGGER.warn(errorMessage);\n                     connection.serviceException(e);\n                  } catch (Exception ex) {\n                     ActiveMQServerLogger.LOGGER.debug(ex);\n                  }\n               }\n            });\n         }\n      }\n   };\n   if (store != null) {\n      if (!store.checkMemory(false, task)) {\n         this.connection.getContext().setDontSendReponse(false);\n         connection.enableTtl();\n         throw new ResourceAllocationException(\"Queue is full \" + address);\n      }\n   } else {\n      task.run();\n   }\n}",
        "reject_response": "private void sendShouldBlockProducer(final ProducerInfo producerInfo,\n                                     final Message messageSend,\n                                     final boolean sendProducerAck,\n                                     final PagingStore store,\n                                     final ActiveMQDestination dest,\n                                     final AtomicInteger count,\n                                     final org.apache.activemq.artemis.api.core.Message coreMsg,\n                                     final SimpleString address) throws ResourceAllocationException {\n   final Runnable task = () -> {\n      Exception exceptionToSend = null;\n\n      try {\n         getCoreSession().send(coreMsg, false, dest.isTemporary());\n      } catch (Exception e) {\n         logger.warn(e.getMessage(), e);\n         exceptionToSend = e;\n      }\n      connection.enableTtl();\n      if (count == null || count.decrementAndGet() == 0) {\n         if (exceptionToSend != null) {\n            this.connection.getContext().setDontSendReponse(false);\n            connection.sendException(exceptionToSend);\n         } else {\n            server.getStorageManager().afterCompleteOperations(new IOCallback() {\n               @Override\n               public void done() {\n                  if (sendProducerAck) {\n                     try {\n                        ProducerAck ack = new ProducerAck(producerInfo.getProducerId(), messageSend.getSize());\n                        connection.dispatchAsync(ack);\n                     } catch (Exception e) {\n                        connection.getContext().setDontSendReponse(false);\n                        ActiveMQServerLogger.LOGGER.warn(e.getMessage(), e);\n                        connection.sendException(e);\n                     }\n                  } else {\n                     connection.getContext().setDontSendReponse(false);\n                     try {\n                        Response response = new Response();\n                        response.setCorrelationId(messageSend.getCommandId());\n                        connection.dispatchAsync(response);\n                     } catch (Exception e) {\n                        ActiveMQServerLogger.LOGGER.warn(e.getMessage(), e);\n                        connection.sendException(e);\n                     }\n                  }\n               }\n\n               @Override\n               public void onError(int errorCode, String errorMessage) {\n                  try {\n                     final IOException e = new IOException(errorMessage);\n                     ActiveMQServerLogger.LOGGER.warn(errorMessage);\n                     connection.serviceException(e);\n                  } catch (Exception ex) {\n                     ActiveMQServerLogger.LOGGER.debug(ex);\n                  }\n               }\n            });\n         }\n      }\n   };\n   if (store != null) {\n      if (!store.checkMemory(false, task)) {\n         this.connection.getContext().setDontSendReponse(false);\n         connection.enableTtl();\n         throw new ResourceAllocationException(\"Queue is full \" + address);\n      }\n   } else {\n      task.run();\n   }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2425,
        "instruction": "public Collection<Resource> populateResources() throws SystemException {\n\n  //Get full url with parameters\n  String specWithParams = getSpec(clusterName, clusterSet, hostSet, metrics.keySet(), temporalInfo);\n\n  //URL\n  String spec = null;\n  //Parameters\n  String params = null;\n\n  String[] tokens = questionMarkPattern.split(specWithParams, 2);\n\n  try {\n    spec = tokens[0];\n    params = tokens[1];\n  } catch (ArrayIndexOutOfBoundsException e) {\n    LOG.info(e.toString());\n  }\n\n\n  BufferedReader reader = null;\n  try {\n\n    //Check if host is live\n    if (!hostProvider.isGangliaCollectorHostLive(clusterName)) {\n      LOG.info(\"Ganglia host is not live\");\n      return Collections.emptySet();\n    }\n\n    //Check if Ganglia server component is live\n    if (!hostProvider.isGangliaCollectorComponentLive(clusterName)) {\n      return Collections.emptySet();\n    }\n\n    reader = new BufferedReader(new InputStreamReader(\n        getStreamProvider().readFrom(spec, \"POST\", params)));\n\n    String feedStart = reader.readLine();\n    if (feedStart == null || feedStart.isEmpty()) {\n      LOG.info(\"Empty feed while getting ganglia metrics for spec => \"+\n        spec);\n      return Collections.emptySet();\n    }\n    int startTime = convertToNumber(feedStart).intValue();\n\n    String dsName = reader.readLine();\n    if (dsName == null || dsName.isEmpty()) {\n      LOG.info(\"Feed without body while reading ganglia metrics for spec \" +\n        \"=> \" + spec);\n      return Collections.emptySet();\n    }\n\n    while(!\"[~EOF]\".equals(dsName)) {\n      GangliaMetric metric = new GangliaMetric();\n      List<GangliaMetric.TemporalMetric> listTemporalMetrics =\n          new ArrayList<GangliaMetric.TemporalMetric>();\n\n      metric.setDs_name(dsName);\n      metric.setCluster_name(reader.readLine());\n      metric.setHost_name(reader.readLine());\n      metric.setMetric_name(reader.readLine());\n\n      String timeStr = reader.readLine();\n      String stepStr = reader.readLine();\n      if (timeStr == null || timeStr.isEmpty() || stepStr == null\n          || stepStr.isEmpty()) {\n        LOG.info(\"Unexpected end of stream reached while getting ganglia \" +\n            \"metrics for spec => \" + spec);\n        return Collections.emptySet();\n      }\n      int time = convertToNumber(timeStr).intValue();\n      int step = convertToNumber(stepStr).intValue();\n\n      String val     = reader.readLine();\n      String lastVal = null;\n\n      while(val!=null && !\"[~EOM]\".equals(val)) {\n        if (val.startsWith(\"[~r]\")) {\n          Integer repeat = Integer.valueOf(val.substring(4)) - 1;\n          for (int i = 0; i < repeat; ++i) {\n            if (! \"[~n]\".equals(lastVal)) {\n              GangliaMetric.TemporalMetric tm = new GangliaMetric.TemporalMetric(lastVal, time);\n              if (tm.isValid()) listTemporalMetrics.add(tm);\n            }\n            time += step;\n          }\n        } else {\n          if (! \"[~n]\".equals(val)) {\n            GangliaMetric.TemporalMetric tm = new GangliaMetric.TemporalMetric(val, time);\n            if (tm.isValid()) listTemporalMetrics.add(tm);\n          }\n          time += step;\n        }\n        lastVal = val;\n        val = reader.readLine();\n      }\n\n      metric.setDatapointsFromList(listTemporalMetrics);\n\n      ResourceKey key = new ResourceKey(metric.getHost_name(), metric.getCluster_name());\n      Set<Resource> resourceSet = resources.get(key);\n      if (resourceSet != null) {\n        for (Resource resource : resourceSet) {\n          populateResource(resource, metric);\n        }\n      }\n\n      dsName = reader.readLine();\n      if (dsName == null || dsName.isEmpty()) {\n        LOG.info(\"Unexpected end of stream reached while getting ganglia \" +\n          \"metrics for spec => \" + spec);\n        return Collections.emptySet();\n      }\n    }\n    String feedEnd = reader.readLine();\n    if (feedEnd == null || feedEnd.isEmpty()) {\n      LOG.info(\"Error reading end of feed while getting ganglia metrics \" +\n        \"for spec => \" + spec);\n    } else {\n\n      int endTime = convertToNumber(feedEnd).intValue();\n      int totalTime = endTime - startTime;\n      if (LOG.isInfoEnabled() && totalTime > POPULATION_TIME_UPPER_LIMIT) {\n        LOG.info(\"Ganglia resource population time: \" + totalTime);\n      }\n    }\n  } catch (IOException e) {\n    if (LOG.isErrorEnabled()) {\n      LOG.error(\"Caught exception getting Ganglia metrics : spec=\" + spec);\n    }\n  } finally {\n    if (reader != null) {\n      try {\n        reader.close();\n      } catch (IOException e) {\n        if (LOG.isWarnEnabled()) {\n          LOG.warn(\"Unable to close http input steam : spec=\" + spec, e);\n        }\n      }\n    }\n  }\n  //todo: filter out resources and return keepers\n  return Collections.emptySet();\n}",
        "accept_response": "public Collection<Resource> populateResources() throws SystemException {\n\n  //Get full url with parameters\n  String specWithParams = getSpec(clusterName, clusterSet, hostSet, metrics.keySet(), temporalInfo);\n\n  //URL\n  String spec = null;\n  //Parameters\n  String params = null;\n\n  String[] tokens = questionMarkPattern.split(specWithParams, 2);\n\n  try {\n    spec = tokens[0];\n    params = tokens[1];\n  } catch (ArrayIndexOutOfBoundsException e) {\n    LOG.info(e.toString());\n  }\n\n\n  BufferedReader reader = null;\n  try {\n\n    //Check if host is live\n    if (!hostProvider.isGangliaCollectorHostLive(clusterName)) {\n      LOG.info(\"Ganglia host is not live\");\n      return Collections.emptySet();\n    }\n\n    //Check if Ganglia server component is live\n    if (!hostProvider.isGangliaCollectorComponentLive(clusterName)) {\n      LOG.debug(\"Ganglia server component is not live\");\n      return Collections.emptySet();\n    }\n\n    reader = new BufferedReader(new InputStreamReader(\n        getStreamProvider().readFrom(spec, \"POST\", params)));\n\n    String feedStart = reader.readLine();\n    if (feedStart == null || feedStart.isEmpty()) {\n      LOG.info(\"Empty feed while getting ganglia metrics for spec => \"+\n        spec);\n      return Collections.emptySet();\n    }\n    int startTime = convertToNumber(feedStart).intValue();\n\n    String dsName = reader.readLine();\n    if (dsName == null || dsName.isEmpty()) {\n      LOG.info(\"Feed without body while reading ganglia metrics for spec \" +\n        \"=> \" + spec);\n      return Collections.emptySet();\n    }\n\n    while(!\"[~EOF]\".equals(dsName)) {\n      GangliaMetric metric = new GangliaMetric();\n      List<GangliaMetric.TemporalMetric> listTemporalMetrics =\n          new ArrayList<GangliaMetric.TemporalMetric>();\n\n      metric.setDs_name(dsName);\n      metric.setCluster_name(reader.readLine());\n      metric.setHost_name(reader.readLine());\n      metric.setMetric_name(reader.readLine());\n\n      String timeStr = reader.readLine();\n      String stepStr = reader.readLine();\n      if (timeStr == null || timeStr.isEmpty() || stepStr == null\n          || stepStr.isEmpty()) {\n        LOG.info(\"Unexpected end of stream reached while getting ganglia \" +\n            \"metrics for spec => \" + spec);\n        return Collections.emptySet();\n      }\n      int time = convertToNumber(timeStr).intValue();\n      int step = convertToNumber(stepStr).intValue();\n\n      String val     = reader.readLine();\n      String lastVal = null;\n\n      while(val!=null && !\"[~EOM]\".equals(val)) {\n        if (val.startsWith(\"[~r]\")) {\n          Integer repeat = Integer.valueOf(val.substring(4)) - 1;\n          for (int i = 0; i < repeat; ++i) {\n            if (! \"[~n]\".equals(lastVal)) {\n              GangliaMetric.TemporalMetric tm = new GangliaMetric.TemporalMetric(lastVal, time);\n              if (tm.isValid()) listTemporalMetrics.add(tm);\n            }\n            time += step;\n          }\n        } else {\n          if (! \"[~n]\".equals(val)) {\n            GangliaMetric.TemporalMetric tm = new GangliaMetric.TemporalMetric(val, time);\n            if (tm.isValid()) listTemporalMetrics.add(tm);\n          }\n          time += step;\n        }\n        lastVal = val;\n        val = reader.readLine();\n      }\n\n      metric.setDatapointsFromList(listTemporalMetrics);\n\n      ResourceKey key = new ResourceKey(metric.getHost_name(), metric.getCluster_name());\n      Set<Resource> resourceSet = resources.get(key);\n      if (resourceSet != null) {\n        for (Resource resource : resourceSet) {\n          populateResource(resource, metric);\n        }\n      }\n\n      dsName = reader.readLine();\n      if (dsName == null || dsName.isEmpty()) {\n        LOG.info(\"Unexpected end of stream reached while getting ganglia \" +\n          \"metrics for spec => \" + spec);\n        return Collections.emptySet();\n      }\n    }\n    String feedEnd = reader.readLine();\n    if (feedEnd == null || feedEnd.isEmpty()) {\n      LOG.info(\"Error reading end of feed while getting ganglia metrics \" +\n        \"for spec => \" + spec);\n    } else {\n\n      int endTime = convertToNumber(feedEnd).intValue();\n      int totalTime = endTime - startTime;\n      if (LOG.isInfoEnabled() && totalTime > POPULATION_TIME_UPPER_LIMIT) {\n        LOG.info(\"Ganglia resource population time: \" + totalTime);\n      }\n    }\n  } catch (IOException e) {\n    if (LOG.isErrorEnabled()) {\n      LOG.error(\"Caught exception getting Ganglia metrics : spec=\" + spec);\n    }\n  } finally {\n    if (reader != null) {\n      try {\n        reader.close();\n      } catch (IOException e) {\n        if (LOG.isWarnEnabled()) {\n          LOG.warn(\"Unable to close http input steam : spec=\" + spec, e);\n        }\n      }\n    }\n  }\n  //todo: filter out resources and return keepers\n  return Collections.emptySet();\n}",
        "reject_response": "public Collection<Resource> populateResources() throws SystemException {\n\n  //Get full url with parameters\n  String specWithParams = getSpec(clusterName, clusterSet, hostSet, metrics.keySet(), temporalInfo);\n\n  //URL\n  String spec = null;\n  //Parameters\n  String params = null;\n\n  String[] tokens = questionMarkPattern.split(specWithParams, 2);\n\n  try {\n    spec = tokens[0];\n    params = tokens[1];\n  } catch (ArrayIndexOutOfBoundsException e) {\n    LOG.info(e.toString());\n  }\n\n\n  BufferedReader reader = null;\n  try {\n\n    //Check if host is live\n    if (!hostProvider.isGangliaCollectorHostLive(clusterName)) {\n      LOG.info(\"Ganglia host is not live\");\n      return Collections.emptySet();\n    }\n\n    //Check if Ganglia server component is live\n    if (!hostProvider.isGangliaCollectorComponentLive(clusterName)) {\n      LOG.info(\"Ganglia server component is not live\");\n      return Collections.emptySet();\n    }\n\n    reader = new BufferedReader(new InputStreamReader(\n        getStreamProvider().readFrom(spec, \"POST\", params)));\n\n    String feedStart = reader.readLine();\n    if (feedStart == null || feedStart.isEmpty()) {\n      LOG.info(\"Empty feed while getting ganglia metrics for spec => \"+\n        spec);\n      return Collections.emptySet();\n    }\n    int startTime = convertToNumber(feedStart).intValue();\n\n    String dsName = reader.readLine();\n    if (dsName == null || dsName.isEmpty()) {\n      LOG.info(\"Feed without body while reading ganglia metrics for spec \" +\n        \"=> \" + spec);\n      return Collections.emptySet();\n    }\n\n    while(!\"[~EOF]\".equals(dsName)) {\n      GangliaMetric metric = new GangliaMetric();\n      List<GangliaMetric.TemporalMetric> listTemporalMetrics =\n          new ArrayList<GangliaMetric.TemporalMetric>();\n\n      metric.setDs_name(dsName);\n      metric.setCluster_name(reader.readLine());\n      metric.setHost_name(reader.readLine());\n      metric.setMetric_name(reader.readLine());\n\n      String timeStr = reader.readLine();\n      String stepStr = reader.readLine();\n      if (timeStr == null || timeStr.isEmpty() || stepStr == null\n          || stepStr.isEmpty()) {\n        LOG.info(\"Unexpected end of stream reached while getting ganglia \" +\n            \"metrics for spec => \" + spec);\n        return Collections.emptySet();\n      }\n      int time = convertToNumber(timeStr).intValue();\n      int step = convertToNumber(stepStr).intValue();\n\n      String val     = reader.readLine();\n      String lastVal = null;\n\n      while(val!=null && !\"[~EOM]\".equals(val)) {\n        if (val.startsWith(\"[~r]\")) {\n          Integer repeat = Integer.valueOf(val.substring(4)) - 1;\n          for (int i = 0; i < repeat; ++i) {\n            if (! \"[~n]\".equals(lastVal)) {\n              GangliaMetric.TemporalMetric tm = new GangliaMetric.TemporalMetric(lastVal, time);\n              if (tm.isValid()) listTemporalMetrics.add(tm);\n            }\n            time += step;\n          }\n        } else {\n          if (! \"[~n]\".equals(val)) {\n            GangliaMetric.TemporalMetric tm = new GangliaMetric.TemporalMetric(val, time);\n            if (tm.isValid()) listTemporalMetrics.add(tm);\n          }\n          time += step;\n        }\n        lastVal = val;\n        val = reader.readLine();\n      }\n\n      metric.setDatapointsFromList(listTemporalMetrics);\n\n      ResourceKey key = new ResourceKey(metric.getHost_name(), metric.getCluster_name());\n      Set<Resource> resourceSet = resources.get(key);\n      if (resourceSet != null) {\n        for (Resource resource : resourceSet) {\n          populateResource(resource, metric);\n        }\n      }\n\n      dsName = reader.readLine();\n      if (dsName == null || dsName.isEmpty()) {\n        LOG.info(\"Unexpected end of stream reached while getting ganglia \" +\n          \"metrics for spec => \" + spec);\n        return Collections.emptySet();\n      }\n    }\n    String feedEnd = reader.readLine();\n    if (feedEnd == null || feedEnd.isEmpty()) {\n      LOG.info(\"Error reading end of feed while getting ganglia metrics \" +\n        \"for spec => \" + spec);\n    } else {\n\n      int endTime = convertToNumber(feedEnd).intValue();\n      int totalTime = endTime - startTime;\n      if (LOG.isInfoEnabled() && totalTime > POPULATION_TIME_UPPER_LIMIT) {\n        LOG.info(\"Ganglia resource population time: \" + totalTime);\n      }\n    }\n  } catch (IOException e) {\n    if (LOG.isErrorEnabled()) {\n      LOG.error(\"Caught exception getting Ganglia metrics : spec=\" + spec);\n    }\n  } finally {\n    if (reader != null) {\n      try {\n        reader.close();\n      } catch (IOException e) {\n        if (LOG.isWarnEnabled()) {\n          LOG.warn(\"Unable to close http input steam : spec=\" + spec, e);\n        }\n      }\n    }\n  }\n  //todo: filter out resources and return keepers\n  return Collections.emptySet();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2409,
        "instruction": "private void sortFilters() {\n  Collections.sort(filterConfigList, new Comparator<Map<String, Object>>() {\n\n    @Override\n    public int compare(Map<String, Object> o1, Map<String, Object> o2) {\n      Object o1Sort = o1.get(\"sort_order\");\n      Object o2Sort = o2.get(\"sort_order\");\n      if (o1Sort == null || o2Sort == null) {\n        return 0;\n      }\n\n      int o1Value = parseSort(o1, o1Sort);\n      int o2Value = parseSort(o2, o2Sort);\n\n      return o1Value - o2Value;\n    }\n\n    private int parseSort(Map<String, Object> map, Object o) {\n      if (!(o instanceof Number)) {\n        try {\n          return (new Double(Double.parseDouble(o.toString()))).intValue();\n        } catch (Throwable t) {\n          return 0;\n        }\n      } else {\n        return ((Number) o).intValue();\n      }\n    }\n  });\n}",
        "accept_response": "private void sortFilters() {\n  Collections.sort(filterConfigList, new Comparator<Map<String, Object>>() {\n\n    @Override\n    public int compare(Map<String, Object> o1, Map<String, Object> o2) {\n      Object o1Sort = o1.get(\"sort_order\");\n      Object o2Sort = o2.get(\"sort_order\");\n      if (o1Sort == null || o2Sort == null) {\n        return 0;\n      }\n\n      int o1Value = parseSort(o1, o1Sort);\n      int o2Value = parseSort(o2, o2Sort);\n\n      return o1Value - o2Value;\n    }\n\n    private int parseSort(Map<String, Object> map, Object o) {\n      if (!(o instanceof Number)) {\n        try {\n          return (new Double(Double.parseDouble(o.toString()))).intValue();\n        } catch (Throwable t) {\n          LOG.error(\"Value is not of type Number. class=\" + o.getClass().getName() + \", value=\" + o.toString()\n            + \", map=\" + map.toString());\n          return 0;\n        }\n      } else {\n        return ((Number) o).intValue();\n      }\n    }\n  });\n}",
        "reject_response": "private void sortFilters() {\n  Collections.sort(filterConfigList, new Comparator<Map<String, Object>>() {\n\n    @Override\n    public int compare(Map<String, Object> o1, Map<String, Object> o2) {\n      Object o1Sort = o1.get(\"sort_order\");\n      Object o2Sort = o2.get(\"sort_order\");\n      if (o1Sort == null || o2Sort == null) {\n        return 0;\n      }\n\n      int o1Value = parseSort(o1, o1Sort);\n      int o2Value = parseSort(o2, o2Sort);\n\n      return o1Value - o2Value;\n    }\n\n    private int parseSort(Map<String, Object> map, Object o) {\n      if (!(o instanceof Number)) {\n        try {\n          return (new Double(Double.parseDouble(o.toString()))).intValue();\n        } catch (Throwable t) {\n          logger.error(\"Value is not of type Number. class=\"\n            + o2Sort.getClass().getName() + \", value=\"\n            + o2Sort.toString() + \", map=\" + o2.toString());\n          return 0;\n        }\n      } else {\n        return ((Number) o).intValue();\n      }\n    }\n  });\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2386,
        "instruction": "private TabletLocation checkLock(TabletLocation tl) {\n  // the goal of this class is to minimize calls out to lockChecker under that assumption that its a resource synchronized among many threads... want to\n  // avoid fine grained synchronization when binning lots of mutations or ranges... remember decisions from the lockChecker in thread local unsynchronized\n  // memory\n\n  if (tl == null)\n    return null;\n\n  Pair<String,String> lock = new Pair<String,String>(tl.tablet_location, tl.tablet_session);\n\n  if (okLocks.contains(lock))\n    return tl;\n\n  if (invalidLocks.contains(lock))\n    return null;\n\n  if (lockChecker.isLockHeld(tl.tablet_location, tl.tablet_session)) {\n    okLocks.add(lock);\n    return tl;\n  }\n\n  if (log.isTraceEnabled())\n\n  invalidLocks.add(lock);\n\n  return null;\n}",
        "accept_response": "private TabletLocation checkLock(TabletLocation tl) {\n  // the goal of this class is to minimize calls out to lockChecker under that assumption that its a resource synchronized among many threads... want to\n  // avoid fine grained synchronization when binning lots of mutations or ranges... remember decisions from the lockChecker in thread local unsynchronized\n  // memory\n\n  if (tl == null)\n    return null;\n\n  Pair<String,String> lock = new Pair<String,String>(tl.tablet_location, tl.tablet_session);\n\n  if (okLocks.contains(lock))\n    return tl;\n\n  if (invalidLocks.contains(lock))\n    return null;\n\n  if (lockChecker.isLockHeld(tl.tablet_location, tl.tablet_session)) {\n    okLocks.add(lock);\n    return tl;\n  }\n\n  if (log.isTraceEnabled())\n    log.trace(\"Tablet server {} {} no longer holds its lock\", tl.tablet_location, tl.tablet_session);\n\n  invalidLocks.add(lock);\n\n  return null;\n}",
        "reject_response": "private TabletLocation checkLock(TabletLocation tl) {\n  // the goal of this class is to minimize calls out to lockChecker under that assumption that its a resource synchronized among many threads... want to\n  // avoid fine grained synchronization when binning lots of mutations or ranges... remember decisions from the lockChecker in thread local unsynchronized\n  // memory\n\n  if (tl == null)\n    return null;\n\n  Pair<String,String> lock = new Pair<String,String>(tl.tablet_location, tl.tablet_session);\n\n  if (okLocks.contains(lock))\n    return tl;\n\n  if (invalidLocks.contains(lock))\n    return null;\n\n  if (lockChecker.isLockHeld(tl.tablet_location, tl.tablet_session)) {\n    okLocks.add(lock);\n    return tl;\n  }\n\n  if (log.isTraceEnabled())\n    log.trace(\"Tablet server \" + tl.tablet_location + \" \" + tl.tablet_session + \" no longer holds its lock\");\n\n  invalidLocks.add(lock);\n\n  return null;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2484,
        "instruction": "private List<BigtableSource> splitKeyRangeIntoBundleSizedSubranges(\n    long sampleSizeBytes, long desiredBundleSizeBytes, ByteKeyRange range) {\n  // Catch the trivial cases. Split is small enough already, or this is the last region.\n  if (sampleSizeBytes <= desiredBundleSizeBytes) {\n    return Collections.singletonList(\n        this.withStartKey(range.getStartKey()).withEndKey(range.getEndKey()));\n  }\n\n  checkArgument(\n      sampleSizeBytes > 0, \"Sample size %s bytes must be greater than 0.\", sampleSizeBytes);\n  checkArgument(\n      desiredBundleSizeBytes > 0,\n      \"Desired bundle size %s bytes must be greater than 0.\",\n      desiredBundleSizeBytes);\n\n  int splitCount = (int) Math.ceil(((double) sampleSizeBytes) / (desiredBundleSizeBytes));\n  List<ByteKey> splitKeys = range.split(splitCount);\n  ImmutableList.Builder<BigtableSource> splits = ImmutableList.builder();\n  Iterator<ByteKey> keys = splitKeys.iterator();\n  ByteKey prev = keys.next();\n  while (keys.hasNext()) {\n    ByteKey next = keys.next();\n    splits.add(\n        this\n            .withStartKey(prev)\n            .withEndKey(next)\n            .withEstimatedSizeBytes(sampleSizeBytes / splitCount));\n    prev = next;\n  }\n  return splits.build();\n}",
        "accept_response": "private List<BigtableSource> splitKeyRangeIntoBundleSizedSubranges(\n    long sampleSizeBytes, long desiredBundleSizeBytes, ByteKeyRange range) {\n  // Catch the trivial cases. Split is small enough already, or this is the last region.\n  LOG.debug(\n      \"Subsplit for sampleSizeBytes {} and desiredBundleSizeBytes {}\",\n      sampleSizeBytes,\n      desiredBundleSizeBytes);\n  if (sampleSizeBytes <= desiredBundleSizeBytes) {\n    return Collections.singletonList(\n        this.withStartKey(range.getStartKey()).withEndKey(range.getEndKey()));\n  }\n\n  checkArgument(\n      sampleSizeBytes > 0, \"Sample size %s bytes must be greater than 0.\", sampleSizeBytes);\n  checkArgument(\n      desiredBundleSizeBytes > 0,\n      \"Desired bundle size %s bytes must be greater than 0.\",\n      desiredBundleSizeBytes);\n\n  int splitCount = (int) Math.ceil(((double) sampleSizeBytes) / (desiredBundleSizeBytes));\n  List<ByteKey> splitKeys = range.split(splitCount);\n  ImmutableList.Builder<BigtableSource> splits = ImmutableList.builder();\n  Iterator<ByteKey> keys = splitKeys.iterator();\n  ByteKey prev = keys.next();\n  while (keys.hasNext()) {\n    ByteKey next = keys.next();\n    splits.add(\n        this\n            .withStartKey(prev)\n            .withEndKey(next)\n            .withEstimatedSizeBytes(sampleSizeBytes / splitCount));\n    prev = next;\n  }\n  return splits.build();\n}",
        "reject_response": "private List<BigtableSource> splitKeyRangeIntoBundleSizedSubranges(\n    long sampleSizeBytes, long desiredBundleSizeBytes, ByteKeyRange range) {\n  // Catch the trivial cases. Split is small enough already, or this is the last region.\n  logger.debug(\n  if (sampleSizeBytes <= desiredBundleSizeBytes) {\n    return Collections.singletonList(\n        this.withStartKey(range.getStartKey()).withEndKey(range.getEndKey()));\n  }\n\n  checkArgument(\n      sampleSizeBytes > 0, \"Sample size %s bytes must be greater than 0.\", sampleSizeBytes);\n  checkArgument(\n      desiredBundleSizeBytes > 0,\n      \"Desired bundle size %s bytes must be greater than 0.\",\n      desiredBundleSizeBytes);\n\n  int splitCount = (int) Math.ceil(((double) sampleSizeBytes) / (desiredBundleSizeBytes));\n  List<ByteKey> splitKeys = range.split(splitCount);\n  ImmutableList.Builder<BigtableSource> splits = ImmutableList.builder();\n  Iterator<ByteKey> keys = splitKeys.iterator();\n  ByteKey prev = keys.next();\n  while (keys.hasNext()) {\n    ByteKey next = keys.next();\n    splits.add(\n        this\n            .withStartKey(prev)\n            .withEndKey(next)\n            .withEstimatedSizeBytes(sampleSizeBytes / splitCount));\n    prev = next;\n  }\n  return splits.build();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2904,
        "instruction": "public DNSLookupContinuation checkSPF(SPFSession spfData)\n        throws PermErrorException,\n        NoneException, TempErrorException,\n        NeutralException {\n    String attExplanation = (String) spfData.getAttribute(ATTRIBUTE_DEFAULT_EXPLANATION_POLICY_EXPLANATION);\n    try {\n        String explanation = macroExpand.expand(attExplanation, spfData, MacroExpand.EXPLANATION);\n\n        spfData.setExplanation(explanation);\n    } catch (PermErrorException e) {\n        // Should never happen !\n    }\n    return null;\n}",
        "accept_response": "public DNSLookupContinuation checkSPF(SPFSession spfData)\n        throws PermErrorException,\n        NoneException, TempErrorException,\n        NeutralException {\n    String attExplanation = (String) spfData.getAttribute(ATTRIBUTE_DEFAULT_EXPLANATION_POLICY_EXPLANATION);\n    try {\n        String explanation = macroExpand.expand(attExplanation, spfData, MacroExpand.EXPLANATION);\n\n        spfData.setExplanation(explanation);\n    } catch (PermErrorException e) {\n        // Should never happen !\n        LOGGER.debug(\"Invalid defaulfExplanation: {}\", attExplanation);\n    }\n    return null;\n}",
        "reject_response": "public DNSLookupContinuation checkSPF(SPFSession spfData)\n        throws PermErrorException,\n        NoneException, TempErrorException,\n        NeutralException {\n    String attExplanation = (String) spfData.getAttribute(ATTRIBUTE_DEFAULT_EXPLANATION_POLICY_EXPLANATION);\n    try {\n        String explanation = macroExpand.expand(attExplanation, spfData, MacroExpand.EXPLANATION);\n\n        spfData.setExplanation(explanation);\n    } catch (PermErrorException e) {\n        // Should never happen !\n        log.debug(\"Invalid defaulfExplanation: \" + attExplanation);\n    }\n    return null;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3042,
        "instruction": "TreeSet<String> fetchZkChildren(Watcher watcher) throws InterruptedException, KeeperException {\n  while (true) {\n    try {\n      TreeSet<String> orderedChildren = new TreeSet<>();\n\n      List<String> childNames = zookeeper.getChildren(dir, watcher, true);\n      stats.setQueueLength(childNames.size());\n      for (String childName : childNames) {\n        // Check format\n        if (!childName.regionMatches(0, PREFIX, 0, PREFIX.length())) {\n          continue;\n        }\n        orderedChildren.add(childName);\n      }\n      return orderedChildren;\n    } catch (KeeperException.NoNodeException e) {\n      zookeeper.makePath(dir, false, true);\n      // go back to the loop and try again\n    }\n  }\n}",
        "accept_response": "TreeSet<String> fetchZkChildren(Watcher watcher) throws InterruptedException, KeeperException {\n  while (true) {\n    try {\n      TreeSet<String> orderedChildren = new TreeSet<>();\n\n      List<String> childNames = zookeeper.getChildren(dir, watcher, true);\n      stats.setQueueLength(childNames.size());\n      for (String childName : childNames) {\n        // Check format\n        if (!childName.regionMatches(0, PREFIX, 0, PREFIX.length())) {\n          LOG.debug(\"Found child node with improper name: {}\", childName);\n          continue;\n        }\n        orderedChildren.add(childName);\n      }\n      return orderedChildren;\n    } catch (KeeperException.NoNodeException e) {\n      zookeeper.makePath(dir, false, true);\n      // go back to the loop and try again\n    }\n  }\n}",
        "reject_response": "TreeSet<String> fetchZkChildren(Watcher watcher) throws InterruptedException, KeeperException {\n  while (true) {\n    try {\n      TreeSet<String> orderedChildren = new TreeSet<>();\n\n      List<String> childNames = zookeeper.getChildren(dir, watcher, true);\n      stats.setQueueLength(childNames.size());\n      for (String childName : childNames) {\n        // Check format\n        if (!childName.regionMatches(0, PREFIX, 0, PREFIX.length())) {\n          LOG.debug(\"Found child node with improper name: \" + childName);\n          continue;\n        }\n        orderedChildren.add(childName);\n      }\n      return orderedChildren;\n    } catch (KeeperException.NoNodeException e) {\n      zookeeper.makePath(dir, false, true);\n      // go back to the loop and try again\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2672,
        "instruction": "private void loadMap(XMLConfiguration conf) {\n    String[] profileNames = conf.getStringArray(\"profile.name\");\n    if (profileNames.length == 0) {\n        LOG.warn(\"Profile file: \" + conf.getFileName() + \" is empty\");\n        return;\n    }\n    Map<String, Map<String, String>> profileMap = new TreeMap<>(String.CASE_INSENSITIVE_ORDER);\n    for (int profileIdx = 0; profileIdx < profileNames.length; profileIdx++) {\n        String profileName = profileNames[profileIdx];\n        if (profileMap.containsKey(profileName)) {\n            continue;\n        }\n        Configuration profileSubset = conf.subset(\"profile(\" + profileIdx + \").plugins\");\n        profileMap.put(profileName, getProfilePluginMap(profileSubset));\n    }\n    profilesMap.putAll(profileMap);\n}",
        "accept_response": "private void loadMap(XMLConfiguration conf) {\n    String[] profileNames = conf.getStringArray(\"profile.name\");\n    if (profileNames.length == 0) {\n        LOG.warn(\"Profile file: \" + conf.getFileName() + \" is empty\");\n        return;\n    }\n    Map<String, Map<String, String>> profileMap = new TreeMap<>(String.CASE_INSENSITIVE_ORDER);\n    for (int profileIdx = 0; profileIdx < profileNames.length; profileIdx++) {\n        String profileName = profileNames[profileIdx];\n        if (profileMap.containsKey(profileName)) {\n            LOG.warn(\"Duplicate profile definition found in \" + conf.getFileName() + \" for: \" + profileName);\n            continue;\n        }\n        Configuration profileSubset = conf.subset(\"profile(\" + profileIdx + \").plugins\");\n        profileMap.put(profileName, getProfilePluginMap(profileSubset));\n    }\n    profilesMap.putAll(profileMap);\n}",
        "reject_response": "private void loadMap(XMLConfiguration conf) {\n    String[] profileNames = conf.getStringArray(\"profile.name\");\n    if (profileNames.length == 0) {\n        LOG.warn(\"Profile file: \" + conf.getFileName() + \" is empty\");\n        return;\n    }\n    Map<String, Map<String, String>> profileMap = new TreeMap<>(String.CASE_INSENSITIVE_ORDER);\n    for (int profileIdx = 0; profileIdx < profileNames.length; profileIdx++) {\n        String profileName = profileNames[profileIdx];\n        if (profileMap.containsKey(profileName)) {\n            log.warn(\"Duplicate profile definition found in \" + conf.getFileName() + \" for: \" + profileName);\n            continue;\n        }\n        Configuration profileSubset = conf.subset(\"profile(\" + profileIdx + \").plugins\");\n        profileMap.put(profileName, getProfilePluginMap(profileSubset));\n    }\n    profilesMap.putAll(profileMap);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3100,
        "instruction": "private void checkRead() throws IOException {\n  if (current == totalCountLoadedSoFar) {\n    if (current != 0) {\n      long timeAssembling = System.currentTimeMillis() - startedAssemblingCurrentBlockAt;\n      totalTimeSpentProcessingRecords += timeAssembling;\n      if (DEBUG) LOG.debug(\"Assembled and processed \" + totalCountLoadedSoFar + \" records from \" + columnCount + \" columns in \" + totalTimeSpentProcessingRecords + \" ms: \" + ((float) totalCountLoadedSoFar / totalTimeSpentProcessingRecords) + \" rec/ms, \" + ((float) totalCountLoadedSoFar * columnCount / totalTimeSpentProcessingRecords) + \" cell/ms\");\n      long totalTime = totalTimeSpentProcessingRecords + totalTimeSpentReadingBytes;\n      long percentReading = 100 * totalTimeSpentReadingBytes / totalTime;\n      long percentProcessing = 100 * totalTimeSpentProcessingRecords / totalTime;\n    }\n\n    if (DEBUG) LOG.debug(\"at row \" + current + \". reading next block\");\n    long t0 = System.currentTimeMillis();\n    PageReadStore pages = reader.readNextRowGroup();\n    if (pages == null) {\n      throw new IOException(\"expecting more rows but reached last block. Read \" + current + \" out of \" + total);\n    }\n    long timeSpentReading = System.currentTimeMillis() - t0;\n    totalTimeSpentReadingBytes += timeSpentReading;\n    BenchmarkCounter.incrementTime(timeSpentReading);\n    if (DEBUG) {\n      LOG.debug(\"block read in memory in \" + timeSpentReading + \" ms. row count = \" + pages.getRowCount());\n      LOG.debug(\"initializing Record assembly with requested schema \" + requestedSchema);\n    }\n    MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema);\n    recordReader = columnIO.getRecordReader(pages, recordConverter, recordFilter);\n    startedAssemblingCurrentBlockAt = System.currentTimeMillis();\n    totalCountLoadedSoFar += pages.getRowCount();\n    ++ currentBlock;\n  }\n}",
        "accept_response": "private void checkRead() throws IOException {\n  if (current == totalCountLoadedSoFar) {\n    if (current != 0) {\n      long timeAssembling = System.currentTimeMillis() - startedAssemblingCurrentBlockAt;\n      totalTimeSpentProcessingRecords += timeAssembling;\n      if (DEBUG) LOG.debug(\"Assembled and processed \" + totalCountLoadedSoFar + \" records from \" + columnCount + \" columns in \" + totalTimeSpentProcessingRecords + \" ms: \" + ((float) totalCountLoadedSoFar / totalTimeSpentProcessingRecords) + \" rec/ms, \" + ((float) totalCountLoadedSoFar * columnCount / totalTimeSpentProcessingRecords) + \" cell/ms\");\n      long totalTime = totalTimeSpentProcessingRecords + totalTimeSpentReadingBytes;\n      long percentReading = 100 * totalTimeSpentReadingBytes / totalTime;\n      long percentProcessing = 100 * totalTimeSpentProcessingRecords / totalTime;\n      if (DEBUG) LOG.debug(\"time spent so far \" + percentReading + \"% reading (\"+totalTimeSpentReadingBytes+\" ms) and \" + percentProcessing + \"% processing (\"+totalTimeSpentProcessingRecords+\" ms)\");\n    }\n\n    if (DEBUG) LOG.debug(\"at row \" + current + \". reading next block\");\n    long t0 = System.currentTimeMillis();\n    PageReadStore pages = reader.readNextRowGroup();\n    if (pages == null) {\n      throw new IOException(\"expecting more rows but reached last block. Read \" + current + \" out of \" + total);\n    }\n    long timeSpentReading = System.currentTimeMillis() - t0;\n    totalTimeSpentReadingBytes += timeSpentReading;\n    BenchmarkCounter.incrementTime(timeSpentReading);\n    if (DEBUG) {\n      LOG.debug(\"block read in memory in \" + timeSpentReading + \" ms. row count = \" + pages.getRowCount());\n      LOG.debug(\"initializing Record assembly with requested schema \" + requestedSchema);\n    }\n    MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema);\n    recordReader = columnIO.getRecordReader(pages, recordConverter, recordFilter);\n    startedAssemblingCurrentBlockAt = System.currentTimeMillis();\n    totalCountLoadedSoFar += pages.getRowCount();\n    ++ currentBlock;\n  }\n}",
        "reject_response": "private void checkRead() throws IOException {\n  if (current == totalCountLoadedSoFar) {\n    if (current != 0) {\n      long timeAssembling = System.currentTimeMillis() - startedAssemblingCurrentBlockAt;\n      totalTimeSpentProcessingRecords += timeAssembling;\n      if (DEBUG) LOG.debug(\"Assembled and processed \" + totalCountLoadedSoFar + \" records from \" + columnCount + \" columns in \" + totalTimeSpentProcessingRecords + \" ms: \" + ((float) totalCountLoadedSoFar / totalTimeSpentProcessingRecords) + \" rec/ms, \" + ((float) totalCountLoadedSoFar * columnCount / totalTimeSpentProcessingRecords) + \" cell/ms\");\n      long totalTime = totalTimeSpentProcessingRecords + totalTimeSpentReadingBytes;\n      long percentReading = 100 * totalTimeSpentReadingBytes / totalTime;\n      long percentProcessing = 100 * totalTimeSpentProcessingRecords / totalTime;\n      LOG.info(\"time spent so far \" + percentReading + \"% reading (\"+totalTimeSpentReadingBytes+\" ms) and \" + percentProcessing + \"% processing (\"+totalTimeSpentProcessingRecords+\" ms)\");\n    }\n\n    if (DEBUG) LOG.debug(\"at row \" + current + \". reading next block\");\n    long t0 = System.currentTimeMillis();\n    PageReadStore pages = reader.readNextRowGroup();\n    if (pages == null) {\n      throw new IOException(\"expecting more rows but reached last block. Read \" + current + \" out of \" + total);\n    }\n    long timeSpentReading = System.currentTimeMillis() - t0;\n    totalTimeSpentReadingBytes += timeSpentReading;\n    BenchmarkCounter.incrementTime(timeSpentReading);\n    if (DEBUG) {\n      LOG.debug(\"block read in memory in \" + timeSpentReading + \" ms. row count = \" + pages.getRowCount());\n      LOG.debug(\"initializing Record assembly with requested schema \" + requestedSchema);\n    }\n    MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema);\n    recordReader = columnIO.getRecordReader(pages, recordConverter, recordFilter);\n    startedAssemblingCurrentBlockAt = System.currentTimeMillis();\n    totalCountLoadedSoFar += pages.getRowCount();\n    ++ currentBlock;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3224,
        "instruction": "private SaslClient createSaslClient(final String servicePrincipal, final String loginContext)\n        throws LoginException {\n    try {\n        if (!initializedLogin) {\n            synchronized (this) {\n                if (login == null) {\n                    // note that the login object is static: it's shared amongst all zookeeper-related connections.\n                    // in order to ensure the login is initialized only once, it must be synchronized the code snippet.\n                    login = new Login(loginContext, new SaslClientCallbackHandler(null, \"Client\"), clientConfig);\n                    login.startThreadIfNeeded();\n                    initializedLogin = true;\n                }\n            }\n        }\n        return SecurityUtils.createSaslClient(login.getSubject(),\n                servicePrincipal, \"zookeeper\", \"zk-sasl-md5\", LOG, \"Client\");\n    } catch (LoginException e) {\n        // We throw LoginExceptions...\n        throw e;\n    } catch (Exception e) {\n        // ..but consume (with a log message) all other types of exceptions.\n        LOG.error(\"Exception while trying to create SASL client: \" + e);\n        return null;\n    }\n}",
        "accept_response": "private SaslClient createSaslClient(final String servicePrincipal, final String loginContext)\n        throws LoginException {\n    try {\n        if (!initializedLogin) {\n            synchronized (this) {\n                if (login == null) {\n                    LOG.debug(\"JAAS loginContext is: {}\", loginContext);\n                    // note that the login object is static: it's shared amongst all zookeeper-related connections.\n                    // in order to ensure the login is initialized only once, it must be synchronized the code snippet.\n                    login = new Login(loginContext, new SaslClientCallbackHandler(null, \"Client\"), clientConfig);\n                    login.startThreadIfNeeded();\n                    initializedLogin = true;\n                }\n            }\n        }\n        return SecurityUtils.createSaslClient(login.getSubject(),\n                servicePrincipal, \"zookeeper\", \"zk-sasl-md5\", LOG, \"Client\");\n    } catch (LoginException e) {\n        // We throw LoginExceptions...\n        throw e;\n    } catch (Exception e) {\n        // ..but consume (with a log message) all other types of exceptions.\n        LOG.error(\"Exception while trying to create SASL client: \" + e);\n        return null;\n    }\n}",
        "reject_response": "private SaslClient createSaslClient(final String servicePrincipal, final String loginContext)\n        throws LoginException {\n    try {\n        if (!initializedLogin) {\n            synchronized (this) {\n                if (login == null) {\n                    if (LOG.isDebugEnabled()) {\n                        LOG.debug(\"JAAS loginContext is: \" + loginContext);\n                    }\n                    // note that the login object is static: it's shared amongst all zookeeper-related connections.\n                    // in order to ensure the login is initialized only once, it must be synchronized the code snippet.\n                    login = new Login(loginContext, new SaslClientCallbackHandler(null, \"Client\"), clientConfig);\n                    login.startThreadIfNeeded();\n                    initializedLogin = true;\n                }\n            }\n        }\n        return SecurityUtils.createSaslClient(login.getSubject(),\n                servicePrincipal, \"zookeeper\", \"zk-sasl-md5\", LOG, \"Client\");\n    } catch (LoginException e) {\n        // We throw LoginExceptions...\n        throw e;\n    } catch (Exception e) {\n        // ..but consume (with a log message) all other types of exceptions.\n        LOG.error(\"Exception while trying to create SASL client: \" + e);\n        return null;\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3031,
        "instruction": "public synchronized ClusterDescription refreshClusterStatus(Map<String, String> providerStatus) {\n  ClusterDescription cd = getClusterStatus();\n  long now = now();\n  cd.setInfoTime(StatusKeys.INFO_STATUS_TIME_HUMAN,\n                 StatusKeys.INFO_STATUS_TIME_MILLIS,\n                 now);\n  if (providerStatus != null) {\n    for (Map.Entry<String, String> entry : providerStatus.entrySet()) {\n      cd.setInfo(entry.getKey(), entry.getValue());\n    }\n  }\n  MapOperations infoOps = new MapOperations(\"info\", cd.info);\n  infoOps.mergeWithoutOverwrite(applicationInfo);\n  SliderUtils.addBuildInfo(infoOps, \"status\");\n  cd.statistics = new HashMap<>();\n\n  // build the map of node -> container IDs\n  Map<String, List<String>> instanceMap = createRoleToInstanceMap();\n  cd.instances = instanceMap;\n\n  //build the map of node -> containers\n  Map<String, Map<String, ClusterNode>> clusterNodes =\n    createRoleToClusterNodeMap();\n  cd.status = new HashMap<>();\n  cd.status.put(ClusterDescriptionKeys.KEY_CLUSTER_LIVE, clusterNodes);\n\n\n  for (RoleStatus role : getRoleStatusMap().values()) {\n    String rolename = role.getName();\n    if (hasUniqueNames(instanceDefinition.getResourceOperations(),\n        role.getGroup())) {\n      cd.setRoleOpt(rolename, COMPONENT_PRIORITY, role.getPriority());\n      cd.setRoleOpt(rolename, ROLE_GROUP, role.getGroup());\n      MapOperations groupOptions = instanceDefinition.getResourceOperations()\n          .getComponent(role.getGroup());\n      SliderUtils.mergeMapsIgnoreDuplicateKeys(cd.getRole(rolename),\n          groupOptions.options);\n    }\n    String prefix = instanceDefinition.getAppConfOperations()\n        .getComponentOpt(role.getGroup(), ROLE_PREFIX, null);\n    if (SliderUtils.isSet(prefix)) {\n      cd.setRoleOpt(rolename, ROLE_PREFIX, SliderUtils.trimPrefix(prefix));\n    }\n    List<String> instances = instanceMap.get(rolename);\n    int nodeCount = instances != null ? instances.size(): 0;\n    cd.setRoleOpt(rolename, COMPONENT_INSTANCES,\n                  role.getDesired());\n    cd.setRoleOpt(rolename, ROLE_ACTUAL_INSTANCES, nodeCount);\n    cd.setRoleOpt(rolename, ROLE_REQUESTED_INSTANCES, role.getRequested());\n    cd.setRoleOpt(rolename, ROLE_RELEASING_INSTANCES, role.getReleasing());\n    cd.setRoleOpt(rolename, ROLE_FAILED_INSTANCES, role.getFailed());\n    cd.setRoleOpt(rolename, ROLE_FAILED_STARTING_INSTANCES, role.getStartFailed());\n    cd.setRoleOpt(rolename, ROLE_FAILED_RECENTLY_INSTANCES, role.getFailedRecently());\n    cd.setRoleOpt(rolename, ROLE_NODE_FAILED_INSTANCES, role.getNodeFailed());\n    cd.setRoleOpt(rolename, ROLE_PREEMPTED_INSTANCES, role.getPreempted());\n    if (role.isAntiAffinePlacement()) {\n      cd.setRoleOpt(rolename, ROLE_PENDING_AA_INSTANCES, role.getPendingAntiAffineRequests());\n    }\n    Map<String, Integer> stats = role.buildStatistics();\n    cd.statistics.put(rolename, stats);\n  }\n\n  Map<String, Integer> sliderstats = getLiveStatistics();\n  cd.statistics.put(SliderKeys.COMPONENT_AM, sliderstats);\n\n  // liveness\n  cd.liveness = getApplicationLivenessInformation();\n\n  return cd;\n}",
        "accept_response": "public synchronized ClusterDescription refreshClusterStatus(Map<String, String> providerStatus) {\n  ClusterDescription cd = getClusterStatus();\n  long now = now();\n  cd.setInfoTime(StatusKeys.INFO_STATUS_TIME_HUMAN,\n                 StatusKeys.INFO_STATUS_TIME_MILLIS,\n                 now);\n  if (providerStatus != null) {\n    for (Map.Entry<String, String> entry : providerStatus.entrySet()) {\n      cd.setInfo(entry.getKey(), entry.getValue());\n    }\n  }\n  MapOperations infoOps = new MapOperations(\"info\", cd.info);\n  infoOps.mergeWithoutOverwrite(applicationInfo);\n  SliderUtils.addBuildInfo(infoOps, \"status\");\n  cd.statistics = new HashMap<>();\n\n  // build the map of node -> container IDs\n  Map<String, List<String>> instanceMap = createRoleToInstanceMap();\n  cd.instances = instanceMap;\n\n  //build the map of node -> containers\n  Map<String, Map<String, ClusterNode>> clusterNodes =\n    createRoleToClusterNodeMap();\n  log.debug(\"app state clusterNodes {} \", clusterNodes.toString());\n  cd.status = new HashMap<>();\n  cd.status.put(ClusterDescriptionKeys.KEY_CLUSTER_LIVE, clusterNodes);\n\n\n  for (RoleStatus role : getRoleStatusMap().values()) {\n    String rolename = role.getName();\n    if (hasUniqueNames(instanceDefinition.getResourceOperations(),\n        role.getGroup())) {\n      cd.setRoleOpt(rolename, COMPONENT_PRIORITY, role.getPriority());\n      cd.setRoleOpt(rolename, ROLE_GROUP, role.getGroup());\n      MapOperations groupOptions = instanceDefinition.getResourceOperations()\n          .getComponent(role.getGroup());\n      SliderUtils.mergeMapsIgnoreDuplicateKeys(cd.getRole(rolename),\n          groupOptions.options);\n    }\n    String prefix = instanceDefinition.getAppConfOperations()\n        .getComponentOpt(role.getGroup(), ROLE_PREFIX, null);\n    if (SliderUtils.isSet(prefix)) {\n      cd.setRoleOpt(rolename, ROLE_PREFIX, SliderUtils.trimPrefix(prefix));\n    }\n    List<String> instances = instanceMap.get(rolename);\n    int nodeCount = instances != null ? instances.size(): 0;\n    cd.setRoleOpt(rolename, COMPONENT_INSTANCES,\n                  role.getDesired());\n    cd.setRoleOpt(rolename, ROLE_ACTUAL_INSTANCES, nodeCount);\n    cd.setRoleOpt(rolename, ROLE_REQUESTED_INSTANCES, role.getRequested());\n    cd.setRoleOpt(rolename, ROLE_RELEASING_INSTANCES, role.getReleasing());\n    cd.setRoleOpt(rolename, ROLE_FAILED_INSTANCES, role.getFailed());\n    cd.setRoleOpt(rolename, ROLE_FAILED_STARTING_INSTANCES, role.getStartFailed());\n    cd.setRoleOpt(rolename, ROLE_FAILED_RECENTLY_INSTANCES, role.getFailedRecently());\n    cd.setRoleOpt(rolename, ROLE_NODE_FAILED_INSTANCES, role.getNodeFailed());\n    cd.setRoleOpt(rolename, ROLE_PREEMPTED_INSTANCES, role.getPreempted());\n    if (role.isAntiAffinePlacement()) {\n      cd.setRoleOpt(rolename, ROLE_PENDING_AA_INSTANCES, role.getPendingAntiAffineRequests());\n    }\n    Map<String, Integer> stats = role.buildStatistics();\n    cd.statistics.put(rolename, stats);\n  }\n\n  Map<String, Integer> sliderstats = getLiveStatistics();\n  cd.statistics.put(SliderKeys.COMPONENT_AM, sliderstats);\n\n  // liveness\n  cd.liveness = getApplicationLivenessInformation();\n\n  return cd;\n}",
        "reject_response": "public synchronized ClusterDescription refreshClusterStatus(Map<String, String> providerStatus) {\n  ClusterDescription cd = getClusterStatus();\n  long now = now();\n  cd.setInfoTime(StatusKeys.INFO_STATUS_TIME_HUMAN,\n                 StatusKeys.INFO_STATUS_TIME_MILLIS,\n                 now);\n  if (providerStatus != null) {\n    for (Map.Entry<String, String> entry : providerStatus.entrySet()) {\n      cd.setInfo(entry.getKey(), entry.getValue());\n    }\n  }\n  MapOperations infoOps = new MapOperations(\"info\", cd.info);\n  infoOps.mergeWithoutOverwrite(applicationInfo);\n  SliderUtils.addBuildInfo(infoOps, \"status\");\n  cd.statistics = new HashMap<>();\n\n  // build the map of node -> container IDs\n  Map<String, List<String>> instanceMap = createRoleToInstanceMap();\n  cd.instances = instanceMap;\n\n  //build the map of node -> containers\n  Map<String, Map<String, ClusterNode>> clusterNodes =\n    createRoleToClusterNodeMap();\n  log.info(\"app state clusterNodes {} \", clusterNodes.toString());\n  cd.status = new HashMap<>();\n  cd.status.put(ClusterDescriptionKeys.KEY_CLUSTER_LIVE, clusterNodes);\n\n\n  for (RoleStatus role : getRoleStatusMap().values()) {\n    String rolename = role.getName();\n    if (hasUniqueNames(instanceDefinition.getResourceOperations(),\n        role.getGroup())) {\n      cd.setRoleOpt(rolename, COMPONENT_PRIORITY, role.getPriority());\n      cd.setRoleOpt(rolename, ROLE_GROUP, role.getGroup());\n      MapOperations groupOptions = instanceDefinition.getResourceOperations()\n          .getComponent(role.getGroup());\n      SliderUtils.mergeMapsIgnoreDuplicateKeys(cd.getRole(rolename),\n          groupOptions.options);\n    }\n    String prefix = instanceDefinition.getAppConfOperations()\n        .getComponentOpt(role.getGroup(), ROLE_PREFIX, null);\n    if (SliderUtils.isSet(prefix)) {\n      cd.setRoleOpt(rolename, ROLE_PREFIX, SliderUtils.trimPrefix(prefix));\n    }\n    List<String> instances = instanceMap.get(rolename);\n    int nodeCount = instances != null ? instances.size(): 0;\n    cd.setRoleOpt(rolename, COMPONENT_INSTANCES,\n                  role.getDesired());\n    cd.setRoleOpt(rolename, ROLE_ACTUAL_INSTANCES, nodeCount);\n    cd.setRoleOpt(rolename, ROLE_REQUESTED_INSTANCES, role.getRequested());\n    cd.setRoleOpt(rolename, ROLE_RELEASING_INSTANCES, role.getReleasing());\n    cd.setRoleOpt(rolename, ROLE_FAILED_INSTANCES, role.getFailed());\n    cd.setRoleOpt(rolename, ROLE_FAILED_STARTING_INSTANCES, role.getStartFailed());\n    cd.setRoleOpt(rolename, ROLE_FAILED_RECENTLY_INSTANCES, role.getFailedRecently());\n    cd.setRoleOpt(rolename, ROLE_NODE_FAILED_INSTANCES, role.getNodeFailed());\n    cd.setRoleOpt(rolename, ROLE_PREEMPTED_INSTANCES, role.getPreempted());\n    if (role.isAntiAffinePlacement()) {\n      cd.setRoleOpt(rolename, ROLE_PENDING_AA_INSTANCES, role.getPendingAntiAffineRequests());\n    }\n    Map<String, Integer> stats = role.buildStatistics();\n    cd.statistics.put(rolename, stats);\n  }\n\n  Map<String, Integer> sliderstats = getLiveStatistics();\n  cd.statistics.put(SliderKeys.COMPONENT_AM, sliderstats);\n\n  // liveness\n  cd.liveness = getApplicationLivenessInformation();\n\n  return cd;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2462,
        "instruction": "protected void waitForServerToStop(ActiveMQServer server) throws InterruptedException {\n   if (server == null)\n      return;\n   final long wait = 5000;\n   long timetowait = System.currentTimeMillis() + wait;\n   while (server.isStarted() && System.currentTimeMillis() < timetowait) {\n      Thread.sleep(50);\n   }\n\n   if (server.isStarted()) {\n      fail(\"Server didn't start: \" + server);\n   }\n}",
        "accept_response": "protected void waitForServerToStop(ActiveMQServer server) throws InterruptedException {\n   if (server == null)\n      return;\n   final long wait = 5000;\n   long timetowait = System.currentTimeMillis() + wait;\n   while (server.isStarted() && System.currentTimeMillis() < timetowait) {\n      Thread.sleep(50);\n   }\n\n   if (server.isStarted()) {\n      logger.info(threadDump(\"Server didn't start\"));\n      fail(\"Server didn't start: \" + server);\n   }\n}",
        "reject_response": "protected void waitForServerToStop(ActiveMQServer server) throws InterruptedException {\n   if (server == null)\n      return;\n   final long wait = 5000;\n   long timetowait = System.currentTimeMillis() + wait;\n   while (server.isStarted() && System.currentTimeMillis() < timetowait) {\n      Thread.sleep(50);\n   }\n\n   if (server.isStarted()) {\n      log.info(threadDump(\"Server didn't start\"));\n      fail(\"Server didn't start: \" + server);\n   }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3043,
        "instruction": "@NotNull\n@Override\npublic Iterable<DistributionQueueEntry> getEntries(int skip, int limit) {\n    ResourceResolver resourceResolver = null;\n    try {\n        resourceResolver = DistributionUtils.loginService(resolverFactory, serviceName);\n        Resource queueRoot = ResourceQueueUtils.getRootResource(resourceResolver, queueRootPath);\n\n        List<DistributionQueueEntry> entries =  ResourceQueueUtils.getEntries(queueRoot, skip, limit);\n\n\n        return entries;\n    } catch (LoginException e) {\n        throw new RuntimeException(e);\n    } catch (PersistenceException e) {\n        throw new RuntimeException(e);\n    } finally {\n        DistributionUtils.safelyLogout(resourceResolver);\n    }\n}",
        "accept_response": "@NotNull\n@Override\npublic Iterable<DistributionQueueEntry> getEntries(int skip, int limit) {\n    ResourceResolver resourceResolver = null;\n    try {\n        resourceResolver = DistributionUtils.loginService(resolverFactory, serviceName);\n        Resource queueRoot = ResourceQueueUtils.getRootResource(resourceResolver, queueRootPath);\n\n        List<DistributionQueueEntry> entries =  ResourceQueueUtils.getEntries(queueRoot, skip, limit);\n\n        log.debug(\"queue[{}] getEntries entries={}\", queueName, entries.size());\n\n        return entries;\n    } catch (LoginException e) {\n        throw new RuntimeException(e);\n    } catch (PersistenceException e) {\n        throw new RuntimeException(e);\n    } finally {\n        DistributionUtils.safelyLogout(resourceResolver);\n    }\n}",
        "reject_response": "@NotNull\n@Override\npublic Iterable<DistributionQueueEntry> getEntries(int skip, int limit) {\n    ResourceResolver resourceResolver = null;\n    try {\n        resourceResolver = DistributionUtils.loginService(resolverFactory, serviceName);\n        Resource queueRoot = ResourceQueueUtils.getRootResource(resourceResolver, queueRootPath);\n\n        List<DistributionQueueEntry> entries =  ResourceQueueUtils.getEntries(queueRoot, skip, limit);\n\n        log.debug(\"queue[{}] getEntries entries={}\", new Object[] { queueName, entries.size() });\n\n        return entries;\n    } catch (LoginException e) {\n        throw new RuntimeException(e);\n    } catch (PersistenceException e) {\n        throw new RuntimeException(e);\n    } finally {\n        DistributionUtils.safelyLogout(resourceResolver);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2408,
        "instruction": "private void loadFilters() {\n  sortFilters();\n\n  List<Input> toRemoveInputList = new ArrayList<Input>();\n  for (Input input : inputManager.getInputList()) {\n    for (Map<String, Object> map : filterConfigList) {\n      if (map == null) {\n        continue;\n      }\n      mergeBlocks(globalConfigs, map);\n\n      String value = (String) map.get(\"filter\");\n      if (StringUtils.isEmpty(value)) {\n        LOG.error(\"Filter block doesn't have filter element\");\n        continue;\n      }\n      Filter filter = (Filter) AliasUtil.getClassInstance(value, AliasType.FILTER);\n      if (filter == null) {\n        continue;\n      }\n      filter.loadConfig(map);\n      filter.setInput(input);\n\n      if (filter.isEnabled()) {\n        filter.setOutputManager(outputManager);\n        input.addFilter(filter);\n        filter.logConfgs(Level.INFO);\n      } else {\n        LOG.debug(\"Ignoring filter \" + filter.getShortDescription() + \" for input \" + input.getShortDescription());\n      }\n    }\n\n    if (input.getFirstFilter() == null) {\n      toRemoveInputList.add(input);\n    }\n  }\n\n  for (Input toRemoveInput : toRemoveInputList) {\n    LOG.warn(\"There are no filters, we will ignore this input. \" + toRemoveInput.getShortDescription());\n    inputManager.removeInput(toRemoveInput);\n  }\n}",
        "accept_response": "private void loadFilters() {\n  sortFilters();\n\n  List<Input> toRemoveInputList = new ArrayList<Input>();\n  for (Input input : inputManager.getInputList()) {\n    for (Map<String, Object> map : filterConfigList) {\n      if (map == null) {\n        continue;\n      }\n      mergeBlocks(globalConfigs, map);\n\n      String value = (String) map.get(\"filter\");\n      if (StringUtils.isEmpty(value)) {\n        LOG.error(\"Filter block doesn't have filter element\");\n        continue;\n      }\n      Filter filter = (Filter) AliasUtil.getClassInstance(value, AliasType.FILTER);\n      if (filter == null) {\n        LOG.error(\"Filter object could not be found\");\n        continue;\n      }\n      filter.loadConfig(map);\n      filter.setInput(input);\n\n      if (filter.isEnabled()) {\n        filter.setOutputManager(outputManager);\n        input.addFilter(filter);\n        filter.logConfgs(Level.INFO);\n      } else {\n        LOG.debug(\"Ignoring filter \" + filter.getShortDescription() + \" for input \" + input.getShortDescription());\n      }\n    }\n\n    if (input.getFirstFilter() == null) {\n      toRemoveInputList.add(input);\n    }\n  }\n\n  for (Input toRemoveInput : toRemoveInputList) {\n    LOG.warn(\"There are no filters, we will ignore this input. \" + toRemoveInput.getShortDescription());\n    inputManager.removeInput(toRemoveInput);\n  }\n}",
        "reject_response": "private void loadFilters() {\n  sortFilters();\n\n  List<Input> toRemoveInputList = new ArrayList<Input>();\n  for (Input input : inputManager.getInputList()) {\n    for (Map<String, Object> map : filterConfigList) {\n      if (map == null) {\n        continue;\n      }\n      mergeBlocks(globalConfigs, map);\n\n      String value = (String) map.get(\"filter\");\n      if (StringUtils.isEmpty(value)) {\n        LOG.error(\"Filter block doesn't have filter element\");\n        continue;\n      }\n      Filter filter = (Filter) AliasUtil.getClassInstance(value, AliasType.FILTER);\n      if (filter == null) {\n        logger.error(\"Filter Object is null\");\n        continue;\n      }\n      filter.loadConfig(map);\n      filter.setInput(input);\n\n      if (filter.isEnabled()) {\n        filter.setOutputManager(outputManager);\n        input.addFilter(filter);\n        filter.logConfgs(Level.INFO);\n      } else {\n        LOG.debug(\"Ignoring filter \" + filter.getShortDescription() + \" for input \" + input.getShortDescription());\n      }\n    }\n\n    if (input.getFirstFilter() == null) {\n      toRemoveInputList.add(input);\n    }\n  }\n\n  for (Input toRemoveInput : toRemoveInputList) {\n    LOG.warn(\"There are no filters, we will ignore this input. \" + toRemoveInput.getShortDescription());\n    inputManager.removeInput(toRemoveInput);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3259,
        "instruction": "protected ObjectName makeObjectName(String path, ZKMBeanInfo bean) throws MalformedObjectNameException {\n    if (path == null) {\n        return null;\n    }\n    StringBuilder beanName = new StringBuilder(DOMAIN + \":\");\n    int counter = 0;\n    counter = tokenize(beanName, path, counter);\n    tokenize(beanName, bean.getName(), counter);\n    beanName.deleteCharAt(beanName.length() - 1);\n    try {\n        return new ObjectName(beanName.toString());\n    } catch (MalformedObjectNameException e) {\n        throw e;\n    }\n}",
        "accept_response": "protected ObjectName makeObjectName(String path, ZKMBeanInfo bean) throws MalformedObjectNameException {\n    if (path == null) {\n        return null;\n    }\n    StringBuilder beanName = new StringBuilder(DOMAIN + \":\");\n    int counter = 0;\n    counter = tokenize(beanName, path, counter);\n    tokenize(beanName, bean.getName(), counter);\n    beanName.deleteCharAt(beanName.length() - 1);\n    try {\n        return new ObjectName(beanName.toString());\n    } catch (MalformedObjectNameException e) {\n        LOG.warn(\"Invalid name \\\"{}\\\" for class {}\", beanName, bean.getClass());\n        throw e;\n    }\n}",
        "reject_response": "protected ObjectName makeObjectName(String path, ZKMBeanInfo bean) throws MalformedObjectNameException {\n    if (path == null) {\n        return null;\n    }\n    StringBuilder beanName = new StringBuilder(DOMAIN + \":\");\n    int counter = 0;\n    counter = tokenize(beanName, path, counter);\n    tokenize(beanName, bean.getName(), counter);\n    beanName.deleteCharAt(beanName.length() - 1);\n    try {\n        return new ObjectName(beanName.toString());\n    } catch (MalformedObjectNameException e) {\n        LOG.warn(\"Invalid name \\\"\" + beanName.toString() + \"\\\" for class \" + bean.getClass().toString());\n        throw e;\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3129,
        "instruction": "@Override\npublic void unregisterRunningContainer(ContainerId containerId, int taskCommId, ContainerEndReason endReason, String diagnostics) {\n  ContainerInfo containerInfo = registeredContainers.remove(containerId);\n  if (containerInfo.taskAttemptId != null) {\n    registeredAttempts.remove(containerInfo.taskAttemptId);\n  }\n  try {\n    taskCommunicators[taskCommId].registerContainerEnd(containerId, endReason, diagnostics);\n  } catch (Exception e) {\n    String msg = \"Error in TaskCommunicator when unregistering Container\"\n        + \", communicator=\" + Utils.getTaskCommIdentifierString(taskCommId, context)\n        + \", containerId=\" + containerId;\n    LOG.error(msg, e);\n    sendEvent(\n        new DAGAppMasterEventUserServiceFatalError(\n            DAGAppMasterEventType.TASK_COMMUNICATOR_SERVICE_FATAL_ERROR,\n            msg, e));\n  }\n}",
        "accept_response": "@Override\npublic void unregisterRunningContainer(ContainerId containerId, int taskCommId, ContainerEndReason endReason, String diagnostics) {\n  LOG.debug(\"Unregistering Container from TaskAttemptListener: {}\", containerId);\n  ContainerInfo containerInfo = registeredContainers.remove(containerId);\n  if (containerInfo.taskAttemptId != null) {\n    registeredAttempts.remove(containerInfo.taskAttemptId);\n  }\n  try {\n    taskCommunicators[taskCommId].registerContainerEnd(containerId, endReason, diagnostics);\n  } catch (Exception e) {\n    String msg = \"Error in TaskCommunicator when unregistering Container\"\n        + \", communicator=\" + Utils.getTaskCommIdentifierString(taskCommId, context)\n        + \", containerId=\" + containerId;\n    LOG.error(msg, e);\n    sendEvent(\n        new DAGAppMasterEventUserServiceFatalError(\n            DAGAppMasterEventType.TASK_COMMUNICATOR_SERVICE_FATAL_ERROR,\n            msg, e));\n  }\n}",
        "reject_response": "@Override\npublic void unregisterRunningContainer(ContainerId containerId, int taskCommId, ContainerEndReason endReason, String diagnostics) {\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"Unregistering Container from TaskAttemptListener: \" + containerId);\n  }\n  ContainerInfo containerInfo = registeredContainers.remove(containerId);\n  if (containerInfo.taskAttemptId != null) {\n    registeredAttempts.remove(containerInfo.taskAttemptId);\n  }\n  try {\n    taskCommunicators[taskCommId].registerContainerEnd(containerId, endReason, diagnostics);\n  } catch (Exception e) {\n    String msg = \"Error in TaskCommunicator when unregistering Container\"\n        + \", communicator=\" + Utils.getTaskCommIdentifierString(taskCommId, context)\n        + \", containerId=\" + containerId;\n    LOG.error(msg, e);\n    sendEvent(\n        new DAGAppMasterEventUserServiceFatalError(\n            DAGAppMasterEventType.TASK_COMMUNICATOR_SERVICE_FATAL_ERROR,\n            msg, e));\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2658,
        "instruction": "public Subject login(Properties credentials) {\n  if (!isIntegratedSecurity()) {\n    return null;\n  }\n\n  if (credentials == null)\n    return null;\n\n  // this makes sure it starts with a clean user object\n  ThreadContext.remove();\n\n  Subject currentUser = SecurityUtils.getSubject();\n  GeodeAuthenticationToken token = new GeodeAuthenticationToken(credentials);\n  try {\n    currentUser.login(token);\n  } catch (ShiroException e) {\n    logger.info(e.getMessage(), e);\n    throw new AuthenticationFailedException(\n        \"Authentication error. Please check your credentials.\", e);\n  }\n\n  return currentUser;\n}",
        "accept_response": "public Subject login(Properties credentials) {\n  if (!isIntegratedSecurity()) {\n    return null;\n  }\n\n  if (credentials == null)\n    return null;\n\n  // this makes sure it starts with a clean user object\n  ThreadContext.remove();\n\n  Subject currentUser = SecurityUtils.getSubject();\n  GeodeAuthenticationToken token = new GeodeAuthenticationToken(credentials);\n  try {\n    logger.debug(\"Logging in \" + token.getPrincipal());\n    currentUser.login(token);\n  } catch (ShiroException e) {\n    logger.info(e.getMessage(), e);\n    throw new AuthenticationFailedException(\n        \"Authentication error. Please check your credentials.\", e);\n  }\n\n  return currentUser;\n}",
        "reject_response": "public Subject login(Properties credentials) {\n  if (!isIntegratedSecurity()) {\n    return null;\n  }\n\n  if (credentials == null)\n    return null;\n\n  // this makes sure it starts with a clean user object\n  ThreadContext.remove();\n\n  Subject currentUser = SecurityUtils.getSubject();\n  GeodeAuthenticationToken token = new GeodeAuthenticationToken(credentials);\n  try {\n    logger.info(\"Logging in \" + token.getPrincipal());\n    currentUser.login(token);\n  } catch (ShiroException e) {\n    logger.info(e.getMessage(), e);\n    throw new AuthenticationFailedException(\n        \"Authentication error. Please check your credentials.\", e);\n  }\n\n  return currentUser;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2612,
        "instruction": "@Override\npublic void run(SourceContext<T> sourceContext) throws Exception {\n\tif (subscribedPartitionsToStartOffsets == null) {\n\t\tthrow new Exception(\"The partitions were not set for the consumer\");\n\t}\n\n\t// initialize commit metrics and default offset callback method\n\tthis.successfulCommits = this.getRuntimeContext().getMetricGroup().counter(COMMITS_SUCCEEDED_METRICS_COUNTER);\n\tthis.failedCommits =  this.getRuntimeContext().getMetricGroup().counter(COMMITS_FAILED_METRICS_COUNTER);\n\n\tthis.offsetCommitCallback = new KafkaCommitCallback() {\n\t\t@Override\n\t\tpublic void onSuccess() {\n\t\t\tsuccessfulCommits.inc();\n\t\t}\n\n\t\t@Override\n\t\tpublic void onException(Throwable cause) {\n\t\t\tLOG.warn(\"Async Kafka commit failed.\", cause);\n\t\t\tfailedCommits.inc();\n\t\t}\n\t};\n\n\t// mark the subtask as temporarily idle if there are no initial seed partitions;\n\t// once this subtask discovers some partitions and starts collecting records, the subtask's\n\t// status will automatically be triggered back to be active.\n\tif (subscribedPartitionsToStartOffsets.isEmpty()) {\n\t\tsourceContext.markAsTemporarilyIdle();\n\t}\n\n\t// from this point forward:\n\t//   - 'snapshotState' will draw offsets from the fetcher,\n\t//     instead of being built from `subscribedPartitionsToStartOffsets`\n\t//   - 'notifyCheckpointComplete' will start to do work (i.e. commit offsets to\n\t//     Kafka through the fetcher, if configured to do so)\n\tthis.kafkaFetcher = createFetcher(\n\t\t\tsourceContext,\n\t\t\tsubscribedPartitionsToStartOffsets,\n\t\t\tperiodicWatermarkAssigner,\n\t\t\tpunctuatedWatermarkAssigner,\n\t\t\t(StreamingRuntimeContext) getRuntimeContext(),\n\t\t\toffsetCommitMode);\n\n\tif (!running) {\n\t\treturn;\n\t}\n\n\t// depending on whether we were restored with the current state version (1.3),\n\t// remaining logic branches off into 2 paths:\n\t//  1) New state - partition discovery loop executed as separate thread, with this\n\t//                 thread running the main fetcher loop\n\t//  2) Old state - partition discovery is disabled and only the main fetcher loop is executed\n\n\tif (discoveryIntervalMillis != PARTITION_DISCOVERY_DISABLED) {\n\t\tfinal AtomicReference<Exception> discoveryLoopErrorRef = new AtomicReference<>();\n\t\tthis.discoveryLoopThread = new Thread(new Runnable() {\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\t// --------------------- partition discovery loop ---------------------\n\n\t\t\t\t\tList<KafkaTopicPartition> discoveredPartitions;\n\n\t\t\t\t\t// throughout the loop, we always eagerly check if we are still running before\n\t\t\t\t\t// performing the next operation, so that we can escape the loop as soon as possible\n\n\t\t\t\t\twhile (running) {\n\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tdiscoveredPartitions = partitionDiscoverer.discoverPartitions();\n\t\t\t\t\t\t} catch (AbstractPartitionDiscoverer.WakeupException | AbstractPartitionDiscoverer.ClosedException e) {\n\t\t\t\t\t\t\t// the partition discoverer may have been closed or woken up before or during the discovery;\n\t\t\t\t\t\t\t// this would only happen if the consumer was canceled; simply escape the loop\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t// no need to add the discovered partitions if we were closed during the meantime\n\t\t\t\t\t\tif (running && !discoveredPartitions.isEmpty()) {\n\t\t\t\t\t\t\tkafkaFetcher.addDiscoveredPartitions(discoveredPartitions);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t// do not waste any time sleeping if we're not running anymore\n\t\t\t\t\t\tif (running && discoveryIntervalMillis != 0) {\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\tThread.sleep(discoveryIntervalMillis);\n\t\t\t\t\t\t\t} catch (InterruptedException iex) {\n\t\t\t\t\t\t\t\t// may be interrupted if the consumer was canceled midway; simply escape the loop\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tdiscoveryLoopErrorRef.set(e);\n\t\t\t\t} finally {\n\t\t\t\t\t// calling cancel will also let the fetcher loop escape\n\t\t\t\t\tcancel();\n\t\t\t\t}\n\t\t\t}\n\t\t});\n\n\t\tdiscoveryLoopThread.start();\n\t\tkafkaFetcher.runFetchLoop();\n\n\t\t// --------------------------------------------------------------------\n\n\t\t// make sure that the partition discoverer is properly closed\n\t\tpartitionDiscoverer.close();\n\t\tdiscoveryLoopThread.join();\n\n\t\t// rethrow any fetcher errors\n\t\tfinal Exception discoveryLoopError = discoveryLoopErrorRef.get();\n\t\tif (discoveryLoopError != null) {\n\t\t\tthrow new RuntimeException(discoveryLoopError);\n\t\t}\n\t} else {\n\t\t// won't be using the discoverer\n\t\tpartitionDiscoverer.close();\n\n\t\tkafkaFetcher.runFetchLoop();\n\t}\n}",
        "accept_response": "@Override\npublic void run(SourceContext<T> sourceContext) throws Exception {\n\tif (subscribedPartitionsToStartOffsets == null) {\n\t\tthrow new Exception(\"The partitions were not set for the consumer\");\n\t}\n\n\t// initialize commit metrics and default offset callback method\n\tthis.successfulCommits = this.getRuntimeContext().getMetricGroup().counter(COMMITS_SUCCEEDED_METRICS_COUNTER);\n\tthis.failedCommits =  this.getRuntimeContext().getMetricGroup().counter(COMMITS_FAILED_METRICS_COUNTER);\n\n\tthis.offsetCommitCallback = new KafkaCommitCallback() {\n\t\t@Override\n\t\tpublic void onSuccess() {\n\t\t\tsuccessfulCommits.inc();\n\t\t}\n\n\t\t@Override\n\t\tpublic void onException(Throwable cause) {\n\t\t\tLOG.warn(\"Async Kafka commit failed.\", cause);\n\t\t\tfailedCommits.inc();\n\t\t}\n\t};\n\n\t// mark the subtask as temporarily idle if there are no initial seed partitions;\n\t// once this subtask discovers some partitions and starts collecting records, the subtask's\n\t// status will automatically be triggered back to be active.\n\tif (subscribedPartitionsToStartOffsets.isEmpty()) {\n\t\tsourceContext.markAsTemporarilyIdle();\n\t}\n\n\t// from this point forward:\n\t//   - 'snapshotState' will draw offsets from the fetcher,\n\t//     instead of being built from `subscribedPartitionsToStartOffsets`\n\t//   - 'notifyCheckpointComplete' will start to do work (i.e. commit offsets to\n\t//     Kafka through the fetcher, if configured to do so)\n\tthis.kafkaFetcher = createFetcher(\n\t\t\tsourceContext,\n\t\t\tsubscribedPartitionsToStartOffsets,\n\t\t\tperiodicWatermarkAssigner,\n\t\t\tpunctuatedWatermarkAssigner,\n\t\t\t(StreamingRuntimeContext) getRuntimeContext(),\n\t\t\toffsetCommitMode);\n\n\tif (!running) {\n\t\treturn;\n\t}\n\n\t// depending on whether we were restored with the current state version (1.3),\n\t// remaining logic branches off into 2 paths:\n\t//  1) New state - partition discovery loop executed as separate thread, with this\n\t//                 thread running the main fetcher loop\n\t//  2) Old state - partition discovery is disabled and only the main fetcher loop is executed\n\n\tif (discoveryIntervalMillis != PARTITION_DISCOVERY_DISABLED) {\n\t\tfinal AtomicReference<Exception> discoveryLoopErrorRef = new AtomicReference<>();\n\t\tthis.discoveryLoopThread = new Thread(new Runnable() {\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\t// --------------------- partition discovery loop ---------------------\n\n\t\t\t\t\tList<KafkaTopicPartition> discoveredPartitions;\n\n\t\t\t\t\t// throughout the loop, we always eagerly check if we are still running before\n\t\t\t\t\t// performing the next operation, so that we can escape the loop as soon as possible\n\n\t\t\t\t\twhile (running) {\n\t\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\t\tLOG.debug(\"Consumer subtask {} is trying to discover new partitions ...\", getRuntimeContext().getIndexOfThisSubtask());\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tdiscoveredPartitions = partitionDiscoverer.discoverPartitions();\n\t\t\t\t\t\t} catch (AbstractPartitionDiscoverer.WakeupException | AbstractPartitionDiscoverer.ClosedException e) {\n\t\t\t\t\t\t\t// the partition discoverer may have been closed or woken up before or during the discovery;\n\t\t\t\t\t\t\t// this would only happen if the consumer was canceled; simply escape the loop\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t// no need to add the discovered partitions if we were closed during the meantime\n\t\t\t\t\t\tif (running && !discoveredPartitions.isEmpty()) {\n\t\t\t\t\t\t\tkafkaFetcher.addDiscoveredPartitions(discoveredPartitions);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t// do not waste any time sleeping if we're not running anymore\n\t\t\t\t\t\tif (running && discoveryIntervalMillis != 0) {\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\tThread.sleep(discoveryIntervalMillis);\n\t\t\t\t\t\t\t} catch (InterruptedException iex) {\n\t\t\t\t\t\t\t\t// may be interrupted if the consumer was canceled midway; simply escape the loop\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tdiscoveryLoopErrorRef.set(e);\n\t\t\t\t} finally {\n\t\t\t\t\t// calling cancel will also let the fetcher loop escape\n\t\t\t\t\tcancel();\n\t\t\t\t}\n\t\t\t}\n\t\t});\n\n\t\tdiscoveryLoopThread.start();\n\t\tkafkaFetcher.runFetchLoop();\n\n\t\t// --------------------------------------------------------------------\n\n\t\t// make sure that the partition discoverer is properly closed\n\t\tpartitionDiscoverer.close();\n\t\tdiscoveryLoopThread.join();\n\n\t\t// rethrow any fetcher errors\n\t\tfinal Exception discoveryLoopError = discoveryLoopErrorRef.get();\n\t\tif (discoveryLoopError != null) {\n\t\t\tthrow new RuntimeException(discoveryLoopError);\n\t\t}\n\t} else {\n\t\t// won't be using the discoverer\n\t\tpartitionDiscoverer.close();\n\n\t\tkafkaFetcher.runFetchLoop();\n\t}\n}",
        "reject_response": "@Override\npublic void run(SourceContext<T> sourceContext) throws Exception {\n\tif (subscribedPartitionsToStartOffsets == null) {\n\t\tthrow new Exception(\"The partitions were not set for the consumer\");\n\t}\n\n\t// initialize commit metrics and default offset callback method\n\tthis.successfulCommits = this.getRuntimeContext().getMetricGroup().counter(COMMITS_SUCCEEDED_METRICS_COUNTER);\n\tthis.failedCommits =  this.getRuntimeContext().getMetricGroup().counter(COMMITS_FAILED_METRICS_COUNTER);\n\n\tthis.offsetCommitCallback = new KafkaCommitCallback() {\n\t\t@Override\n\t\tpublic void onSuccess() {\n\t\t\tsuccessfulCommits.inc();\n\t\t}\n\n\t\t@Override\n\t\tpublic void onException(Throwable cause) {\n\t\t\tLOG.warn(\"Async Kafka commit failed.\", cause);\n\t\t\tfailedCommits.inc();\n\t\t}\n\t};\n\n\t// mark the subtask as temporarily idle if there are no initial seed partitions;\n\t// once this subtask discovers some partitions and starts collecting records, the subtask's\n\t// status will automatically be triggered back to be active.\n\tif (subscribedPartitionsToStartOffsets.isEmpty()) {\n\t\tsourceContext.markAsTemporarilyIdle();\n\t}\n\n\t// from this point forward:\n\t//   - 'snapshotState' will draw offsets from the fetcher,\n\t//     instead of being built from `subscribedPartitionsToStartOffsets`\n\t//   - 'notifyCheckpointComplete' will start to do work (i.e. commit offsets to\n\t//     Kafka through the fetcher, if configured to do so)\n\tthis.kafkaFetcher = createFetcher(\n\t\t\tsourceContext,\n\t\t\tsubscribedPartitionsToStartOffsets,\n\t\t\tperiodicWatermarkAssigner,\n\t\t\tpunctuatedWatermarkAssigner,\n\t\t\t(StreamingRuntimeContext) getRuntimeContext(),\n\t\t\toffsetCommitMode);\n\n\tif (!running) {\n\t\treturn;\n\t}\n\n\t// depending on whether we were restored with the current state version (1.3),\n\t// remaining logic branches off into 2 paths:\n\t//  1) New state - partition discovery loop executed as separate thread, with this\n\t//                 thread running the main fetcher loop\n\t//  2) Old state - partition discovery is disabled and only the main fetcher loop is executed\n\n\tif (discoveryIntervalMillis != PARTITION_DISCOVERY_DISABLED) {\n\t\tfinal AtomicReference<Exception> discoveryLoopErrorRef = new AtomicReference<>();\n\t\tthis.discoveryLoopThread = new Thread(new Runnable() {\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\t// --------------------- partition discovery loop ---------------------\n\n\t\t\t\t\tList<KafkaTopicPartition> discoveredPartitions;\n\n\t\t\t\t\t// throughout the loop, we always eagerly check if we are still running before\n\t\t\t\t\t// performing the next operation, so that we can escape the loop as soon as possible\n\n\t\t\t\t\twhile (running) {\n\t\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\t\tLOG.debug(\"Consumer subtask {} is trying to discover new partitions ...\");\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tdiscoveredPartitions = partitionDiscoverer.discoverPartitions();\n\t\t\t\t\t\t} catch (AbstractPartitionDiscoverer.WakeupException | AbstractPartitionDiscoverer.ClosedException e) {\n\t\t\t\t\t\t\t// the partition discoverer may have been closed or woken up before or during the discovery;\n\t\t\t\t\t\t\t// this would only happen if the consumer was canceled; simply escape the loop\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t// no need to add the discovered partitions if we were closed during the meantime\n\t\t\t\t\t\tif (running && !discoveredPartitions.isEmpty()) {\n\t\t\t\t\t\t\tkafkaFetcher.addDiscoveredPartitions(discoveredPartitions);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t// do not waste any time sleeping if we're not running anymore\n\t\t\t\t\t\tif (running && discoveryIntervalMillis != 0) {\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\tThread.sleep(discoveryIntervalMillis);\n\t\t\t\t\t\t\t} catch (InterruptedException iex) {\n\t\t\t\t\t\t\t\t// may be interrupted if the consumer was canceled midway; simply escape the loop\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tdiscoveryLoopErrorRef.set(e);\n\t\t\t\t} finally {\n\t\t\t\t\t// calling cancel will also let the fetcher loop escape\n\t\t\t\t\tcancel();\n\t\t\t\t}\n\t\t\t}\n\t\t});\n\n\t\tdiscoveryLoopThread.start();\n\t\tkafkaFetcher.runFetchLoop();\n\n\t\t// --------------------------------------------------------------------\n\n\t\t// make sure that the partition discoverer is properly closed\n\t\tpartitionDiscoverer.close();\n\t\tdiscoveryLoopThread.join();\n\n\t\t// rethrow any fetcher errors\n\t\tfinal Exception discoveryLoopError = discoveryLoopErrorRef.get();\n\t\tif (discoveryLoopError != null) {\n\t\t\tthrow new RuntimeException(discoveryLoopError);\n\t\t}\n\t} else {\n\t\t// won't be using the discoverer\n\t\tpartitionDiscoverer.close();\n\n\t\tkafkaFetcher.runFetchLoop();\n\t}\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3058,
        "instruction": "private void doEdit(SolrQueryRequest req, SolrQueryResponse rsp, String path, final String key, final Object plugin)\n    throws IOException {\n  ConfigEditablePlugin configEditablePlugin = null;\n\n  if (plugin == null) {\n    throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No \" + key + \" plugin configured\");\n  }\n  if (plugin instanceof ConfigEditablePlugin) {\n    configEditablePlugin = (ConfigEditablePlugin) plugin;\n  } else {\n    throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, key + \" plugin is not editable\");\n  }\n\n  if (req.getContentStreams() == null) {\n    throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No contentStream\");\n  }\n  List<CommandOperation> ops = CommandOperation.readCommands(req.getContentStreams(), rsp);\n  if (ops == null) {\n    throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No commands\");\n  }\n  for (int count = 1; count <= 3 ; count++ ) {\n    SecurityConfig securityConfig = getSecurityConfig(true);\n    Map<String, Object> data = securityConfig.getData();\n    Map<String, Object> latestConf = (Map<String, Object>) data.get(key);\n    if (latestConf == null) {\n      throw new SolrException(SERVER_ERROR, \"No configuration present for \" + key);\n    }\n    List<CommandOperation> commandsCopy = CommandOperation.clone(ops);\n    Map<String, Object> out = configEditablePlugin.edit(Utils.getDeepCopy(latestConf, 4) , commandsCopy);\n    if (out == null) {\n      List<Map> errs = CommandOperation.captureErrors(commandsCopy);\n      if (!errs.isEmpty()) {\n        rsp.add(CommandOperation.ERR_MSGS, errs);\n        return;\n      }\n      log.debug(\"No edits made\");\n      return;\n    } else {\n      if(!Objects.equals(latestConf.get(\"class\") , out.get(\"class\"))){\n        throw new SolrException(SERVER_ERROR, \"class cannot be modified\");\n      }\n      Map meta = getMapValue(out, \"\");\n      meta.put(\"v\", securityConfig.getVersion()+1);//encode the expected zkversion\n      data.put(key, out);\n\n      if(persistConf(securityConfig)) {\n        securityConfEdited();\n        return;\n      }\n    }\n  }\n  throw new SolrException(SERVER_ERROR, \"Failed to persist security config after 3 attempts. Giving up\");\n}",
        "accept_response": "private void doEdit(SolrQueryRequest req, SolrQueryResponse rsp, String path, final String key, final Object plugin)\n    throws IOException {\n  ConfigEditablePlugin configEditablePlugin = null;\n\n  if (plugin == null) {\n    throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No \" + key + \" plugin configured\");\n  }\n  if (plugin instanceof ConfigEditablePlugin) {\n    configEditablePlugin = (ConfigEditablePlugin) plugin;\n  } else {\n    throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, key + \" plugin is not editable\");\n  }\n\n  if (req.getContentStreams() == null) {\n    throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No contentStream\");\n  }\n  List<CommandOperation> ops = CommandOperation.readCommands(req.getContentStreams(), rsp);\n  if (ops == null) {\n    throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No commands\");\n  }\n  for (int count = 1; count <= 3 ; count++ ) {\n    SecurityConfig securityConfig = getSecurityConfig(true);\n    Map<String, Object> data = securityConfig.getData();\n    Map<String, Object> latestConf = (Map<String, Object>) data.get(key);\n    if (latestConf == null) {\n      throw new SolrException(SERVER_ERROR, \"No configuration present for \" + key);\n    }\n    List<CommandOperation> commandsCopy = CommandOperation.clone(ops);\n    Map<String, Object> out = configEditablePlugin.edit(Utils.getDeepCopy(latestConf, 4) , commandsCopy);\n    if (out == null) {\n      List<Map> errs = CommandOperation.captureErrors(commandsCopy);\n      if (!errs.isEmpty()) {\n        rsp.add(CommandOperation.ERR_MSGS, errs);\n        return;\n      }\n      log.debug(\"No edits made\");\n      return;\n    } else {\n      if(!Objects.equals(latestConf.get(\"class\") , out.get(\"class\"))){\n        throw new SolrException(SERVER_ERROR, \"class cannot be modified\");\n      }\n      Map meta = getMapValue(out, \"\");\n      meta.put(\"v\", securityConfig.getVersion()+1);//encode the expected zkversion\n      data.put(key, out);\n\n      if(persistConf(securityConfig)) {\n        securityConfEdited();\n        return;\n      }\n    }\n    log.debug(\"Security edit operation failed {} time(s)\", count);\n  }\n  throw new SolrException(SERVER_ERROR, \"Failed to persist security config after 3 attempts. Giving up\");\n}",
        "reject_response": "private void doEdit(SolrQueryRequest req, SolrQueryResponse rsp, String path, final String key, final Object plugin)\n    throws IOException {\n  ConfigEditablePlugin configEditablePlugin = null;\n\n  if (plugin == null) {\n    throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No \" + key + \" plugin configured\");\n  }\n  if (plugin instanceof ConfigEditablePlugin) {\n    configEditablePlugin = (ConfigEditablePlugin) plugin;\n  } else {\n    throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, key + \" plugin is not editable\");\n  }\n\n  if (req.getContentStreams() == null) {\n    throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No contentStream\");\n  }\n  List<CommandOperation> ops = CommandOperation.readCommands(req.getContentStreams(), rsp);\n  if (ops == null) {\n    throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No commands\");\n  }\n  for (int count = 1; count <= 3 ; count++ ) {\n    SecurityConfig securityConfig = getSecurityConfig(true);\n    Map<String, Object> data = securityConfig.getData();\n    Map<String, Object> latestConf = (Map<String, Object>) data.get(key);\n    if (latestConf == null) {\n      throw new SolrException(SERVER_ERROR, \"No configuration present for \" + key);\n    }\n    List<CommandOperation> commandsCopy = CommandOperation.clone(ops);\n    Map<String, Object> out = configEditablePlugin.edit(Utils.getDeepCopy(latestConf, 4) , commandsCopy);\n    if (out == null) {\n      List<Map> errs = CommandOperation.captureErrors(commandsCopy);\n      if (!errs.isEmpty()) {\n        rsp.add(CommandOperation.ERR_MSGS, errs);\n        return;\n      }\n      log.debug(\"No edits made\");\n      return;\n    } else {\n      if(!Objects.equals(latestConf.get(\"class\") , out.get(\"class\"))){\n        throw new SolrException(SERVER_ERROR, \"class cannot be modified\");\n      }\n      Map meta = getMapValue(out, \"\");\n      meta.put(\"v\", securityConfig.getVersion()+1);//encode the expected zkversion\n      data.put(key, out);\n\n      if(persistConf(securityConfig)) {\n        securityConfEdited();\n        return;\n      }\n    }\n    log.debug(\"Security edit operation failed {} time(s)\" + count);\n  }\n  throw new SolrException(SERVER_ERROR, \"Failed to persist security config after 3 attempts. Giving up\");\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2650,
        "instruction": "private void initializeLogger() {\n\n  // Override default log configuration by properties which are provided in\n  // properties file.\n  loadLogDetailsFromPropertyFile();\n\n  // Override log configuration by properties which are provided in\n  // through system properties.\n  loadLogDetailsFromSystemProperties();\n\n  // Log messages stored in messagesToBeLogged\n  messagesToBeLogged = \"\";\n}",
        "accept_response": "private void initializeLogger() {\n\n  // Override default log configuration by properties which are provided in\n  // properties file.\n  loadLogDetailsFromPropertyFile();\n\n  // Override log configuration by properties which are provided in\n  // through system properties.\n  loadLogDetailsFromSystemProperties();\n\n  // Log messages stored in messagesToBeLogged\n  logger.info(messagesToBeLogged);\n  messagesToBeLogged = \"\";\n}",
        "reject_response": "private void initializeLogger() {\n\n  // Override default log configuration by properties which are provided in\n  // properties file.\n  loadLogDetailsFromPropertyFile();\n\n  // Override log configuration by properties which are provided in\n  // through system properties.\n  loadLogDetailsFromSystemProperties();\n\n  // Log messages stored in messagesToBeLogged\n  if (LOGGER.infoEnabled()) {\n    LOGGER.info(messagesToBeLogged);\n    messagesToBeLogged = \"\";\n  }\n  messagesToBeLogged = \"\";\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2866,
        "instruction": "@Override\npublic Binding convert(Binding b)\n{\n    if ( parentBinding == null || parentBinding.isEmpty() )\n        return b ;\n\n    // This is the result.  Could have BindingBase.setParent etc.\n    BindingMap b2 = BindingFactory.create(parentBinding) ;\n\n    // Copy the resultSet bindings to the combined result binding with checking.\n    for ( Iterator<Var> iter = b.vars() ; iter.hasNext(); )\n    {\n        Var v = iter.next();\n        Node n = b.get(v) ;\n        if ( b2.contains(v) )\n        {\n            Node n2 = b2.get(v) ;\n            if ( n2.equals(n) )\n                Log.warn(this, \"Binding already for \"+v+\" (same value)\" ) ;\n            else\n            {\n                throw new ARQInternalErrorException(\"Incompatible bindings for \"+v) ;\n            }\n        }\n        b2.add(v, n) ;\n    }\n    return b2 ;\n}",
        "accept_response": "@Override\npublic Binding convert(Binding b)\n{\n    if ( parentBinding == null || parentBinding.isEmpty() )\n        return b ;\n\n    // This is the result.  Could have BindingBase.setParent etc.\n    BindingMap b2 = BindingFactory.create(parentBinding) ;\n\n    // Copy the resultSet bindings to the combined result binding with checking.\n    for ( Iterator<Var> iter = b.vars() ; iter.hasNext(); )\n    {\n        Var v = iter.next();\n        Node n = b.get(v) ;\n        if ( b2.contains(v) )\n        {\n            Node n2 = b2.get(v) ;\n            if ( n2.equals(n) )\n                Log.warn(this, \"Binding already for \"+v+\" (same value)\" ) ;\n            else\n            {\n                Log.error(this, \"Binding already for \"+v+\" (different values)\" ) ;\n                throw new ARQInternalErrorException(\"Incompatible bindings for \"+v) ;\n            }\n        }\n        b2.add(v, n) ;\n    }\n    return b2 ;\n}",
        "reject_response": "@Override\npublic Binding convert(Binding b)\n{\n    if ( parentBinding == null || parentBinding.isEmpty() )\n        return b ;\n\n    // This is the result.  Could have BindingBase.setParent etc.\n    BindingMap b2 = BindingFactory.create(parentBinding) ;\n\n    // Copy the resultSet bindings to the combined result binding with checking.\n    for ( Iterator<Var> iter = b.vars() ; iter.hasNext(); )\n    {\n        Var v = iter.next();\n        Node n = b.get(v) ;\n        if ( b2.contains(v) )\n        {\n            Node n2 = b2.get(v) ;\n            if ( n2.equals(n) )\n                Log.warn(this, \"Binding already for \"+v+\" (same value)\" ) ;\n            else\n            {\n                Log.fatal(this, \"Binding already for \"+v+\" (different values)\" ) ;\n                throw new ARQInternalErrorException(\"Incompatible bindings for \"+v) ;\n            }\n        }\n        b2.add(v, n) ;\n    }\n    return b2 ;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2963,
        "instruction": "public void processTopNJob(String crawlDb, long topN, float min,\n    String output, Configuration config)\n    throws IOException, ClassNotFoundException, InterruptedException {\n\n  if (LOG.isInfoEnabled()) {\n    LOG.info(\"CrawlDb topN: starting (topN=\" + topN + \", min=\" + min + \")\");\n  }\n\n  Path outFolder = new Path(output);\n  Path tempDir = new Path(\n      config.get(\"mapreduce.cluster.temp.dir\", \".\") + \"/readdb-topN-temp-\"\n          + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));\n\n  Job job = NutchJob.getInstance(config);\n  job.setJobName(\"topN prepare \" + crawlDb);\n  FileInputFormat.addInputPath(job, new Path(crawlDb, CrawlDb.CURRENT_NAME));\n  job.setInputFormatClass(SequenceFileInputFormat.class);\n\n  job.setJarByClass(CrawlDbReader.class);\n  job.setMapperClass(CrawlDbTopNMapper.class);\n  job.setReducerClass(Reducer.class);\n\n  FileOutputFormat.setOutputPath(job, tempDir);\n  job.setOutputFormatClass(SequenceFileOutputFormat.class);\n  job.setOutputKeyClass(FloatWritable.class);\n  job.setOutputValueClass(Text.class);\n\n  job.getConfiguration().setFloat(\"db.reader.topn.min\", min);\n\n  FileSystem fs = tempDir.getFileSystem(config);\n  try {\n    boolean success = job.waitForCompletion(true);\n    if (!success) {\n      String message = \"CrawlDbReader job did not succeed, job status:\"\n          + job.getStatus().getState() + \", reason: \"\n          + job.getStatus().getFailureInfo();\n      LOG.error(message);\n      fs.delete(tempDir, true);\n      throw new RuntimeException(message);\n    }\n  } catch (IOException | InterruptedException | ClassNotFoundException e) {\n    LOG.error(StringUtils.stringifyException(e));\n    fs.delete(tempDir, true);\n    throw e;\n  }\n\n  LOG.info(\"CrawlDb topN: collecting topN scores.\");\n  job = NutchJob.getInstance(config);\n  job.setJobName(\"topN collect \" + crawlDb);\n  job.getConfiguration().setLong(\"db.reader.topn\", topN);\n\n  FileInputFormat.addInputPath(job, tempDir);\n  job.setInputFormatClass(SequenceFileInputFormat.class);\n  job.setMapperClass(Mapper.class);\n  job.setReducerClass(CrawlDbTopNReducer.class);\n  job.setJarByClass(CrawlDbReader.class);\n\n  FileOutputFormat.setOutputPath(job, outFolder);\n  job.setOutputFormatClass(TextOutputFormat.class);\n  job.setOutputKeyClass(FloatWritable.class);\n  job.setOutputValueClass(Text.class);\n\n  job.setNumReduceTasks(1); // create a single file.\n\n  try {\n    boolean success = job.waitForCompletion(true);\n    if (!success) {\n      String message = \"CrawlDbReader job did not succeed, job status:\"\n          + job.getStatus().getState() + \", reason: \"\n          + job.getStatus().getFailureInfo();\n      LOG.error(message);\n      fs.delete(tempDir, true);\n      throw new RuntimeException(message);\n    }\n  } catch (IOException | InterruptedException | ClassNotFoundException e) {\n    LOG.error(StringUtils.stringifyException(e));\n    fs.delete(tempDir, true);\n    throw e;\n  }\n\n  fs.delete(tempDir, true);\n  LOG.info(\"CrawlDb topN: done\");\n}",
        "accept_response": "public void processTopNJob(String crawlDb, long topN, float min,\n    String output, Configuration config)\n    throws IOException, ClassNotFoundException, InterruptedException {\n\n  if (LOG.isInfoEnabled()) {\n    LOG.info(\"CrawlDb topN: starting (topN=\" + topN + \", min=\" + min + \")\");\n    LOG.info(\"CrawlDb db: {}\", crawlDb);\n  }\n\n  Path outFolder = new Path(output);\n  Path tempDir = new Path(\n      config.get(\"mapreduce.cluster.temp.dir\", \".\") + \"/readdb-topN-temp-\"\n          + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));\n\n  Job job = NutchJob.getInstance(config);\n  job.setJobName(\"topN prepare \" + crawlDb);\n  FileInputFormat.addInputPath(job, new Path(crawlDb, CrawlDb.CURRENT_NAME));\n  job.setInputFormatClass(SequenceFileInputFormat.class);\n\n  job.setJarByClass(CrawlDbReader.class);\n  job.setMapperClass(CrawlDbTopNMapper.class);\n  job.setReducerClass(Reducer.class);\n\n  FileOutputFormat.setOutputPath(job, tempDir);\n  job.setOutputFormatClass(SequenceFileOutputFormat.class);\n  job.setOutputKeyClass(FloatWritable.class);\n  job.setOutputValueClass(Text.class);\n\n  job.getConfiguration().setFloat(\"db.reader.topn.min\", min);\n\n  FileSystem fs = tempDir.getFileSystem(config);\n  try {\n    boolean success = job.waitForCompletion(true);\n    if (!success) {\n      String message = \"CrawlDbReader job did not succeed, job status:\"\n          + job.getStatus().getState() + \", reason: \"\n          + job.getStatus().getFailureInfo();\n      LOG.error(message);\n      fs.delete(tempDir, true);\n      throw new RuntimeException(message);\n    }\n  } catch (IOException | InterruptedException | ClassNotFoundException e) {\n    LOG.error(StringUtils.stringifyException(e));\n    fs.delete(tempDir, true);\n    throw e;\n  }\n\n  LOG.info(\"CrawlDb topN: collecting topN scores.\");\n  job = NutchJob.getInstance(config);\n  job.setJobName(\"topN collect \" + crawlDb);\n  job.getConfiguration().setLong(\"db.reader.topn\", topN);\n\n  FileInputFormat.addInputPath(job, tempDir);\n  job.setInputFormatClass(SequenceFileInputFormat.class);\n  job.setMapperClass(Mapper.class);\n  job.setReducerClass(CrawlDbTopNReducer.class);\n  job.setJarByClass(CrawlDbReader.class);\n\n  FileOutputFormat.setOutputPath(job, outFolder);\n  job.setOutputFormatClass(TextOutputFormat.class);\n  job.setOutputKeyClass(FloatWritable.class);\n  job.setOutputValueClass(Text.class);\n\n  job.setNumReduceTasks(1); // create a single file.\n\n  try {\n    boolean success = job.waitForCompletion(true);\n    if (!success) {\n      String message = \"CrawlDbReader job did not succeed, job status:\"\n          + job.getStatus().getState() + \", reason: \"\n          + job.getStatus().getFailureInfo();\n      LOG.error(message);\n      fs.delete(tempDir, true);\n      throw new RuntimeException(message);\n    }\n  } catch (IOException | InterruptedException | ClassNotFoundException e) {\n    LOG.error(StringUtils.stringifyException(e));\n    fs.delete(tempDir, true);\n    throw e;\n  }\n\n  fs.delete(tempDir, true);\n  LOG.info(\"CrawlDb topN: done\");\n}",
        "reject_response": "public void processTopNJob(String crawlDb, long topN, float min,\n    String output, Configuration config)\n    throws IOException, ClassNotFoundException, InterruptedException {\n\n  if (LOG.isInfoEnabled()) {\n    LOG.info(\"CrawlDb topN: starting (topN=\" + topN + \", min=\" + min + \")\");\n    LOG.info(\"CrawlDb db: \" + crawlDb);\n  }\n\n  Path outFolder = new Path(output);\n  Path tempDir = new Path(\n      config.get(\"mapreduce.cluster.temp.dir\", \".\") + \"/readdb-topN-temp-\"\n          + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));\n\n  Job job = NutchJob.getInstance(config);\n  job.setJobName(\"topN prepare \" + crawlDb);\n  FileInputFormat.addInputPath(job, new Path(crawlDb, CrawlDb.CURRENT_NAME));\n  job.setInputFormatClass(SequenceFileInputFormat.class);\n\n  job.setJarByClass(CrawlDbReader.class);\n  job.setMapperClass(CrawlDbTopNMapper.class);\n  job.setReducerClass(Reducer.class);\n\n  FileOutputFormat.setOutputPath(job, tempDir);\n  job.setOutputFormatClass(SequenceFileOutputFormat.class);\n  job.setOutputKeyClass(FloatWritable.class);\n  job.setOutputValueClass(Text.class);\n\n  job.getConfiguration().setFloat(\"db.reader.topn.min\", min);\n\n  FileSystem fs = tempDir.getFileSystem(config);\n  try {\n    boolean success = job.waitForCompletion(true);\n    if (!success) {\n      String message = \"CrawlDbReader job did not succeed, job status:\"\n          + job.getStatus().getState() + \", reason: \"\n          + job.getStatus().getFailureInfo();\n      LOG.error(message);\n      fs.delete(tempDir, true);\n      throw new RuntimeException(message);\n    }\n  } catch (IOException | InterruptedException | ClassNotFoundException e) {\n    LOG.error(StringUtils.stringifyException(e));\n    fs.delete(tempDir, true);\n    throw e;\n  }\n\n  LOG.info(\"CrawlDb topN: collecting topN scores.\");\n  job = NutchJob.getInstance(config);\n  job.setJobName(\"topN collect \" + crawlDb);\n  job.getConfiguration().setLong(\"db.reader.topn\", topN);\n\n  FileInputFormat.addInputPath(job, tempDir);\n  job.setInputFormatClass(SequenceFileInputFormat.class);\n  job.setMapperClass(Mapper.class);\n  job.setReducerClass(CrawlDbTopNReducer.class);\n  job.setJarByClass(CrawlDbReader.class);\n\n  FileOutputFormat.setOutputPath(job, outFolder);\n  job.setOutputFormatClass(TextOutputFormat.class);\n  job.setOutputKeyClass(FloatWritable.class);\n  job.setOutputValueClass(Text.class);\n\n  job.setNumReduceTasks(1); // create a single file.\n\n  try {\n    boolean success = job.waitForCompletion(true);\n    if (!success) {\n      String message = \"CrawlDbReader job did not succeed, job status:\"\n          + job.getStatus().getState() + \", reason: \"\n          + job.getStatus().getFailureInfo();\n      LOG.error(message);\n      fs.delete(tempDir, true);\n      throw new RuntimeException(message);\n    }\n  } catch (IOException | InterruptedException | ClassNotFoundException e) {\n    LOG.error(StringUtils.stringifyException(e));\n    fs.delete(tempDir, true);\n    throw e;\n  }\n\n  fs.delete(tempDir, true);\n  LOG.info(\"CrawlDb topN: done\");\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2661,
        "instruction": "private void configureClientSSLSocket(Socket socket, int timeout) throws IOException {\n  if (socket instanceof SSLSocket) {\n    SSLSocket sslSocket = (SSLSocket) socket;\n\n    sslSocket.setUseClientMode(true);\n    sslSocket.setEnableSessionCreation(true);\n\n    if (sslConfig.doEndpointIdentification()) {\n      SSLParameters sslParameters = sslSocket.getSSLParameters();\n      sslParameters.setEndpointIdentificationAlgorithm(\"HTTPS\");\n      sslSocket.setSSLParameters(sslParameters);\n    } else {\n      if (!hostnameValidationDisabledLogShown) {\n        hostnameValidationDisabledLogShown = true;\n      }\n    }\n\n    String[] protocols = this.sslConfig.getProtocolsAsStringArray();\n\n    // restrict cyphers\n    if (protocols != null && !\"any\".equalsIgnoreCase(protocols[0])) {\n      sslSocket.setEnabledProtocols(protocols);\n    }\n    String[] ciphers = this.sslConfig.getCiphersAsStringArray();\n    if (ciphers != null && !\"any\".equalsIgnoreCase(ciphers[0])) {\n      sslSocket.setEnabledCipherSuites(ciphers);\n    }\n\n    try {\n      if (timeout > 0) {\n        sslSocket.setSoTimeout(timeout);\n      }\n      sslSocket.startHandshake();\n    }\n    // Pre jkd11, startHandshake is throwing SocketTimeoutException.\n    // in jdk 11 it is throwing SSLProtocolException with a cause of SocketTimeoutException.\n    // this is to keep the exception consistent across jdk\n    catch (SSLProtocolException ex) {\n      if (ex.getCause() instanceof SocketTimeoutException) {\n        throw (SocketTimeoutException) ex.getCause();\n      } else {\n        throw ex;\n      }\n    } catch (SSLHandshakeException ex) {\n      logger.fatal(String.format(\"Problem forming SSL connection to %s[%s].\",\n          socket.getInetAddress(), socket.getPort()), ex);\n      throw ex;\n    } catch (SSLPeerUnverifiedException ex) {\n      if (this.sslConfig.isRequireAuth()) {\n        logger.fatal(\"SSL authentication exception.\", ex);\n        throw ex;\n      }\n    }\n  }\n}",
        "accept_response": "private void configureClientSSLSocket(Socket socket, int timeout) throws IOException {\n  if (socket instanceof SSLSocket) {\n    SSLSocket sslSocket = (SSLSocket) socket;\n\n    sslSocket.setUseClientMode(true);\n    sslSocket.setEnableSessionCreation(true);\n\n    if (sslConfig.doEndpointIdentification()) {\n      SSLParameters sslParameters = sslSocket.getSSLParameters();\n      sslParameters.setEndpointIdentificationAlgorithm(\"HTTPS\");\n      sslSocket.setSSLParameters(sslParameters);\n    } else {\n      if (!hostnameValidationDisabledLogShown) {\n        logger.info(\"Your SSL configuration disables hostname validation. \"\n            + \"ssl-endpoint-identification-enabled should be set to true when SSL is enabled. \"\n            + \"Please refer to the Apache GEODE SSL Documentation for SSL Property: ssl\u2011endpoint\u2011identification\u2011enabled\");\n        hostnameValidationDisabledLogShown = true;\n      }\n    }\n\n    String[] protocols = this.sslConfig.getProtocolsAsStringArray();\n\n    // restrict cyphers\n    if (protocols != null && !\"any\".equalsIgnoreCase(protocols[0])) {\n      sslSocket.setEnabledProtocols(protocols);\n    }\n    String[] ciphers = this.sslConfig.getCiphersAsStringArray();\n    if (ciphers != null && !\"any\".equalsIgnoreCase(ciphers[0])) {\n      sslSocket.setEnabledCipherSuites(ciphers);\n    }\n\n    try {\n      if (timeout > 0) {\n        sslSocket.setSoTimeout(timeout);\n      }\n      sslSocket.startHandshake();\n    }\n    // Pre jkd11, startHandshake is throwing SocketTimeoutException.\n    // in jdk 11 it is throwing SSLProtocolException with a cause of SocketTimeoutException.\n    // this is to keep the exception consistent across jdk\n    catch (SSLProtocolException ex) {\n      if (ex.getCause() instanceof SocketTimeoutException) {\n        throw (SocketTimeoutException) ex.getCause();\n      } else {\n        throw ex;\n      }\n    } catch (SSLHandshakeException ex) {\n      logger.fatal(String.format(\"Problem forming SSL connection to %s[%s].\",\n          socket.getInetAddress(), socket.getPort()), ex);\n      throw ex;\n    } catch (SSLPeerUnverifiedException ex) {\n      if (this.sslConfig.isRequireAuth()) {\n        logger.fatal(\"SSL authentication exception.\", ex);\n        throw ex;\n      }\n    }\n  }\n}",
        "reject_response": "private void configureClientSSLSocket(Socket socket, int timeout) throws IOException {\n  if (socket instanceof SSLSocket) {\n    SSLSocket sslSocket = (SSLSocket) socket;\n\n    sslSocket.setUseClientMode(true);\n    sslSocket.setEnableSessionCreation(true);\n\n    if (sslConfig.doEndpointIdentification()) {\n      SSLParameters sslParameters = sslSocket.getSSLParameters();\n      sslParameters.setEndpointIdentificationAlgorithm(\"HTTPS\");\n      sslSocket.setSSLParameters(sslParameters);\n    } else {\n      if (!hostnameValidationDisabledLogShown) {\n      logger.warn(\"Your SSL configuration disables hostname validation. \"\n          + \"ssl-endpoint-identification-enabled should be set to true when SSL is enabled. \"\n          + \"Please refer to the Apache GEODE SSL Documentation for SSL Property: ssl\u2011endpoint\u2011identification\u2011enabled\");\n        hostnameValidationDisabledLogShown = true;\n      }\n    }\n\n    String[] protocols = this.sslConfig.getProtocolsAsStringArray();\n\n    // restrict cyphers\n    if (protocols != null && !\"any\".equalsIgnoreCase(protocols[0])) {\n      sslSocket.setEnabledProtocols(protocols);\n    }\n    String[] ciphers = this.sslConfig.getCiphersAsStringArray();\n    if (ciphers != null && !\"any\".equalsIgnoreCase(ciphers[0])) {\n      sslSocket.setEnabledCipherSuites(ciphers);\n    }\n\n    try {\n      if (timeout > 0) {\n        sslSocket.setSoTimeout(timeout);\n      }\n      sslSocket.startHandshake();\n    }\n    // Pre jkd11, startHandshake is throwing SocketTimeoutException.\n    // in jdk 11 it is throwing SSLProtocolException with a cause of SocketTimeoutException.\n    // this is to keep the exception consistent across jdk\n    catch (SSLProtocolException ex) {\n      if (ex.getCause() instanceof SocketTimeoutException) {\n        throw (SocketTimeoutException) ex.getCause();\n      } else {\n        throw ex;\n      }\n    } catch (SSLHandshakeException ex) {\n      logger.fatal(String.format(\"Problem forming SSL connection to %s[%s].\",\n          socket.getInetAddress(), socket.getPort()), ex);\n      throw ex;\n    } catch (SSLPeerUnverifiedException ex) {\n      if (this.sslConfig.isRequireAuth()) {\n        logger.fatal(\"SSL authentication exception.\", ex);\n        throw ex;\n      }\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3115,
        "instruction": "@Override\npublic boolean isValid(int timeout) throws SQLException {\n  try {\n    if (tajoClient.isConnected()) {\n      ResultSet resultSet = tajoClient.executeQueryAndGetResult(\"SELECT 1;\");\n      boolean next = resultSet.next();\n      boolean valid = next && resultSet.getLong(1) == 1;\n      resultSet.close();\n      return valid;\n    } else {\n      return false;\n    }\n  } catch (ServiceException e) {\n    return false;\n  } catch (IOException e) {\n    LOG.error(\"JDBC connection is not valid.\", e);\n    return false;\n  }\n}",
        "accept_response": "@Override\npublic boolean isValid(int timeout) throws SQLException {\n  try {\n    if (tajoClient.isConnected()) {\n      ResultSet resultSet = tajoClient.executeQueryAndGetResult(\"SELECT 1;\");\n      boolean next = resultSet.next();\n      boolean valid = next && resultSet.getLong(1) == 1;\n      resultSet.close();\n      return valid;\n    } else {\n      return false;\n    }\n  } catch (ServiceException e) {\n    LOG.error(\"TajoMaster is not available.\", e);\n    return false;\n  } catch (IOException e) {\n    LOG.error(\"JDBC connection is not valid.\", e);\n    return false;\n  }\n}",
        "reject_response": "@Override\npublic boolean isValid(int timeout) throws SQLException {\n  try {\n    if (tajoClient.isConnected()) {\n      ResultSet resultSet = tajoClient.executeQueryAndGetResult(\"SELECT 1;\");\n      boolean next = resultSet.next();\n      boolean valid = next && resultSet.getLong(1) == 1;\n      resultSet.close();\n      return valid;\n    } else {\n      return false;\n    }\n  } catch (ServiceException e) {\n    LOG.error(\"TajoMaster is not available.\");\n    return false;\n  } catch (IOException e) {\n    LOG.error(\"JDBC connection is not valid.\", e);\n    return false;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2519,
        "instruction": "public void run()\n{\n    TokenMetadata metadata = StorageService.instance.getTokenMetadata().cloneOnlyTokenMap();\n    if (!metadata.isMember(FBUtilities.getBroadcastAddressAndPort()))\n    {\n        logger.debug(\"Node is not part of the ring; not recording size estimates\");\n        return;\n    }\n\n    logger.trace(\"Recording size estimates\");\n\n    for (Keyspace keyspace : Keyspace.nonLocalStrategy())\n    {\n        Collection<Range<Token>> localRanges = StorageService.instance.getPrimaryRangesForEndpoint(keyspace.getName(),\n                FBUtilities.getBroadcastAddressAndPort());\n        for (ColumnFamilyStore table : keyspace.getColumnFamilyStores())\n        {\n            long start = System.nanoTime();\n            recordSizeEstimates(table, localRanges);\n            long passed = System.nanoTime() - start;\n            if (logger.isTraceEnabled())\n        }\n    }\n}",
        "accept_response": "public void run()\n{\n    TokenMetadata metadata = StorageService.instance.getTokenMetadata().cloneOnlyTokenMap();\n    if (!metadata.isMember(FBUtilities.getBroadcastAddressAndPort()))\n    {\n        logger.debug(\"Node is not part of the ring; not recording size estimates\");\n        return;\n    }\n\n    logger.trace(\"Recording size estimates\");\n\n    for (Keyspace keyspace : Keyspace.nonLocalStrategy())\n    {\n        Collection<Range<Token>> localRanges = StorageService.instance.getPrimaryRangesForEndpoint(keyspace.getName(),\n                FBUtilities.getBroadcastAddressAndPort());\n        for (ColumnFamilyStore table : keyspace.getColumnFamilyStores())\n        {\n            long start = System.nanoTime();\n            recordSizeEstimates(table, localRanges);\n            long passed = System.nanoTime() - start;\n            if (logger.isTraceEnabled())\n            \tlogger.trace(\"Spent {} milliseconds on estimating {}.{} size\",\n                         TimeUnit.NANOSECONDS.toMillis(passed),\n                         table.metadata.keyspace,\n                         table.metadata.name);\n        }\n    }\n}",
        "reject_response": "public void run()\n{\n    TokenMetadata metadata = StorageService.instance.getTokenMetadata().cloneOnlyTokenMap();\n    if (!metadata.isMember(FBUtilities.getBroadcastAddressAndPort()))\n    {\n        logger.debug(\"Node is not part of the ring; not recording size estimates\");\n        return;\n    }\n\n    logger.trace(\"Recording size estimates\");\n\n    for (Keyspace keyspace : Keyspace.nonLocalStrategy())\n    {\n        Collection<Range<Token>> localRanges = StorageService.instance.getPrimaryRangesForEndpoint(keyspace.getName(),\n                FBUtilities.getBroadcastAddressAndPort());\n        for (ColumnFamilyStore table : keyspace.getColumnFamilyStores())\n        {\n            long start = System.nanoTime();\n            recordSizeEstimates(table, localRanges);\n            long passed = System.nanoTime() - start;\n            if (logger.isTraceEnabled())\n            logger.trace(\"Spent {} milliseconds on estimating {}.{} size\",\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3167,
        "instruction": "private static void trySetXercesSecurityManager(DocumentBuilderFactory dbf, XmlOptionsBean options) {\n    // Try built-in JVM one first, standalone if not\n    for (String securityManagerClassName : new String[]{\n            //\"com.sun.org.apache.xerces.internal.util.SecurityManager\",\n            \"org.apache.xerces.util.SecurityManager\"\n    }) {\n        try {\n            Object mgr = Class.forName(securityManagerClassName).getDeclaredConstructor().newInstance();\n            Method setLimit = mgr.getClass().getMethod(\"setEntityExpansionLimit\", Integer.TYPE);\n            setLimit.invoke(mgr, options.getEntityExpansionLimit());\n            dbf.setAttribute(XMLBeansConstants.SECURITY_MANAGER, mgr);\n            // Stop once one can be setup without error\n            return;\n        } catch (ClassNotFoundException e) {\n            // continue without log, this is expected in some setups\n        } catch (Throwable e) {     // NOSONAR - also catch things like NoClassDefError here\n            if(System.currentTimeMillis() > lastLog + TimeUnit.MINUTES.toMillis(5)) {\n                logger.warn(\"DocumentBuilderFactory Security Manager could not be setup [log suppressed for 5 minutes]\", e);\n                lastLog = System.currentTimeMillis();\n            }\n        }\n    }\n\n    // separate old version of Xerces not found => use the builtin way of setting the property\n    try {\n        dbf.setAttribute(XMLBeansConstants.ENTITY_EXPANSION_LIMIT, options.getEntityExpansionLimit());\n    } catch (Throwable e) {\n        if(System.currentTimeMillis() > lastLog + TimeUnit.MINUTES.toMillis(5)) {\n            lastLog = System.currentTimeMillis();\n        }\n    }\n}",
        "accept_response": "private static void trySetXercesSecurityManager(DocumentBuilderFactory dbf, XmlOptionsBean options) {\n    // Try built-in JVM one first, standalone if not\n    for (String securityManagerClassName : new String[]{\n            //\"com.sun.org.apache.xerces.internal.util.SecurityManager\",\n            \"org.apache.xerces.util.SecurityManager\"\n    }) {\n        try {\n            Object mgr = Class.forName(securityManagerClassName).getDeclaredConstructor().newInstance();\n            Method setLimit = mgr.getClass().getMethod(\"setEntityExpansionLimit\", Integer.TYPE);\n            setLimit.invoke(mgr, options.getEntityExpansionLimit());\n            dbf.setAttribute(XMLBeansConstants.SECURITY_MANAGER, mgr);\n            // Stop once one can be setup without error\n            return;\n        } catch (ClassNotFoundException e) {\n            // continue without log, this is expected in some setups\n        } catch (Throwable e) {     // NOSONAR - also catch things like NoClassDefError here\n            if(System.currentTimeMillis() > lastLog + TimeUnit.MINUTES.toMillis(5)) {\n                logger.warn(\"DocumentBuilderFactory Security Manager could not be setup [log suppressed for 5 minutes]\", e);\n                lastLog = System.currentTimeMillis();\n            }\n        }\n    }\n\n    // separate old version of Xerces not found => use the builtin way of setting the property\n    try {\n        dbf.setAttribute(XMLBeansConstants.ENTITY_EXPANSION_LIMIT, options.getEntityExpansionLimit());\n    } catch (Throwable e) {\n        if(System.currentTimeMillis() > lastLog + TimeUnit.MINUTES.toMillis(5)) {\n            logger.warn(\"DocumentBuilderFactory Entity Expansion Limit could not be setup [log suppressed for 5 minutes]\", e);\n            lastLog = System.currentTimeMillis();\n        }\n    }\n}",
        "reject_response": "private static void trySetXercesSecurityManager(DocumentBuilderFactory dbf, XmlOptionsBean options) {\n    // Try built-in JVM one first, standalone if not\n    for (String securityManagerClassName : new String[]{\n            //\"com.sun.org.apache.xerces.internal.util.SecurityManager\",\n            \"org.apache.xerces.util.SecurityManager\"\n    }) {\n        try {\n            Object mgr = Class.forName(securityManagerClassName).getDeclaredConstructor().newInstance();\n            Method setLimit = mgr.getClass().getMethod(\"setEntityExpansionLimit\", Integer.TYPE);\n            setLimit.invoke(mgr, options.getEntityExpansionLimit());\n            dbf.setAttribute(XMLBeansConstants.SECURITY_MANAGER, mgr);\n            // Stop once one can be setup without error\n            return;\n        } catch (ClassNotFoundException e) {\n            // continue without log, this is expected in some setups\n        } catch (Throwable e) {     // NOSONAR - also catch things like NoClassDefError here\n            if(System.currentTimeMillis() > lastLog + TimeUnit.MINUTES.toMillis(5)) {\n                logger.warn(\"DocumentBuilderFactory Security Manager could not be setup [log suppressed for 5 minutes]\", e);\n                lastLog = System.currentTimeMillis();\n            }\n        }\n    }\n\n    // separate old version of Xerces not found => use the builtin way of setting the property\n    try {\n        dbf.setAttribute(XMLBeansConstants.ENTITY_EXPANSION_LIMIT, options.getEntityExpansionLimit());\n    } catch (Throwable e) {\n        if(System.currentTimeMillis() > lastLog + TimeUnit.MINUTES.toMillis(5)) {\n            logger.log(XBLogger.WARN, \"DocumentBuilderFactory Entity Expansion Limit could not be setup [log suppressed for 5 minutes]\", e);\n            lastLog = System.currentTimeMillis();\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2578,
        "instruction": "@Override\npublic void runTheCheck(final Long entityId, final String entityName, final Long statusCode, String foreignKeyColumn) {\n    final List<EntityDatatableChecks> tableRequiredBeforeClientActivation = entityDatatableChecksRepository.findByEntityAndStatus(\n            entityName, statusCode);\n\n    if (tableRequiredBeforeClientActivation != null) {\n        List<String> reqDatatables = new ArrayList<>();\n        for (EntityDatatableChecks t : tableRequiredBeforeClientActivation) {\n\n            final String datatableName = t.getDatatableName();\n            final Long countEntries = readWriteNonCoreDataService.countDatatableEntries(datatableName, entityId, foreignKeyColumn);\n\n            if (countEntries.intValue() == 0) {\n                reqDatatables.add(datatableName);\n            }\n        }\n        if (reqDatatables.size() > 0) { throw new DatatableEntryRequiredException(reqDatatables.toString()); }\n    }\n\n}",
        "accept_response": "@Override\npublic void runTheCheck(final Long entityId, final String entityName, final Long statusCode, String foreignKeyColumn) {\n    final List<EntityDatatableChecks> tableRequiredBeforeClientActivation = entityDatatableChecksRepository.findByEntityAndStatus(\n            entityName, statusCode);\n\n    if (tableRequiredBeforeClientActivation != null) {\n        List<String> reqDatatables = new ArrayList<>();\n        for (EntityDatatableChecks t : tableRequiredBeforeClientActivation) {\n\n            final String datatableName = t.getDatatableName();\n            final Long countEntries = readWriteNonCoreDataService.countDatatableEntries(datatableName, entityId, foreignKeyColumn);\n\n            logger.info(\"The are {} entries in the table {}\", countEntries, datatableName);\n            if (countEntries.intValue() == 0) {\n                reqDatatables.add(datatableName);\n            }\n        }\n        if (reqDatatables.size() > 0) { throw new DatatableEntryRequiredException(reqDatatables.toString()); }\n    }\n\n}",
        "reject_response": "@Override\npublic void runTheCheck(final Long entityId, final String entityName, final Long statusCode, String foreignKeyColumn) {\n    final List<EntityDatatableChecks> tableRequiredBeforeClientActivation = entityDatatableChecksRepository.findByEntityAndStatus(\n            entityName, statusCode);\n\n    if (tableRequiredBeforeClientActivation != null) {\n        List<String> reqDatatables = new ArrayList<>();\n        for (EntityDatatableChecks t : tableRequiredBeforeClientActivation) {\n\n            final String datatableName = t.getDatatableName();\n            final Long countEntries = readWriteNonCoreDataService.countDatatableEntries(datatableName, entityId, foreignKeyColumn);\n\n            logger.info(\"The are \" + countEntries + \" entries in the table \" + datatableName);\n            if (countEntries.intValue() == 0) {\n                reqDatatables.add(datatableName);\n            }\n        }\n        if (reqDatatables.size() > 0) { throw new DatatableEntryRequiredException(reqDatatables.toString()); }\n    }\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3026,
        "instruction": "private void submitBatchQuery() {\n    int count = 0;\n    executedRangeMap.clear();\n    final List<Document> pipeline = new ArrayList<>();\n    final List<DBObject> match = new ArrayList<>();\n\n    while (queryIterator.hasNext() && count < QUERY_BATCH_SIZE){\n        count++;\n        final RyaStatement query = queryIterator.next();\n        executedRangeMap.putAll(query, rangeMap.get(query));\n        final DBObject currentQuery = strategy.getQuery(query);\n        match.add(currentQuery);\n    }\n\n    if (match.size() > 1) {\n        pipeline.add(new Document(\"$match\", new Document(\"$or\", match)));\n    } else if (match.size() == 1) {\n        pipeline.add(new Document(\"$match\", match.get(0)));\n    } else {\n        batchQueryResultsIterator = Iterators.emptyIterator();\n        return;\n    }\n\n    // Executing redact aggregation to only return documents the user has access to.\n    pipeline.addAll(AggregationUtil.createRedactPipeline(auths));\n\n    final AggregateIterable<Document> aggIter = coll.aggregate(pipeline);\n    aggIter.batchSize(1000);\n    batchQueryResultsIterator = aggIter.iterator();\n}",
        "accept_response": "private void submitBatchQuery() {\n    int count = 0;\n    executedRangeMap.clear();\n    final List<Document> pipeline = new ArrayList<>();\n    final List<DBObject> match = new ArrayList<>();\n\n    while (queryIterator.hasNext() && count < QUERY_BATCH_SIZE){\n        count++;\n        final RyaStatement query = queryIterator.next();\n        executedRangeMap.putAll(query, rangeMap.get(query));\n        final DBObject currentQuery = strategy.getQuery(query);\n        match.add(currentQuery);\n    }\n\n    if (match.size() > 1) {\n        pipeline.add(new Document(\"$match\", new Document(\"$or\", match)));\n    } else if (match.size() == 1) {\n        pipeline.add(new Document(\"$match\", match.get(0)));\n    } else {\n        batchQueryResultsIterator = Iterators.emptyIterator();\n        return;\n    }\n\n    // Executing redact aggregation to only return documents the user has access to.\n    pipeline.addAll(AggregationUtil.createRedactPipeline(auths));\n    log.trace(pipeline);\n\n    final AggregateIterable<Document> aggIter = coll.aggregate(pipeline);\n    aggIter.batchSize(1000);\n    batchQueryResultsIterator = aggIter.iterator();\n}",
        "reject_response": "private void submitBatchQuery() {\n    int count = 0;\n    executedRangeMap.clear();\n    final List<Document> pipeline = new ArrayList<>();\n    final List<DBObject> match = new ArrayList<>();\n\n    while (queryIterator.hasNext() && count < QUERY_BATCH_SIZE){\n        count++;\n        final RyaStatement query = queryIterator.next();\n        executedRangeMap.putAll(query, rangeMap.get(query));\n        final DBObject currentQuery = strategy.getQuery(query);\n        match.add(currentQuery);\n    }\n\n    if (match.size() > 1) {\n        pipeline.add(new Document(\"$match\", new Document(\"$or\", match)));\n    } else if (match.size() == 1) {\n        pipeline.add(new Document(\"$match\", match.get(0)));\n    } else {\n        batchQueryResultsIterator = Iterators.emptyIterator();\n        return;\n    }\n\n    // Executing redact aggregation to only return documents the user has access to.\n    pipeline.addAll(AggregationUtil.createRedactPipeline(auths));\n    log.info(pipeline);\n\n    final AggregateIterable<Document> aggIter = coll.aggregate(pipeline);\n    aggIter.batchSize(1000);\n    batchQueryResultsIterator = aggIter.iterator();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3109,
        "instruction": "public static List<FunctionDesc> findLegacyFunctions() {\n  List<FunctionDesc> sqlFuncs = new ArrayList<FunctionDesc>();\n\n  Set<Class> functionClasses = ClassUtil.findClasses(Function.class, \"org.apache.tajo.engine.function\");\n\n  for (Class eachClass : functionClasses) {\n    if(eachClass.isInterface() || Modifier.isAbstract(eachClass.getModifiers())) {\n      continue;\n    }\n    Function function = null;\n    try {\n      function = (Function)eachClass.newInstance();\n    } catch (Exception e) {\n      continue;\n    }\n    String functionName = function.getClass().getAnnotation(Description.class).functionName();\n    String[] synonyms = function.getClass().getAnnotation(Description.class).synonyms();\n    String description = function.getClass().getAnnotation(Description.class).description();\n    String detail = function.getClass().getAnnotation(Description.class).detail();\n    String example = function.getClass().getAnnotation(Description.class).example();\n    TajoDataTypes.Type returnType = function.getClass().getAnnotation(Description.class).returnType();\n    ParamTypes[] paramArray = function.getClass().getAnnotation(Description.class).paramTypes();\n\n    String[] allFunctionNames = null;\n    if(synonyms != null && synonyms.length > 0) {\n      allFunctionNames = new String[1 + synonyms.length];\n      allFunctionNames[0] = functionName;\n      System.arraycopy(synonyms, 0, allFunctionNames, 1, synonyms.length);\n    } else {\n      allFunctionNames = new String[]{functionName};\n    }\n\n    for(String eachFunctionName: allFunctionNames) {\n      for (ParamTypes params : paramArray) {\n        ParamOptionTypes[] paramOptionArray;\n        if(params.paramOptionTypes() == null ||\n            params.paramOptionTypes().getClass().getAnnotation(ParamTypes.class) == null) {\n          paramOptionArray = new ParamOptionTypes[0];\n        } else {\n          paramOptionArray = params.paramOptionTypes().getClass().getAnnotation(ParamTypes.class).paramOptionTypes();\n        }\n\n        TajoDataTypes.Type[] paramTypes = params.paramTypes();\n        if (paramOptionArray.length > 0)\n          paramTypes = params.paramTypes().clone();\n\n        for (int i=0; i < paramOptionArray.length + 1; i++) {\n          FunctionDesc functionDesc = new FunctionDesc(eachFunctionName,\n              function.getClass(), function.getFunctionType(),\n              CatalogUtil.newSimpleDataType(returnType),\n              paramTypes.length == 0 ? CatalogUtil.newSimpleDataTypeArray() : CatalogUtil.newSimpleDataTypeArray(paramTypes));\n\n          functionDesc.setDescription(description);\n          functionDesc.setExample(example);\n          functionDesc.setDetail(detail);\n          sqlFuncs.add(functionDesc);\n\n          if (i != paramOptionArray.length) {\n            paramTypes = new TajoDataTypes.Type[paramTypes.length +\n                paramOptionArray[i].paramOptionTypes().length];\n            System.arraycopy(params.paramTypes(), 0, paramTypes, 0, paramTypes.length);\n            System.arraycopy(paramOptionArray[i].paramOptionTypes(), 0, paramTypes, paramTypes.length,\n                paramOptionArray[i].paramOptionTypes().length);\n          }\n        }\n      }\n    }\n  }\n\n  return sqlFuncs;\n}",
        "accept_response": "public static List<FunctionDesc> findLegacyFunctions() {\n  List<FunctionDesc> sqlFuncs = new ArrayList<FunctionDesc>();\n\n  Set<Class> functionClasses = ClassUtil.findClasses(Function.class, \"org.apache.tajo.engine.function\");\n\n  for (Class eachClass : functionClasses) {\n    if(eachClass.isInterface() || Modifier.isAbstract(eachClass.getModifiers())) {\n      continue;\n    }\n    Function function = null;\n    try {\n      function = (Function)eachClass.newInstance();\n    } catch (Exception e) {\n      LOG.warn(eachClass + \" cannot instantiate Function class because of \" + e.getMessage(), e);\n      continue;\n    }\n    String functionName = function.getClass().getAnnotation(Description.class).functionName();\n    String[] synonyms = function.getClass().getAnnotation(Description.class).synonyms();\n    String description = function.getClass().getAnnotation(Description.class).description();\n    String detail = function.getClass().getAnnotation(Description.class).detail();\n    String example = function.getClass().getAnnotation(Description.class).example();\n    TajoDataTypes.Type returnType = function.getClass().getAnnotation(Description.class).returnType();\n    ParamTypes[] paramArray = function.getClass().getAnnotation(Description.class).paramTypes();\n\n    String[] allFunctionNames = null;\n    if(synonyms != null && synonyms.length > 0) {\n      allFunctionNames = new String[1 + synonyms.length];\n      allFunctionNames[0] = functionName;\n      System.arraycopy(synonyms, 0, allFunctionNames, 1, synonyms.length);\n    } else {\n      allFunctionNames = new String[]{functionName};\n    }\n\n    for(String eachFunctionName: allFunctionNames) {\n      for (ParamTypes params : paramArray) {\n        ParamOptionTypes[] paramOptionArray;\n        if(params.paramOptionTypes() == null ||\n            params.paramOptionTypes().getClass().getAnnotation(ParamTypes.class) == null) {\n          paramOptionArray = new ParamOptionTypes[0];\n        } else {\n          paramOptionArray = params.paramOptionTypes().getClass().getAnnotation(ParamTypes.class).paramOptionTypes();\n        }\n\n        TajoDataTypes.Type[] paramTypes = params.paramTypes();\n        if (paramOptionArray.length > 0)\n          paramTypes = params.paramTypes().clone();\n\n        for (int i=0; i < paramOptionArray.length + 1; i++) {\n          FunctionDesc functionDesc = new FunctionDesc(eachFunctionName,\n              function.getClass(), function.getFunctionType(),\n              CatalogUtil.newSimpleDataType(returnType),\n              paramTypes.length == 0 ? CatalogUtil.newSimpleDataTypeArray() : CatalogUtil.newSimpleDataTypeArray(paramTypes));\n\n          functionDesc.setDescription(description);\n          functionDesc.setExample(example);\n          functionDesc.setDetail(detail);\n          sqlFuncs.add(functionDesc);\n\n          if (i != paramOptionArray.length) {\n            paramTypes = new TajoDataTypes.Type[paramTypes.length +\n                paramOptionArray[i].paramOptionTypes().length];\n            System.arraycopy(params.paramTypes(), 0, paramTypes, 0, paramTypes.length);\n            System.arraycopy(paramOptionArray[i].paramOptionTypes(), 0, paramTypes, paramTypes.length,\n                paramOptionArray[i].paramOptionTypes().length);\n          }\n        }\n      }\n    }\n  }\n\n  return sqlFuncs;\n}",
        "reject_response": "public static List<FunctionDesc> findLegacyFunctions() {\n  List<FunctionDesc> sqlFuncs = new ArrayList<FunctionDesc>();\n\n  Set<Class> functionClasses = ClassUtil.findClasses(Function.class, \"org.apache.tajo.engine.function\");\n\n  for (Class eachClass : functionClasses) {\n    if(eachClass.isInterface() || Modifier.isAbstract(eachClass.getModifiers())) {\n      continue;\n    }\n    Function function = null;\n    try {\n      function = (Function)eachClass.newInstance();\n    } catch (Exception e) {\n      LOG.warn(eachClass + \" cannot instantiate Function class because of \" + e.getMessage());\n      continue;\n    }\n    String functionName = function.getClass().getAnnotation(Description.class).functionName();\n    String[] synonyms = function.getClass().getAnnotation(Description.class).synonyms();\n    String description = function.getClass().getAnnotation(Description.class).description();\n    String detail = function.getClass().getAnnotation(Description.class).detail();\n    String example = function.getClass().getAnnotation(Description.class).example();\n    TajoDataTypes.Type returnType = function.getClass().getAnnotation(Description.class).returnType();\n    ParamTypes[] paramArray = function.getClass().getAnnotation(Description.class).paramTypes();\n\n    String[] allFunctionNames = null;\n    if(synonyms != null && synonyms.length > 0) {\n      allFunctionNames = new String[1 + synonyms.length];\n      allFunctionNames[0] = functionName;\n      System.arraycopy(synonyms, 0, allFunctionNames, 1, synonyms.length);\n    } else {\n      allFunctionNames = new String[]{functionName};\n    }\n\n    for(String eachFunctionName: allFunctionNames) {\n      for (ParamTypes params : paramArray) {\n        ParamOptionTypes[] paramOptionArray;\n        if(params.paramOptionTypes() == null ||\n            params.paramOptionTypes().getClass().getAnnotation(ParamTypes.class) == null) {\n          paramOptionArray = new ParamOptionTypes[0];\n        } else {\n          paramOptionArray = params.paramOptionTypes().getClass().getAnnotation(ParamTypes.class).paramOptionTypes();\n        }\n\n        TajoDataTypes.Type[] paramTypes = params.paramTypes();\n        if (paramOptionArray.length > 0)\n          paramTypes = params.paramTypes().clone();\n\n        for (int i=0; i < paramOptionArray.length + 1; i++) {\n          FunctionDesc functionDesc = new FunctionDesc(eachFunctionName,\n              function.getClass(), function.getFunctionType(),\n              CatalogUtil.newSimpleDataType(returnType),\n              paramTypes.length == 0 ? CatalogUtil.newSimpleDataTypeArray() : CatalogUtil.newSimpleDataTypeArray(paramTypes));\n\n          functionDesc.setDescription(description);\n          functionDesc.setExample(example);\n          functionDesc.setDetail(detail);\n          sqlFuncs.add(functionDesc);\n\n          if (i != paramOptionArray.length) {\n            paramTypes = new TajoDataTypes.Type[paramTypes.length +\n                paramOptionArray[i].paramOptionTypes().length];\n            System.arraycopy(params.paramTypes(), 0, paramTypes, 0, paramTypes.length);\n            System.arraycopy(paramOptionArray[i].paramOptionTypes(), 0, paramTypes, paramTypes.length,\n                paramOptionArray[i].paramOptionTypes().length);\n          }\n        }\n      }\n    }\n  }\n\n  return sqlFuncs;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3103,
        "instruction": "@Override\npublic final Collection<FunctionDesc> getFunctions() {\n  try {\n    return new ServerCallable<Collection<FunctionDesc>>(this.pool, getCatalogServerAddr(), CatalogProtocol.class, false) {\n      public Collection<FunctionDesc> call(NettyClientBase client) throws ServiceException {\n        List<FunctionDesc> list = new ArrayList<FunctionDesc>();\n        GetFunctionsResponse response;\n        CatalogProtocolService.BlockingInterface stub = getStub(client);\n        response = stub.getFunctions(null, NullProto.newBuilder().build());\n        int size = response.getFunctionDescCount();\n        for (int i = 0; i < size; i++) {\n          try {\n            list.add(new FunctionDesc(response.getFunctionDesc(i)));\n          } catch (ClassNotFoundException e) {\n            return null;\n          }\n        }\n        return list;\n      }\n    }.withRetries();\n  } catch (ServiceException e) {\n    LOG.error(e.getMessage(), e);\n    return null;\n  }\n}",
        "accept_response": "@Override\npublic final Collection<FunctionDesc> getFunctions() {\n  try {\n    return new ServerCallable<Collection<FunctionDesc>>(this.pool, getCatalogServerAddr(), CatalogProtocol.class, false) {\n      public Collection<FunctionDesc> call(NettyClientBase client) throws ServiceException {\n        List<FunctionDesc> list = new ArrayList<FunctionDesc>();\n        GetFunctionsResponse response;\n        CatalogProtocolService.BlockingInterface stub = getStub(client);\n        response = stub.getFunctions(null, NullProto.newBuilder().build());\n        int size = response.getFunctionDescCount();\n        for (int i = 0; i < size; i++) {\n          try {\n            list.add(new FunctionDesc(response.getFunctionDesc(i)));\n          } catch (ClassNotFoundException e) {\n            LOG.error(e, e);\n            return null;\n          }\n        }\n        return list;\n      }\n    }.withRetries();\n  } catch (ServiceException e) {\n    LOG.error(e.getMessage(), e);\n    return null;\n  }\n}",
        "reject_response": "@Override\npublic final Collection<FunctionDesc> getFunctions() {\n  try {\n    return new ServerCallable<Collection<FunctionDesc>>(this.pool, getCatalogServerAddr(), CatalogProtocol.class, false) {\n      public Collection<FunctionDesc> call(NettyClientBase client) throws ServiceException {\n        List<FunctionDesc> list = new ArrayList<FunctionDesc>();\n        GetFunctionsResponse response;\n        CatalogProtocolService.BlockingInterface stub = getStub(client);\n        response = stub.getFunctions(null, NullProto.newBuilder().build());\n        int size = response.getFunctionDescCount();\n        for (int i = 0; i < size; i++) {\n          try {\n            list.add(new FunctionDesc(response.getFunctionDesc(i)));\n          } catch (ClassNotFoundException e) {\n            LOG.error(e);\n            return null;\n          }\n        }\n        return list;\n      }\n    }.withRetries();\n  } catch (ServiceException e) {\n    LOG.error(e.getMessage(), e);\n    return null;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2464,
        "instruction": "private void checkFilesUsage() throws Exception {\n   int invmSize = InVMRegistry.instance.size();\n   if (invmSize > 0) {\n      InVMRegistry.instance.clear();\n      fail(\"invm registry still had acceptors registered\");\n   }\n}",
        "accept_response": "private void checkFilesUsage() throws Exception {\n   int invmSize = InVMRegistry.instance.size();\n   if (invmSize > 0) {\n      InVMRegistry.instance.clear();\n      logger.info(threadDump(\"Thread dump\"));\n      fail(\"invm registry still had acceptors registered\");\n   }\n}",
        "reject_response": "private void checkFilesUsage() throws Exception {\n   int invmSize = InVMRegistry.instance.size();\n   if (invmSize > 0) {\n      InVMRegistry.instance.clear();\n      log.info(threadDump(\"Thread dump\"));\n      fail(\"invm registry still had acceptors registered\");\n   }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2915,
        "instruction": "public <K, V> void send(ProducerRecord<K, V> record, Serializer<K> keySerializer, Serializer<V> valueSerializer,\n                        StreamPartitioner<K, V> partitioner) {\n    byte[] keyBytes = keySerializer.serialize(record.topic(), record.key());\n    byte[] valBytes = valueSerializer.serialize(record.topic(), record.value());\n    Integer partition = record.partition();\n    if (partition == null && partitioner != null) {\n        List<PartitionInfo> partitions = this.producer.partitionsFor(record.topic());\n        if (partitions != null)\n            partition = partitioner.partition(record.key(), record.value(), partitions.size());\n    }\n\n    ProducerRecord<byte[], byte[]> serializedRecord =\n            new ProducerRecord<>(record.topic(), partition, record.timestamp(), keyBytes, valBytes);\n    final String topic = serializedRecord.topic();\n\n    this.producer.send(serializedRecord, new Callback() {\n        @Override\n        public void onCompletion(RecordMetadata metadata, Exception exception) {\n            if (exception == null) {\n                TopicPartition tp = new TopicPartition(metadata.topic(), metadata.partition());\n                offsets.put(tp, metadata.offset());\n            } else {\n                String prefix = String.format(\"task [%s]\", streamTaskId);\n            }\n        }\n    });\n}",
        "accept_response": "public <K, V> void send(ProducerRecord<K, V> record, Serializer<K> keySerializer, Serializer<V> valueSerializer,\n                        StreamPartitioner<K, V> partitioner) {\n    byte[] keyBytes = keySerializer.serialize(record.topic(), record.key());\n    byte[] valBytes = valueSerializer.serialize(record.topic(), record.value());\n    Integer partition = record.partition();\n    if (partition == null && partitioner != null) {\n        List<PartitionInfo> partitions = this.producer.partitionsFor(record.topic());\n        if (partitions != null)\n            partition = partitioner.partition(record.key(), record.value(), partitions.size());\n    }\n\n    ProducerRecord<byte[], byte[]> serializedRecord =\n            new ProducerRecord<>(record.topic(), partition, record.timestamp(), keyBytes, valBytes);\n    final String topic = serializedRecord.topic();\n\n    this.producer.send(serializedRecord, new Callback() {\n        @Override\n        public void onCompletion(RecordMetadata metadata, Exception exception) {\n            if (exception == null) {\n                TopicPartition tp = new TopicPartition(metadata.topic(), metadata.partition());\n                offsets.put(tp, metadata.offset());\n            } else {\n                String prefix = String.format(\"task [%s]\", streamTaskId);\n                log.error(\"{} Error sending record to topic {}\", prefix, topic, exception);\n            }\n        }\n    });\n}",
        "reject_response": "public <K, V> void send(ProducerRecord<K, V> record, Serializer<K> keySerializer, Serializer<V> valueSerializer,\n                        StreamPartitioner<K, V> partitioner) {\n    byte[] keyBytes = keySerializer.serialize(record.topic(), record.key());\n    byte[] valBytes = valueSerializer.serialize(record.topic(), record.value());\n    Integer partition = record.partition();\n    if (partition == null && partitioner != null) {\n        List<PartitionInfo> partitions = this.producer.partitionsFor(record.topic());\n        if (partitions != null)\n            partition = partitioner.partition(record.key(), record.value(), partitions.size());\n    }\n\n    ProducerRecord<byte[], byte[]> serializedRecord =\n            new ProducerRecord<>(record.topic(), partition, record.timestamp(), keyBytes, valBytes);\n    final String topic = serializedRecord.topic();\n\n    this.producer.send(serializedRecord, new Callback() {\n        @Override\n        public void onCompletion(RecordMetadata metadata, Exception exception) {\n            if (exception == null) {\n                TopicPartition tp = new TopicPartition(metadata.topic(), metadata.partition());\n                offsets.put(tp, metadata.offset());\n            } else {\n                String prefix = String.format(\"task [%s]\", streamTaskId);\n                log.error(\"Error sending record to topic {}\", topic, exception);\n            }\n        }\n    });\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2415,
        "instruction": "@Override\npublic DeleteStatusMetaData deleteHostComponents(\n    Set<ServiceComponentHostRequest> requests) throws AmbariException, AuthorizationException {\n\n  Set<ServiceComponentHostRequest> expanded = new HashSet<>();\n\n  // if any request are for the whole host, they need to be expanded\n  for (ServiceComponentHostRequest request : requests) {\n    if (null == request.getComponentName()) {\n      if (null == request.getClusterName() || request.getClusterName().isEmpty() ||\n          null == request.getHostname() || request.getHostname().isEmpty()) {\n        throw new IllegalArgumentException(\"Cluster name and hostname must be specified.\");\n      }\n      Cluster cluster = clusters.getCluster(request.getClusterName());\n\n      if (!AuthorizationHelper.isAuthorized(ResourceType.CLUSTER, cluster.getResourceId(),\n          EnumSet.of(RoleAuthorization.SERVICE_ADD_DELETE_SERVICES,RoleAuthorization.HOST_ADD_DELETE_COMPONENTS))) {\n        throw new AuthorizationException(\"The authenticated user is not authorized to delete service components from hosts\");\n      }\n\n      for (ServiceComponentHost sch : cluster.getServiceComponentHosts(request.getHostname())) {\n        ServiceComponentHostRequest schr = new ServiceComponentHostRequest(request.getClusterName(),\n            sch.getServiceName(), sch.getServiceComponentName(), sch.getHostName(), null);\n        expanded.add(schr);\n      }\n    }\n    else {\n      expanded.add(request);\n    }\n  }\n\n  Map<ServiceComponent, Set<ServiceComponentHost>> safeToRemoveSCHs = new HashMap<>();\n  DeleteStatusMetaData deleteStatusMetaData = new DeleteStatusMetaData();\n\n  for (ServiceComponentHostRequest request : expanded) {\n\n    validateServiceComponentHostRequest(request);\n\n    Cluster cluster = clusters.getCluster(request.getClusterName());\n\n    if (StringUtils.isEmpty(request.getServiceName())) {\n      request.setServiceName(findServiceName(cluster, request.getComponentName()));\n    }\n\n\n    Service service = cluster.getService(request.getServiceName());\n    ServiceComponent component = service.getServiceComponent(request.getComponentName());\n    ServiceComponentHost componentHost = component.getServiceComponentHost(request.getHostname());\n\n    setRestartRequiredServices(service, request.getComponentName());\n    try {\n      checkIfHostComponentsInDeleteFriendlyState(request, cluster);\n      if (!safeToRemoveSCHs.containsKey(component)) {\n        safeToRemoveSCHs.put(component, new HashSet<ServiceComponentHost>());\n      }\n      safeToRemoveSCHs.get(component).add(componentHost);\n    } catch (Exception ex) {\n      deleteStatusMetaData.addException(request.getHostname() + \"/\" + request.getComponentName(), ex);\n    }\n  }\n\n  for (Entry<ServiceComponent, Set<ServiceComponentHost>> entry : safeToRemoveSCHs.entrySet()) {\n    for (ServiceComponentHost componentHost : entry.getValue()) {\n      try {\n        deleteHostComponent(entry.getKey(), componentHost);\n        deleteStatusMetaData.addDeletedKey(componentHost.getHostName() + \"/\" + componentHost.getServiceComponentName());\n\n      } catch (Exception ex) {\n        deleteStatusMetaData.addException(componentHost.getHostName() + \"/\" + componentHost.getServiceComponentName(), ex);\n      }\n    }\n  }\n\n  //Do not break behavior for existing clients where delete request contains only 1 host component.\n  //Response for these requests will have empty body with appropriate error code.\n  if (deleteStatusMetaData.getDeletedKeys().size() + deleteStatusMetaData.getExceptionForKeys().size() == 1) {\n    if (deleteStatusMetaData.getDeletedKeys().size() == 1) {\n      return null;\n    }\n    Exception ex =  deleteStatusMetaData.getExceptionForKeys().values().iterator().next();\n    if (ex instanceof AmbariException) {\n      throw (AmbariException)ex;\n    } else {\n      throw new AmbariException(ex.getMessage(), ex);\n    }\n  }\n\n  // set restartRequired flag for  monitoring services\n  if (!safeToRemoveSCHs.isEmpty()) {\n    setMonitoringServicesRestartRequired(requests);\n  }\n  return deleteStatusMetaData;\n}",
        "accept_response": "@Override\npublic DeleteStatusMetaData deleteHostComponents(\n    Set<ServiceComponentHostRequest> requests) throws AmbariException, AuthorizationException {\n\n  Set<ServiceComponentHostRequest> expanded = new HashSet<>();\n\n  // if any request are for the whole host, they need to be expanded\n  for (ServiceComponentHostRequest request : requests) {\n    if (null == request.getComponentName()) {\n      if (null == request.getClusterName() || request.getClusterName().isEmpty() ||\n          null == request.getHostname() || request.getHostname().isEmpty()) {\n        throw new IllegalArgumentException(\"Cluster name and hostname must be specified.\");\n      }\n      Cluster cluster = clusters.getCluster(request.getClusterName());\n\n      if (!AuthorizationHelper.isAuthorized(ResourceType.CLUSTER, cluster.getResourceId(),\n          EnumSet.of(RoleAuthorization.SERVICE_ADD_DELETE_SERVICES,RoleAuthorization.HOST_ADD_DELETE_COMPONENTS))) {\n        throw new AuthorizationException(\"The authenticated user is not authorized to delete service components from hosts\");\n      }\n\n      for (ServiceComponentHost sch : cluster.getServiceComponentHosts(request.getHostname())) {\n        ServiceComponentHostRequest schr = new ServiceComponentHostRequest(request.getClusterName(),\n            sch.getServiceName(), sch.getServiceComponentName(), sch.getHostName(), null);\n        expanded.add(schr);\n      }\n    }\n    else {\n      expanded.add(request);\n    }\n  }\n\n  Map<ServiceComponent, Set<ServiceComponentHost>> safeToRemoveSCHs = new HashMap<>();\n  DeleteStatusMetaData deleteStatusMetaData = new DeleteStatusMetaData();\n\n  for (ServiceComponentHostRequest request : expanded) {\n\n    validateServiceComponentHostRequest(request);\n\n    Cluster cluster = clusters.getCluster(request.getClusterName());\n\n    if (StringUtils.isEmpty(request.getServiceName())) {\n      request.setServiceName(findServiceName(cluster, request.getComponentName()));\n    }\n\n    LOG.info(\"Received a hostComponent DELETE request\"\n      + \", clusterName=\" + request.getClusterName()\n      + \", serviceName=\" + request.getServiceName()\n      + \", componentName=\" + request.getComponentName()\n      + \", hostname=\" + request.getHostname()\n      + \", request=\" + request);\n\n    Service service = cluster.getService(request.getServiceName());\n    ServiceComponent component = service.getServiceComponent(request.getComponentName());\n    ServiceComponentHost componentHost = component.getServiceComponentHost(request.getHostname());\n\n    setRestartRequiredServices(service, request.getComponentName());\n    try {\n      checkIfHostComponentsInDeleteFriendlyState(request, cluster);\n      if (!safeToRemoveSCHs.containsKey(component)) {\n        safeToRemoveSCHs.put(component, new HashSet<ServiceComponentHost>());\n      }\n      safeToRemoveSCHs.get(component).add(componentHost);\n    } catch (Exception ex) {\n      deleteStatusMetaData.addException(request.getHostname() + \"/\" + request.getComponentName(), ex);\n    }\n  }\n\n  for (Entry<ServiceComponent, Set<ServiceComponentHost>> entry : safeToRemoveSCHs.entrySet()) {\n    for (ServiceComponentHost componentHost : entry.getValue()) {\n      try {\n        deleteHostComponent(entry.getKey(), componentHost);\n        deleteStatusMetaData.addDeletedKey(componentHost.getHostName() + \"/\" + componentHost.getServiceComponentName());\n\n      } catch (Exception ex) {\n        deleteStatusMetaData.addException(componentHost.getHostName() + \"/\" + componentHost.getServiceComponentName(), ex);\n      }\n    }\n  }\n\n  //Do not break behavior for existing clients where delete request contains only 1 host component.\n  //Response for these requests will have empty body with appropriate error code.\n  if (deleteStatusMetaData.getDeletedKeys().size() + deleteStatusMetaData.getExceptionForKeys().size() == 1) {\n    if (deleteStatusMetaData.getDeletedKeys().size() == 1) {\n      return null;\n    }\n    Exception ex =  deleteStatusMetaData.getExceptionForKeys().values().iterator().next();\n    if (ex instanceof AmbariException) {\n      throw (AmbariException)ex;\n    } else {\n      throw new AmbariException(ex.getMessage(), ex);\n    }\n  }\n\n  // set restartRequired flag for  monitoring services\n  if (!safeToRemoveSCHs.isEmpty()) {\n    setMonitoringServicesRestartRequired(requests);\n  }\n  return deleteStatusMetaData;\n}",
        "reject_response": "@Override\npublic DeleteStatusMetaData deleteHostComponents(\n    Set<ServiceComponentHostRequest> requests) throws AmbariException, AuthorizationException {\n\n  Set<ServiceComponentHostRequest> expanded = new HashSet<>();\n\n  // if any request are for the whole host, they need to be expanded\n  for (ServiceComponentHostRequest request : requests) {\n    if (null == request.getComponentName()) {\n      if (null == request.getClusterName() || request.getClusterName().isEmpty() ||\n          null == request.getHostname() || request.getHostname().isEmpty()) {\n        throw new IllegalArgumentException(\"Cluster name and hostname must be specified.\");\n      }\n      Cluster cluster = clusters.getCluster(request.getClusterName());\n\n      if (!AuthorizationHelper.isAuthorized(ResourceType.CLUSTER, cluster.getResourceId(),\n          EnumSet.of(RoleAuthorization.SERVICE_ADD_DELETE_SERVICES,RoleAuthorization.HOST_ADD_DELETE_COMPONENTS))) {\n        throw new AuthorizationException(\"The authenticated user is not authorized to delete service components from hosts\");\n      }\n\n      for (ServiceComponentHost sch : cluster.getServiceComponentHosts(request.getHostname())) {\n        ServiceComponentHostRequest schr = new ServiceComponentHostRequest(request.getClusterName(),\n            sch.getServiceName(), sch.getServiceComponentName(), sch.getHostName(), null);\n        expanded.add(schr);\n      }\n    }\n    else {\n      expanded.add(request);\n    }\n  }\n\n  Map<ServiceComponent, Set<ServiceComponentHost>> safeToRemoveSCHs = new HashMap<>();\n  DeleteStatusMetaData deleteStatusMetaData = new DeleteStatusMetaData();\n\n  for (ServiceComponentHostRequest request : expanded) {\n\n    validateServiceComponentHostRequest(request);\n\n    Cluster cluster = clusters.getCluster(request.getClusterName());\n\n    if (StringUtils.isEmpty(request.getServiceName())) {\n      request.setServiceName(findServiceName(cluster, request.getComponentName()));\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Received a hostComponent DELETE request\"\n          + \", clusterName=\" + request.getClusterName()\n          + \", serviceName=\" + request.getServiceName()\n          + \", componentName=\" + request.getComponentName()\n          + \", hostname=\" + request.getHostname()\n          + \", request=\" + request);\n    }\n\n    Service service = cluster.getService(request.getServiceName());\n    ServiceComponent component = service.getServiceComponent(request.getComponentName());\n    ServiceComponentHost componentHost = component.getServiceComponentHost(request.getHostname());\n\n    setRestartRequiredServices(service, request.getComponentName());\n    try {\n      checkIfHostComponentsInDeleteFriendlyState(request, cluster);\n      if (!safeToRemoveSCHs.containsKey(component)) {\n        safeToRemoveSCHs.put(component, new HashSet<ServiceComponentHost>());\n      }\n      safeToRemoveSCHs.get(component).add(componentHost);\n    } catch (Exception ex) {\n      deleteStatusMetaData.addException(request.getHostname() + \"/\" + request.getComponentName(), ex);\n    }\n  }\n\n  for (Entry<ServiceComponent, Set<ServiceComponentHost>> entry : safeToRemoveSCHs.entrySet()) {\n    for (ServiceComponentHost componentHost : entry.getValue()) {\n      try {\n        deleteHostComponent(entry.getKey(), componentHost);\n        deleteStatusMetaData.addDeletedKey(componentHost.getHostName() + \"/\" + componentHost.getServiceComponentName());\n\n      } catch (Exception ex) {\n        deleteStatusMetaData.addException(componentHost.getHostName() + \"/\" + componentHost.getServiceComponentName(), ex);\n      }\n    }\n  }\n\n  //Do not break behavior for existing clients where delete request contains only 1 host component.\n  //Response for these requests will have empty body with appropriate error code.\n  if (deleteStatusMetaData.getDeletedKeys().size() + deleteStatusMetaData.getExceptionForKeys().size() == 1) {\n    if (deleteStatusMetaData.getDeletedKeys().size() == 1) {\n      return null;\n    }\n    Exception ex =  deleteStatusMetaData.getExceptionForKeys().values().iterator().next();\n    if (ex instanceof AmbariException) {\n      throw (AmbariException)ex;\n    } else {\n      throw new AmbariException(ex.getMessage(), ex);\n    }\n  }\n\n  // set restartRequired flag for  monitoring services\n  if (!safeToRemoveSCHs.isEmpty()) {\n    setMonitoringServicesRestartRequired(requests);\n  }\n  return deleteStatusMetaData;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2515,
        "instruction": "private void convertLegacyData()\n{\n    try\n    {\n        if (Schema.instance.getCFMetaData(\"system_auth\", \"permissions\") != null)\n        {\n            logger.info(\"Converting legacy permissions data\");\n            CQLStatement insertStatement =\n                QueryProcessor.getStatement(String.format(\"INSERT INTO %s.%s (role, resource, permissions) \" +\n                                                          \"VALUES (?, ?, ?)\",\n                                                          AuthKeyspace.NAME,\n                                                          AuthKeyspace.ROLE_PERMISSIONS),\n                                            ClientState.forInternalCalls()).statement;\n            CQLStatement indexStatement =\n                QueryProcessor.getStatement(String.format(\"INSERT INTO %s.%s (resource, role) VALUES (?,?)\",\n                                                          AuthKeyspace.NAME,\n                                                          AuthKeyspace.RESOURCE_ROLE_INDEX),\n                                            ClientState.forInternalCalls()).statement;\n\n            UntypedResultSet permissions = process(\"SELECT * FROM system_auth.permissions\");\n            for (UntypedResultSet.Row row : permissions)\n            {\n                final IResource resource = Resources.fromName(row.getString(\"resource\"));\n                Predicate<String> isApplicable = new Predicate<String>()\n                {\n                    public boolean apply(String s)\n                    {\n                        return resource.applicablePermissions().contains(Permission.valueOf(s));\n                    }\n                };\n                SetSerializer<String> serializer = SetSerializer.getInstance(UTF8Serializer.instance, UTF8Type.instance);\n                Set<String> originalPerms = serializer.deserialize(row.getBytes(\"permissions\"));\n                Set<String> filteredPerms = ImmutableSet.copyOf(Iterables.filter(originalPerms, isApplicable));\n                insertStatement.execute(QueryState.forInternalCalls(),\n                                        QueryOptions.forInternalCalls(ConsistencyLevel.ONE,\n                                                                      Lists.newArrayList(row.getBytes(\"username\"),\n                                                                                         row.getBytes(\"resource\"),\n                                                                                         serializer.serialize(filteredPerms))));\n\n                indexStatement.execute(QueryState.forInternalCalls(),\n                                       QueryOptions.forInternalCalls(ConsistencyLevel.ONE,\n                                                                     Lists.newArrayList(row.getBytes(\"resource\"),\n                                                                                        row.getBytes(\"username\"))));\n\n            }\n            logger.info(\"Completed conversion of legacy permissions\");\n        }\n    }\n    catch (Exception e)\n    {\n        logger.info(\"Unable to complete conversion of legacy permissions data (perhaps not enough nodes are upgraded yet). \" +\n                    \"Conversion should not be considered complete\");\n    }\n}",
        "accept_response": "private void convertLegacyData()\n{\n    try\n    {\n        if (Schema.instance.getCFMetaData(\"system_auth\", \"permissions\") != null)\n        {\n            logger.info(\"Converting legacy permissions data\");\n            CQLStatement insertStatement =\n                QueryProcessor.getStatement(String.format(\"INSERT INTO %s.%s (role, resource, permissions) \" +\n                                                          \"VALUES (?, ?, ?)\",\n                                                          AuthKeyspace.NAME,\n                                                          AuthKeyspace.ROLE_PERMISSIONS),\n                                            ClientState.forInternalCalls()).statement;\n            CQLStatement indexStatement =\n                QueryProcessor.getStatement(String.format(\"INSERT INTO %s.%s (resource, role) VALUES (?,?)\",\n                                                          AuthKeyspace.NAME,\n                                                          AuthKeyspace.RESOURCE_ROLE_INDEX),\n                                            ClientState.forInternalCalls()).statement;\n\n            UntypedResultSet permissions = process(\"SELECT * FROM system_auth.permissions\");\n            for (UntypedResultSet.Row row : permissions)\n            {\n                final IResource resource = Resources.fromName(row.getString(\"resource\"));\n                Predicate<String> isApplicable = new Predicate<String>()\n                {\n                    public boolean apply(String s)\n                    {\n                        return resource.applicablePermissions().contains(Permission.valueOf(s));\n                    }\n                };\n                SetSerializer<String> serializer = SetSerializer.getInstance(UTF8Serializer.instance, UTF8Type.instance);\n                Set<String> originalPerms = serializer.deserialize(row.getBytes(\"permissions\"));\n                Set<String> filteredPerms = ImmutableSet.copyOf(Iterables.filter(originalPerms, isApplicable));\n                insertStatement.execute(QueryState.forInternalCalls(),\n                                        QueryOptions.forInternalCalls(ConsistencyLevel.ONE,\n                                                                      Lists.newArrayList(row.getBytes(\"username\"),\n                                                                                         row.getBytes(\"resource\"),\n                                                                                         serializer.serialize(filteredPerms))));\n\n                indexStatement.execute(QueryState.forInternalCalls(),\n                                       QueryOptions.forInternalCalls(ConsistencyLevel.ONE,\n                                                                     Lists.newArrayList(row.getBytes(\"resource\"),\n                                                                                        row.getBytes(\"username\"))));\n\n            }\n            logger.info(\"Completed conversion of legacy permissions\");\n        }\n    }\n    catch (Exception e)\n    {\n        logger.info(\"Unable to complete conversion of legacy permissions data (perhaps not enough nodes are upgraded yet). \" +\n                    \"Conversion should not be considered complete\");\n        logger.trace(\"Conversion error\", e);\n    }\n}",
        "reject_response": "private void convertLegacyData()\n{\n    try\n    {\n        if (Schema.instance.getCFMetaData(\"system_auth\", \"permissions\") != null)\n        {\n            logger.info(\"Converting legacy permissions data\");\n            CQLStatement insertStatement =\n                QueryProcessor.getStatement(String.format(\"INSERT INTO %s.%s (role, resource, permissions) \" +\n                                                          \"VALUES (?, ?, ?)\",\n                                                          AuthKeyspace.NAME,\n                                                          AuthKeyspace.ROLE_PERMISSIONS),\n                                            ClientState.forInternalCalls()).statement;\n            CQLStatement indexStatement =\n                QueryProcessor.getStatement(String.format(\"INSERT INTO %s.%s (resource, role) VALUES (?,?)\",\n                                                          AuthKeyspace.NAME,\n                                                          AuthKeyspace.RESOURCE_ROLE_INDEX),\n                                            ClientState.forInternalCalls()).statement;\n\n            UntypedResultSet permissions = process(\"SELECT * FROM system_auth.permissions\");\n            for (UntypedResultSet.Row row : permissions)\n            {\n                final IResource resource = Resources.fromName(row.getString(\"resource\"));\n                Predicate<String> isApplicable = new Predicate<String>()\n                {\n                    public boolean apply(String s)\n                    {\n                        return resource.applicablePermissions().contains(Permission.valueOf(s));\n                    }\n                };\n                SetSerializer<String> serializer = SetSerializer.getInstance(UTF8Serializer.instance, UTF8Type.instance);\n                Set<String> originalPerms = serializer.deserialize(row.getBytes(\"permissions\"));\n                Set<String> filteredPerms = ImmutableSet.copyOf(Iterables.filter(originalPerms, isApplicable));\n                insertStatement.execute(QueryState.forInternalCalls(),\n                                        QueryOptions.forInternalCalls(ConsistencyLevel.ONE,\n                                                                      Lists.newArrayList(row.getBytes(\"username\"),\n                                                                                         row.getBytes(\"resource\"),\n                                                                                         serializer.serialize(filteredPerms))));\n\n                indexStatement.execute(QueryState.forInternalCalls(),\n                                       QueryOptions.forInternalCalls(ConsistencyLevel.ONE,\n                                                                     Lists.newArrayList(row.getBytes(\"resource\"),\n                                                                                        row.getBytes(\"username\"))));\n\n            }\n            logger.info(\"Completed conversion of legacy permissions\");\n        }\n    }\n    catch (Exception e)\n    {\n        logger.info(\"Unable to complete conversion of legacy permissions data (perhaps not enough nodes are upgraded yet). \" +\n                    \"Conversion should not be considered complete\");\n        logger.debug(\"Conversion error\", e);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2466,
        "instruction": "private void cleanupPools() {\n   OperationContextImpl.clearContext();\n\n   // We shutdown the global pools to give a better isolation between tests\n   try {\n      ServerLocatorImpl.clearThreadPools();\n   } catch (Throwable e) {\n      System.err.println(threadDump(e.getMessage()));\n   }\n\n   try {\n      NettyConnector.clearThreadPools();\n   } catch (Exception e) {\n      logger.info(threadDump(e.getMessage()));\n      System.err.println(threadDump(e.getMessage()));\n   }\n}",
        "accept_response": "private void cleanupPools() {\n   OperationContextImpl.clearContext();\n\n   // We shutdown the global pools to give a better isolation between tests\n   try {\n      ServerLocatorImpl.clearThreadPools();\n   } catch (Throwable e) {\n      logger.info(threadDump(e.getMessage()));\n      System.err.println(threadDump(e.getMessage()));\n   }\n\n   try {\n      NettyConnector.clearThreadPools();\n   } catch (Exception e) {\n      logger.info(threadDump(e.getMessage()));\n      System.err.println(threadDump(e.getMessage()));\n   }\n}",
        "reject_response": "private void cleanupPools() {\n   OperationContextImpl.clearContext();\n\n   // We shutdown the global pools to give a better isolation between tests\n   try {\n      ServerLocatorImpl.clearThreadPools();\n   } catch (Throwable e) {\n      log.info(threadDump(e.getMessage()));\n      System.err.println(threadDump(e.getMessage()));\n   }\n\n   try {\n      NettyConnector.clearThreadPools();\n   } catch (Exception e) {\n      logger.info(threadDump(e.getMessage()));\n      System.err.println(threadDump(e.getMessage()));\n   }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2936,
        "instruction": "private Map<String, String> initDbcpProps(KylinConfig config) {\n    // metadataUrl is like \"kylin_default_instance@jdbc,url=jdbc:mysql://localhost:3306/kylin,username=root,password=xxx\"\n    StorageURL metadataUrl = config.getMetadataUrl();\n    JDBCResourceStore.checkScheme(metadataUrl);\n\n    LinkedHashMap<String, String> ret = new LinkedHashMap<>(metadataUrl.getAllParameters());\n    List<String> mandatoryItems = Arrays.asList(\"url\", \"username\", PASSWORD);\n\n    for (String item : mandatoryItems) {\n        Preconditions.checkNotNull(ret.get(item),\n                \"Setting item \\\"\" + item + \"\\\" is mandatory for Jdbc connections.\");\n    }\n\n    // Check whether password encrypted\n    if (\"true\".equals(ret.get(\"passwordEncrypted\"))) {\n        String password = ret.get(\"password\");\n        ret.put(\"password\", EncryptUtil.decrypt(password));\n        ret.remove(\"passwordEncrypted\");\n    }\n\n\n    putIfMissing(ret, \"driverClassName\", \"com.mysql.jdbc.Driver\");\n    putIfMissing(ret, \"maxActive\", \"5\");\n    putIfMissing(ret, \"maxIdle\", \"5\");\n    putIfMissing(ret, \"maxWait\", \"1000\");\n    putIfMissing(ret, \"removeAbandoned\", \"true\");\n    putIfMissing(ret, \"removeAbandonedTimeout\", \"180\");\n    putIfMissing(ret, \"testOnBorrow\", \"true\");\n    putIfMissing(ret, \"testWhileIdle\", \"true\");\n    putIfMissing(ret, \"validationQuery\", \"select 1\");\n    return ret;\n}",
        "accept_response": "private Map<String, String> initDbcpProps(KylinConfig config) {\n    // metadataUrl is like \"kylin_default_instance@jdbc,url=jdbc:mysql://localhost:3306/kylin,username=root,password=xxx\"\n    StorageURL metadataUrl = config.getMetadataUrl();\n    JDBCResourceStore.checkScheme(metadataUrl);\n\n    LinkedHashMap<String, String> ret = new LinkedHashMap<>(metadataUrl.getAllParameters());\n    List<String> mandatoryItems = Arrays.asList(\"url\", \"username\", PASSWORD);\n\n    for (String item : mandatoryItems) {\n        Preconditions.checkNotNull(ret.get(item),\n                \"Setting item \\\"\" + item + \"\\\" is mandatory for Jdbc connections.\");\n    }\n\n    // Check whether password encrypted\n    if (\"true\".equals(ret.get(\"passwordEncrypted\"))) {\n        String password = ret.get(\"password\");\n        ret.put(\"password\", EncryptUtil.decrypt(password));\n        ret.remove(\"passwordEncrypted\");\n    }\n\n    logger.info(\"Connecting to Jdbc with url:{} by user {}\", ret.get(\"url\"), ret.get(\"username\"));\n\n    putIfMissing(ret, \"driverClassName\", \"com.mysql.jdbc.Driver\");\n    putIfMissing(ret, \"maxActive\", \"5\");\n    putIfMissing(ret, \"maxIdle\", \"5\");\n    putIfMissing(ret, \"maxWait\", \"1000\");\n    putIfMissing(ret, \"removeAbandoned\", \"true\");\n    putIfMissing(ret, \"removeAbandonedTimeout\", \"180\");\n    putIfMissing(ret, \"testOnBorrow\", \"true\");\n    putIfMissing(ret, \"testWhileIdle\", \"true\");\n    putIfMissing(ret, \"validationQuery\", \"select 1\");\n    return ret;\n}",
        "reject_response": "private Map<String, String> initDbcpProps(KylinConfig config) {\n    // metadataUrl is like \"kylin_default_instance@jdbc,url=jdbc:mysql://localhost:3306/kylin,username=root,password=xxx\"\n    StorageURL metadataUrl = config.getMetadataUrl();\n    JDBCResourceStore.checkScheme(metadataUrl);\n\n    LinkedHashMap<String, String> ret = new LinkedHashMap<>(metadataUrl.getAllParameters());\n    List<String> mandatoryItems = Arrays.asList(\"url\", \"username\", PASSWORD);\n\n    for (String item : mandatoryItems) {\n        Preconditions.checkNotNull(ret.get(item),\n                \"Setting item \\\"\" + item + \"\\\" is mandatory for Jdbc connections.\");\n    }\n\n    // Check whether password encrypted\n    if (\"true\".equals(ret.get(\"passwordEncrypted\"))) {\n        String password = ret.get(\"password\");\n        ret.put(\"password\", EncryptUtil.decrypt(password));\n        ret.remove(\"passwordEncrypted\");\n    }\n\n    logger.info(\"Connecting to Jdbc with url:{0} by user {1}\", ret.get(\"url\"), ret.get(\"username\"));\n\n    putIfMissing(ret, \"driverClassName\", \"com.mysql.jdbc.Driver\");\n    putIfMissing(ret, \"maxActive\", \"5\");\n    putIfMissing(ret, \"maxIdle\", \"5\");\n    putIfMissing(ret, \"maxWait\", \"1000\");\n    putIfMissing(ret, \"removeAbandoned\", \"true\");\n    putIfMissing(ret, \"removeAbandonedTimeout\", \"180\");\n    putIfMissing(ret, \"testOnBorrow\", \"true\");\n    putIfMissing(ret, \"testWhileIdle\", \"true\");\n    putIfMissing(ret, \"validationQuery\", \"select 1\");\n    return ret;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2627,
        "instruction": "@RequestMapping(value = \"/dataBrowserQuery\", method = RequestMethod.GET)\npublic void dataBrowserQuery(HttpServletRequest request, HttpServletResponse response)\n    throws IOException {\n  // get query string\n  String query = request.getParameter(\"query\");\n  String members = request.getParameter(\"members\");\n  int limit = 0;\n\n  try {\n    limit = Integer.valueOf(request.getParameter(\"limit\"));\n  } catch (NumberFormatException e) {\n    limit = 0;\n  }\n\n  ObjectNode queryResult = mapper.createObjectNode();\n  try {\n\n    if (StringUtils.isNotBlank(query)) {\n      // get cluster object\n      Cluster cluster = Repository.get().getCluster();\n      String userName = request.getUserPrincipal().getName();\n\n      // Call execute query method\n      queryResult = cluster.executeQuery(query, members, limit);\n\n      // Add query in history if query is executed successfully\n      if (!queryResult.has(\"error\")) {\n        // Add html escaped query to history\n        String escapedQuery = StringEscapeUtils.escapeHtml(query);\n        cluster.addQueryInHistory(escapedQuery, userName);\n      }\n    }\n  } catch (Exception e) {\n    logger.debug(\"Exception Occurred : \", e);\n  }\n\n  response.getOutputStream().write(queryResult.toString().getBytes());\n}",
        "accept_response": "@RequestMapping(value = \"/dataBrowserQuery\", method = RequestMethod.GET)\npublic void dataBrowserQuery(HttpServletRequest request, HttpServletResponse response)\n    throws IOException {\n  // get query string\n  String query = request.getParameter(\"query\");\n  String members = request.getParameter(\"members\");\n  int limit = 0;\n\n  try {\n    limit = Integer.valueOf(request.getParameter(\"limit\"));\n  } catch (NumberFormatException e) {\n    limit = 0;\n    logger.debug(e);\n  }\n\n  ObjectNode queryResult = mapper.createObjectNode();\n  try {\n\n    if (StringUtils.isNotBlank(query)) {\n      // get cluster object\n      Cluster cluster = Repository.get().getCluster();\n      String userName = request.getUserPrincipal().getName();\n\n      // Call execute query method\n      queryResult = cluster.executeQuery(query, members, limit);\n\n      // Add query in history if query is executed successfully\n      if (!queryResult.has(\"error\")) {\n        // Add html escaped query to history\n        String escapedQuery = StringEscapeUtils.escapeHtml(query);\n        cluster.addQueryInHistory(escapedQuery, userName);\n      }\n    }\n  } catch (Exception e) {\n    logger.debug(\"Exception Occurred : \", e);\n  }\n\n  response.getOutputStream().write(queryResult.toString().getBytes());\n}",
        "reject_response": "@RequestMapping(value = \"/dataBrowserQuery\", method = RequestMethod.GET)\npublic void dataBrowserQuery(HttpServletRequest request, HttpServletResponse response)\n    throws IOException {\n  // get query string\n  String query = request.getParameter(\"query\");\n  String members = request.getParameter(\"members\");\n  int limit = 0;\n\n  try {\n    limit = Integer.valueOf(request.getParameter(\"limit\"));\n  } catch (NumberFormatException e) {\n    limit = 0;\n    if (LOGGER.finerEnabled()) {\n      LOGGER.finer(e.getMessage());\n    }\n  }\n\n  ObjectNode queryResult = mapper.createObjectNode();\n  try {\n\n    if (StringUtils.isNotBlank(query)) {\n      // get cluster object\n      Cluster cluster = Repository.get().getCluster();\n      String userName = request.getUserPrincipal().getName();\n\n      // Call execute query method\n      queryResult = cluster.executeQuery(query, members, limit);\n\n      // Add query in history if query is executed successfully\n      if (!queryResult.has(\"error\")) {\n        // Add html escaped query to history\n        String escapedQuery = StringEscapeUtils.escapeHtml(query);\n        cluster.addQueryInHistory(escapedQuery, userName);\n      }\n    }\n  } catch (Exception e) {\n    logger.debug(\"Exception Occurred : \", e);\n  }\n\n  response.getOutputStream().write(queryResult.toString().getBytes());\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2463,
        "instruction": "protected void waitForServerToStart(ActiveMQServer server, boolean activation) throws InterruptedException {\n   if (server == null)\n      return;\n   final long wait = 5000;\n   long timetowait = System.currentTimeMillis() + wait;\n   while (!server.isStarted() && System.currentTimeMillis() < timetowait) {\n      Thread.sleep(50);\n   }\n\n   if (!server.isStarted()) {\n      fail(\"server didn't start: \" + server);\n   }\n\n   if (activation) {\n      if (!server.getHAPolicy().isBackup()) {\n         if (!server.waitForActivation(wait, TimeUnit.MILLISECONDS))\n            fail(\"Server didn't initialize: \" + server);\n      }\n   }\n}",
        "accept_response": "protected void waitForServerToStart(ActiveMQServer server, boolean activation) throws InterruptedException {\n   if (server == null)\n      return;\n   final long wait = 5000;\n   long timetowait = System.currentTimeMillis() + wait;\n   while (!server.isStarted() && System.currentTimeMillis() < timetowait) {\n      Thread.sleep(50);\n   }\n\n   if (!server.isStarted()) {\n      logger.info(threadDump(\"Server didn't start\"));\n      fail(\"server didn't start: \" + server);\n   }\n\n   if (activation) {\n      if (!server.getHAPolicy().isBackup()) {\n         if (!server.waitForActivation(wait, TimeUnit.MILLISECONDS))\n            fail(\"Server didn't initialize: \" + server);\n      }\n   }\n}",
        "reject_response": "protected void waitForServerToStart(ActiveMQServer server, boolean activation) throws InterruptedException {\n   if (server == null)\n      return;\n   final long wait = 5000;\n   long timetowait = System.currentTimeMillis() + wait;\n   while (!server.isStarted() && System.currentTimeMillis() < timetowait) {\n      Thread.sleep(50);\n   }\n\n   if (!server.isStarted()) {\n      log.info(threadDump(\"Server didn't start\"));\n      fail(\"server didn't start: \" + server);\n   }\n\n   if (activation) {\n      if (!server.getHAPolicy().isBackup()) {\n         if (!server.waitForActivation(wait, TimeUnit.MILLISECONDS))\n            fail(\"Server didn't initialize: \" + server);\n      }\n   }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2622,
        "instruction": "@Override\npublic void contextInitialized(ServletContextEvent event) {\n\n  messagesToBeLogged = messagesToBeLogged\n      .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_CONTEXT_INITIALIZED\")));\n\n  // Load Pulse version details\n  loadPulseVersionDetails();\n\n  // Load Pulse Properties\n  pulseProperties = loadProperties(PulseConstants.PULSE_PROPERTIES_FILE);\n\n  if (pulseProperties.isEmpty()) {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_NOT_FOUND\")));\n  } else {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_FOUND\")));\n\n    // set Pulse product support into the Pulse controller for access from\n    // client side\n    // to display the appropriate ui depending on which product is supported\n    // in present deployment\n    String pulseProduct =\n        pulseProperties.getProperty(PulseConstants.APPLICATION_PROPERTY_PULSE_PRODUCTSUPPORT);\n  }\n\n  pulseSecurityProperties = loadProperties(PulseConstants.PULSE_SECURITY_PROPERTIES_FILE);\n\n  // Initialize logger\n  initializeLogger();\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n  logger.info(resourceBundle.getString(\"LOG_MSG_CHECK_APP_RUNNING_MODE\"));\n\n  boolean sysIsEmbedded = Boolean.getBoolean(PulseConstants.SYSTEM_PROPERTY_PULSE_EMBEDDED);\n\n  if (sysIsEmbedded) {\n    // Application Pulse is running in Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_EMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(true);\n\n    sysPulseUseLocator = Boolean.FALSE;\n    try {\n      // Get host name of machine running pulse in embedded mode\n      sysPulseHost = InetAddress.getLocalHost().getCanonicalHostName();\n    } catch (Exception e) {\n      // Set default host name\n      sysPulseHost = PulseConstants.GEMFIRE_DEFAULT_HOST;\n    }\n    sysPulsePort = PulseConstants.GEMFIRE_DEFAULT_PORT;\n\n  } else {\n    // Application Pulse is running in Non-Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_NONEMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(false);\n\n    // Load JMX User Details\n    loadJMXUserDetails();\n    // Load locator and/or manager details\n    loadLocatorManagerDetails();\n\n    useGemFireCredentials = areWeUsingGemFireSecurityProfile(event);\n  }\n\n  // Set user details in repository\n  repository.setJmxUserName(jmxUserName);\n  repository.setJmxUserPassword(jmxUserPassword);\n\n  // Set locator/Manager details in repository\n  repository.setJmxUseLocator(sysPulseUseLocator);\n  repository.setJmxHost(sysPulseHost);\n  repository.setJmxPort(sysPulsePort);\n\n  // set SSL info\n  initializeSSL();\n  repository.setUseSSLLocator(sysPulseUseSSLLocator);\n  repository.setUseSSLManager(sysPulseUseSSLManager);\n\n  repository.setUseGemFireCredentials(useGemFireCredentials);\n\n}",
        "accept_response": "@Override\npublic void contextInitialized(ServletContextEvent event) {\n\n  messagesToBeLogged = messagesToBeLogged\n      .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_CONTEXT_INITIALIZED\")));\n\n  // Load Pulse version details\n  loadPulseVersionDetails();\n\n  // Load Pulse Properties\n  pulseProperties = loadProperties(PulseConstants.PULSE_PROPERTIES_FILE);\n\n  if (pulseProperties.isEmpty()) {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_NOT_FOUND\")));\n  } else {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_FOUND\")));\n\n    // set Pulse product support into the Pulse controller for access from\n    // client side\n    // to display the appropriate ui depending on which product is supported\n    // in present deployment\n    String pulseProduct =\n        pulseProperties.getProperty(PulseConstants.APPLICATION_PROPERTY_PULSE_PRODUCTSUPPORT);\n  }\n\n  pulseSecurityProperties = loadProperties(PulseConstants.PULSE_SECURITY_PROPERTIES_FILE);\n\n  // Initialize logger\n  initializeLogger();\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n  logger.info(resourceBundle.getString(\"LOG_MSG_CHECK_APP_RUNNING_MODE\"));\n\n  boolean sysIsEmbedded = Boolean.getBoolean(PulseConstants.SYSTEM_PROPERTY_PULSE_EMBEDDED);\n\n  if (sysIsEmbedded) {\n    // Application Pulse is running in Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_EMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(true);\n\n    sysPulseUseLocator = Boolean.FALSE;\n    try {\n      // Get host name of machine running pulse in embedded mode\n      sysPulseHost = InetAddress.getLocalHost().getCanonicalHostName();\n    } catch (Exception e) {\n      logger.debug(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_UNKNOWN_HOST\"), e);\n      // Set default host name\n      sysPulseHost = PulseConstants.GEMFIRE_DEFAULT_HOST;\n    }\n    sysPulsePort = PulseConstants.GEMFIRE_DEFAULT_PORT;\n\n  } else {\n    // Application Pulse is running in Non-Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_NONEMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(false);\n\n    // Load JMX User Details\n    loadJMXUserDetails();\n    // Load locator and/or manager details\n    loadLocatorManagerDetails();\n\n    useGemFireCredentials = areWeUsingGemFireSecurityProfile(event);\n  }\n\n  // Set user details in repository\n  repository.setJmxUserName(jmxUserName);\n  repository.setJmxUserPassword(jmxUserPassword);\n\n  // Set locator/Manager details in repository\n  repository.setJmxUseLocator(sysPulseUseLocator);\n  repository.setJmxHost(sysPulseHost);\n  repository.setJmxPort(sysPulsePort);\n\n  // set SSL info\n  initializeSSL();\n  repository.setUseSSLLocator(sysPulseUseSSLLocator);\n  repository.setUseSSLManager(sysPulseUseSSLManager);\n\n  repository.setUseGemFireCredentials(useGemFireCredentials);\n\n}",
        "reject_response": "@Override\npublic void contextInitialized(ServletContextEvent event) {\n\n  messagesToBeLogged = messagesToBeLogged\n      .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_CONTEXT_INITIALIZED\")));\n\n  // Load Pulse version details\n  loadPulseVersionDetails();\n\n  // Load Pulse Properties\n  pulseProperties = loadProperties(PulseConstants.PULSE_PROPERTIES_FILE);\n\n  if (pulseProperties.isEmpty()) {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_NOT_FOUND\")));\n  } else {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_FOUND\")));\n\n    // set Pulse product support into the Pulse controller for access from\n    // client side\n    // to display the appropriate ui depending on which product is supported\n    // in present deployment\n    String pulseProduct =\n        pulseProperties.getProperty(PulseConstants.APPLICATION_PROPERTY_PULSE_PRODUCTSUPPORT);\n  }\n\n  pulseSecurityProperties = loadProperties(PulseConstants.PULSE_SECURITY_PROPERTIES_FILE);\n\n  // Initialize logger\n  initializeLogger();\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n  logger.info(resourceBundle.getString(\"LOG_MSG_CHECK_APP_RUNNING_MODE\"));\n\n  boolean sysIsEmbedded = Boolean.getBoolean(PulseConstants.SYSTEM_PROPERTY_PULSE_EMBEDDED);\n\n  if (sysIsEmbedded) {\n    // Application Pulse is running in Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_EMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(true);\n\n    sysPulseUseLocator = Boolean.FALSE;\n    try {\n      // Get host name of machine running pulse in embedded mode\n      sysPulseHost = InetAddress.getLocalHost().getCanonicalHostName();\n    } catch (Exception e) {\n      if (LOGGER.fineEnabled()) {\n        LOGGER.fine(\n            resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_UNKNOWN_HOST\") + e.getMessage());\n      }\n      // Set default host name\n      sysPulseHost = PulseConstants.GEMFIRE_DEFAULT_HOST;\n    }\n    sysPulsePort = PulseConstants.GEMFIRE_DEFAULT_PORT;\n\n  } else {\n    // Application Pulse is running in Non-Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_NONEMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(false);\n\n    // Load JMX User Details\n    loadJMXUserDetails();\n    // Load locator and/or manager details\n    loadLocatorManagerDetails();\n\n    useGemFireCredentials = areWeUsingGemFireSecurityProfile(event);\n  }\n\n  // Set user details in repository\n  repository.setJmxUserName(jmxUserName);\n  repository.setJmxUserPassword(jmxUserPassword);\n\n  // Set locator/Manager details in repository\n  repository.setJmxUseLocator(sysPulseUseLocator);\n  repository.setJmxHost(sysPulseHost);\n  repository.setJmxPort(sysPulsePort);\n\n  // set SSL info\n  initializeSSL();\n  repository.setUseSSLLocator(sysPulseUseSSLLocator);\n  repository.setUseSSLManager(sysPulseUseSSLManager);\n\n  repository.setUseGemFireCredentials(useGemFireCredentials);\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2864,
        "instruction": "private void reset()\n{\n\n    try\n    {\n        storageLock.writeLock().lock();\n\n        if (dataFile != null)\n        {\n            dataFile.close();\n        }\n\n        File dataFileTemp = new File(rafDir, fileName + \".data\");\n        Files.delete(dataFileTemp.toPath());\n\n        if (keyFile != null)\n        {\n            keyFile.close();\n        }\n        File keyFileTemp = new File(rafDir, fileName + \".key\");\n        Files.delete(keyFileTemp.toPath());\n\n        dataFile = new IndexedDisk(new File(rafDir, fileName + \".data\"), getElementSerializer());\n        keyFile = new IndexedDisk(new File(rafDir, fileName + \".key\"), getElementSerializer());\n\n        this.recycle.clear();\n        this.keyHash.clear();\n    }\n    catch (IOException e)\n    {\n        log.error(logCacheName + \"Failure resetting state\", e);\n    }\n    finally\n    {\n        storageLock.writeLock().unlock();\n    }\n}",
        "accept_response": "private void reset()\n{\n    if (log.isInfoEnabled())\n    {\n        log.info(logCacheName + \"Resetting cache\");\n    }\n\n    try\n    {\n        storageLock.writeLock().lock();\n\n        if (dataFile != null)\n        {\n            dataFile.close();\n        }\n\n        File dataFileTemp = new File(rafDir, fileName + \".data\");\n        Files.delete(dataFileTemp.toPath());\n\n        if (keyFile != null)\n        {\n            keyFile.close();\n        }\n        File keyFileTemp = new File(rafDir, fileName + \".key\");\n        Files.delete(keyFileTemp.toPath());\n\n        dataFile = new IndexedDisk(new File(rafDir, fileName + \".data\"), getElementSerializer());\n        keyFile = new IndexedDisk(new File(rafDir, fileName + \".key\"), getElementSerializer());\n\n        this.recycle.clear();\n        this.keyHash.clear();\n    }\n    catch (IOException e)\n    {\n        log.error(logCacheName + \"Failure resetting state\", e);\n    }\n    finally\n    {\n        storageLock.writeLock().unlock();\n    }\n}",
        "reject_response": "private void reset()\n{\n    if (log.isInfoEnabled())\n    {\n        log.warn(logCacheName + \"Resetting cache\");\n    }\n\n    try\n    {\n        storageLock.writeLock().lock();\n\n        if (dataFile != null)\n        {\n            dataFile.close();\n        }\n\n        File dataFileTemp = new File(rafDir, fileName + \".data\");\n        Files.delete(dataFileTemp.toPath());\n\n        if (keyFile != null)\n        {\n            keyFile.close();\n        }\n        File keyFileTemp = new File(rafDir, fileName + \".key\");\n        Files.delete(keyFileTemp.toPath());\n\n        dataFile = new IndexedDisk(new File(rafDir, fileName + \".data\"), getElementSerializer());\n        keyFile = new IndexedDisk(new File(rafDir, fileName + \".key\"), getElementSerializer());\n\n        this.recycle.clear();\n        this.keyHash.clear();\n    }\n    catch (IOException e)\n    {\n        log.error(logCacheName + \"Failure resetting state\", e);\n    }\n    finally\n    {\n        storageLock.writeLock().unlock();\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2682,
        "instruction": "private String updateFileExtension(String fileName, CompressionCodec codec) {\n\n    if (codec != null) {\n        fileName += codec.getDefaultExtension();\n    }\n    return fileName;\n}",
        "accept_response": "private String updateFileExtension(String fileName, CompressionCodec codec) {\n\n    if (codec != null) {\n        fileName += codec.getDefaultExtension();\n    }\n    LOG.debug(\"File name for write: \" + fileName);\n    return fileName;\n}",
        "reject_response": "private String updateFileExtension(String fileName, CompressionCodec codec) {\n\n    if (codec != null) {\n        fileName += codec.getDefaultExtension();\n    }\n    Log.debug(\"File name for write: \" + fileName);\n    return fileName;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3136,
        "instruction": "private ContainerTask getContainerTask(ContainerId containerId) throws IOException {\n  ContainerInfo containerInfo = registeredContainers.get(containerId);\n  ContainerTask task;\n  if (containerInfo == null) {\n    if (getContext().isKnownContainer(containerId)) {\n      LOG.info(\"Container with id: \" + containerId\n          + \" is valid, but no longer registered, and will be killed\");\n    } else {\n      LOG.info(\"Container with id: \" + containerId\n          + \" is invalid and will be killed\");\n    }\n    task = TASK_FOR_INVALID_JVM;\n  } else {\n    synchronized (containerInfo) {\n      getContext().containerAlive(containerId);\n      if (containerInfo.taskSpec != null) {\n        if (!containerInfo.taskPulled) {\n          containerInfo.taskPulled = true;\n          task = constructContainerTask(containerInfo);\n        } else {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Task \" + containerInfo.taskSpec.getTaskAttemptID() +\n                \" already sent to container: \" + containerId);\n          }\n          task = null;\n        }\n      } else {\n        task = null;\n      }\n    }\n  }\n  return task;\n}",
        "accept_response": "private ContainerTask getContainerTask(ContainerId containerId) throws IOException {\n  ContainerInfo containerInfo = registeredContainers.get(containerId);\n  ContainerTask task;\n  if (containerInfo == null) {\n    if (getContext().isKnownContainer(containerId)) {\n      LOG.info(\"Container with id: \" + containerId\n          + \" is valid, but no longer registered, and will be killed\");\n    } else {\n      LOG.info(\"Container with id: \" + containerId\n          + \" is invalid and will be killed\");\n    }\n    task = TASK_FOR_INVALID_JVM;\n  } else {\n    synchronized (containerInfo) {\n      getContext().containerAlive(containerId);\n      if (containerInfo.taskSpec != null) {\n        if (!containerInfo.taskPulled) {\n          containerInfo.taskPulled = true;\n          task = constructContainerTask(containerInfo);\n        } else {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Task \" + containerInfo.taskSpec.getTaskAttemptID() +\n                \" already sent to container: \" + containerId);\n          }\n          task = null;\n        }\n      } else {\n        task = null;\n        LOG.debug(\"No task assigned yet for running container: {}\", containerId);\n      }\n    }\n  }\n  return task;\n}",
        "reject_response": "private ContainerTask getContainerTask(ContainerId containerId) throws IOException {\n  ContainerInfo containerInfo = registeredContainers.get(containerId);\n  ContainerTask task;\n  if (containerInfo == null) {\n    if (getContext().isKnownContainer(containerId)) {\n      LOG.info(\"Container with id: \" + containerId\n          + \" is valid, but no longer registered, and will be killed\");\n    } else {\n      LOG.info(\"Container with id: \" + containerId\n          + \" is invalid and will be killed\");\n    }\n    task = TASK_FOR_INVALID_JVM;\n  } else {\n    synchronized (containerInfo) {\n      getContext().containerAlive(containerId);\n      if (containerInfo.taskSpec != null) {\n        if (!containerInfo.taskPulled) {\n          containerInfo.taskPulled = true;\n          task = constructContainerTask(containerInfo);\n        } else {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Task \" + containerInfo.taskSpec.getTaskAttemptID() +\n                \" already sent to container: \" + containerId);\n          }\n          task = null;\n        }\n      } else {\n        task = null;\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"No task assigned yet for running container: \" + containerId);\n        }\n      }\n    }\n  }\n  return task;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2819,
        "instruction": "public void start() {\n    if (getBoolean(IGNITE_JVM_PAUSE_DETECTOR_DISABLED, false)) {\n        if (log.isDebugEnabled())\n\n        return;\n    }\n\n    final Thread worker = new Thread(\"jvm-pause-detector-worker\") {\n        private long prev = System.currentTimeMillis();\n\n        @Override public void run() {\n            if (log.isDebugEnabled())\n                log.debug(getName() + \" has been started.\");\n\n            while (true) {\n                try {\n                    Thread.sleep(PRECISION);\n\n                    final long now = System.currentTimeMillis();\n                    final long pause = now - PRECISION - prev;\n\n                    prev = now;\n\n                    if (pause >= THRESHOLD) {\n                        log.warning(\"Possible too long JVM pause: \" + pause + \" milliseconds.\");\n\n                        synchronized (LongJVMPauseDetector.this) {\n                            final int next = (int)(longPausesCnt % EVT_CNT);\n\n                            longPausesCnt++;\n\n                            longPausesTotalDuration += pause;\n\n                            longPausesTimestamps[next] = now;\n\n                            longPausesDurations[next] = pause;\n                        }\n                    }\n                }\n                catch (InterruptedException e) {\n                    log.error(getName() + \" has been interrupted\", e);\n\n                    break;\n                }\n            }\n        }\n    };\n\n    if (!workerRef.compareAndSet(null, worker)) {\n        log.warning(LongJVMPauseDetector.class.getSimpleName() + \" already started!\");\n\n        return;\n    }\n\n    worker.setDaemon(true);\n    worker.start();\n\n    if (log.isDebugEnabled())\n        log.debug(\"LongJVMPauseDetector was successfully started\");\n}",
        "accept_response": "public void start() {\n    if (getBoolean(IGNITE_JVM_PAUSE_DETECTOR_DISABLED, false)) {\n        if (log.isDebugEnabled())\n            log.debug(\"JVM Pause Detector is disabled.\");\n\n        return;\n    }\n\n    final Thread worker = new Thread(\"jvm-pause-detector-worker\") {\n        private long prev = System.currentTimeMillis();\n\n        @Override public void run() {\n            if (log.isDebugEnabled())\n                log.debug(getName() + \" has been started.\");\n\n            while (true) {\n                try {\n                    Thread.sleep(PRECISION);\n\n                    final long now = System.currentTimeMillis();\n                    final long pause = now - PRECISION - prev;\n\n                    prev = now;\n\n                    if (pause >= THRESHOLD) {\n                        log.warning(\"Possible too long JVM pause: \" + pause + \" milliseconds.\");\n\n                        synchronized (LongJVMPauseDetector.this) {\n                            final int next = (int)(longPausesCnt % EVT_CNT);\n\n                            longPausesCnt++;\n\n                            longPausesTotalDuration += pause;\n\n                            longPausesTimestamps[next] = now;\n\n                            longPausesDurations[next] = pause;\n                        }\n                    }\n                }\n                catch (InterruptedException e) {\n                    log.error(getName() + \" has been interrupted\", e);\n\n                    break;\n                }\n            }\n        }\n    };\n\n    if (!workerRef.compareAndSet(null, worker)) {\n        log.warning(LongJVMPauseDetector.class.getSimpleName() + \" already started!\");\n\n        return;\n    }\n\n    worker.setDaemon(true);\n    worker.start();\n\n    if (log.isDebugEnabled())\n        log.debug(\"LongJVMPauseDetector was successfully started\");\n}",
        "reject_response": "public void start() {\n    if (getBoolean(IGNITE_JVM_PAUSE_DETECTOR_DISABLED, false)) {\n        if (log.isDebugEnabled())\n        if (LOG.isDebugEnabled())\n            LOG.debug(\"JVM Pause Detector is disabled.\");\n\n        return;\n    }\n\n    final Thread worker = new Thread(\"jvm-pause-detector-worker\") {\n        private long prev = System.currentTimeMillis();\n\n        @Override public void run() {\n            if (log.isDebugEnabled())\n                log.debug(getName() + \" has been started.\");\n\n            while (true) {\n                try {\n                    Thread.sleep(PRECISION);\n\n                    final long now = System.currentTimeMillis();\n                    final long pause = now - PRECISION - prev;\n\n                    prev = now;\n\n                    if (pause >= THRESHOLD) {\n                        log.warning(\"Possible too long JVM pause: \" + pause + \" milliseconds.\");\n\n                        synchronized (LongJVMPauseDetector.this) {\n                            final int next = (int)(longPausesCnt % EVT_CNT);\n\n                            longPausesCnt++;\n\n                            longPausesTotalDuration += pause;\n\n                            longPausesTimestamps[next] = now;\n\n                            longPausesDurations[next] = pause;\n                        }\n                    }\n                }\n                catch (InterruptedException e) {\n                    log.error(getName() + \" has been interrupted\", e);\n\n                    break;\n                }\n            }\n        }\n    };\n\n    if (!workerRef.compareAndSet(null, worker)) {\n        log.warning(LongJVMPauseDetector.class.getSimpleName() + \" already started!\");\n\n        return;\n    }\n\n    worker.setDaemon(true);\n    worker.start();\n\n    if (log.isDebugEnabled())\n        log.debug(\"LongJVMPauseDetector was successfully started\");\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2393,
        "instruction": "private void binRanges(TabletLocator tabletLocator, List<Range> ranges, Map<String,Map<KeyExtent,List<Range>>> binnedRanges) throws AccumuloException,\n    AccumuloSecurityException, TableNotFoundException {\n\n  int lastFailureSize = Integer.MAX_VALUE;\n\n  while (true) {\n\n    binnedRanges.clear();\n    List<Range> failures = tabletLocator.binRanges(context, ranges, binnedRanges);\n\n    if (failures.size() > 0) {\n      // tried to only do table state checks when failures.size() == ranges.size(), however this did\n      // not work because nothing ever invalidated entries in the tabletLocator cache... so even though\n      // the table was deleted the tablet locator entries for the deleted table were not cleared... so\n      // need to always do the check when failures occur\n      if (failures.size() >= lastFailureSize)\n        if (!Tables.exists(instance, table))\n          throw new TableDeletedException(table);\n        else if (Tables.getTableState(instance, table) == TableState.OFFLINE)\n          throw new TableOfflineException(instance, table);\n\n      lastFailureSize = failures.size();\n\n      if (log.isTraceEnabled())\n      try {\n        Thread.sleep(100);\n      } catch (InterruptedException e) {\n        throw new RuntimeException(e);\n      }\n    } else {\n      break;\n    }\n\n  }\n\n  // truncate the ranges to within the tablets... this makes it easier to know what work\n  // needs to be redone when failures occurs and tablets have merged or split\n  Map<String,Map<KeyExtent,List<Range>>> binnedRanges2 = new HashMap<String,Map<KeyExtent,List<Range>>>();\n  for (Entry<String,Map<KeyExtent,List<Range>>> entry : binnedRanges.entrySet()) {\n    Map<KeyExtent,List<Range>> tabletMap = new HashMap<KeyExtent,List<Range>>();\n    binnedRanges2.put(entry.getKey(), tabletMap);\n    for (Entry<KeyExtent,List<Range>> tabletRanges : entry.getValue().entrySet()) {\n      Range tabletRange = tabletRanges.getKey().toDataRange();\n      List<Range> clippedRanges = new ArrayList<Range>();\n      tabletMap.put(tabletRanges.getKey(), clippedRanges);\n      for (Range range : tabletRanges.getValue())\n        clippedRanges.add(tabletRange.clip(range));\n    }\n  }\n\n  binnedRanges.clear();\n  binnedRanges.putAll(binnedRanges2);\n}",
        "accept_response": "private void binRanges(TabletLocator tabletLocator, List<Range> ranges, Map<String,Map<KeyExtent,List<Range>>> binnedRanges) throws AccumuloException,\n    AccumuloSecurityException, TableNotFoundException {\n\n  int lastFailureSize = Integer.MAX_VALUE;\n\n  while (true) {\n\n    binnedRanges.clear();\n    List<Range> failures = tabletLocator.binRanges(context, ranges, binnedRanges);\n\n    if (failures.size() > 0) {\n      // tried to only do table state checks when failures.size() == ranges.size(), however this did\n      // not work because nothing ever invalidated entries in the tabletLocator cache... so even though\n      // the table was deleted the tablet locator entries for the deleted table were not cleared... so\n      // need to always do the check when failures occur\n      if (failures.size() >= lastFailureSize)\n        if (!Tables.exists(instance, table))\n          throw new TableDeletedException(table);\n        else if (Tables.getTableState(instance, table) == TableState.OFFLINE)\n          throw new TableOfflineException(instance, table);\n\n      lastFailureSize = failures.size();\n\n      if (log.isTraceEnabled())\n        log.trace(\"Failed to bin {} ranges, tablet locations were null, retrying in 100ms\", failures.size());\n      try {\n        Thread.sleep(100);\n      } catch (InterruptedException e) {\n        throw new RuntimeException(e);\n      }\n    } else {\n      break;\n    }\n\n  }\n\n  // truncate the ranges to within the tablets... this makes it easier to know what work\n  // needs to be redone when failures occurs and tablets have merged or split\n  Map<String,Map<KeyExtent,List<Range>>> binnedRanges2 = new HashMap<String,Map<KeyExtent,List<Range>>>();\n  for (Entry<String,Map<KeyExtent,List<Range>>> entry : binnedRanges.entrySet()) {\n    Map<KeyExtent,List<Range>> tabletMap = new HashMap<KeyExtent,List<Range>>();\n    binnedRanges2.put(entry.getKey(), tabletMap);\n    for (Entry<KeyExtent,List<Range>> tabletRanges : entry.getValue().entrySet()) {\n      Range tabletRange = tabletRanges.getKey().toDataRange();\n      List<Range> clippedRanges = new ArrayList<Range>();\n      tabletMap.put(tabletRanges.getKey(), clippedRanges);\n      for (Range range : tabletRanges.getValue())\n        clippedRanges.add(tabletRange.clip(range));\n    }\n  }\n\n  binnedRanges.clear();\n  binnedRanges.putAll(binnedRanges2);\n}",
        "reject_response": "private void binRanges(TabletLocator tabletLocator, List<Range> ranges, Map<String,Map<KeyExtent,List<Range>>> binnedRanges) throws AccumuloException,\n    AccumuloSecurityException, TableNotFoundException {\n\n  int lastFailureSize = Integer.MAX_VALUE;\n\n  while (true) {\n\n    binnedRanges.clear();\n    List<Range> failures = tabletLocator.binRanges(context, ranges, binnedRanges);\n\n    if (failures.size() > 0) {\n      // tried to only do table state checks when failures.size() == ranges.size(), however this did\n      // not work because nothing ever invalidated entries in the tabletLocator cache... so even though\n      // the table was deleted the tablet locator entries for the deleted table were not cleared... so\n      // need to always do the check when failures occur\n      if (failures.size() >= lastFailureSize)\n        if (!Tables.exists(instance, table))\n          throw new TableDeletedException(table);\n        else if (Tables.getTableState(instance, table) == TableState.OFFLINE)\n          throw new TableOfflineException(instance, table);\n\n      lastFailureSize = failures.size();\n\n      if (log.isTraceEnabled())\n        log.trace(\"Failed to bin \" + failures.size() + \" ranges, tablet locations were null, retrying in 100ms\");\n      try {\n        Thread.sleep(100);\n      } catch (InterruptedException e) {\n        throw new RuntimeException(e);\n      }\n    } else {\n      break;\n    }\n\n  }\n\n  // truncate the ranges to within the tablets... this makes it easier to know what work\n  // needs to be redone when failures occurs and tablets have merged or split\n  Map<String,Map<KeyExtent,List<Range>>> binnedRanges2 = new HashMap<String,Map<KeyExtent,List<Range>>>();\n  for (Entry<String,Map<KeyExtent,List<Range>>> entry : binnedRanges.entrySet()) {\n    Map<KeyExtent,List<Range>> tabletMap = new HashMap<KeyExtent,List<Range>>();\n    binnedRanges2.put(entry.getKey(), tabletMap);\n    for (Entry<KeyExtent,List<Range>> tabletRanges : entry.getValue().entrySet()) {\n      Range tabletRange = tabletRanges.getKey().toDataRange();\n      List<Range> clippedRanges = new ArrayList<Range>();\n      tabletMap.put(tabletRanges.getKey(), clippedRanges);\n      for (Range range : tabletRanges.getValue())\n        clippedRanges.add(tabletRange.clip(range));\n    }\n  }\n\n  binnedRanges.clear();\n  binnedRanges.putAll(binnedRanges2);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2662,
        "instruction": "public synchronized void shutdown() {\n  if (!shutdown) {\n    ChannelFuture closeFuture = serverChannel.closeFuture();\n    Future<?> c = workerGroup.shutdownGracefully();\n    Future<?> c2 = bossGroup.shutdownGracefully();\n    serverChannel.close();\n    c.syncUninterruptibly();\n    c2.syncUninterruptibly();\n    regionProvider.close();\n    if (mainThread != null) {\n      mainThread.interrupt();\n    }\n    for (ScheduledFuture<?> f : expirationFutures.values()) {\n      f.cancel(true);\n    }\n    expirationFutures.clear();\n    expirationExecutor.shutdownNow();\n    closeFuture.syncUninterruptibly();\n    shutdown = true;\n  }\n}",
        "accept_response": "public synchronized void shutdown() {\n  if (!shutdown) {\n    logger.info(\"GeodeRedisServer shutting down\");\n    ChannelFuture closeFuture = serverChannel.closeFuture();\n    Future<?> c = workerGroup.shutdownGracefully();\n    Future<?> c2 = bossGroup.shutdownGracefully();\n    serverChannel.close();\n    c.syncUninterruptibly();\n    c2.syncUninterruptibly();\n    regionProvider.close();\n    if (mainThread != null) {\n      mainThread.interrupt();\n    }\n    for (ScheduledFuture<?> f : expirationFutures.values()) {\n      f.cancel(true);\n    }\n    expirationFutures.clear();\n    expirationExecutor.shutdownNow();\n    closeFuture.syncUninterruptibly();\n    shutdown = true;\n  }\n}",
        "reject_response": "public synchronized void shutdown() {\n  if (!shutdown) {\n    if (logger.infoEnabled()) {\n      logger.info(\"GeodeRedisServer shutting down\");\n    }\n    ChannelFuture closeFuture = serverChannel.closeFuture();\n    Future<?> c = workerGroup.shutdownGracefully();\n    Future<?> c2 = bossGroup.shutdownGracefully();\n    serverChannel.close();\n    c.syncUninterruptibly();\n    c2.syncUninterruptibly();\n    regionProvider.close();\n    if (mainThread != null) {\n      mainThread.interrupt();\n    }\n    for (ScheduledFuture<?> f : expirationFutures.values()) {\n      f.cancel(true);\n    }\n    expirationFutures.clear();\n    expirationExecutor.shutdownNow();\n    closeFuture.syncUninterruptibly();\n    shutdown = true;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2826,
        "instruction": "@SuppressWarnings(\"TooBroadScope\")\n@Override public WALPointer log(WALRecord rec) throws IgniteCheckedException, StorageException {\n    if (serializer == null || mode == WALMode.NONE)\n        return null;\n\n    FileWriteHandle currWrHandle = currentHandle();\n\n    // Logging was not resumed yet.\n    if (currWrHandle == null)\n        return null;\n\n    // Need to calculate record size first.\n    rec.size(serializer.size(rec));\n\n    while (true) {\n        if (rec.rollOver()){\n            assert cctx.database().checkpointLockIsHeldByThread();\n\n            long idx = currWrHandle.idx;\n\n            currWrHandle.buf.close();\n\n            currWrHandle = rollOver(currWrHandle);\n\n            if (log != null && log.isInfoEnabled())\n        }\n\n        WALPointer ptr = currWrHandle.addRecord(rec);\n\n        if (ptr != null) {\n            metrics.onWalRecordLogged();\n\n            lastWALPtr.set(ptr);\n\n            if (walAutoArchiveAfterInactivity > 0)\n                lastRecordLoggedMs.set(U.currentTimeMillis());\n\n            return ptr;\n        }\n        else\n            currWrHandle = rollOver(currWrHandle);\n\n        checkNode();\n\n        if (isStopping())\n            throw new IgniteCheckedException(\"Stopping.\");\n    }\n}",
        "accept_response": "@SuppressWarnings(\"TooBroadScope\")\n@Override public WALPointer log(WALRecord rec) throws IgniteCheckedException, StorageException {\n    if (serializer == null || mode == WALMode.NONE)\n        return null;\n\n    FileWriteHandle currWrHandle = currentHandle();\n\n    // Logging was not resumed yet.\n    if (currWrHandle == null)\n        return null;\n\n    // Need to calculate record size first.\n    rec.size(serializer.size(rec));\n\n    while (true) {\n        if (rec.rollOver()){\n            assert cctx.database().checkpointLockIsHeldByThread();\n\n            long idx = currWrHandle.idx;\n\n            currWrHandle.buf.close();\n\n            currWrHandle = rollOver(currWrHandle);\n\n            if (log != null && log.isInfoEnabled())\n                log.info(\"Rollover segment [\" + idx + \" to \" + currWrHandle.idx + \"], recordType=\" + rec.type());\n        }\n\n        WALPointer ptr = currWrHandle.addRecord(rec);\n\n        if (ptr != null) {\n            metrics.onWalRecordLogged();\n\n            lastWALPtr.set(ptr);\n\n            if (walAutoArchiveAfterInactivity > 0)\n                lastRecordLoggedMs.set(U.currentTimeMillis());\n\n            return ptr;\n        }\n        else\n            currWrHandle = rollOver(currWrHandle);\n\n        checkNode();\n\n        if (isStopping())\n            throw new IgniteCheckedException(\"Stopping.\");\n    }\n}",
        "reject_response": "@SuppressWarnings(\"TooBroadScope\")\n@Override public WALPointer log(WALRecord rec) throws IgniteCheckedException, StorageException {\n    if (serializer == null || mode == WALMode.NONE)\n        return null;\n\n    FileWriteHandle currWrHandle = currentHandle();\n\n    // Logging was not resumed yet.\n    if (currWrHandle == null)\n        return null;\n\n    // Need to calculate record size first.\n    rec.size(serializer.size(rec));\n\n    while (true) {\n        if (rec.rollOver()){\n            assert cctx.database().checkpointLockIsHeldByThread();\n\n            long idx = currWrHandle.idx;\n\n            currWrHandle.buf.close();\n\n            currWrHandle = rollOver(currWrHandle);\n\n            if (log != null && log.isInfoEnabled())\n            if (log != null && log.isDebugEnabled())\n                log.debug(\"Rollover segment [\" + idx + \" to \" + currWrHandle.idx + \"], recordType=\" + rec.type());\n        }\n\n        WALPointer ptr = currWrHandle.addRecord(rec);\n\n        if (ptr != null) {\n            metrics.onWalRecordLogged();\n\n            lastWALPtr.set(ptr);\n\n            if (walAutoArchiveAfterInactivity > 0)\n                lastRecordLoggedMs.set(U.currentTimeMillis());\n\n            return ptr;\n        }\n        else\n            currWrHandle = rollOver(currWrHandle);\n\n        checkNode();\n\n        if (isStopping())\n            throw new IgniteCheckedException(\"Stopping.\");\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3180,
        "instruction": "@Override\npublic void restartContainerAsync(ContainerId containerId){\n  if (!(callbackHandler instanceof AbstractCallbackHandler)) {\n    LOG.error(\"Callback handler does not implement container restart \"\n        + \"callback methods\");\n    return;\n  }\n  AbstractCallbackHandler handler = (AbstractCallbackHandler) callbackHandler;\n  if (containers.get(containerId) == null) {\n    handler.onContainerRestartError(\n        containerId, RPCUtil.getRemoteException(\n            \"Container \" + containerId + \" is not started\"));\n  }\n  try {\n    events.put(new ContainerEvent(containerId,\n        client.getNodeIdOfStartedContainer(containerId),\n        null, ContainerEventType.RESTART_CONTAINER));\n  } catch (InterruptedException e) {\n    handler.onContainerRestartError(containerId, e);\n  }\n}",
        "accept_response": "@Override\npublic void restartContainerAsync(ContainerId containerId){\n  if (!(callbackHandler instanceof AbstractCallbackHandler)) {\n    LOG.error(\"Callback handler does not implement container restart \"\n        + \"callback methods\");\n    return;\n  }\n  AbstractCallbackHandler handler = (AbstractCallbackHandler) callbackHandler;\n  if (containers.get(containerId) == null) {\n    handler.onContainerRestartError(\n        containerId, RPCUtil.getRemoteException(\n            \"Container \" + containerId + \" is not started\"));\n  }\n  try {\n    events.put(new ContainerEvent(containerId,\n        client.getNodeIdOfStartedContainer(containerId),\n        null, ContainerEventType.RESTART_CONTAINER));\n  } catch (InterruptedException e) {\n    LOG.warn(\"Exception when scheduling the event of restart of Container {}\", containerId);\n    handler.onContainerRestartError(containerId, e);\n  }\n}",
        "reject_response": "@Override\npublic void restartContainerAsync(ContainerId containerId){\n  if (!(callbackHandler instanceof AbstractCallbackHandler)) {\n    LOG.error(\"Callback handler does not implement container restart \"\n        + \"callback methods\");\n    return;\n  }\n  AbstractCallbackHandler handler = (AbstractCallbackHandler) callbackHandler;\n  if (containers.get(containerId) == null) {\n    handler.onContainerRestartError(\n        containerId, RPCUtil.getRemoteException(\n            \"Container \" + containerId + \" is not started\"));\n  }\n  try {\n    events.put(new ContainerEvent(containerId,\n        client.getNodeIdOfStartedContainer(containerId),\n        null, ContainerEventType.RESTART_CONTAINER));\n  } catch (InterruptedException e) {\n    LOG.warn(\"Exception when scheduling the event of restart of \"\n        + \"Container \" + containerId);\n    handler.onContainerRestartError(containerId, e);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2489,
        "instruction": "@VisibleForTesting\nstatic void startJob(\n    Job job,\n    ApiErrorExtractor errorExtractor,\n    Bigquery client,\n    Sleeper sleeper,\n    BackOff backoff) throws IOException, InterruptedException {\n  JobReference jobRef = job.getJobReference();\n  Exception lastException = null;\n  do {\n    try {\n      client.jobs().insert(jobRef.getProjectId(), job).execute();\n      return; // SUCCEEDED\n    } catch (GoogleJsonResponseException e) {\n      if (errorExtractor.itemAlreadyExists(e)) {\n        return; // SUCCEEDED\n      }\n      // ignore and retry\n      LOG.info(\"Ignore the error and retry inserting the job.\", e);\n      lastException = e;\n    } catch (IOException e) {\n      // ignore and retry\n      LOG.info(\"Ignore the error and retry inserting the job.\", e);\n      lastException = e;\n    }\n  } while (nextBackOff(sleeper, backoff));\n  throw new IOException(\n      String.format(\n          \"Unable to insert job: %s, aborting after %d .\",\n          jobRef.getJobId(), MAX_RPC_RETRIES),\n      lastException);\n}",
        "accept_response": "@VisibleForTesting\nstatic void startJob(\n    Job job,\n    ApiErrorExtractor errorExtractor,\n    Bigquery client,\n    Sleeper sleeper,\n    BackOff backoff) throws IOException, InterruptedException {\n  JobReference jobRef = job.getJobReference();\n  Exception lastException = null;\n  do {\n    try {\n      client.jobs().insert(jobRef.getProjectId(), job).execute();\n      LOG.info(\"Started BigQuery job: {}.\\n{}\", jobRef,\n          formatBqStatusCommand(jobRef.getProjectId(), jobRef.getJobId()));\n      return; // SUCCEEDED\n    } catch (GoogleJsonResponseException e) {\n      if (errorExtractor.itemAlreadyExists(e)) {\n        return; // SUCCEEDED\n      }\n      // ignore and retry\n      LOG.info(\"Ignore the error and retry inserting the job.\", e);\n      lastException = e;\n    } catch (IOException e) {\n      // ignore and retry\n      LOG.info(\"Ignore the error and retry inserting the job.\", e);\n      lastException = e;\n    }\n  } while (nextBackOff(sleeper, backoff));\n  throw new IOException(\n      String.format(\n          \"Unable to insert job: %s, aborting after %d .\",\n          jobRef.getJobId(), MAX_RPC_RETRIES),\n      lastException);\n}",
        "reject_response": "@VisibleForTesting\nstatic void startJob(\n    Job job,\n    ApiErrorExtractor errorExtractor,\n    Bigquery client,\n    Sleeper sleeper,\n    BackOff backoff) throws IOException, InterruptedException {\n  JobReference jobRef = job.getJobReference();\n  Exception lastException = null;\n  do {\n    try {\n      client.jobs().insert(jobRef.getProjectId(), job).execute();\n      LOG.info(\"Started BigQuery job: {}.\", jobRef);\n      return; // SUCCEEDED\n    } catch (GoogleJsonResponseException e) {\n      if (errorExtractor.itemAlreadyExists(e)) {\n        return; // SUCCEEDED\n      }\n      // ignore and retry\n      LOG.info(\"Ignore the error and retry inserting the job.\", e);\n      lastException = e;\n    } catch (IOException e) {\n      // ignore and retry\n      LOG.info(\"Ignore the error and retry inserting the job.\", e);\n      lastException = e;\n    }\n  } while (nextBackOff(sleeper, backoff));\n  throw new IOException(\n      String.format(\n          \"Unable to insert job: %s, aborting after %d .\",\n          jobRef.getJobId(), MAX_RPC_RETRIES),\n      lastException);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2642,
        "instruction": "Double getDoubleAttribute(Object object, String name) {\n  if (object == null) {\n    return Double.valueOf(0);\n  }\n\n  try {\n    if (object instanceof Float) {\n      return BigDecimal.valueOf((Float) object).doubleValue();\n    } else if (object instanceof Double) {\n      return (Double) object;\n    } else {\n      return Double.valueOf(0);\n    }\n  } catch (Exception e) {\n    logger.info(\"Exception occurred: \", e);\n    return Double.valueOf(0);\n  }\n}",
        "accept_response": "Double getDoubleAttribute(Object object, String name) {\n  if (object == null) {\n    return Double.valueOf(0);\n  }\n\n  try {\n    if (object instanceof Float) {\n      return BigDecimal.valueOf((Float) object).doubleValue();\n    } else if (object instanceof Double) {\n      return (Double) object;\n    } else {\n      logger.info(\n          \"************************Unexpected type for attribute: {}; Expected type: {}; Received type: {}************************\",\n          name, Double.class.getName(), object.getClass().getName());\n      return Double.valueOf(0);\n    }\n  } catch (Exception e) {\n    logger.info(\"Exception occurred: \", e);\n    return Double.valueOf(0);\n  }\n}",
        "reject_response": "Double getDoubleAttribute(Object object, String name) {\n  if (object == null) {\n    return Double.valueOf(0);\n  }\n\n  try {\n    if (object instanceof Float) {\n      return BigDecimal.valueOf((Float) object).doubleValue();\n    } else if (object instanceof Double) {\n      return (Double) object;\n    } else {\n      if (LOGGER.infoEnabled()) {\n        LOGGER.info(\"************************Unexpected type for attribute: \" + name\n            + \" Expected type: \" + Double.class.getName() + \" Received type: \"\n            + object.getClass().getName() + \"************************\");\n      }\n      return Double.valueOf(0);\n    }\n  } catch (Exception e) {\n    logger.info(\"Exception occurred: \", e);\n    return Double.valueOf(0);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2935,
        "instruction": "private void reconfigureConnectorTasksWithRetry(long initialRequestTime, final String connName) {\n    reconfigureConnector(connName, new Callback<Void>() {\n        @Override\n        public void onCompletion(Throwable error, Void result) {\n            // If we encountered an error, we don't have much choice but to just retry. If we don't, we could get\n            // stuck with a connector that thinks it has generated tasks, but wasn't actually successful and therefore\n            // never makes progress. The retry has to run through a DistributedHerderRequest since this callback could be happening\n            // from the HTTP request forwarding thread.\n            if (error != null) {\n                if (isPossibleExpiredKeyException(initialRequestTime, error)) {\n                    log.debug(\"Failed to reconfigure connector's tasks, possibly due to expired session key. Retrying after backoff\");\n                } else {\n                    log.error(\"Failed to reconfigure connector's tasks, retrying after backoff:\", error);\n                }\n                addRequest(RECONFIGURE_CONNECTOR_TASKS_BACKOFF_MS,\n                        new Callable<Void>() {\n                            @Override\n                            public Void call() throws Exception {\n                                reconfigureConnectorTasksWithRetry(initialRequestTime, connName);\n                                return null;\n                            }\n                        }, new Callback<Void>() {\n                            @Override\n                            public void onCompletion(Throwable error, Void result) {\n                                if (error != null) {\n                                    log.error(\"Unexpected error during connector task reconfiguration: \", error);\n                                }\n                            }\n                        }\n                );\n            }\n        }\n    });\n}",
        "accept_response": "private void reconfigureConnectorTasksWithRetry(long initialRequestTime, final String connName) {\n    reconfigureConnector(connName, new Callback<Void>() {\n        @Override\n        public void onCompletion(Throwable error, Void result) {\n            // If we encountered an error, we don't have much choice but to just retry. If we don't, we could get\n            // stuck with a connector that thinks it has generated tasks, but wasn't actually successful and therefore\n            // never makes progress. The retry has to run through a DistributedHerderRequest since this callback could be happening\n            // from the HTTP request forwarding thread.\n            if (error != null) {\n                if (isPossibleExpiredKeyException(initialRequestTime, error)) {\n                    log.debug(\"Failed to reconfigure connector's tasks, possibly due to expired session key. Retrying after backoff\");\n                } else {\n                    log.error(\"Failed to reconfigure connector's tasks, retrying after backoff:\", error);\n                }\n                addRequest(RECONFIGURE_CONNECTOR_TASKS_BACKOFF_MS,\n                        new Callable<Void>() {\n                            @Override\n                            public Void call() throws Exception {\n                                reconfigureConnectorTasksWithRetry(initialRequestTime, connName);\n                                return null;\n                            }\n                        }, new Callback<Void>() {\n                            @Override\n                            public void onCompletion(Throwable error, Void result) {\n                                if (error != null) {\n                                    log.error(\"Unexpected error during connector task reconfiguration: \", error);\n                                    log.error(\"Task reconfiguration for {} failed unexpectedly, this connector will not be properly reconfigured unless manually triggered.\", connName);\n                                }\n                            }\n                        }\n                );\n            }\n        }\n    });\n}",
        "reject_response": "private void reconfigureConnectorTasksWithRetry(long initialRequestTime, final String connName) {\n    reconfigureConnector(connName, new Callback<Void>() {\n        @Override\n        public void onCompletion(Throwable error, Void result) {\n            // If we encountered an error, we don't have much choice but to just retry. If we don't, we could get\n            // stuck with a connector that thinks it has generated tasks, but wasn't actually successful and therefore\n            // never makes progress. The retry has to run through a DistributedHerderRequest since this callback could be happening\n            // from the HTTP request forwarding thread.\n            if (error != null) {\n                if (isPossibleExpiredKeyException(initialRequestTime, error)) {\n                    log.debug(\"Failed to reconfigure connector's tasks, possibly due to expired session key. Retrying after backoff\");\n                } else {\n                    log.error(\"Failed to reconfigure connector's tasks, retrying after backoff:\", error);\n                }\n                addRequest(RECONFIGURE_CONNECTOR_TASKS_BACKOFF_MS,\n                        new Callable<Void>() {\n                            @Override\n                            public Void call() throws Exception {\n                                reconfigureConnectorTasksWithRetry(initialRequestTime, connName);\n                                return null;\n                            }\n                        }, new Callback<Void>() {\n                            @Override\n                            public void onCompletion(Throwable error, Void result) {\n                                if (error != null) {\n                                    log.error(\"Unexpected error during connector task reconfiguration: \", error);\n                                log.error(\"Unexpected error during connector task reconfiguration: \", error);\n                                log.error(\"Task reconfiguration for {} failed unexpectedly, this connector will not be properly reconfigured unless manually triggered.\", connName);\n                                }\n                            }\n                        }\n                );\n            }\n        }\n    });\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2426,
        "instruction": "public void doStart() throws Exception {\n\n    if (lockAcquireSleepInterval < lockable.getLockKeepAlivePeriod()) {\n        LOG.warn(\"LockableService keep alive period: \" + lockable.getLockKeepAlivePeriod()\n                + \", which renews the lease, is greater than lockAcquireSleepInterval: \" + lockAcquireSleepInterval\n                + \", the lease duration. These values will allow the lease to expire.\");\n    }\n\n    LOG.info(getLeaseHolderId() + \" attempting to acquire exclusive lease to become the master\");\n    String sql = getStatements().getLeaseObtainStatement();\n    LOG.debug(getLeaseHolderId() + \" locking Query is \"+sql);\n\n    long now = 0l;\n    while (!isStopping()) {\n        Connection connection = null;\n        PreparedStatement statement = null;\n        try {\n            connection = getConnection();\n            initTimeDiff(connection);\n\n            statement = connection.prepareStatement(sql);\n            setQueryTimeout(statement);\n\n            now = System.currentTimeMillis() + diffFromCurrentTime;\n            statement.setString(1, getLeaseHolderId());\n            statement.setLong(2, now + lockAcquireSleepInterval);\n            statement.setLong(3, now);\n\n            int result = statement.executeUpdate();\n            if (result == 1) {\n                // we got the lease, verify we still have it\n                if (keepAlive()) {\n                    break;\n                }\n            }\n\n            reportLeasOwnerShipAndDuration(connection);\n\n        } catch (Exception e) {\n            LOG.warn(getLeaseHolderId() + \" lease acquire failure: \"+ e, e);\n            if (isStopping()) {\n                throw new Exception(\n                        \"Cannot start broker as being asked to shut down. \"\n                                + \"Interrupted attempt to acquire lock: \"\n                                + e, e);\n            }\n            if (handleStartException) {\n                lockable.getBrokerService().handleIOException(IOExceptionSupport.create(e));\n            }\n        } finally {\n            close(statement);\n            close(connection);\n        }\n\n        TimeUnit.MILLISECONDS.sleep(lockAcquireSleepInterval);\n    }\n    if (isStopping()) {\n        throw new RuntimeException(getLeaseHolderId() + \" failing lease acquire due to stop\");\n    }\n\n    LOG.info(getLeaseHolderId() + \", becoming master with lease expiry \" + new Date(now + lockAcquireSleepInterval) + \" on dataSource: \" + dataSource);\n}",
        "accept_response": "public void doStart() throws Exception {\n\n    if (lockAcquireSleepInterval < lockable.getLockKeepAlivePeriod()) {\n        LOG.warn(\"LockableService keep alive period: \" + lockable.getLockKeepAlivePeriod()\n                + \", which renews the lease, is greater than lockAcquireSleepInterval: \" + lockAcquireSleepInterval\n                + \", the lease duration. These values will allow the lease to expire.\");\n    }\n\n    LOG.info(getLeaseHolderId() + \" attempting to acquire exclusive lease to become the master\");\n    String sql = getStatements().getLeaseObtainStatement();\n    LOG.debug(getLeaseHolderId() + \" locking Query is \"+sql);\n\n    long now = 0l;\n    while (!isStopping()) {\n        Connection connection = null;\n        PreparedStatement statement = null;\n        try {\n            connection = getConnection();\n            initTimeDiff(connection);\n\n            statement = connection.prepareStatement(sql);\n            setQueryTimeout(statement);\n\n            now = System.currentTimeMillis() + diffFromCurrentTime;\n            statement.setString(1, getLeaseHolderId());\n            statement.setLong(2, now + lockAcquireSleepInterval);\n            statement.setLong(3, now);\n\n            int result = statement.executeUpdate();\n            if (result == 1) {\n                // we got the lease, verify we still have it\n                if (keepAlive()) {\n                    break;\n                }\n            }\n\n            reportLeasOwnerShipAndDuration(connection);\n\n        } catch (Exception e) {\n            LOG.warn(getLeaseHolderId() + \" lease acquire failure: \"+ e, e);\n            if (isStopping()) {\n                throw new Exception(\n                        \"Cannot start broker as being asked to shut down. \"\n                                + \"Interrupted attempt to acquire lock: \"\n                                + e, e);\n            }\n            if (handleStartException) {\n                lockable.getBrokerService().handleIOException(IOExceptionSupport.create(e));\n            }\n        } finally {\n            close(statement);\n            close(connection);\n        }\n\n        LOG.debug(getLeaseHolderId() + \" failed to acquire lease.  Sleeping for \" + lockAcquireSleepInterval + \" milli(s) before trying again...\");\n        TimeUnit.MILLISECONDS.sleep(lockAcquireSleepInterval);\n    }\n    if (isStopping()) {\n        throw new RuntimeException(getLeaseHolderId() + \" failing lease acquire due to stop\");\n    }\n\n    LOG.info(getLeaseHolderId() + \", becoming master with lease expiry \" + new Date(now + lockAcquireSleepInterval) + \" on dataSource: \" + dataSource);\n}",
        "reject_response": "public void doStart() throws Exception {\n\n    if (lockAcquireSleepInterval < lockable.getLockKeepAlivePeriod()) {\n        LOG.warn(\"LockableService keep alive period: \" + lockable.getLockKeepAlivePeriod()\n                + \", which renews the lease, is greater than lockAcquireSleepInterval: \" + lockAcquireSleepInterval\n                + \", the lease duration. These values will allow the lease to expire.\");\n    }\n\n    LOG.info(getLeaseHolderId() + \" attempting to acquire exclusive lease to become the master\");\n    String sql = getStatements().getLeaseObtainStatement();\n    LOG.debug(getLeaseHolderId() + \" locking Query is \"+sql);\n\n    long now = 0l;\n    while (!isStopping()) {\n        Connection connection = null;\n        PreparedStatement statement = null;\n        try {\n            connection = getConnection();\n            initTimeDiff(connection);\n\n            statement = connection.prepareStatement(sql);\n            setQueryTimeout(statement);\n\n            now = System.currentTimeMillis() + diffFromCurrentTime;\n            statement.setString(1, getLeaseHolderId());\n            statement.setLong(2, now + lockAcquireSleepInterval);\n            statement.setLong(3, now);\n\n            int result = statement.executeUpdate();\n            if (result == 1) {\n                // we got the lease, verify we still have it\n                if (keepAlive()) {\n                    break;\n                }\n            }\n\n            reportLeasOwnerShipAndDuration(connection);\n\n        } catch (Exception e) {\n            LOG.warn(getLeaseHolderId() + \" lease acquire failure: \"+ e, e);\n            if (isStopping()) {\n                throw new Exception(\n                        \"Cannot start broker as being asked to shut down. \"\n                                + \"Interrupted attempt to acquire lock: \"\n                                + e, e);\n            }\n            if (handleStartException) {\n                lockable.getBrokerService().handleIOException(IOExceptionSupport.create(e));\n            }\n        } finally {\n            close(statement);\n            close(connection);\n        }\n\n        LOG.info(getLeaseHolderId() + \" failed to acquire lease.  Sleeping for \" + lockAcquireSleepInterval + \" milli(s) before trying again...\");\n        TimeUnit.MILLISECONDS.sleep(lockAcquireSleepInterval);\n    }\n    if (isStopping()) {\n        throw new RuntimeException(getLeaseHolderId() + \" failing lease acquire due to stop\");\n    }\n\n    LOG.info(getLeaseHolderId() + \", becoming master with lease expiry \" + new Date(now + lockAcquireSleepInterval) + \" on dataSource: \" + dataSource);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3070,
        "instruction": "private void init(String hostToBind, int portToBind) {\n\n  IOMode ioMode = IOMode.valueOf(conf.ioMode());\n  EventLoopGroup bossGroup =\n    NettyUtils.createEventLoop(ioMode, conf.serverThreads(), \"shuffle-server\");\n  EventLoopGroup workerGroup = bossGroup;\n\n  PooledByteBufAllocator allocator = NettyUtils.createPooledByteBufAllocator(\n    conf.preferDirectBufs(), true /* allowCache */, conf.serverThreads());\n\n  bootstrap = new ServerBootstrap()\n    .group(bossGroup, workerGroup)\n    .channel(NettyUtils.getServerChannelClass(ioMode))\n    .option(ChannelOption.ALLOCATOR, allocator)\n    .childOption(ChannelOption.ALLOCATOR, allocator);\n\n  if (conf.backLog() > 0) {\n    bootstrap.option(ChannelOption.SO_BACKLOG, conf.backLog());\n  }\n\n  if (conf.receiveBuf() > 0) {\n    bootstrap.childOption(ChannelOption.SO_RCVBUF, conf.receiveBuf());\n  }\n\n  if (conf.sendBuf() > 0) {\n    bootstrap.childOption(ChannelOption.SO_SNDBUF, conf.sendBuf());\n  }\n\n  bootstrap.childHandler(new ChannelInitializer<SocketChannel>() {\n    @Override\n    protected void initChannel(SocketChannel ch) throws Exception {\n      RpcHandler rpcHandler = appRpcHandler;\n      for (TransportServerBootstrap bootstrap : bootstraps) {\n        rpcHandler = bootstrap.doBootstrap(ch, rpcHandler);\n      }\n      context.initializePipeline(ch, rpcHandler);\n    }\n  });\n\n  InetSocketAddress address = hostToBind == null ?\n      new InetSocketAddress(portToBind): new InetSocketAddress(hostToBind, portToBind);\n  channelFuture = bootstrap.bind(address);\n  channelFuture.syncUninterruptibly();\n\n  port = ((InetSocketAddress) channelFuture.channel().localAddress()).getPort();\n}",
        "accept_response": "private void init(String hostToBind, int portToBind) {\n\n  IOMode ioMode = IOMode.valueOf(conf.ioMode());\n  EventLoopGroup bossGroup =\n    NettyUtils.createEventLoop(ioMode, conf.serverThreads(), \"shuffle-server\");\n  EventLoopGroup workerGroup = bossGroup;\n\n  PooledByteBufAllocator allocator = NettyUtils.createPooledByteBufAllocator(\n    conf.preferDirectBufs(), true /* allowCache */, conf.serverThreads());\n\n  bootstrap = new ServerBootstrap()\n    .group(bossGroup, workerGroup)\n    .channel(NettyUtils.getServerChannelClass(ioMode))\n    .option(ChannelOption.ALLOCATOR, allocator)\n    .childOption(ChannelOption.ALLOCATOR, allocator);\n\n  if (conf.backLog() > 0) {\n    bootstrap.option(ChannelOption.SO_BACKLOG, conf.backLog());\n  }\n\n  if (conf.receiveBuf() > 0) {\n    bootstrap.childOption(ChannelOption.SO_RCVBUF, conf.receiveBuf());\n  }\n\n  if (conf.sendBuf() > 0) {\n    bootstrap.childOption(ChannelOption.SO_SNDBUF, conf.sendBuf());\n  }\n\n  bootstrap.childHandler(new ChannelInitializer<SocketChannel>() {\n    @Override\n    protected void initChannel(SocketChannel ch) throws Exception {\n      RpcHandler rpcHandler = appRpcHandler;\n      for (TransportServerBootstrap bootstrap : bootstraps) {\n        rpcHandler = bootstrap.doBootstrap(ch, rpcHandler);\n      }\n      context.initializePipeline(ch, rpcHandler);\n    }\n  });\n\n  InetSocketAddress address = hostToBind == null ?\n      new InetSocketAddress(portToBind): new InetSocketAddress(hostToBind, portToBind);\n  channelFuture = bootstrap.bind(address);\n  channelFuture.syncUninterruptibly();\n\n  port = ((InetSocketAddress) channelFuture.channel().localAddress()).getPort();\n  logger.debug(\"Shuffle server started on port: {}\", port);\n}",
        "reject_response": "private void init(String hostToBind, int portToBind) {\n\n  IOMode ioMode = IOMode.valueOf(conf.ioMode());\n  EventLoopGroup bossGroup =\n    NettyUtils.createEventLoop(ioMode, conf.serverThreads(), \"shuffle-server\");\n  EventLoopGroup workerGroup = bossGroup;\n\n  PooledByteBufAllocator allocator = NettyUtils.createPooledByteBufAllocator(\n    conf.preferDirectBufs(), true /* allowCache */, conf.serverThreads());\n\n  bootstrap = new ServerBootstrap()\n    .group(bossGroup, workerGroup)\n    .channel(NettyUtils.getServerChannelClass(ioMode))\n    .option(ChannelOption.ALLOCATOR, allocator)\n    .childOption(ChannelOption.ALLOCATOR, allocator);\n\n  if (conf.backLog() > 0) {\n    bootstrap.option(ChannelOption.SO_BACKLOG, conf.backLog());\n  }\n\n  if (conf.receiveBuf() > 0) {\n    bootstrap.childOption(ChannelOption.SO_RCVBUF, conf.receiveBuf());\n  }\n\n  if (conf.sendBuf() > 0) {\n    bootstrap.childOption(ChannelOption.SO_SNDBUF, conf.sendBuf());\n  }\n\n  bootstrap.childHandler(new ChannelInitializer<SocketChannel>() {\n    @Override\n    protected void initChannel(SocketChannel ch) throws Exception {\n      RpcHandler rpcHandler = appRpcHandler;\n      for (TransportServerBootstrap bootstrap : bootstraps) {\n        rpcHandler = bootstrap.doBootstrap(ch, rpcHandler);\n      }\n      context.initializePipeline(ch, rpcHandler);\n    }\n  });\n\n  InetSocketAddress address = hostToBind == null ?\n      new InetSocketAddress(portToBind): new InetSocketAddress(hostToBind, portToBind);\n  channelFuture = bootstrap.bind(address);\n  channelFuture.syncUninterruptibly();\n\n  port = ((InetSocketAddress) channelFuture.channel().localAddress()).getPort();\n  logger.debug(\"Shuffle server started on port :\" + port);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3085,
        "instruction": "public void fail(Long offset) {\n    if (offset < _emittedToOffset - _spoutConfig.maxOffsetBehind) {\n        LOG.info(\n                \"Skipping failed tuple at offset=\" + offset +\n                        \" because it's more than maxOffsetBehind=\" + _spoutConfig.maxOffsetBehind +\n                        \" behind _emittedToOffset=\" + _emittedToOffset\n        );\n    } else {\n        numberFailed++;\n        if (numberAcked == 0 && numberFailed > _spoutConfig.maxOffsetBehind) {\n            throw new RuntimeException(\"Too many tuple failures\");\n        }\n\n        this._failedMsgRetryManager.failed(offset);\n    }\n}",
        "accept_response": "public void fail(Long offset) {\n    if (offset < _emittedToOffset - _spoutConfig.maxOffsetBehind) {\n        LOG.info(\n                \"Skipping failed tuple at offset=\" + offset +\n                        \" because it's more than maxOffsetBehind=\" + _spoutConfig.maxOffsetBehind +\n                        \" behind _emittedToOffset=\" + _emittedToOffset\n        );\n    } else {\n        LOG.debug(\"failing at offset={} with _pending.size()={} pending and _emittedToOffset={}\", offset, _pending.size(), _emittedToOffset);\n        numberFailed++;\n        if (numberAcked == 0 && numberFailed > _spoutConfig.maxOffsetBehind) {\n            throw new RuntimeException(\"Too many tuple failures\");\n        }\n\n        this._failedMsgRetryManager.failed(offset);\n    }\n}",
        "reject_response": "public void fail(Long offset) {\n    if (offset < _emittedToOffset - _spoutConfig.maxOffsetBehind) {\n        LOG.info(\n                \"Skipping failed tuple at offset=\" + offset +\n                        \" because it's more than maxOffsetBehind=\" + _spoutConfig.maxOffsetBehind +\n                        \" behind _emittedToOffset=\" + _emittedToOffset\n        );\n    } else {\n        LOG.debug(\"failing at offset=\" + offset + \" with _pending.size()=\" + _pending.size() + \" pending and _emittedToOffset=\" + _emittedToOffset);\n        numberFailed++;\n        if (numberAcked == 0 && numberFailed > _spoutConfig.maxOffsetBehind) {\n            throw new RuntimeException(\"Too many tuple failures\");\n        }\n\n        this._failedMsgRetryManager.failed(offset);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2989,
        "instruction": "@GET\n@Path(KMSRESTConstants.KEYS_NAMES_RESOURCE)\n@Produces(MediaType.APPLICATION_JSON)\npublic Response getKeyNames(@Context HttpServletRequest request) throws Exception {\n  try {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Entering getKeyNames method.\");\n    }\n    KMSWebApp.getAdminCallsMeter().mark();\n    UserGroupInformation user = HttpUserGroupInformation.get();\n    assertAccess(Type.GET_KEYS, user, KMSOp.GET_KEYS, request.getRemoteAddr());\n    List<String> json = user.doAs(new PrivilegedExceptionAction<List<String>>() {\n      @Override\n      public List<String> run() throws Exception {\n        return provider.getKeys();\n      }\n    });\n    kmsAudit.ok(user, KMSOp.GET_KEYS, \"\");\n    return Response.ok().type(MediaType.APPLICATION_JSON).entity(json).build();\n  } catch (Exception e) {\n    LOG.error(\"Exception in getkeyNames.\", e);\n    throw e;\n  }\n}",
        "accept_response": "@GET\n@Path(KMSRESTConstants.KEYS_NAMES_RESOURCE)\n@Produces(MediaType.APPLICATION_JSON)\npublic Response getKeyNames(@Context HttpServletRequest request) throws Exception {\n  try {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Entering getKeyNames method.\");\n    }\n    KMSWebApp.getAdminCallsMeter().mark();\n    UserGroupInformation user = HttpUserGroupInformation.get();\n    assertAccess(Type.GET_KEYS, user, KMSOp.GET_KEYS, request.getRemoteAddr());\n    List<String> json = user.doAs(new PrivilegedExceptionAction<List<String>>() {\n      @Override\n      public List<String> run() throws Exception {\n        return provider.getKeys();\n      }\n    });\n    kmsAudit.ok(user, KMSOp.GET_KEYS, \"\");\n    if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Exiting getKeyNames method.\");\n    }\n    return Response.ok().type(MediaType.APPLICATION_JSON).entity(json).build();\n  } catch (Exception e) {\n    LOG.error(\"Exception in getkeyNames.\", e);\n    throw e;\n  }\n}",
        "reject_response": "@GET\n@Path(KMSRESTConstants.KEYS_NAMES_RESOURCE)\n@Produces(MediaType.APPLICATION_JSON)\npublic Response getKeyNames(@Context HttpServletRequest request) throws Exception {\n  try {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Entering getKeyNames method.\");\n    }\n    KMSWebApp.getAdminCallsMeter().mark();\n    UserGroupInformation user = HttpUserGroupInformation.get();\n    assertAccess(Type.GET_KEYS, user, KMSOp.GET_KEYS, request.getRemoteAddr());\n    List<String> json = user.doAs(new PrivilegedExceptionAction<List<String>>() {\n      @Override\n      public List<String> run() throws Exception {\n        return provider.getKeys();\n      }\n    });\n    kmsAudit.ok(user, KMSOp.GET_KEYS, \"\");\n    if (LOG.isDebugEnabled()) {\n    LOG.info(\"Exiting getKeyNames method.\");\n    }\n    return Response.ok().type(MediaType.APPLICATION_JSON).entity(json).build();\n  } catch (Exception e) {\n    LOG.error(\"Exception in getkeyNames.\", e);\n    throw e;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2468,
        "instruction": "@Override\npublic void start() {\n   if (persisted) {\n      persistedCacheEntries = storageManager.getPersistedKeyValuePairs(id);\n\n      if (persistedCacheEntries != null) {\n         for (Map.Entry<String, PersistedKeyValuePair> cacheEntry : persistedCacheEntries.entrySet()) {\n            cache.put(cacheEntry.getKey(), cacheEntry.getValue().getValue());\n\n         }\n      }\n   }\n\n   running = true;\n}",
        "accept_response": "@Override\npublic void start() {\n   if (persisted) {\n      persistedCacheEntries = storageManager.getPersistedKeyValuePairs(id);\n\n      if (persistedCacheEntries != null) {\n         for (Map.Entry<String, PersistedKeyValuePair> cacheEntry : persistedCacheEntries.entrySet()) {\n            cache.put(cacheEntry.getKey(), cacheEntry.getValue().getValue());\n\n            logger.debug(\"Restored persisted cache entry during start: {}\", cacheEntry);\n         }\n      }\n   }\n\n   running = true;\n}",
        "reject_response": "@Override\npublic void start() {\n   if (persisted) {\n      persistedCacheEntries = storageManager.getPersistedKeyValuePairs(id);\n\n      if (persistedCacheEntries != null) {\n         for (Map.Entry<String, PersistedKeyValuePair> cacheEntry : persistedCacheEntries.entrySet()) {\n            cache.put(cacheEntry.getKey(), cacheEntry.getValue().getValue());\n\n            logger.info(cacheEntry.toString());\n         }\n      }\n   }\n\n   running = true;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2714,
        "instruction": "@Override\npublic void run() {\n  try {\n    long rowKeyBase;\n    StringBuilder buf = new StringBuilder();\n    byte[][] columnFamilies = dataGenerator.getColumnFamilies();\n    while ((rowKeyBase = getNextKeyToUpdate()) < endKey) {\n      if (RandomUtils.nextInt(100) < updatePercent) {\n        byte[] rowKey = dataGenerator.getDeterministicUniqueKey(rowKeyBase);\n        Increment inc = new Increment(rowKey);\n        Append app = new Append(rowKey);\n        numKeys.addAndGet(1);\n        int columnCount = 0;\n        for (byte[] cf : columnFamilies) {\n          long cfHash = Arrays.hashCode(cf);\n          inc.addColumn(cf, INCREMENT, cfHash);\n          buf.setLength(0); // Clear the buffer\n          buf.append(\"#\").append(Bytes.toString(INCREMENT));\n          buf.append(\":\").append(MutationType.INCREMENT.getNumber());\n          app.add(cf, MUTATE_INFO, Bytes.toBytes(buf.toString()));\n          ++columnCount;\n          if (!isBatchUpdate) {\n            mutate(table, inc, rowKeyBase);\n            numCols.addAndGet(1);\n            inc = new Increment(rowKey);\n            mutate(table, app, rowKeyBase);\n            numCols.addAndGet(1);\n            app = new Append(rowKey);\n          }\n          Get get = new Get(rowKey);\n          get.addFamily(cf);\n          try {\n            get = dataGenerator.beforeGet(rowKeyBase, get);\n          } catch (Exception e) {\n            // Ideally wont happen\n            LOG.warn(\"Failed to modify the get from the load generator  = [\" + get.getRow()\n                + \"], column family = [\" + Bytes.toString(cf) + \"]\", e);\n          }\n          Result result = getRow(get, rowKeyBase, cf);\n          Map<byte[], byte[]> columnValues =\n            result != null ? result.getFamilyMap(cf) : null;\n          if (columnValues == null) {\n            int specialPermCellInsertionFactor = Integer.parseInt(dataGenerator.getArgs()[2]);\n            if (((int) rowKeyBase % specialPermCellInsertionFactor == 0)) {\n              LOG.info(\"Null result expected for the rowkey \" + Bytes.toString(rowKey));\n            } else {\n              failedKeySet.add(rowKeyBase);\n            }\n          }\n          if(columnValues != null) {\n            for (byte[] column : columnValues.keySet()) {\n              if (Bytes.equals(column, INCREMENT) || Bytes.equals(column, MUTATE_INFO)) {\n                continue;\n              }\n              MutationType mt = MutationType\n                  .valueOf(RandomUtils.nextInt(MutationType.values().length));\n              long columnHash = Arrays.hashCode(column);\n              long hashCode = cfHash + columnHash;\n              byte[] hashCodeBytes = Bytes.toBytes(hashCode);\n              byte[] checkedValue = HConstants.EMPTY_BYTE_ARRAY;\n              if (hashCode % 2 == 0) {\n                Cell kv = result.getColumnLatestCell(cf, column);\n                checkedValue = kv != null ? CellUtil.cloneValue(kv) : null;\n                Preconditions.checkNotNull(checkedValue,\n                    \"Column value to be checked should not be null\");\n              }\n              buf.setLength(0); // Clear the buffer\n              buf.append(\"#\").append(Bytes.toString(column)).append(\":\");\n              ++columnCount;\n              switch (mt) {\n              case PUT:\n                Put put = new Put(rowKey);\n                put.addColumn(cf, column, hashCodeBytes);\n                mutate(table, put, rowKeyBase, rowKey, cf, column, checkedValue);\n                buf.append(MutationType.PUT.getNumber());\n                break;\n              case DELETE:\n                Delete delete = new Delete(rowKey);\n                // Delete all versions since a put\n                // could be called multiple times if CM is used\n                delete.addColumns(cf, column);\n                mutate(table, delete, rowKeyBase, rowKey, cf, column, checkedValue);\n                buf.append(MutationType.DELETE.getNumber());\n                break;\n              default:\n                buf.append(MutationType.APPEND.getNumber());\n                app.add(cf, column, hashCodeBytes);\n              }\n              app.add(cf, MUTATE_INFO, Bytes.toBytes(buf.toString()));\n              if (!isBatchUpdate) {\n                mutate(table, app, rowKeyBase);\n                numCols.addAndGet(1);\n                app = new Append(rowKey);\n              }\n            }\n          }\n        }\n        if (isBatchUpdate) {\n          if (verbose) {\n            LOG.debug(\"Preparing increment and append for key = [\"\n              + rowKey + \"], \" + columnCount + \" columns\");\n          }\n          mutate(table, inc, rowKeyBase);\n          mutate(table, app, rowKeyBase);\n          numCols.addAndGet(columnCount);\n        }\n      }\n      if (trackWroteKeys) {\n        wroteKeys.add(rowKeyBase);\n      }\n    }\n  } finally {\n    closeHTable();\n    numThreadsWorking.decrementAndGet();\n  }\n}",
        "accept_response": "@Override\npublic void run() {\n  try {\n    long rowKeyBase;\n    StringBuilder buf = new StringBuilder();\n    byte[][] columnFamilies = dataGenerator.getColumnFamilies();\n    while ((rowKeyBase = getNextKeyToUpdate()) < endKey) {\n      if (RandomUtils.nextInt(100) < updatePercent) {\n        byte[] rowKey = dataGenerator.getDeterministicUniqueKey(rowKeyBase);\n        Increment inc = new Increment(rowKey);\n        Append app = new Append(rowKey);\n        numKeys.addAndGet(1);\n        int columnCount = 0;\n        for (byte[] cf : columnFamilies) {\n          long cfHash = Arrays.hashCode(cf);\n          inc.addColumn(cf, INCREMENT, cfHash);\n          buf.setLength(0); // Clear the buffer\n          buf.append(\"#\").append(Bytes.toString(INCREMENT));\n          buf.append(\":\").append(MutationType.INCREMENT.getNumber());\n          app.add(cf, MUTATE_INFO, Bytes.toBytes(buf.toString()));\n          ++columnCount;\n          if (!isBatchUpdate) {\n            mutate(table, inc, rowKeyBase);\n            numCols.addAndGet(1);\n            inc = new Increment(rowKey);\n            mutate(table, app, rowKeyBase);\n            numCols.addAndGet(1);\n            app = new Append(rowKey);\n          }\n          Get get = new Get(rowKey);\n          get.addFamily(cf);\n          try {\n            get = dataGenerator.beforeGet(rowKeyBase, get);\n          } catch (Exception e) {\n            // Ideally wont happen\n            LOG.warn(\"Failed to modify the get from the load generator  = [\" + get.getRow()\n                + \"], column family = [\" + Bytes.toString(cf) + \"]\", e);\n          }\n          Result result = getRow(get, rowKeyBase, cf);\n          Map<byte[], byte[]> columnValues =\n            result != null ? result.getFamilyMap(cf) : null;\n          if (columnValues == null) {\n            int specialPermCellInsertionFactor = Integer.parseInt(dataGenerator.getArgs()[2]);\n            if (((int) rowKeyBase % specialPermCellInsertionFactor == 0)) {\n              LOG.info(\"Null result expected for the rowkey \" + Bytes.toString(rowKey));\n            } else {\n              failedKeySet.add(rowKeyBase);\n              LOG.error(\"Failed to update the row with key = [\" + Bytes.toString(rowKey)\n                  + \"], since we could not get the original row\");\n            }\n          }\n          if(columnValues != null) {\n            for (byte[] column : columnValues.keySet()) {\n              if (Bytes.equals(column, INCREMENT) || Bytes.equals(column, MUTATE_INFO)) {\n                continue;\n              }\n              MutationType mt = MutationType\n                  .valueOf(RandomUtils.nextInt(MutationType.values().length));\n              long columnHash = Arrays.hashCode(column);\n              long hashCode = cfHash + columnHash;\n              byte[] hashCodeBytes = Bytes.toBytes(hashCode);\n              byte[] checkedValue = HConstants.EMPTY_BYTE_ARRAY;\n              if (hashCode % 2 == 0) {\n                Cell kv = result.getColumnLatestCell(cf, column);\n                checkedValue = kv != null ? CellUtil.cloneValue(kv) : null;\n                Preconditions.checkNotNull(checkedValue,\n                    \"Column value to be checked should not be null\");\n              }\n              buf.setLength(0); // Clear the buffer\n              buf.append(\"#\").append(Bytes.toString(column)).append(\":\");\n              ++columnCount;\n              switch (mt) {\n              case PUT:\n                Put put = new Put(rowKey);\n                put.addColumn(cf, column, hashCodeBytes);\n                mutate(table, put, rowKeyBase, rowKey, cf, column, checkedValue);\n                buf.append(MutationType.PUT.getNumber());\n                break;\n              case DELETE:\n                Delete delete = new Delete(rowKey);\n                // Delete all versions since a put\n                // could be called multiple times if CM is used\n                delete.addColumns(cf, column);\n                mutate(table, delete, rowKeyBase, rowKey, cf, column, checkedValue);\n                buf.append(MutationType.DELETE.getNumber());\n                break;\n              default:\n                buf.append(MutationType.APPEND.getNumber());\n                app.add(cf, column, hashCodeBytes);\n              }\n              app.add(cf, MUTATE_INFO, Bytes.toBytes(buf.toString()));\n              if (!isBatchUpdate) {\n                mutate(table, app, rowKeyBase);\n                numCols.addAndGet(1);\n                app = new Append(rowKey);\n              }\n            }\n          }\n        }\n        if (isBatchUpdate) {\n          if (verbose) {\n            LOG.debug(\"Preparing increment and append for key = [\"\n              + rowKey + \"], \" + columnCount + \" columns\");\n          }\n          mutate(table, inc, rowKeyBase);\n          mutate(table, app, rowKeyBase);\n          numCols.addAndGet(columnCount);\n        }\n      }\n      if (trackWroteKeys) {\n        wroteKeys.add(rowKeyBase);\n      }\n    }\n  } finally {\n    closeHTable();\n    numThreadsWorking.decrementAndGet();\n  }\n}",
        "reject_response": "@Override\npublic void run() {\n  try {\n    long rowKeyBase;\n    StringBuilder buf = new StringBuilder();\n    byte[][] columnFamilies = dataGenerator.getColumnFamilies();\n    while ((rowKeyBase = getNextKeyToUpdate()) < endKey) {\n      if (RandomUtils.nextInt(100) < updatePercent) {\n        byte[] rowKey = dataGenerator.getDeterministicUniqueKey(rowKeyBase);\n        Increment inc = new Increment(rowKey);\n        Append app = new Append(rowKey);\n        numKeys.addAndGet(1);\n        int columnCount = 0;\n        for (byte[] cf : columnFamilies) {\n          long cfHash = Arrays.hashCode(cf);\n          inc.addColumn(cf, INCREMENT, cfHash);\n          buf.setLength(0); // Clear the buffer\n          buf.append(\"#\").append(Bytes.toString(INCREMENT));\n          buf.append(\":\").append(MutationType.INCREMENT.getNumber());\n          app.add(cf, MUTATE_INFO, Bytes.toBytes(buf.toString()));\n          ++columnCount;\n          if (!isBatchUpdate) {\n            mutate(table, inc, rowKeyBase);\n            numCols.addAndGet(1);\n            inc = new Increment(rowKey);\n            mutate(table, app, rowKeyBase);\n            numCols.addAndGet(1);\n            app = new Append(rowKey);\n          }\n          Get get = new Get(rowKey);\n          get.addFamily(cf);\n          try {\n            get = dataGenerator.beforeGet(rowKeyBase, get);\n          } catch (Exception e) {\n            // Ideally wont happen\n            LOG.warn(\"Failed to modify the get from the load generator  = [\" + get.getRow()\n                + \"], column family = [\" + Bytes.toString(cf) + \"]\", e);\n          }\n          Result result = getRow(get, rowKeyBase, cf);\n          Map<byte[], byte[]> columnValues =\n            result != null ? result.getFamilyMap(cf) : null;\n          if (columnValues == null) {\n            int specialPermCellInsertionFactor = Integer.parseInt(dataGenerator.getArgs()[2]);\n            if (((int) rowKeyBase % specialPermCellInsertionFactor == 0)) {\n              LOG.info(\"Null result expected for the rowkey \" + Bytes.toString(rowKey));\n            } else {\n              failedKeySet.add(rowKeyBase);\n              LOG.error(\"Failed to update the row with key = [\" + rowKey\n            }\n          }\n          if(columnValues != null) {\n            for (byte[] column : columnValues.keySet()) {\n              if (Bytes.equals(column, INCREMENT) || Bytes.equals(column, MUTATE_INFO)) {\n                continue;\n              }\n              MutationType mt = MutationType\n                  .valueOf(RandomUtils.nextInt(MutationType.values().length));\n              long columnHash = Arrays.hashCode(column);\n              long hashCode = cfHash + columnHash;\n              byte[] hashCodeBytes = Bytes.toBytes(hashCode);\n              byte[] checkedValue = HConstants.EMPTY_BYTE_ARRAY;\n              if (hashCode % 2 == 0) {\n                Cell kv = result.getColumnLatestCell(cf, column);\n                checkedValue = kv != null ? CellUtil.cloneValue(kv) : null;\n                Preconditions.checkNotNull(checkedValue,\n                    \"Column value to be checked should not be null\");\n              }\n              buf.setLength(0); // Clear the buffer\n              buf.append(\"#\").append(Bytes.toString(column)).append(\":\");\n              ++columnCount;\n              switch (mt) {\n              case PUT:\n                Put put = new Put(rowKey);\n                put.addColumn(cf, column, hashCodeBytes);\n                mutate(table, put, rowKeyBase, rowKey, cf, column, checkedValue);\n                buf.append(MutationType.PUT.getNumber());\n                break;\n              case DELETE:\n                Delete delete = new Delete(rowKey);\n                // Delete all versions since a put\n                // could be called multiple times if CM is used\n                delete.addColumns(cf, column);\n                mutate(table, delete, rowKeyBase, rowKey, cf, column, checkedValue);\n                buf.append(MutationType.DELETE.getNumber());\n                break;\n              default:\n                buf.append(MutationType.APPEND.getNumber());\n                app.add(cf, column, hashCodeBytes);\n              }\n              app.add(cf, MUTATE_INFO, Bytes.toBytes(buf.toString()));\n              if (!isBatchUpdate) {\n                mutate(table, app, rowKeyBase);\n                numCols.addAndGet(1);\n                app = new Append(rowKey);\n              }\n            }\n          }\n        }\n        if (isBatchUpdate) {\n          if (verbose) {\n            LOG.debug(\"Preparing increment and append for key = [\"\n              + rowKey + \"], \" + columnCount + \" columns\");\n          }\n          mutate(table, inc, rowKeyBase);\n          mutate(table, app, rowKeyBase);\n          numCols.addAndGet(columnCount);\n        }\n      }\n      if (trackWroteKeys) {\n        wroteKeys.add(rowKeyBase);\n      }\n    }\n  } finally {\n    closeHTable();\n    numThreadsWorking.decrementAndGet();\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3135,
        "instruction": "@Override\npublic TezHeartbeatResponse heartbeat(TezHeartbeatRequest request) throws IOException,\n    TezException {\n  ContainerId containerId = ConverterUtils.toContainerId(request.getContainerIdentifier());\n  long requestId = request.getRequestId();\n\n  ContainerInfo containerInfo = registeredContainers.get(containerId);\n  if (containerInfo == null) {\n    LOG.warn(\"Received task heartbeat from unknown container with id: \" + containerId +\n        \", asking it to die\");\n    TezHeartbeatResponse response = new TezHeartbeatResponse();\n    response.setLastRequestId(requestId);\n    response.setShouldDie();\n    return response;\n  }\n\n  synchronized (containerInfo) {\n    if (containerInfo.lastRequestId == requestId) {\n      LOG.warn(\"Old sequenceId received: \" + requestId\n          + \", Re-sending last response to client\");\n      return containerInfo.lastResponse;\n    }\n  }\n\n\n\n  TezHeartbeatResponse response = new TezHeartbeatResponse();\n  TezTaskAttemptID taskAttemptID = request.getCurrentTaskAttemptID();\n  if (taskAttemptID != null) {\n    TaskHeartbeatResponse tResponse;\n    synchronized (containerInfo) {\n      ContainerId containerIdFromMap = attemptToContainerMap.get(taskAttemptID);\n      if (containerIdFromMap == null || !containerIdFromMap.equals(containerId)) {\n        throw new TezException(\"Attempt \" + taskAttemptID\n            + \" is not recognized for heartbeat\");\n      }\n\n      if (containerInfo.lastRequestId + 1 != requestId) {\n        throw new TezException(\"Container \" + containerId\n            + \" has invalid request id. Expected: \"\n            + containerInfo.lastRequestId + 1\n            + \" and actual: \" + requestId);\n      }\n    }\n    TaskHeartbeatRequest tRequest = new TaskHeartbeatRequest(request.getContainerIdentifier(),\n        request.getCurrentTaskAttemptID(), request.getEvents(), request.getStartIndex(),\n        request.getPreRoutedStartIndex(), request.getMaxEvents());\n    tResponse = getContext().heartbeat(tRequest);\n    response.setEvents(tResponse.getEvents());\n    response.setNextFromEventId(tResponse.getNextFromEventId());\n    response.setNextPreRoutedEventId(tResponse.getNextPreRoutedEventId());\n  }\n  response.setLastRequestId(requestId);\n  containerInfo.lastRequestId = requestId;\n  containerInfo.lastResponse = response;\n  return response;\n}",
        "accept_response": "@Override\npublic TezHeartbeatResponse heartbeat(TezHeartbeatRequest request) throws IOException,\n    TezException {\n  ContainerId containerId = ConverterUtils.toContainerId(request.getContainerIdentifier());\n  long requestId = request.getRequestId();\n  LOG.debug(\"Received heartbeat from container, request={}\", request);\n\n  ContainerInfo containerInfo = registeredContainers.get(containerId);\n  if (containerInfo == null) {\n    LOG.warn(\"Received task heartbeat from unknown container with id: \" + containerId +\n        \", asking it to die\");\n    TezHeartbeatResponse response = new TezHeartbeatResponse();\n    response.setLastRequestId(requestId);\n    response.setShouldDie();\n    return response;\n  }\n\n  synchronized (containerInfo) {\n    if (containerInfo.lastRequestId == requestId) {\n      LOG.warn(\"Old sequenceId received: \" + requestId\n          + \", Re-sending last response to client\");\n      return containerInfo.lastResponse;\n    }\n  }\n\n\n\n  TezHeartbeatResponse response = new TezHeartbeatResponse();\n  TezTaskAttemptID taskAttemptID = request.getCurrentTaskAttemptID();\n  if (taskAttemptID != null) {\n    TaskHeartbeatResponse tResponse;\n    synchronized (containerInfo) {\n      ContainerId containerIdFromMap = attemptToContainerMap.get(taskAttemptID);\n      if (containerIdFromMap == null || !containerIdFromMap.equals(containerId)) {\n        throw new TezException(\"Attempt \" + taskAttemptID\n            + \" is not recognized for heartbeat\");\n      }\n\n      if (containerInfo.lastRequestId + 1 != requestId) {\n        throw new TezException(\"Container \" + containerId\n            + \" has invalid request id. Expected: \"\n            + containerInfo.lastRequestId + 1\n            + \" and actual: \" + requestId);\n      }\n    }\n    TaskHeartbeatRequest tRequest = new TaskHeartbeatRequest(request.getContainerIdentifier(),\n        request.getCurrentTaskAttemptID(), request.getEvents(), request.getStartIndex(),\n        request.getPreRoutedStartIndex(), request.getMaxEvents());\n    tResponse = getContext().heartbeat(tRequest);\n    response.setEvents(tResponse.getEvents());\n    response.setNextFromEventId(tResponse.getNextFromEventId());\n    response.setNextPreRoutedEventId(tResponse.getNextPreRoutedEventId());\n  }\n  response.setLastRequestId(requestId);\n  containerInfo.lastRequestId = requestId;\n  containerInfo.lastResponse = response;\n  return response;\n}",
        "reject_response": "@Override\npublic TezHeartbeatResponse heartbeat(TezHeartbeatRequest request) throws IOException,\n    TezException {\n  ContainerId containerId = ConverterUtils.toContainerId(request.getContainerIdentifier());\n  long requestId = request.getRequestId();\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"Received heartbeat from container\"\n        + \", request=\" + request);\n  }\n\n  ContainerInfo containerInfo = registeredContainers.get(containerId);\n  if (containerInfo == null) {\n    LOG.warn(\"Received task heartbeat from unknown container with id: \" + containerId +\n        \", asking it to die\");\n    TezHeartbeatResponse response = new TezHeartbeatResponse();\n    response.setLastRequestId(requestId);\n    response.setShouldDie();\n    return response;\n  }\n\n  synchronized (containerInfo) {\n    if (containerInfo.lastRequestId == requestId) {\n      LOG.warn(\"Old sequenceId received: \" + requestId\n          + \", Re-sending last response to client\");\n      return containerInfo.lastResponse;\n    }\n  }\n\n\n\n  TezHeartbeatResponse response = new TezHeartbeatResponse();\n  TezTaskAttemptID taskAttemptID = request.getCurrentTaskAttemptID();\n  if (taskAttemptID != null) {\n    TaskHeartbeatResponse tResponse;\n    synchronized (containerInfo) {\n      ContainerId containerIdFromMap = attemptToContainerMap.get(taskAttemptID);\n      if (containerIdFromMap == null || !containerIdFromMap.equals(containerId)) {\n        throw new TezException(\"Attempt \" + taskAttemptID\n            + \" is not recognized for heartbeat\");\n      }\n\n      if (containerInfo.lastRequestId + 1 != requestId) {\n        throw new TezException(\"Container \" + containerId\n            + \" has invalid request id. Expected: \"\n            + containerInfo.lastRequestId + 1\n            + \" and actual: \" + requestId);\n      }\n    }\n    TaskHeartbeatRequest tRequest = new TaskHeartbeatRequest(request.getContainerIdentifier(),\n        request.getCurrentTaskAttemptID(), request.getEvents(), request.getStartIndex(),\n        request.getPreRoutedStartIndex(), request.getMaxEvents());\n    tResponse = getContext().heartbeat(tRequest);\n    response.setEvents(tResponse.getEvents());\n    response.setNextFromEventId(tResponse.getNextFromEventId());\n    response.setNextPreRoutedEventId(tResponse.getNextPreRoutedEventId());\n  }\n  response.setLastRequestId(requestId);\n  containerInfo.lastRequestId = requestId;\n  containerInfo.lastResponse = response;\n  return response;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2823,
        "instruction": "public void start() {\n    if (getBoolean(IGNITE_JVM_PAUSE_DETECTOR_DISABLED, false)) {\n        if (log.isDebugEnabled())\n            log.debug(\"JVM Pause Detector is disabled.\");\n\n        return;\n    }\n\n    final Thread worker = new Thread(\"jvm-pause-detector-worker\") {\n        private long prev = System.currentTimeMillis();\n\n        @Override public void run() {\n            if (log.isDebugEnabled())\n                log.debug(getName() + \" has been started.\");\n\n            while (true) {\n                try {\n                    Thread.sleep(PRECISION);\n\n                    final long now = System.currentTimeMillis();\n                    final long pause = now - PRECISION - prev;\n\n                    prev = now;\n\n                    if (pause >= THRESHOLD) {\n\n                        synchronized (LongJVMPauseDetector.this) {\n                            final int next = (int)(longPausesCnt % EVT_CNT);\n\n                            longPausesCnt++;\n\n                            longPausesTotalDuration += pause;\n\n                            longPausesTimestamps[next] = now;\n\n                            longPausesDurations[next] = pause;\n                        }\n                    }\n                }\n                catch (InterruptedException e) {\n                    log.error(getName() + \" has been interrupted\", e);\n\n                    break;\n                }\n            }\n        }\n    };\n\n    if (!workerRef.compareAndSet(null, worker)) {\n        log.warning(LongJVMPauseDetector.class.getSimpleName() + \" already started!\");\n\n        return;\n    }\n\n    worker.setDaemon(true);\n    worker.start();\n\n    if (log.isDebugEnabled())\n        log.debug(\"LongJVMPauseDetector was successfully started\");\n}",
        "accept_response": "public void start() {\n    if (getBoolean(IGNITE_JVM_PAUSE_DETECTOR_DISABLED, false)) {\n        if (log.isDebugEnabled())\n            log.debug(\"JVM Pause Detector is disabled.\");\n\n        return;\n    }\n\n    final Thread worker = new Thread(\"jvm-pause-detector-worker\") {\n        private long prev = System.currentTimeMillis();\n\n        @Override public void run() {\n            if (log.isDebugEnabled())\n                log.debug(getName() + \" has been started.\");\n\n            while (true) {\n                try {\n                    Thread.sleep(PRECISION);\n\n                    final long now = System.currentTimeMillis();\n                    final long pause = now - PRECISION - prev;\n\n                    prev = now;\n\n                    if (pause >= THRESHOLD) {\n                        log.warning(\"Possible too long JVM pause: \" + pause + \" milliseconds.\");\n\n                        synchronized (LongJVMPauseDetector.this) {\n                            final int next = (int)(longPausesCnt % EVT_CNT);\n\n                            longPausesCnt++;\n\n                            longPausesTotalDuration += pause;\n\n                            longPausesTimestamps[next] = now;\n\n                            longPausesDurations[next] = pause;\n                        }\n                    }\n                }\n                catch (InterruptedException e) {\n                    log.error(getName() + \" has been interrupted\", e);\n\n                    break;\n                }\n            }\n        }\n    };\n\n    if (!workerRef.compareAndSet(null, worker)) {\n        log.warning(LongJVMPauseDetector.class.getSimpleName() + \" already started!\");\n\n        return;\n    }\n\n    worker.setDaemon(true);\n    worker.start();\n\n    if (log.isDebugEnabled())\n        log.debug(\"LongJVMPauseDetector was successfully started\");\n}",
        "reject_response": "public void start() {\n    if (getBoolean(IGNITE_JVM_PAUSE_DETECTOR_DISABLED, false)) {\n        if (log.isDebugEnabled())\n            log.debug(\"JVM Pause Detector is disabled.\");\n\n        return;\n    }\n\n    final Thread worker = new Thread(\"jvm-pause-detector-worker\") {\n        private long prev = System.currentTimeMillis();\n\n        @Override public void run() {\n            if (log.isDebugEnabled())\n                log.debug(getName() + \" has been started.\");\n\n            while (true) {\n                try {\n                    Thread.sleep(PRECISION);\n\n                    final long now = System.currentTimeMillis();\n                    final long pause = now - PRECISION - prev;\n\n                    prev = now;\n\n                    if (pause >= THRESHOLD) {\n                        LOG.warning(\"Possible too long JVM pause: \" + pause + \" milliseconds.\");\n\n                        synchronized (LongJVMPauseDetector.this) {\n                            final int next = (int)(longPausesCnt % EVT_CNT);\n\n                            longPausesCnt++;\n\n                            longPausesTotalDuration += pause;\n\n                            longPausesTimestamps[next] = now;\n\n                            longPausesDurations[next] = pause;\n                        }\n                    }\n                }\n                catch (InterruptedException e) {\n                    log.error(getName() + \" has been interrupted\", e);\n\n                    break;\n                }\n            }\n        }\n    };\n\n    if (!workerRef.compareAndSet(null, worker)) {\n        log.warning(LongJVMPauseDetector.class.getSimpleName() + \" already started!\");\n\n        return;\n    }\n\n    worker.setDaemon(true);\n    worker.start();\n\n    if (log.isDebugEnabled())\n        log.debug(\"LongJVMPauseDetector was successfully started\");\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3215,
        "instruction": "protected Object retryOperation(ZooKeeperOperation operation)\n    throws KeeperException, InterruptedException {\n    KeeperException exception = null;\n    for (int i = 0; i < retryCount; i++) {\n        try {\n            return operation.execute();\n        } catch (KeeperException.SessionExpiredException e) {\n            LOG.warn(\"Session expired for: \" + zookeeper + \" so reconnecting due to: \" + e, e);\n            throw e;\n        } catch (KeeperException.ConnectionLossException e) {\n            if (exception == null) {\n                exception = e;\n            }\n            retryDelay(i);\n        }\n    }\n    throw exception;\n}",
        "accept_response": "protected Object retryOperation(ZooKeeperOperation operation)\n    throws KeeperException, InterruptedException {\n    KeeperException exception = null;\n    for (int i = 0; i < retryCount; i++) {\n        try {\n            return operation.execute();\n        } catch (KeeperException.SessionExpiredException e) {\n            LOG.warn(\"Session expired for: \" + zookeeper + \" so reconnecting due to: \" + e, e);\n            throw e;\n        } catch (KeeperException.ConnectionLossException e) {\n            if (exception == null) {\n                exception = e;\n            }\n            LOG.debug(\"Attempt {} failed with connection loss so \" +\n                \"attempting to reconnect\", i, e);\n            retryDelay(i);\n        }\n    }\n    throw exception;\n}",
        "reject_response": "protected Object retryOperation(ZooKeeperOperation operation)\n    throws KeeperException, InterruptedException {\n    KeeperException exception = null;\n    for (int i = 0; i < retryCount; i++) {\n        try {\n            return operation.execute();\n        } catch (KeeperException.SessionExpiredException e) {\n            LOG.warn(\"Session expired for: \" + zookeeper + \" so reconnecting due to: \" + e, e);\n            throw e;\n        } catch (KeeperException.ConnectionLossException e) {\n            if (exception == null) {\n                exception = e;\n            }\n            LOG.debug(\"Attempt \" + i + \" failed with connection loss so \" +\n            \t\t\"attempting to reconnect: \" + e, e);\n            retryDelay(i);\n        }\n    }\n    throw exception;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3195,
        "instruction": "private TimelineWriter createTimelineWriter(final Configuration conf) {\n  String timelineWriterClassName = conf.get(\n      YarnConfiguration.TIMELINE_SERVICE_WRITER_CLASS,\n          YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WRITER_CLASS);\n  try {\n    Class<?> timelineWriterClazz = Class.forName(timelineWriterClassName);\n    if (TimelineWriter.class.isAssignableFrom(timelineWriterClazz)) {\n      return (TimelineWriter) ReflectionUtils.newInstance(\n          timelineWriterClazz, conf);\n    } else {\n      throw new YarnRuntimeException(\"Class: \" + timelineWriterClassName\n          + \" not instance of \" + TimelineWriter.class.getCanonicalName());\n    }\n  } catch (ClassNotFoundException e) {\n    throw new YarnRuntimeException(\"Could not instantiate TimelineWriter: \"\n        + timelineWriterClassName, e);\n  }\n}",
        "accept_response": "private TimelineWriter createTimelineWriter(final Configuration conf) {\n  String timelineWriterClassName = conf.get(\n      YarnConfiguration.TIMELINE_SERVICE_WRITER_CLASS,\n          YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WRITER_CLASS);\n  LOG.info(\"Using TimelineWriter: {}\", timelineWriterClassName);\n  try {\n    Class<?> timelineWriterClazz = Class.forName(timelineWriterClassName);\n    if (TimelineWriter.class.isAssignableFrom(timelineWriterClazz)) {\n      return (TimelineWriter) ReflectionUtils.newInstance(\n          timelineWriterClazz, conf);\n    } else {\n      throw new YarnRuntimeException(\"Class: \" + timelineWriterClassName\n          + \" not instance of \" + TimelineWriter.class.getCanonicalName());\n    }\n  } catch (ClassNotFoundException e) {\n    throw new YarnRuntimeException(\"Could not instantiate TimelineWriter: \"\n        + timelineWriterClassName, e);\n  }\n}",
        "reject_response": "private TimelineWriter createTimelineWriter(final Configuration conf) {\n  String timelineWriterClassName = conf.get(\n      YarnConfiguration.TIMELINE_SERVICE_WRITER_CLASS,\n          YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WRITER_CLASS);\n  LOG.info(\"Using TimelineWriter: \" + timelineWriterClassName);\n  try {\n    Class<?> timelineWriterClazz = Class.forName(timelineWriterClassName);\n    if (TimelineWriter.class.isAssignableFrom(timelineWriterClazz)) {\n      return (TimelineWriter) ReflectionUtils.newInstance(\n          timelineWriterClazz, conf);\n    } else {\n      throw new YarnRuntimeException(\"Class: \" + timelineWriterClassName\n          + \" not instance of \" + TimelineWriter.class.getCanonicalName());\n    }\n  } catch (ClassNotFoundException e) {\n    throw new YarnRuntimeException(\"Could not instantiate TimelineWriter: \"\n        + timelineWriterClassName, e);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2381,
        "instruction": "@Override\npublic void delete(String namespace) throws AccumuloException, AccumuloSecurityException, NamespaceNotFoundException, NamespaceNotEmptyException {\n  checkArgument(namespace != null, \"namespace is null\");\n  String namespaceId = Namespaces.getNamespaceId(context.getInstance(), namespace);\n\n  if (namespaceId.equals(Namespaces.ACCUMULO_NAMESPACE_ID) || namespaceId.equals(Namespaces.DEFAULT_NAMESPACE_ID)) {\n    Credentials credentials = context.getCredentials();\n    throw new AccumuloSecurityException(credentials.getPrincipal(), SecurityErrorCode.UNSUPPORTED_OPERATION);\n  }\n\n  if (Namespaces.getTableIds(context.getInstance(), namespaceId).size() > 0) {\n    throw new NamespaceNotEmptyException(namespaceId, namespace, null);\n  }\n\n  List<ByteBuffer> args = Arrays.asList(ByteBuffer.wrap(namespace.getBytes(UTF_8)));\n  Map<String,String> opts = new HashMap<String,String>();\n\n  try {\n    doNamespaceFateOperation(FateOperation.NAMESPACE_DELETE, args, opts);\n  } catch (NamespaceExistsException e) {\n    // should not happen\n    throw new AssertionError(e);\n  }\n\n}",
        "accept_response": "@Override\npublic void delete(String namespace) throws AccumuloException, AccumuloSecurityException, NamespaceNotFoundException, NamespaceNotEmptyException {\n  checkArgument(namespace != null, \"namespace is null\");\n  String namespaceId = Namespaces.getNamespaceId(context.getInstance(), namespace);\n\n  if (namespaceId.equals(Namespaces.ACCUMULO_NAMESPACE_ID) || namespaceId.equals(Namespaces.DEFAULT_NAMESPACE_ID)) {\n    Credentials credentials = context.getCredentials();\n    log.debug(\"{} attempted to delete the {} namespace\", credentials.getPrincipal(), namespaceId);\n    throw new AccumuloSecurityException(credentials.getPrincipal(), SecurityErrorCode.UNSUPPORTED_OPERATION);\n  }\n\n  if (Namespaces.getTableIds(context.getInstance(), namespaceId).size() > 0) {\n    throw new NamespaceNotEmptyException(namespaceId, namespace, null);\n  }\n\n  List<ByteBuffer> args = Arrays.asList(ByteBuffer.wrap(namespace.getBytes(UTF_8)));\n  Map<String,String> opts = new HashMap<String,String>();\n\n  try {\n    doNamespaceFateOperation(FateOperation.NAMESPACE_DELETE, args, opts);\n  } catch (NamespaceExistsException e) {\n    // should not happen\n    throw new AssertionError(e);\n  }\n\n}",
        "reject_response": "@Override\npublic void delete(String namespace) throws AccumuloException, AccumuloSecurityException, NamespaceNotFoundException, NamespaceNotEmptyException {\n  checkArgument(namespace != null, \"namespace is null\");\n  String namespaceId = Namespaces.getNamespaceId(context.getInstance(), namespace);\n\n  if (namespaceId.equals(Namespaces.ACCUMULO_NAMESPACE_ID) || namespaceId.equals(Namespaces.DEFAULT_NAMESPACE_ID)) {\n    Credentials credentials = context.getCredentials();\n    log.debug(credentials.getPrincipal() + \" attempted to delete the \" + namespaceId + \" namespace\");\n    throw new AccumuloSecurityException(credentials.getPrincipal(), SecurityErrorCode.UNSUPPORTED_OPERATION);\n  }\n\n  if (Namespaces.getTableIds(context.getInstance(), namespaceId).size() > 0) {\n    throw new NamespaceNotEmptyException(namespaceId, namespace, null);\n  }\n\n  List<ByteBuffer> args = Arrays.asList(ByteBuffer.wrap(namespace.getBytes(UTF_8)));\n  Map<String,String> opts = new HashMap<String,String>();\n\n  try {\n    doNamespaceFateOperation(FateOperation.NAMESPACE_DELETE, args, opts);\n  } catch (NamespaceExistsException e) {\n    // should not happen\n    throw new AssertionError(e);\n  }\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3189,
        "instruction": "@Override\nprotected void doPostPut(ApplicationId appId, TimelineCollector collector) {\n  try {\n    // Get context info from NM\n    updateTimelineCollectorContext(appId, collector);\n    // Generate token for app collector.\n    org.apache.hadoop.yarn.api.records.Token token = null;\n    if (UserGroupInformation.isSecurityEnabled() &&\n        collector instanceof AppLevelTimelineCollector) {\n      AppLevelTimelineCollector appCollector =\n          (AppLevelTimelineCollector) collector;\n      token = generateTokenAndSetTimer(appId, appCollector);\n    }\n    // Report to NM if a new collector is added.\n    reportNewCollectorInfoToNM(appId, token);\n  } catch (YarnException | IOException e) {\n    // throw exception here as it cannot be used if failed communicate with NM\n    throw new YarnRuntimeException(e);\n  }\n}",
        "accept_response": "@Override\nprotected void doPostPut(ApplicationId appId, TimelineCollector collector) {\n  try {\n    // Get context info from NM\n    updateTimelineCollectorContext(appId, collector);\n    // Generate token for app collector.\n    org.apache.hadoop.yarn.api.records.Token token = null;\n    if (UserGroupInformation.isSecurityEnabled() &&\n        collector instanceof AppLevelTimelineCollector) {\n      AppLevelTimelineCollector appCollector =\n          (AppLevelTimelineCollector) collector;\n      token = generateTokenAndSetTimer(appId, appCollector);\n    }\n    // Report to NM if a new collector is added.\n    reportNewCollectorInfoToNM(appId, token);\n  } catch (YarnException | IOException e) {\n    // throw exception here as it cannot be used if failed communicate with NM\n    LOG.error(\"Failed to communicate with NM Collector Service for {}\", appId);\n    throw new YarnRuntimeException(e);\n  }\n}",
        "reject_response": "@Override\nprotected void doPostPut(ApplicationId appId, TimelineCollector collector) {\n  try {\n    // Get context info from NM\n    updateTimelineCollectorContext(appId, collector);\n    // Generate token for app collector.\n    org.apache.hadoop.yarn.api.records.Token token = null;\n    if (UserGroupInformation.isSecurityEnabled() &&\n        collector instanceof AppLevelTimelineCollector) {\n      AppLevelTimelineCollector appCollector =\n          (AppLevelTimelineCollector) collector;\n      token = generateTokenAndSetTimer(appId, appCollector);\n    }\n    // Report to NM if a new collector is added.\n    reportNewCollectorInfoToNM(appId, token);\n  } catch (YarnException | IOException e) {\n    // throw exception here as it cannot be used if failed communicate with NM\n    LOG.error(\"Failed to communicate with NM Collector Service for \" + appId);\n    throw new YarnRuntimeException(e);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2829,
        "instruction": "@SuppressWarnings(\"TooBroadScope\")\n@Override public WALPointer log(WALRecord rec) throws IgniteCheckedException, StorageException {\n    if (serializer == null || mode == WALMode.NONE)\n        return null;\n\n    FileWriteHandle currWrHandle = currentHandle();\n\n    // Logging was not resumed yet.\n    if (currWrHandle == null)\n        return null;\n\n    // Need to calculate record size first.\n    rec.size(serializer.size(rec));\n\n    while (true) {\n        if (rec.rollOver()){\n            assert cctx.database().checkpointLockIsHeldByThread();\n\n            long idx = currWrHandle.idx;\n\n            currWrHandle.buf.close();\n\n            currWrHandle = rollOver(currWrHandle);\n\n            if (log != null && log.isInfoEnabled())\n        }\n\n        WALPointer ptr = currWrHandle.addRecord(rec);\n\n        if (ptr != null) {\n            metrics.onWalRecordLogged();\n\n            lastWALPtr.set(ptr);\n\n            if (walAutoArchiveAfterInactivity > 0)\n                lastRecordLoggedMs.set(U.currentTimeMillis());\n\n            return ptr;\n        }\n        else\n            currWrHandle = rollOver(currWrHandle);\n\n        checkNode();\n\n        if (isStopping())\n            throw new IgniteCheckedException(\"Stopping.\");\n    }\n}",
        "accept_response": "@SuppressWarnings(\"TooBroadScope\")\n@Override public WALPointer log(WALRecord rec) throws IgniteCheckedException, StorageException {\n    if (serializer == null || mode == WALMode.NONE)\n        return null;\n\n    FileWriteHandle currWrHandle = currentHandle();\n\n    // Logging was not resumed yet.\n    if (currWrHandle == null)\n        return null;\n\n    // Need to calculate record size first.\n    rec.size(serializer.size(rec));\n\n    while (true) {\n        if (rec.rollOver()){\n            assert cctx.database().checkpointLockIsHeldByThread();\n\n            long idx = currWrHandle.idx;\n\n            currWrHandle.buf.close();\n\n            currWrHandle = rollOver(currWrHandle);\n\n            if (log != null && log.isInfoEnabled())\n                log.info(\"Rollover segment [\" + idx + \" to \" + currWrHandle.idx + \"], recordType=\" + rec.type());\n        }\n\n        WALPointer ptr = currWrHandle.addRecord(rec);\n\n        if (ptr != null) {\n            metrics.onWalRecordLogged();\n\n            lastWALPtr.set(ptr);\n\n            if (walAutoArchiveAfterInactivity > 0)\n                lastRecordLoggedMs.set(U.currentTimeMillis());\n\n            return ptr;\n        }\n        else\n            currWrHandle = rollOver(currWrHandle);\n\n        checkNode();\n\n        if (isStopping())\n            throw new IgniteCheckedException(\"Stopping.\");\n    }\n}",
        "reject_response": "@SuppressWarnings(\"TooBroadScope\")\n@Override public WALPointer log(WALRecord rec) throws IgniteCheckedException, StorageException {\n    if (serializer == null || mode == WALMode.NONE)\n        return null;\n\n    FileWriteHandle currWrHandle = currentHandle();\n\n    // Logging was not resumed yet.\n    if (currWrHandle == null)\n        return null;\n\n    // Need to calculate record size first.\n    rec.size(serializer.size(rec));\n\n    while (true) {\n        if (rec.rollOver()){\n            assert cctx.database().checkpointLockIsHeldByThread();\n\n            long idx = currWrHandle.idx;\n\n            currWrHandle.buf.close();\n\n            currWrHandle = rollOver(currWrHandle);\n\n            if (log != null && log.isInfoEnabled())\n            if (log != null && log.isDebugEnabled())\n                log.debug(\"Rollover segment [\" + idx + \" to \" + currWrHandle.idx + \"], recordType=\" + rec.type());\n        }\n\n        WALPointer ptr = currWrHandle.addRecord(rec);\n\n        if (ptr != null) {\n            metrics.onWalRecordLogged();\n\n            lastWALPtr.set(ptr);\n\n            if (walAutoArchiveAfterInactivity > 0)\n                lastRecordLoggedMs.set(U.currentTimeMillis());\n\n            return ptr;\n        }\n        else\n            currWrHandle = rollOver(currWrHandle);\n\n        checkNode();\n\n        if (isStopping())\n            throw new IgniteCheckedException(\"Stopping.\");\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3209,
        "instruction": "public void run() {\n  JobProgressPoller progressUpdator = null;\n  try {\n    progressUpdator = new JobProgressPoller(this, progressUpdateIntervalMs);\n    progressUpdator.start();\n    dateStarted = new Date();\n    result = jobRun();\n    this.exception = null;\n    errorMessage = null;\n    dateFinished = new Date();\n    progressUpdator.terminate();\n  } catch (NullPointerException e) {\n    progressUpdator.terminate();\n    this.exception = e;\n    result = e.getMessage();\n    errorMessage = getStack(e);\n    dateFinished = new Date();\n  } catch (Throwable e) {\n    LOGGER.error(\"Job failed\", e);\n    progressUpdator.terminate();\n    this.exception = e;\n    result = e.getMessage();\n    errorMessage = getStack(e);\n    dateFinished = new Date();\n  } finally {\n    //aborted = false;\n  }\n}",
        "accept_response": "public void run() {\n  JobProgressPoller progressUpdator = null;\n  try {\n    progressUpdator = new JobProgressPoller(this, progressUpdateIntervalMs);\n    progressUpdator.start();\n    dateStarted = new Date();\n    result = jobRun();\n    this.exception = null;\n    errorMessage = null;\n    dateFinished = new Date();\n    progressUpdator.terminate();\n  } catch (NullPointerException e) {\n    LOGGER.error(\"Job failed\", e);\n    progressUpdator.terminate();\n    this.exception = e;\n    result = e.getMessage();\n    errorMessage = getStack(e);\n    dateFinished = new Date();\n  } catch (Throwable e) {\n    LOGGER.error(\"Job failed\", e);\n    progressUpdator.terminate();\n    this.exception = e;\n    result = e.getMessage();\n    errorMessage = getStack(e);\n    dateFinished = new Date();\n  } finally {\n    //aborted = false;\n  }\n}",
        "reject_response": "public void run() {\n  JobProgressPoller progressUpdator = null;\n  try {\n    progressUpdator = new JobProgressPoller(this, progressUpdateIntervalMs);\n    progressUpdator.start();\n    dateStarted = new Date();\n    result = jobRun();\n    this.exception = null;\n    errorMessage = null;\n    dateFinished = new Date();\n    progressUpdator.terminate();\n  } catch (NullPointerException e) {\n    logger().error(\"Job failed\", e);\n    progressUpdator.terminate();\n    this.exception = e;\n    result = e.getMessage();\n    errorMessage = getStack(e);\n    dateFinished = new Date();\n  } catch (Throwable e) {\n    LOGGER.error(\"Job failed\", e);\n    progressUpdator.terminate();\n    this.exception = e;\n    result = e.getMessage();\n    errorMessage = getStack(e);\n    dateFinished = new Date();\n  } finally {\n    //aborted = false;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2564,
        "instruction": "private void handleGLAccountDataIntegrityIssues(final JsonCommand command, final DataIntegrityViolationException dve) {\n    final Throwable realCause = dve.getMostSpecificCause();\n    if (realCause.getMessage().contains(\"acc_gl_code\")) {\n        final String glCode = command.stringValueOfParameterNamed(GLAccountJsonInputParams.GL_CODE.getValue());\n        throw new GLAccountDuplicateException(glCode);\n    }\n\n    throw new PlatformDataIntegrityException(\"error.msg.glAccount.unknown.data.integrity.issue\",\n            \"Unknown data integrity issue with resource GL Account: \" + realCause.getMessage());\n}",
        "accept_response": "private void handleGLAccountDataIntegrityIssues(final JsonCommand command, final DataIntegrityViolationException dve) {\n    final Throwable realCause = dve.getMostSpecificCause();\n    if (realCause.getMessage().contains(\"acc_gl_code\")) {\n        final String glCode = command.stringValueOfParameterNamed(GLAccountJsonInputParams.GL_CODE.getValue());\n        throw new GLAccountDuplicateException(glCode);\n    }\n\n    logger.error(\"Error occured.\", dve);\n    throw new PlatformDataIntegrityException(\"error.msg.glAccount.unknown.data.integrity.issue\",\n            \"Unknown data integrity issue with resource GL Account: \" + realCause.getMessage());\n}",
        "reject_response": "private void handleGLAccountDataIntegrityIssues(final JsonCommand command, final DataIntegrityViolationException dve) {\n    final Throwable realCause = dve.getMostSpecificCause();\n    if (realCause.getMessage().contains(\"acc_gl_code\")) {\n        final String glCode = command.stringValueOfParameterNamed(GLAccountJsonInputParams.GL_CODE.getValue());\n        throw new GLAccountDuplicateException(glCode);\n    }\n\n    logger.error(dve.getMessage(), dve);\n    throw new PlatformDataIntegrityException(\"error.msg.glAccount.unknown.data.integrity.issue\",\n            \"Unknown data integrity issue with resource GL Account: \" + realCause.getMessage());\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2580,
        "instruction": "@Transactional\n@Override\n@CacheEvict(value = \"code_values\", allEntries = true)\npublic CommandProcessingResult deleteCodeValue(final Long codeId, final Long codeValueId) {\n\n    try {\n        this.context.authenticatedUser();\n\n        final Code code = this.codeRepository.findById(codeId)\n                .orElseThrow(() -> new CodeNotFoundException(codeId));\n\n        final CodeValue codeValueToDelete = this.codeValueRepositoryWrapper.findOneWithNotFoundDetection(codeValueId);\n\n        final boolean removed = code.remove(codeValueToDelete);\n        if (removed) {\n            this.codeRepository.saveAndFlush(code);\n        }\n\n        return new CommandProcessingResultBuilder() //\n                .withEntityId(codeId) //\n                .withSubEntityId(codeValueId)//\n                .build();\n    } catch (final DataIntegrityViolationException dve) {\n        final Throwable realCause = dve.getMostSpecificCause();\n        if (realCause.getMessage().contains(\"code_value\")) { throw new PlatformDataIntegrityException(\"error.msg.codeValue.in.use\",\n                \"This code value is in use\", codeValueId); }\n        throw new PlatformDataIntegrityException(\"error.msg.code.value.unknown.data.integrity.issue\",\n                \"Unknown data integrity issue with resource: \" + dve.getMostSpecificCause().getMessage());\n    }\n}",
        "accept_response": "@Transactional\n@Override\n@CacheEvict(value = \"code_values\", allEntries = true)\npublic CommandProcessingResult deleteCodeValue(final Long codeId, final Long codeValueId) {\n\n    try {\n        this.context.authenticatedUser();\n\n        final Code code = this.codeRepository.findById(codeId)\n                .orElseThrow(() -> new CodeNotFoundException(codeId));\n\n        final CodeValue codeValueToDelete = this.codeValueRepositoryWrapper.findOneWithNotFoundDetection(codeValueId);\n\n        final boolean removed = code.remove(codeValueToDelete);\n        if (removed) {\n            this.codeRepository.saveAndFlush(code);\n        }\n\n        return new CommandProcessingResultBuilder() //\n                .withEntityId(codeId) //\n                .withSubEntityId(codeValueId)//\n                .build();\n    } catch (final DataIntegrityViolationException dve) {\n        logger.error(\"Error occured.\", dve);\n        final Throwable realCause = dve.getMostSpecificCause();\n        if (realCause.getMessage().contains(\"code_value\")) { throw new PlatformDataIntegrityException(\"error.msg.codeValue.in.use\",\n                \"This code value is in use\", codeValueId); }\n        throw new PlatformDataIntegrityException(\"error.msg.code.value.unknown.data.integrity.issue\",\n                \"Unknown data integrity issue with resource: \" + dve.getMostSpecificCause().getMessage());\n    }\n}",
        "reject_response": "@Transactional\n@Override\n@CacheEvict(value = \"code_values\", allEntries = true)\npublic CommandProcessingResult deleteCodeValue(final Long codeId, final Long codeValueId) {\n\n    try {\n        this.context.authenticatedUser();\n\n        final Code code = this.codeRepository.findById(codeId)\n                .orElseThrow(() -> new CodeNotFoundException(codeId));\n\n        final CodeValue codeValueToDelete = this.codeValueRepositoryWrapper.findOneWithNotFoundDetection(codeValueId);\n\n        final boolean removed = code.remove(codeValueToDelete);\n        if (removed) {\n            this.codeRepository.saveAndFlush(code);\n        }\n\n        return new CommandProcessingResultBuilder() //\n                .withEntityId(codeId) //\n                .withSubEntityId(codeValueId)//\n                .build();\n    } catch (final DataIntegrityViolationException dve) {\n        logger.error(dve.getMessage(), dve);\n        final Throwable realCause = dve.getMostSpecificCause();\n        if (realCause.getMessage().contains(\"code_value\")) { throw new PlatformDataIntegrityException(\"error.msg.codeValue.in.use\",\n                \"This code value is in use\", codeValueId); }\n        throw new PlatformDataIntegrityException(\"error.msg.code.value.unknown.data.integrity.issue\",\n                \"Unknown data integrity issue with resource: \" + dve.getMostSpecificCause().getMessage());\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3065,
        "instruction": "void deleteReplicaBasedOnCount(ClusterState clusterState,\n                               ZkNodeProps message,\n                               NamedList results,\n                               Runnable onComplete,\n                               boolean parallel)\n        throws KeeperException, InterruptedException {\n  ocmh.checkRequired(message, COLLECTION_PROP, COUNT_PROP);\n  int count = Integer.parseInt(message.getStr(COUNT_PROP));\n  String collectionName = message.getStr(COLLECTION_PROP);\n  String shard = message.getStr(SHARD_ID_PROP);\n  DocCollection coll = clusterState.getCollection(collectionName);\n  Slice slice = null;\n  //Validate if shard is passed.\n  if (shard != null) {\n    slice = coll.getSlice(shard);\n    if (slice == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Invalid shard name : \" +  shard +  \" in collection : \" + collectionName);\n    }\n  }\n\n  Map<Slice, Set<String>> shardToReplicasMapping = new HashMap<Slice, Set<String>>();\n  if (slice != null) {\n    Set<String> replicasToBeDeleted = pickReplicasTobeDeleted(slice, shard, collectionName, count);\n    shardToReplicasMapping.put(slice,replicasToBeDeleted);\n  } else {\n\n    //If there are many replicas left, remove the rest based on count.\n    Collection<Slice> allSlices = coll.getSlices();\n    for (Slice individualSlice : allSlices) {\n      Set<String> replicasToBeDeleted = pickReplicasTobeDeleted(individualSlice, individualSlice.getName(), collectionName, count);\n      shardToReplicasMapping.put(individualSlice, replicasToBeDeleted);\n    }\n  }\n\n  for (Slice shardSlice: shardToReplicasMapping.keySet()) {\n    String shardId = shardSlice.getName();\n    Set<String> replicas = shardToReplicasMapping.get(shardSlice);\n    //callDeleteReplica on all replicas\n    for (String replica: replicas) {\n      deleteCore(shardSlice, collectionName, replica, message, shard, results, onComplete, parallel);\n    }\n    results.add(\"shard_id\", shardId);\n    results.add(\"replicas_deleted\", replicas);\n  }\n\n}",
        "accept_response": "void deleteReplicaBasedOnCount(ClusterState clusterState,\n                               ZkNodeProps message,\n                               NamedList results,\n                               Runnable onComplete,\n                               boolean parallel)\n        throws KeeperException, InterruptedException {\n  ocmh.checkRequired(message, COLLECTION_PROP, COUNT_PROP);\n  int count = Integer.parseInt(message.getStr(COUNT_PROP));\n  String collectionName = message.getStr(COLLECTION_PROP);\n  String shard = message.getStr(SHARD_ID_PROP);\n  DocCollection coll = clusterState.getCollection(collectionName);\n  Slice slice = null;\n  //Validate if shard is passed.\n  if (shard != null) {\n    slice = coll.getSlice(shard);\n    if (slice == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Invalid shard name : \" +  shard +  \" in collection : \" + collectionName);\n    }\n  }\n\n  Map<Slice, Set<String>> shardToReplicasMapping = new HashMap<Slice, Set<String>>();\n  if (slice != null) {\n    Set<String> replicasToBeDeleted = pickReplicasTobeDeleted(slice, shard, collectionName, count);\n    shardToReplicasMapping.put(slice,replicasToBeDeleted);\n  } else {\n\n    //If there are many replicas left, remove the rest based on count.\n    Collection<Slice> allSlices = coll.getSlices();\n    for (Slice individualSlice : allSlices) {\n      Set<String> replicasToBeDeleted = pickReplicasTobeDeleted(individualSlice, individualSlice.getName(), collectionName, count);\n      shardToReplicasMapping.put(individualSlice, replicasToBeDeleted);\n    }\n  }\n\n  for (Slice shardSlice: shardToReplicasMapping.keySet()) {\n    String shardId = shardSlice.getName();\n    Set<String> replicas = shardToReplicasMapping.get(shardSlice);\n    //callDeleteReplica on all replicas\n    for (String replica: replicas) {\n      log.debug(\"Deleting replica {}  for shard {} based on count {}\", replica, shardId, count);\n      deleteCore(shardSlice, collectionName, replica, message, shard, results, onComplete, parallel);\n    }\n    results.add(\"shard_id\", shardId);\n    results.add(\"replicas_deleted\", replicas);\n  }\n\n}",
        "reject_response": "void deleteReplicaBasedOnCount(ClusterState clusterState,\n                               ZkNodeProps message,\n                               NamedList results,\n                               Runnable onComplete,\n                               boolean parallel)\n        throws KeeperException, InterruptedException {\n  ocmh.checkRequired(message, COLLECTION_PROP, COUNT_PROP);\n  int count = Integer.parseInt(message.getStr(COUNT_PROP));\n  String collectionName = message.getStr(COLLECTION_PROP);\n  String shard = message.getStr(SHARD_ID_PROP);\n  DocCollection coll = clusterState.getCollection(collectionName);\n  Slice slice = null;\n  //Validate if shard is passed.\n  if (shard != null) {\n    slice = coll.getSlice(shard);\n    if (slice == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n              \"Invalid shard name : \" +  shard +  \" in collection : \" + collectionName);\n    }\n  }\n\n  Map<Slice, Set<String>> shardToReplicasMapping = new HashMap<Slice, Set<String>>();\n  if (slice != null) {\n    Set<String> replicasToBeDeleted = pickReplicasTobeDeleted(slice, shard, collectionName, count);\n    shardToReplicasMapping.put(slice,replicasToBeDeleted);\n  } else {\n\n    //If there are many replicas left, remove the rest based on count.\n    Collection<Slice> allSlices = coll.getSlices();\n    for (Slice individualSlice : allSlices) {\n      Set<String> replicasToBeDeleted = pickReplicasTobeDeleted(individualSlice, individualSlice.getName(), collectionName, count);\n      shardToReplicasMapping.put(individualSlice, replicasToBeDeleted);\n    }\n  }\n\n  for (Slice shardSlice: shardToReplicasMapping.keySet()) {\n    String shardId = shardSlice.getName();\n    Set<String> replicas = shardToReplicasMapping.get(shardSlice);\n    //callDeleteReplica on all replicas\n    for (String replica: replicas) {\n      log.info(\"Deleting replica {}  for shard {} based on count {}\", replica, shardId, count);\n      deleteCore(shardSlice, collectionName, replica, message, shard, results, onComplete, parallel);\n    }\n    results.add(\"shard_id\", shardId);\n    results.add(\"replicas_deleted\", replicas);\n  }\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3104,
        "instruction": "private void checkRead() throws IOException {\n  if (current == totalCountLoadedSoFar) {\n    if (current != 0) {\n      long timeAssembling = System.currentTimeMillis() - startedAssemblingCurrentBlockAt;\n      totalTimeSpentProcessingRecords += timeAssembling;\n      if (DEBUG) LOG.debug(\"Assembled and processed \" + totalCountLoadedSoFar + \" records from \" + columnCount + \" columns in \" + totalTimeSpentProcessingRecords + \" ms: \" + ((float) totalCountLoadedSoFar / totalTimeSpentProcessingRecords) + \" rec/ms, \" + ((float) totalCountLoadedSoFar * columnCount / totalTimeSpentProcessingRecords) + \" cell/ms\");\n      long totalTime = totalTimeSpentProcessingRecords + totalTimeSpentReadingBytes;\n      long percentReading = 100 * totalTimeSpentReadingBytes / totalTime;\n      long percentProcessing = 100 * totalTimeSpentProcessingRecords / totalTime;\n      if (DEBUG) LOG.debug(\"time spent so far \" + percentReading + \"% reading (\"+totalTimeSpentReadingBytes+\" ms) and \" + percentProcessing + \"% processing (\"+totalTimeSpentProcessingRecords+\" ms)\");\n    }\n\n    if (DEBUG) LOG.debug(\"at row \" + current + \". reading next block\");\n    long t0 = System.currentTimeMillis();\n    PageReadStore pages = reader.readNextRowGroup();\n    if (pages == null) {\n      throw new IOException(\"expecting more rows but reached last block. Read \" + current + \" out of \" + total);\n    }\n    long timeSpentReading = System.currentTimeMillis() - t0;\n    totalTimeSpentReadingBytes += timeSpentReading;\n    BenchmarkCounter.incrementTime(timeSpentReading);\n    if (DEBUG) {\n      LOG.debug(\"block read in memory in \" + timeSpentReading + \" ms. row count = \" + pages.getRowCount());\n    }\n    MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema);\n    recordReader = columnIO.getRecordReader(pages, recordConverter, recordFilter);\n    startedAssemblingCurrentBlockAt = System.currentTimeMillis();\n    totalCountLoadedSoFar += pages.getRowCount();\n    ++ currentBlock;\n  }\n}",
        "accept_response": "private void checkRead() throws IOException {\n  if (current == totalCountLoadedSoFar) {\n    if (current != 0) {\n      long timeAssembling = System.currentTimeMillis() - startedAssemblingCurrentBlockAt;\n      totalTimeSpentProcessingRecords += timeAssembling;\n      if (DEBUG) LOG.debug(\"Assembled and processed \" + totalCountLoadedSoFar + \" records from \" + columnCount + \" columns in \" + totalTimeSpentProcessingRecords + \" ms: \" + ((float) totalCountLoadedSoFar / totalTimeSpentProcessingRecords) + \" rec/ms, \" + ((float) totalCountLoadedSoFar * columnCount / totalTimeSpentProcessingRecords) + \" cell/ms\");\n      long totalTime = totalTimeSpentProcessingRecords + totalTimeSpentReadingBytes;\n      long percentReading = 100 * totalTimeSpentReadingBytes / totalTime;\n      long percentProcessing = 100 * totalTimeSpentProcessingRecords / totalTime;\n      if (DEBUG) LOG.debug(\"time spent so far \" + percentReading + \"% reading (\"+totalTimeSpentReadingBytes+\" ms) and \" + percentProcessing + \"% processing (\"+totalTimeSpentProcessingRecords+\" ms)\");\n    }\n\n    if (DEBUG) LOG.debug(\"at row \" + current + \". reading next block\");\n    long t0 = System.currentTimeMillis();\n    PageReadStore pages = reader.readNextRowGroup();\n    if (pages == null) {\n      throw new IOException(\"expecting more rows but reached last block. Read \" + current + \" out of \" + total);\n    }\n    long timeSpentReading = System.currentTimeMillis() - t0;\n    totalTimeSpentReadingBytes += timeSpentReading;\n    BenchmarkCounter.incrementTime(timeSpentReading);\n    if (DEBUG) {\n      LOG.debug(\"block read in memory in \" + timeSpentReading + \" ms. row count = \" + pages.getRowCount());\n      LOG.debug(\"initializing Record assembly with requested schema \" + requestedSchema);\n    }\n    MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema);\n    recordReader = columnIO.getRecordReader(pages, recordConverter, recordFilter);\n    startedAssemblingCurrentBlockAt = System.currentTimeMillis();\n    totalCountLoadedSoFar += pages.getRowCount();\n    ++ currentBlock;\n  }\n}",
        "reject_response": "private void checkRead() throws IOException {\n  if (current == totalCountLoadedSoFar) {\n    if (current != 0) {\n      long timeAssembling = System.currentTimeMillis() - startedAssemblingCurrentBlockAt;\n      totalTimeSpentProcessingRecords += timeAssembling;\n      if (DEBUG) LOG.debug(\"Assembled and processed \" + totalCountLoadedSoFar + \" records from \" + columnCount + \" columns in \" + totalTimeSpentProcessingRecords + \" ms: \" + ((float) totalCountLoadedSoFar / totalTimeSpentProcessingRecords) + \" rec/ms, \" + ((float) totalCountLoadedSoFar * columnCount / totalTimeSpentProcessingRecords) + \" cell/ms\");\n      long totalTime = totalTimeSpentProcessingRecords + totalTimeSpentReadingBytes;\n      long percentReading = 100 * totalTimeSpentReadingBytes / totalTime;\n      long percentProcessing = 100 * totalTimeSpentProcessingRecords / totalTime;\n      if (DEBUG) LOG.debug(\"time spent so far \" + percentReading + \"% reading (\"+totalTimeSpentReadingBytes+\" ms) and \" + percentProcessing + \"% processing (\"+totalTimeSpentProcessingRecords+\" ms)\");\n    }\n\n    if (DEBUG) LOG.debug(\"at row \" + current + \". reading next block\");\n    long t0 = System.currentTimeMillis();\n    PageReadStore pages = reader.readNextRowGroup();\n    if (pages == null) {\n      throw new IOException(\"expecting more rows but reached last block. Read \" + current + \" out of \" + total);\n    }\n    long timeSpentReading = System.currentTimeMillis() - t0;\n    totalTimeSpentReadingBytes += timeSpentReading;\n    BenchmarkCounter.incrementTime(timeSpentReading);\n    if (DEBUG) {\n      LOG.debug(\"block read in memory in \" + timeSpentReading + \" ms. row count = \" + pages.getRowCount());\n    LOG.info(\"block read in memory in \" + timeSpentReading + \" ms. row count = \" + pages.getRowCount());\n    if (Log.DEBUG) LOG.debug(\"initializing Record assembly with requested schema \" + requestedSchema);\n    }\n    MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema);\n    recordReader = columnIO.getRecordReader(pages, recordConverter, recordFilter);\n    startedAssemblingCurrentBlockAt = System.currentTimeMillis();\n    totalCountLoadedSoFar += pages.getRowCount();\n    ++ currentBlock;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3119,
        "instruction": "private static boolean loadNativeIO() {\n  boolean loaded = true;\n  if (nativeIOPossible) return loaded;\n\n  Class[] parameters = {String.class, FileDescriptor.class, Long.TYPE, Long.TYPE, Integer.TYPE};\n  try {\n    Method getCacheManipulator = MethodUtils.getAccessibleMethod(NativeIO.POSIX.class, \"getCacheManipulator\", new Class[0]);\n    Class posixClass;\n    if (getCacheManipulator != null) {\n      Object posix = MethodUtils.invokeStaticMethod(NativeIO.POSIX.class, \"getCacheManipulator\", null);\n      posixClass = posix.getClass();\n    } else {\n      posixClass = NativeIO.POSIX.class;\n    }\n    posixFadviseIfPossible = MethodUtils.getAccessibleMethod(posixClass, \"posixFadviseIfPossible\", parameters);\n  } catch (Throwable e) {\n    loaded = false;\n  }\n\n  if (posixFadviseIfPossible == null) {\n    loaded = false;\n  }\n  return loaded;\n}",
        "accept_response": "private static boolean loadNativeIO() {\n  boolean loaded = true;\n  if (nativeIOPossible) return loaded;\n\n  Class[] parameters = {String.class, FileDescriptor.class, Long.TYPE, Long.TYPE, Integer.TYPE};\n  try {\n    Method getCacheManipulator = MethodUtils.getAccessibleMethod(NativeIO.POSIX.class, \"getCacheManipulator\", new Class[0]);\n    Class posixClass;\n    if (getCacheManipulator != null) {\n      Object posix = MethodUtils.invokeStaticMethod(NativeIO.POSIX.class, \"getCacheManipulator\", null);\n      posixClass = posix.getClass();\n    } else {\n      posixClass = NativeIO.POSIX.class;\n    }\n    posixFadviseIfPossible = MethodUtils.getAccessibleMethod(posixClass, \"posixFadviseIfPossible\", parameters);\n  } catch (Throwable e) {\n    loaded = false;\n    LOG.warn(\"Failed to access posixFadviseIfPossible :\" + e.getMessage(), e);\n  }\n\n  if (posixFadviseIfPossible == null) {\n    loaded = false;\n  }\n  return loaded;\n}",
        "reject_response": "private static boolean loadNativeIO() {\n  boolean loaded = true;\n  if (nativeIOPossible) return loaded;\n\n  Class[] parameters = {String.class, FileDescriptor.class, Long.TYPE, Long.TYPE, Integer.TYPE};\n  try {\n    Method getCacheManipulator = MethodUtils.getAccessibleMethod(NativeIO.POSIX.class, \"getCacheManipulator\", new Class[0]);\n    Class posixClass;\n    if (getCacheManipulator != null) {\n      Object posix = MethodUtils.invokeStaticMethod(NativeIO.POSIX.class, \"getCacheManipulator\", null);\n      posixClass = posix.getClass();\n    } else {\n      posixClass = NativeIO.POSIX.class;\n    }\n    posixFadviseIfPossible = MethodUtils.getAccessibleMethod(posixClass, \"posixFadviseIfPossible\", parameters);\n  } catch (Throwable e) {\n    loaded = false;\n    LOG.warn(\"Failed to access posixFadviseIfPossible :\" + e.getMessage());\n  }\n\n  if (posixFadviseIfPossible == null) {\n    loaded = false;\n  }\n  return loaded;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3013,
        "instruction": "public synchronized void removeClosedEvaluator(final EvaluatorManager evaluatorManager) {\n\n  final String evaluatorId = evaluatorManager.getId();\n  LOG.log(Level.FINE, \"Removing closed evaluator: {0}\", evaluatorId);\n\n  if (!evaluatorManager.isClosed()) {\n    throw new IllegalArgumentException(\"Removing evaluator that has not been closed yet: \" + evaluatorId);\n  }\n\n  if (!this.evaluators.containsKey(evaluatorId) && !this.closedEvaluatorIds.contains(evaluatorId)) {\n    throw new IllegalArgumentException(\"Removing unknown evaluator: \" + evaluatorId);\n  }\n\n  if (!this.evaluators.containsKey(evaluatorId) && this.closedEvaluatorIds.contains(evaluatorId)) {\n    LOG.log(Level.FINE, \"Removing closed evaluator which has already been removed: {0}\", evaluatorId);\n    return;\n  }\n\n  evaluatorManager.shutdown();\n  this.evaluators.remove(evaluatorId);\n  this.closedEvaluatorIds.add(evaluatorId);\n\n}",
        "accept_response": "public synchronized void removeClosedEvaluator(final EvaluatorManager evaluatorManager) {\n\n  final String evaluatorId = evaluatorManager.getId();\n  LOG.log(Level.FINE, \"Removing closed evaluator: {0}\", evaluatorId);\n\n  if (!evaluatorManager.isClosed()) {\n    throw new IllegalArgumentException(\"Removing evaluator that has not been closed yet: \" + evaluatorId);\n  }\n\n  if (!this.evaluators.containsKey(evaluatorId) && !this.closedEvaluatorIds.contains(evaluatorId)) {\n    throw new IllegalArgumentException(\"Removing unknown evaluator: \" + evaluatorId);\n  }\n\n  if (!this.evaluators.containsKey(evaluatorId) && this.closedEvaluatorIds.contains(evaluatorId)) {\n    LOG.log(Level.FINE, \"Removing closed evaluator which has already been removed: {0}\", evaluatorId);\n    return;\n  }\n\n  evaluatorManager.shutdown();\n  this.evaluators.remove(evaluatorId);\n  this.closedEvaluatorIds.add(evaluatorId);\n\n  LOG.log(Level.FINEST, \"Closed evaluator removed: {0}\", evaluatorId);\n}",
        "reject_response": "public synchronized void removeClosedEvaluator(final EvaluatorManager evaluatorManager) {\n\n  final String evaluatorId = evaluatorManager.getId();\n  LOG.log(Level.FINE, \"Removing closed evaluator: {0}\", evaluatorId);\n\n  if (!evaluatorManager.isClosed()) {\n    throw new IllegalArgumentException(\"Removing evaluator that has not been closed yet: \" + evaluatorId);\n  }\n\n  if (!this.evaluators.containsKey(evaluatorId) && !this.closedEvaluatorIds.contains(evaluatorId)) {\n    throw new IllegalArgumentException(\"Removing unknown evaluator: \" + evaluatorId);\n  }\n\n  if (!this.evaluators.containsKey(evaluatorId) && this.closedEvaluatorIds.contains(evaluatorId)) {\n    LOG.log(Level.FINE, \"Removing closed evaluator which has already been removed: {0}\", evaluatorId);\n    return;\n  }\n\n  evaluatorManager.shutdown();\n  this.evaluators.remove(evaluatorId);\n  this.closedEvaluatorIds.add(evaluatorId);\n\n  LOG.log(Level.FINEST, \"Evaluator {0} removed\", evaluatorId);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2810,
        "instruction": "@Override public V load(K key) {\n    if (log.isDebugEnabled())\n\n    StatefulValue<K, V> val;\n\n    if (writeCoalescing)\n        val = writeCache.get(key);\n    else\n        val = flusher(key).flusherWriteMap.get(key);\n\n    if (val != null) {\n        val.readLock().lock();\n\n        try {\n            switch (val.operation()) {\n                case PUT:\n                    return val.entry().getValue();\n\n                case RMV:\n                    return null;\n\n                default:\n                    assert false : \"Unexpected operation: \" + val.status();\n            }\n        }\n        finally {\n            val.readLock().unlock();\n        }\n    }\n\n    return store.load(key);\n}",
        "accept_response": "@Override public V load(K key) {\n    if (log.isDebugEnabled())\n        log.debug(S.toString(\"Store load\",\n            \"key\", key, true));\n\n    StatefulValue<K, V> val;\n\n    if (writeCoalescing)\n        val = writeCache.get(key);\n    else\n        val = flusher(key).flusherWriteMap.get(key);\n\n    if (val != null) {\n        val.readLock().lock();\n\n        try {\n            switch (val.operation()) {\n                case PUT:\n                    return val.entry().getValue();\n\n                case RMV:\n                    return null;\n\n                default:\n                    assert false : \"Unexpected operation: \" + val.status();\n            }\n        }\n        finally {\n            val.readLock().unlock();\n        }\n    }\n\n    return store.load(key);\n}",
        "reject_response": "@Override public V load(K key) {\n    if (log.isDebugEnabled())\n        log.debug(\"Store load [key=\" + key + ']');\n\n    StatefulValue<K, V> val;\n\n    if (writeCoalescing)\n        val = writeCache.get(key);\n    else\n        val = flusher(key).flusherWriteMap.get(key);\n\n    if (val != null) {\n        val.readLock().lock();\n\n        try {\n            switch (val.operation()) {\n                case PUT:\n                    return val.entry().getValue();\n\n                case RMV:\n                    return null;\n\n                default:\n                    assert false : \"Unexpected operation: \" + val.status();\n            }\n        }\n        finally {\n            val.readLock().unlock();\n        }\n    }\n\n    return store.load(key);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3198,
        "instruction": "private TimelineReader createTimelineReaderStore(final Configuration conf) {\n  String timelineReaderClassName = conf.get(\n      YarnConfiguration.TIMELINE_SERVICE_READER_CLASS,\n      YarnConfiguration.DEFAULT_TIMELINE_SERVICE_READER_CLASS);\n  try {\n    Class<?> timelineReaderClazz = Class.forName(timelineReaderClassName);\n    if (TimelineReader.class.isAssignableFrom(timelineReaderClazz)) {\n      return (TimelineReader) ReflectionUtils.newInstance(\n          timelineReaderClazz, conf);\n    } else {\n      throw new YarnRuntimeException(\"Class: \" + timelineReaderClassName\n          + \" not instance of \" + TimelineReader.class.getCanonicalName());\n    }\n  } catch (ClassNotFoundException e) {\n    throw new YarnRuntimeException(\"Could not instantiate TimelineReader: \"\n        + timelineReaderClassName, e);\n  }\n}",
        "accept_response": "private TimelineReader createTimelineReaderStore(final Configuration conf) {\n  String timelineReaderClassName = conf.get(\n      YarnConfiguration.TIMELINE_SERVICE_READER_CLASS,\n      YarnConfiguration.DEFAULT_TIMELINE_SERVICE_READER_CLASS);\n  LOG.info(\"Using store: {}\", timelineReaderClassName);\n  try {\n    Class<?> timelineReaderClazz = Class.forName(timelineReaderClassName);\n    if (TimelineReader.class.isAssignableFrom(timelineReaderClazz)) {\n      return (TimelineReader) ReflectionUtils.newInstance(\n          timelineReaderClazz, conf);\n    } else {\n      throw new YarnRuntimeException(\"Class: \" + timelineReaderClassName\n          + \" not instance of \" + TimelineReader.class.getCanonicalName());\n    }\n  } catch (ClassNotFoundException e) {\n    throw new YarnRuntimeException(\"Could not instantiate TimelineReader: \"\n        + timelineReaderClassName, e);\n  }\n}",
        "reject_response": "private TimelineReader createTimelineReaderStore(final Configuration conf) {\n  String timelineReaderClassName = conf.get(\n      YarnConfiguration.TIMELINE_SERVICE_READER_CLASS,\n      YarnConfiguration.DEFAULT_TIMELINE_SERVICE_READER_CLASS);\n  LOG.info(\"Using store: \" + timelineReaderClassName);\n  try {\n    Class<?> timelineReaderClazz = Class.forName(timelineReaderClassName);\n    if (TimelineReader.class.isAssignableFrom(timelineReaderClazz)) {\n      return (TimelineReader) ReflectionUtils.newInstance(\n          timelineReaderClazz, conf);\n    } else {\n      throw new YarnRuntimeException(\"Class: \" + timelineReaderClassName\n          + \" not instance of \" + TimelineReader.class.getCanonicalName());\n    }\n  } catch (ClassNotFoundException e) {\n    throw new YarnRuntimeException(\"Could not instantiate TimelineReader: \"\n        + timelineReaderClassName, e);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2870,
        "instruction": "@Override\npublic final Binding nextBinding()\n{\n    try {\n        // Need to make sure to only read this once per iteration\n        boolean shouldCancel = requestingCancel;\n\n        if ( shouldCancel && abortIterator )\n        {\n            // Try to close first to release resources (in case the user\n            // doesn't have a close() call in a finally block)\n            close() ;\n            throw new QueryCancelledException() ;\n        }\n\n        if ( finished )\n            throw new NoSuchElementException(Lib.className(this)) ;\n\n        if ( ! hasNextBinding() )\n            throw new NoSuchElementException(Lib.className(this)) ;\n\n        Binding obj = moveToNextBinding() ;\n        if ( obj == null )\n            throw new NoSuchElementException(Lib.className(this)) ;\n\n        if ( shouldCancel && ! finished )\n        {\n            // But .cancel sets both requestingCancel and abortIterator\n            // This only happens with a continuing iterator.\n    \t\tclose() ;\n    \t}\n\n        return obj ;\n    } catch (QueryFatalException ex)\n    {\n        throw ex ;\n    }\n}",
        "accept_response": "@Override\npublic final Binding nextBinding()\n{\n    try {\n        // Need to make sure to only read this once per iteration\n        boolean shouldCancel = requestingCancel;\n\n        if ( shouldCancel && abortIterator )\n        {\n            // Try to close first to release resources (in case the user\n            // doesn't have a close() call in a finally block)\n            close() ;\n            throw new QueryCancelledException() ;\n        }\n\n        if ( finished )\n            throw new NoSuchElementException(Lib.className(this)) ;\n\n        if ( ! hasNextBinding() )\n            throw new NoSuchElementException(Lib.className(this)) ;\n\n        Binding obj = moveToNextBinding() ;\n        if ( obj == null )\n            throw new NoSuchElementException(Lib.className(this)) ;\n\n        if ( shouldCancel && ! finished )\n        {\n            // But .cancel sets both requestingCancel and abortIterator\n            // This only happens with a continuing iterator.\n    \t\tclose() ;\n    \t}\n\n        return obj ;\n    } catch (QueryFatalException ex)\n    {\n        Log.error(this, \"QueryFatalException\", ex) ;\n        throw ex ;\n    }\n}",
        "reject_response": "@Override\npublic final Binding nextBinding()\n{\n    try {\n        // Need to make sure to only read this once per iteration\n        boolean shouldCancel = requestingCancel;\n\n        if ( shouldCancel && abortIterator )\n        {\n            // Try to close first to release resources (in case the user\n            // doesn't have a close() call in a finally block)\n            close() ;\n            throw new QueryCancelledException() ;\n        }\n\n        if ( finished )\n            throw new NoSuchElementException(Lib.className(this)) ;\n\n        if ( ! hasNextBinding() )\n            throw new NoSuchElementException(Lib.className(this)) ;\n\n        Binding obj = moveToNextBinding() ;\n        if ( obj == null )\n            throw new NoSuchElementException(Lib.className(this)) ;\n\n        if ( shouldCancel && ! finished )\n        {\n            // But .cancel sets both requestingCancel and abortIterator\n            // This only happens with a continuing iterator.\n    \t\tclose() ;\n    \t}\n\n        return obj ;\n    } catch (QueryFatalException ex)\n    {\n        Log.fatal(this, \"QueryFatalException\", ex) ;\n        throw ex ;\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3082,
        "instruction": "private static InputStream getConfigFileInputStream(String configFilePath)\n        throws IOException {\n    if (null == configFilePath) {\n        throw new IOException(\n                \"Could not find config file, name not specified\");\n    }\n\n    HashSet<URL> resources = new HashSet<URL>(findResources(configFilePath));\n    if (resources.isEmpty()) {\n        File configFile = new File(configFilePath);\n        if (configFile.exists()) {\n            return new FileInputStream(configFile);\n        }\n    } else if (resources.size() > 1) {\n        throw new IOException(\n                \"Found multiple \" + configFilePath\n                        + \" resources. You're probably bundling the Storm jars with your topology jar. \"\n                        + resources);\n    } else {\n        URL resource = resources.iterator().next();\n        return resource.openStream();\n    }\n    return null;\n}",
        "accept_response": "private static InputStream getConfigFileInputStream(String configFilePath)\n        throws IOException {\n    if (null == configFilePath) {\n        throw new IOException(\n                \"Could not find config file, name not specified\");\n    }\n\n    HashSet<URL> resources = new HashSet<URL>(findResources(configFilePath));\n    if (resources.isEmpty()) {\n        File configFile = new File(configFilePath);\n        if (configFile.exists()) {\n            return new FileInputStream(configFile);\n        }\n    } else if (resources.size() > 1) {\n        throw new IOException(\n                \"Found multiple \" + configFilePath\n                        + \" resources. You're probably bundling the Storm jars with your topology jar. \"\n                        + resources);\n    } else {\n        LOG.debug(\"Using \"+configFilePath+\" from resources\");\n        URL resource = resources.iterator().next();\n        return resource.openStream();\n    }\n    return null;\n}",
        "reject_response": "private static InputStream getConfigFileInputStream(String configFilePath)\n        throws IOException {\n    if (null == configFilePath) {\n        throw new IOException(\n                \"Could not find config file, name not specified\");\n    }\n\n    HashSet<URL> resources = new HashSet<URL>(findResources(configFilePath));\n    if (resources.isEmpty()) {\n        File configFile = new File(configFilePath);\n        if (configFile.exists()) {\n            return new FileInputStream(configFile);\n        }\n    } else if (resources.size() > 1) {\n        throw new IOException(\n                \"Found multiple \" + configFilePath\n                        + \" resources. You're probably bundling the Storm jars with your topology jar. \"\n                        + resources);\n    } else {\n        LOG.info(\"Using \"+configFilePath+\" from resources\");\n        URL resource = resources.iterator().next();\n        return resource.openStream();\n    }\n    return null;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2983,
        "instruction": "public boolean isValidXAUser(String loginId) {\n\tXXPortalUser pUser = daoManager.getXXPortalUser().findByLoginId(loginId);\n\tif (pUser == null) {\n\t\tlogger.error(\"Error getting user for loginId=\" + loginId);\n\t\treturn false;\n\t} else {\n\t\treturn true;\n\t}\n\n}",
        "accept_response": "public boolean isValidXAUser(String loginId) {\n\tXXPortalUser pUser = daoManager.getXXPortalUser().findByLoginId(loginId);\n\tif (pUser == null) {\n\t\tlogger.error(\"Error getting user for loginId=\" + loginId);\n\t\treturn false;\n\t} else {\n\t\tif(logger.isDebugEnabled()) {\n\t\t\tlogger.debug(loginId + \" is a valid user\");\n\t\t}\n\t\treturn true;\n\t}\n\n}",
        "reject_response": "public boolean isValidXAUser(String loginId) {\n\tXXPortalUser pUser = daoManager.getXXPortalUser().findByLoginId(loginId);\n\tif (pUser == null) {\n\t\tlogger.error(\"Error getting user for loginId=\" + loginId);\n\t\treturn false;\n\t} else {\n\t\tif(logger.isDebugEnabled()) {\n\t\tlogger.info(loginId+\" is a valid user\");\n\t\t}\n\t\treturn true;\n\t}\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2774,
        "instruction": "ClassicHttpResponse callBackend(\n        final HttpHost target,\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain) throws IOException, HttpException  {\n\n    final Date requestDate = getCurrentDate();\n\n    final ClassicHttpResponse backendResponse = chain.proceed(request, scope);\n    try {\n        backendResponse.addHeader(\"Via\", generateViaHeader(backendResponse));\n        return handleBackendResponse(target, request, scope, requestDate, getCurrentDate(), backendResponse);\n    } catch (final IOException | RuntimeException ex) {\n        backendResponse.close();\n        throw ex;\n    }\n}",
        "accept_response": "ClassicHttpResponse callBackend(\n        final HttpHost target,\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain) throws IOException, HttpException  {\n\n    final Date requestDate = getCurrentDate();\n\n    LOG.debug(\"Calling the backend\");\n    final ClassicHttpResponse backendResponse = chain.proceed(request, scope);\n    try {\n        backendResponse.addHeader(\"Via\", generateViaHeader(backendResponse));\n        return handleBackendResponse(target, request, scope, requestDate, getCurrentDate(), backendResponse);\n    } catch (final IOException | RuntimeException ex) {\n        backendResponse.close();\n        throw ex;\n    }\n}",
        "reject_response": "ClassicHttpResponse callBackend(\n        final HttpHost target,\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain) throws IOException, HttpException  {\n\n    final Date requestDate = getCurrentDate();\n\n    log.debug(\"Calling the backend\");\n    final ClassicHttpResponse backendResponse = chain.proceed(request, scope);\n    try {\n        backendResponse.addHeader(\"Via\", generateViaHeader(backendResponse));\n        return handleBackendResponse(target, request, scope, requestDate, getCurrentDate(), backendResponse);\n    } catch (final IOException | RuntimeException ex) {\n        backendResponse.close();\n        throw ex;\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2792,
        "instruction": "public void revalidateCacheEntry(\n        final String cacheKey,\n        final RevalidationCall call) {\n    scheduleRevalidation(cacheKey, new Runnable() {\n\n                    @Override\n                    public void run() {\n                        try (ClassicHttpResponse httpResponse = call.execute()) {\n                            if (httpResponse.getCode() < HttpStatus.SC_SERVER_ERROR && !isStale(httpResponse)) {\n                                jobSuccessful(cacheKey);\n                            } else {\n                                jobFailed(cacheKey);\n                            }\n                        } catch (final IOException ex) {\n                            jobFailed(cacheKey);\n                            LOG.debug(\"Asynchronous revalidation failed due to I/O error\", ex);\n                        } catch (final HttpException ex) {\n                            jobFailed(cacheKey);\n                            LOG.error(\"HTTP protocol exception during asynchronous revalidation\", ex);\n                        } catch (final RuntimeException ex) {\n                            jobFailed(cacheKey);\n                        }\n\n                    }\n\n                });\n}",
        "accept_response": "public void revalidateCacheEntry(\n        final String cacheKey,\n        final RevalidationCall call) {\n    scheduleRevalidation(cacheKey, new Runnable() {\n\n                    @Override\n                    public void run() {\n                        try (ClassicHttpResponse httpResponse = call.execute()) {\n                            if (httpResponse.getCode() < HttpStatus.SC_SERVER_ERROR && !isStale(httpResponse)) {\n                                jobSuccessful(cacheKey);\n                            } else {\n                                jobFailed(cacheKey);\n                            }\n                        } catch (final IOException ex) {\n                            jobFailed(cacheKey);\n                            LOG.debug(\"Asynchronous revalidation failed due to I/O error\", ex);\n                        } catch (final HttpException ex) {\n                            jobFailed(cacheKey);\n                            LOG.error(\"HTTP protocol exception during asynchronous revalidation\", ex);\n                        } catch (final RuntimeException ex) {\n                            jobFailed(cacheKey);\n                            LOG.error(\"Unexpected runtime exception thrown during asynchronous revalidation\", ex);\n                        }\n\n                    }\n\n                });\n}",
        "reject_response": "public void revalidateCacheEntry(\n        final String cacheKey,\n        final RevalidationCall call) {\n    scheduleRevalidation(cacheKey, new Runnable() {\n\n                    @Override\n                    public void run() {\n                        try (ClassicHttpResponse httpResponse = call.execute()) {\n                            if (httpResponse.getCode() < HttpStatus.SC_SERVER_ERROR && !isStale(httpResponse)) {\n                                jobSuccessful(cacheKey);\n                            } else {\n                                jobFailed(cacheKey);\n                            }\n                        } catch (final IOException ex) {\n                            jobFailed(cacheKey);\n                            LOG.debug(\"Asynchronous revalidation failed due to I/O error\", ex);\n                        } catch (final HttpException ex) {\n                            jobFailed(cacheKey);\n                            LOG.error(\"HTTP protocol exception during asynchronous revalidation\", ex);\n                        } catch (final RuntimeException ex) {\n                            jobFailed(cacheKey);\n                            log.error(\"Unexpected runtime exception thrown during asynchronous revalidation\", ex);\n                        }\n\n                    }\n\n                });\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2394,
        "instruction": "private void processFailures(Map<KeyExtent,List<Range>> failures, ResultReceiver receiver, List<Column> columns) throws AccumuloException,\n    AccumuloSecurityException, TableNotFoundException {\n  if (log.isTraceEnabled())\n\n  try {\n    Thread.sleep(failSleepTime);\n  } catch (InterruptedException e) {\n    Thread.currentThread().interrupt();\n\n    // We were interrupted (close called on batchscanner) just exit\n    log.debug(\"Exiting failure processing on interrupt\");\n    return;\n  }\n\n  failSleepTime = Math.min(5000, failSleepTime * 2);\n\n  Map<String,Map<KeyExtent,List<Range>>> binnedRanges = new HashMap<String,Map<KeyExtent,List<Range>>>();\n  List<Range> allRanges = new ArrayList<Range>();\n\n  for (List<Range> ranges : failures.values())\n    allRanges.addAll(ranges);\n\n  // since the first call to binRanges clipped the ranges to within a tablet, we should not get only\n  // bin to the set of failed tablets\n  binRanges(locator, allRanges, binnedRanges);\n\n  doLookups(binnedRanges, receiver, columns);\n}",
        "accept_response": "private void processFailures(Map<KeyExtent,List<Range>> failures, ResultReceiver receiver, List<Column> columns) throws AccumuloException,\n    AccumuloSecurityException, TableNotFoundException {\n  if (log.isTraceEnabled())\n    log.trace(\"Failed to execute multiscans against {} tablets, retrying...\", failures.size());\n\n  try {\n    Thread.sleep(failSleepTime);\n  } catch (InterruptedException e) {\n    Thread.currentThread().interrupt();\n\n    // We were interrupted (close called on batchscanner) just exit\n    log.debug(\"Exiting failure processing on interrupt\");\n    return;\n  }\n\n  failSleepTime = Math.min(5000, failSleepTime * 2);\n\n  Map<String,Map<KeyExtent,List<Range>>> binnedRanges = new HashMap<String,Map<KeyExtent,List<Range>>>();\n  List<Range> allRanges = new ArrayList<Range>();\n\n  for (List<Range> ranges : failures.values())\n    allRanges.addAll(ranges);\n\n  // since the first call to binRanges clipped the ranges to within a tablet, we should not get only\n  // bin to the set of failed tablets\n  binRanges(locator, allRanges, binnedRanges);\n\n  doLookups(binnedRanges, receiver, columns);\n}",
        "reject_response": "private void processFailures(Map<KeyExtent,List<Range>> failures, ResultReceiver receiver, List<Column> columns) throws AccumuloException,\n    AccumuloSecurityException, TableNotFoundException {\n  if (log.isTraceEnabled())\n    log.trace(\"Failed to execute multiscans against \" + failures.size() + \" tablets, retrying...\");\n\n  try {\n    Thread.sleep(failSleepTime);\n  } catch (InterruptedException e) {\n    Thread.currentThread().interrupt();\n\n    // We were interrupted (close called on batchscanner) just exit\n    log.debug(\"Exiting failure processing on interrupt\");\n    return;\n  }\n\n  failSleepTime = Math.min(5000, failSleepTime * 2);\n\n  Map<String,Map<KeyExtent,List<Range>>> binnedRanges = new HashMap<String,Map<KeyExtent,List<Range>>>();\n  List<Range> allRanges = new ArrayList<Range>();\n\n  for (List<Range> ranges : failures.values())\n    allRanges.addAll(ranges);\n\n  // since the first call to binRanges clipped the ranges to within a tablet, we should not get only\n  // bin to the set of failed tablets\n  binRanges(locator, allRanges, binnedRanges);\n\n  doLookups(binnedRanges, receiver, columns);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2485,
        "instruction": "@Override\npublic void close() throws IOException {\n  if (reader != null) {\n    reader.close();\n    reader = null;\n  }\n}",
        "accept_response": "@Override\npublic void close() throws IOException {\n  LOG.info(\"Closing reader after reading {} records.\", recordsReturned);\n  if (reader != null) {\n    reader.close();\n    reader = null;\n  }\n}",
        "reject_response": "@Override\npublic void close() throws IOException {\n  logger.info(\"Closing reader after reading {} records.\", recordsReturned);\n  if (reader != null) {\n    reader.close();\n    reader = null;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2927,
        "instruction": "private void removeStandbyTasks() {\n\n    standbyTasks.clear();\n    standbyTasksByPartition.clear();\n    standbyRecords.clear();\n}",
        "accept_response": "private void removeStandbyTasks() {\n    log.debug(\"{} Removing all standby tasks {}\", logPrefix, standbyTasks.keySet());\n\n    standbyTasks.clear();\n    standbyTasksByPartition.clear();\n    standbyRecords.clear();\n}",
        "reject_response": "private void removeStandbyTasks() {\n    log.info(\"{} Removing all standby tasks {}\", logPrefix, standbyTasks.keySet());\n\n    standbyTasks.clear();\n    standbyTasksByPartition.clear();\n    standbyRecords.clear();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2626,
        "instruction": "@RequestMapping(value = \"/clearAlerts\", method = RequestMethod.GET)\npublic void clearAlerts(HttpServletRequest request, HttpServletResponse response)\n    throws IOException {\n  int alertType;\n  ObjectNode responseJSON = mapper.createObjectNode();\n\n  try {\n    alertType = Integer.valueOf(request.getParameter(\"alertType\"));\n  } catch (NumberFormatException e) {\n    // Empty json response\n    response.getOutputStream().write(responseJSON.toString().getBytes());\n    return;\n  }\n\n  try {\n    boolean isClearAll = Boolean.valueOf(request.getParameter(\"clearAll\"));\n    // get cluster object\n    Cluster cluster = Repository.get().getCluster();\n    cluster.clearAlerts(alertType, isClearAll);\n    responseJSON.put(\"status\", \"deleted\");\n    responseJSON.put(\"systemAlerts\",\n        SystemAlertsService.getAlertsJson(cluster, cluster.getNotificationPageNumber()));\n    responseJSON.put(\"pageNumber\", cluster.getNotificationPageNumber());\n\n    boolean isGFConnected = cluster.isConnectedFlag();\n    if (isGFConnected) {\n      responseJSON.put(\"connectedFlag\", isGFConnected);\n    } else {\n      responseJSON.put(\"connectedFlag\", isGFConnected);\n      responseJSON.put(\"connectedErrorMsg\", cluster.getConnectionErrorMsg());\n    }\n  } catch (Exception e) {\n    logger.debug(\"Exception Occurred : \", e);\n  }\n\n  // Send json response\n  response.getOutputStream().write(responseJSON.toString().getBytes());\n}",
        "accept_response": "@RequestMapping(value = \"/clearAlerts\", method = RequestMethod.GET)\npublic void clearAlerts(HttpServletRequest request, HttpServletResponse response)\n    throws IOException {\n  int alertType;\n  ObjectNode responseJSON = mapper.createObjectNode();\n\n  try {\n    alertType = Integer.valueOf(request.getParameter(\"alertType\"));\n  } catch (NumberFormatException e) {\n    // Empty json response\n    response.getOutputStream().write(responseJSON.toString().getBytes());\n    logger.debug(e);\n    return;\n  }\n\n  try {\n    boolean isClearAll = Boolean.valueOf(request.getParameter(\"clearAll\"));\n    // get cluster object\n    Cluster cluster = Repository.get().getCluster();\n    cluster.clearAlerts(alertType, isClearAll);\n    responseJSON.put(\"status\", \"deleted\");\n    responseJSON.put(\"systemAlerts\",\n        SystemAlertsService.getAlertsJson(cluster, cluster.getNotificationPageNumber()));\n    responseJSON.put(\"pageNumber\", cluster.getNotificationPageNumber());\n\n    boolean isGFConnected = cluster.isConnectedFlag();\n    if (isGFConnected) {\n      responseJSON.put(\"connectedFlag\", isGFConnected);\n    } else {\n      responseJSON.put(\"connectedFlag\", isGFConnected);\n      responseJSON.put(\"connectedErrorMsg\", cluster.getConnectionErrorMsg());\n    }\n  } catch (Exception e) {\n    logger.debug(\"Exception Occurred : \", e);\n  }\n\n  // Send json response\n  response.getOutputStream().write(responseJSON.toString().getBytes());\n}",
        "reject_response": "@RequestMapping(value = \"/clearAlerts\", method = RequestMethod.GET)\npublic void clearAlerts(HttpServletRequest request, HttpServletResponse response)\n    throws IOException {\n  int alertType;\n  ObjectNode responseJSON = mapper.createObjectNode();\n\n  try {\n    alertType = Integer.valueOf(request.getParameter(\"alertType\"));\n  } catch (NumberFormatException e) {\n    // Empty json response\n    response.getOutputStream().write(responseJSON.toString().getBytes());\n    if (LOGGER.finerEnabled()) {\n      LOGGER.finer(e.getMessage());\n    }\n    return;\n  }\n\n  try {\n    boolean isClearAll = Boolean.valueOf(request.getParameter(\"clearAll\"));\n    // get cluster object\n    Cluster cluster = Repository.get().getCluster();\n    cluster.clearAlerts(alertType, isClearAll);\n    responseJSON.put(\"status\", \"deleted\");\n    responseJSON.put(\"systemAlerts\",\n        SystemAlertsService.getAlertsJson(cluster, cluster.getNotificationPageNumber()));\n    responseJSON.put(\"pageNumber\", cluster.getNotificationPageNumber());\n\n    boolean isGFConnected = cluster.isConnectedFlag();\n    if (isGFConnected) {\n      responseJSON.put(\"connectedFlag\", isGFConnected);\n    } else {\n      responseJSON.put(\"connectedFlag\", isGFConnected);\n      responseJSON.put(\"connectedErrorMsg\", cluster.getConnectionErrorMsg());\n    }\n  } catch (Exception e) {\n    logger.debug(\"Exception Occurred : \", e);\n  }\n\n  // Send json response\n  response.getOutputStream().write(responseJSON.toString().getBytes());\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2574,
        "instruction": "@Transactional\n@Override\npublic CommandProcessingResult createCheck(final JsonCommand command) {\n\n    try {\n        this.context.authenticatedUser();\n\n        this.fromApiJsonDeserializer.validateForCreate(command.json());\n\n        // check if the datatable is linked to the entity\n\n        String datatableName = command.stringValueOfParameterNamed(\"datatableName\");\n        DatatableData datatableData = this.readWriteNonCoreDataService.retrieveDatatable(datatableName);\n\n        if (datatableData == null) { throw new DatatableNotFoundException(datatableName); }\n\n        final String entity = command.stringValueOfParameterNamed(\"entity\");\n        final String foreignKeyColumnName = EntityTables.getForeignKeyColumnNameOnDatatable(entity);\n        final boolean columnExist = datatableData.hasColumn(foreignKeyColumnName);\n\n\n        if (!columnExist) { throw new EntityDatatableCheckNotSupportedException(datatableData.getRegisteredTableName(), entity); }\n\n        final Long productId = command.longValueOfParameterNamed(\"productId\");\n        final Long status = command.longValueOfParameterNamed(\"status\");\n\n        List<EntityDatatableChecks> entityDatatableCheck = null;\n        if (productId == null) {\n            entityDatatableCheck = this.entityDatatableChecksRepository.findByEntityStatusAndDatatableIdAndNoProduct(entity, status,\n                    datatableName);\n            if (!entityDatatableCheck.isEmpty()) { throw new EntityDatatableCheckAlreadyExistsException(entity, status, datatableName); }\n        } else {\n            if (entity.equals(\"m_loan\")) {\n                // if invalid loan product id, throws exception\n                this.loanProductReadPlatformService.retrieveLoanProduct(productId);\n            } else if (entity.equals(\"m_savings_account\")) {\n                // if invalid savings product id, throws exception\n                this.savingsProductReadPlatformService.retrieveOne(productId);\n            } else {\n                throw new EntityDatatableCheckNotSupportedException(entity, productId);\n            }\n            entityDatatableCheck = this.entityDatatableChecksRepository.findByEntityStatusAndDatatableIdAndProductId(entity, status,\n                    datatableName, productId);\n            if (!entityDatatableCheck.isEmpty()) { throw new EntityDatatableCheckAlreadyExistsException(entity, status, datatableName,\n                    productId); }\n        }\n\n        final EntityDatatableChecks check = EntityDatatableChecks.fromJson(command);\n\n        this.entityDatatableChecksRepository.saveAndFlush(check);\n\n        return new CommandProcessingResultBuilder() //\n                .withCommandId(command.commandId()) //\n                .withEntityId(check.getId()) //\n                .build();\n    } catch (final DataAccessException e) {\n        handleReportDataIntegrityIssues(command, e.getMostSpecificCause(), e);\n        return CommandProcessingResult.empty();\n    } catch (final PersistenceException dve) {\n        Throwable throwable = ExceptionUtils.getRootCause(dve.getCause());\n        handleReportDataIntegrityIssues(command, throwable, dve);\n        return CommandProcessingResult.empty();\n    }\n}",
        "accept_response": "@Transactional\n@Override\npublic CommandProcessingResult createCheck(final JsonCommand command) {\n\n    try {\n        this.context.authenticatedUser();\n\n        this.fromApiJsonDeserializer.validateForCreate(command.json());\n\n        // check if the datatable is linked to the entity\n\n        String datatableName = command.stringValueOfParameterNamed(\"datatableName\");\n        DatatableData datatableData = this.readWriteNonCoreDataService.retrieveDatatable(datatableName);\n\n        if (datatableData == null) { throw new DatatableNotFoundException(datatableName); }\n\n        final String entity = command.stringValueOfParameterNamed(\"entity\");\n        final String foreignKeyColumnName = EntityTables.getForeignKeyColumnNameOnDatatable(entity);\n        final boolean columnExist = datatableData.hasColumn(foreignKeyColumnName);\n\n        logger.info(\"{} has column {} ? {}\", new Object[] { datatableData.getRegisteredTableName(), foreignKeyColumnName, columnExist });\n\n        if (!columnExist) { throw new EntityDatatableCheckNotSupportedException(datatableData.getRegisteredTableName(), entity); }\n\n        final Long productId = command.longValueOfParameterNamed(\"productId\");\n        final Long status = command.longValueOfParameterNamed(\"status\");\n\n        List<EntityDatatableChecks> entityDatatableCheck = null;\n        if (productId == null) {\n            entityDatatableCheck = this.entityDatatableChecksRepository.findByEntityStatusAndDatatableIdAndNoProduct(entity, status,\n                    datatableName);\n            if (!entityDatatableCheck.isEmpty()) { throw new EntityDatatableCheckAlreadyExistsException(entity, status, datatableName); }\n        } else {\n            if (entity.equals(\"m_loan\")) {\n                // if invalid loan product id, throws exception\n                this.loanProductReadPlatformService.retrieveLoanProduct(productId);\n            } else if (entity.equals(\"m_savings_account\")) {\n                // if invalid savings product id, throws exception\n                this.savingsProductReadPlatformService.retrieveOne(productId);\n            } else {\n                throw new EntityDatatableCheckNotSupportedException(entity, productId);\n            }\n            entityDatatableCheck = this.entityDatatableChecksRepository.findByEntityStatusAndDatatableIdAndProductId(entity, status,\n                    datatableName, productId);\n            if (!entityDatatableCheck.isEmpty()) { throw new EntityDatatableCheckAlreadyExistsException(entity, status, datatableName,\n                    productId); }\n        }\n\n        final EntityDatatableChecks check = EntityDatatableChecks.fromJson(command);\n\n        this.entityDatatableChecksRepository.saveAndFlush(check);\n\n        return new CommandProcessingResultBuilder() //\n                .withCommandId(command.commandId()) //\n                .withEntityId(check.getId()) //\n                .build();\n    } catch (final DataAccessException e) {\n        handleReportDataIntegrityIssues(command, e.getMostSpecificCause(), e);\n        return CommandProcessingResult.empty();\n    } catch (final PersistenceException dve) {\n        Throwable throwable = ExceptionUtils.getRootCause(dve.getCause());\n        handleReportDataIntegrityIssues(command, throwable, dve);\n        return CommandProcessingResult.empty();\n    }\n}",
        "reject_response": "@Transactional\n@Override\npublic CommandProcessingResult createCheck(final JsonCommand command) {\n\n    try {\n        this.context.authenticatedUser();\n\n        this.fromApiJsonDeserializer.validateForCreate(command.json());\n\n        // check if the datatable is linked to the entity\n\n        String datatableName = command.stringValueOfParameterNamed(\"datatableName\");\n        DatatableData datatableData = this.readWriteNonCoreDataService.retrieveDatatable(datatableName);\n\n        if (datatableData == null) { throw new DatatableNotFoundException(datatableName); }\n\n        final String entity = command.stringValueOfParameterNamed(\"entity\");\n        final String foreignKeyColumnName = EntityTables.getForeignKeyColumnNameOnDatatable(entity);\n        final boolean columnExist = datatableData.hasColumn(foreignKeyColumnName);\n\n        logger.info(datatableData.getRegisteredTableName() + \"has column \" + foreignKeyColumnName + \" ? \" + columnExist);\n\n        if (!columnExist) { throw new EntityDatatableCheckNotSupportedException(datatableData.getRegisteredTableName(), entity); }\n\n        final Long productId = command.longValueOfParameterNamed(\"productId\");\n        final Long status = command.longValueOfParameterNamed(\"status\");\n\n        List<EntityDatatableChecks> entityDatatableCheck = null;\n        if (productId == null) {\n            entityDatatableCheck = this.entityDatatableChecksRepository.findByEntityStatusAndDatatableIdAndNoProduct(entity, status,\n                    datatableName);\n            if (!entityDatatableCheck.isEmpty()) { throw new EntityDatatableCheckAlreadyExistsException(entity, status, datatableName); }\n        } else {\n            if (entity.equals(\"m_loan\")) {\n                // if invalid loan product id, throws exception\n                this.loanProductReadPlatformService.retrieveLoanProduct(productId);\n            } else if (entity.equals(\"m_savings_account\")) {\n                // if invalid savings product id, throws exception\n                this.savingsProductReadPlatformService.retrieveOne(productId);\n            } else {\n                throw new EntityDatatableCheckNotSupportedException(entity, productId);\n            }\n            entityDatatableCheck = this.entityDatatableChecksRepository.findByEntityStatusAndDatatableIdAndProductId(entity, status,\n                    datatableName, productId);\n            if (!entityDatatableCheck.isEmpty()) { throw new EntityDatatableCheckAlreadyExistsException(entity, status, datatableName,\n                    productId); }\n        }\n\n        final EntityDatatableChecks check = EntityDatatableChecks.fromJson(command);\n\n        this.entityDatatableChecksRepository.saveAndFlush(check);\n\n        return new CommandProcessingResultBuilder() //\n                .withCommandId(command.commandId()) //\n                .withEntityId(check.getId()) //\n                .build();\n    } catch (final DataAccessException e) {\n        handleReportDataIntegrityIssues(command, e.getMostSpecificCause(), e);\n        return CommandProcessingResult.empty();\n    } catch (final PersistenceException dve) {\n        Throwable throwable = ExceptionUtils.getRootCause(dve.getCause());\n        handleReportDataIntegrityIssues(command, throwable, dve);\n        return CommandProcessingResult.empty();\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2783,
        "instruction": "ClassicHttpResponse cacheAndReturnResponse(\n        final HttpHost target,\n        final HttpRequest request,\n        final ClassicHttpResponse backendResponse,\n        final ExecChain.Scope scope,\n        final Date requestSent,\n        final Date responseReceived) throws IOException {\n    final ByteArrayBuffer buf;\n    final HttpEntity entity = backendResponse.getEntity();\n    if (entity != null) {\n        buf = new ByteArrayBuffer(1024);\n        final InputStream inStream = entity.getContent();\n        final byte[] tmp = new byte[2048];\n        long total = 0;\n        int l;\n        while ((l = inStream.read(tmp)) != -1) {\n            buf.append(tmp, 0, l);\n            total += l;\n            if (total > cacheConfig.getMaxObjectSize()) {\n                LOG.debug(\"Backend response content length exceeds maximum\");\n                backendResponse.setEntity(new CombinedEntity(entity, buf));\n                return backendResponse;\n            }\n        }\n    } else {\n        buf = null;\n    }\n    backendResponse.close();\n\n    final HttpCacheEntry cacheEntry;\n    if (cacheConfig.isFreshnessCheckEnabled()) {\n        final HttpCacheEntry existingEntry = responseCache.getCacheEntry(target, request);\n        if (DateUtils.isAfter(existingEntry, backendResponse, HttpHeaders.DATE)) {\n            LOG.debug(\"Backend already contains fresher cache entry\");\n            cacheEntry = existingEntry;\n        } else {\n            cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n            LOG.debug(\"Backend response successfully cached\");\n        }\n    } else {\n        cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n        LOG.debug(\"Backend response successfully cached (freshness check skipped)\");\n    }\n    return convert(responseGenerator.generateResponse(request, cacheEntry), scope);\n}",
        "accept_response": "ClassicHttpResponse cacheAndReturnResponse(\n        final HttpHost target,\n        final HttpRequest request,\n        final ClassicHttpResponse backendResponse,\n        final ExecChain.Scope scope,\n        final Date requestSent,\n        final Date responseReceived) throws IOException {\n    LOG.debug(\"Caching backend response\");\n    final ByteArrayBuffer buf;\n    final HttpEntity entity = backendResponse.getEntity();\n    if (entity != null) {\n        buf = new ByteArrayBuffer(1024);\n        final InputStream inStream = entity.getContent();\n        final byte[] tmp = new byte[2048];\n        long total = 0;\n        int l;\n        while ((l = inStream.read(tmp)) != -1) {\n            buf.append(tmp, 0, l);\n            total += l;\n            if (total > cacheConfig.getMaxObjectSize()) {\n                LOG.debug(\"Backend response content length exceeds maximum\");\n                backendResponse.setEntity(new CombinedEntity(entity, buf));\n                return backendResponse;\n            }\n        }\n    } else {\n        buf = null;\n    }\n    backendResponse.close();\n\n    final HttpCacheEntry cacheEntry;\n    if (cacheConfig.isFreshnessCheckEnabled()) {\n        final HttpCacheEntry existingEntry = responseCache.getCacheEntry(target, request);\n        if (DateUtils.isAfter(existingEntry, backendResponse, HttpHeaders.DATE)) {\n            LOG.debug(\"Backend already contains fresher cache entry\");\n            cacheEntry = existingEntry;\n        } else {\n            cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n            LOG.debug(\"Backend response successfully cached\");\n        }\n    } else {\n        cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n        LOG.debug(\"Backend response successfully cached (freshness check skipped)\");\n    }\n    return convert(responseGenerator.generateResponse(request, cacheEntry), scope);\n}",
        "reject_response": "ClassicHttpResponse cacheAndReturnResponse(\n        final HttpHost target,\n        final HttpRequest request,\n        final ClassicHttpResponse backendResponse,\n        final ExecChain.Scope scope,\n        final Date requestSent,\n        final Date responseReceived) throws IOException {\n    log.debug(\"Caching backend response\");\n    final ByteArrayBuffer buf;\n    final HttpEntity entity = backendResponse.getEntity();\n    if (entity != null) {\n        buf = new ByteArrayBuffer(1024);\n        final InputStream inStream = entity.getContent();\n        final byte[] tmp = new byte[2048];\n        long total = 0;\n        int l;\n        while ((l = inStream.read(tmp)) != -1) {\n            buf.append(tmp, 0, l);\n            total += l;\n            if (total > cacheConfig.getMaxObjectSize()) {\n                LOG.debug(\"Backend response content length exceeds maximum\");\n                backendResponse.setEntity(new CombinedEntity(entity, buf));\n                return backendResponse;\n            }\n        }\n    } else {\n        buf = null;\n    }\n    backendResponse.close();\n\n    final HttpCacheEntry cacheEntry;\n    if (cacheConfig.isFreshnessCheckEnabled()) {\n        final HttpCacheEntry existingEntry = responseCache.getCacheEntry(target, request);\n        if (DateUtils.isAfter(existingEntry, backendResponse, HttpHeaders.DATE)) {\n            LOG.debug(\"Backend already contains fresher cache entry\");\n            cacheEntry = existingEntry;\n        } else {\n            cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n            LOG.debug(\"Backend response successfully cached\");\n        }\n    } else {\n        cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n        LOG.debug(\"Backend response successfully cached (freshness check skipped)\");\n    }\n    return convert(responseGenerator.generateResponse(request, cacheEntry), scope);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3191,
        "instruction": "private org.apache.hadoop.yarn.api.records.Token generateTokenAndSetTimer(\n    ApplicationId appId, AppLevelTimelineCollector appCollector)\n    throws IOException {\n  Token<TimelineDelegationTokenIdentifier> timelineToken =\n      generateTokenForAppCollector(appCollector.getAppUser());\n  TimelineDelegationTokenIdentifier tokenId =\n      timelineToken.decodeIdentifier();\n  long renewalDelay = getRenewalDelay(tokenRenewInterval);\n  long regenerationDelay = getRegenerationDelay(tokenId.getMaxDate());\n  if (renewalDelay > 0 || regenerationDelay > 0) {\n    boolean isTimerForRenewal = renewalDelay < regenerationDelay;\n    Future<?> renewalOrRegenerationFuture = tokenRenewalExecutor.schedule(\n        new CollectorTokenRenewer(appId, isTimerForRenewal),\n        isTimerForRenewal? renewalDelay : regenerationDelay,\n        TimeUnit.MILLISECONDS);\n    appCollector.setDelegationTokenAndFutureForApp(timelineToken,\n        renewalOrRegenerationFuture, tokenId.getMaxDate(),\n        tokenId.getRenewer().toString());\n  }\n  return org.apache.hadoop.yarn.api.records.Token.newInstance(\n      timelineToken.getIdentifier(), timelineToken.getKind().toString(),\n      timelineToken.getPassword(), timelineToken.getService().toString());\n}",
        "accept_response": "private org.apache.hadoop.yarn.api.records.Token generateTokenAndSetTimer(\n    ApplicationId appId, AppLevelTimelineCollector appCollector)\n    throws IOException {\n  Token<TimelineDelegationTokenIdentifier> timelineToken =\n      generateTokenForAppCollector(appCollector.getAppUser());\n  TimelineDelegationTokenIdentifier tokenId =\n      timelineToken.decodeIdentifier();\n  long renewalDelay = getRenewalDelay(tokenRenewInterval);\n  long regenerationDelay = getRegenerationDelay(tokenId.getMaxDate());\n  if (renewalDelay > 0 || regenerationDelay > 0) {\n    boolean isTimerForRenewal = renewalDelay < regenerationDelay;\n    Future<?> renewalOrRegenerationFuture = tokenRenewalExecutor.schedule(\n        new CollectorTokenRenewer(appId, isTimerForRenewal),\n        isTimerForRenewal? renewalDelay : regenerationDelay,\n        TimeUnit.MILLISECONDS);\n    appCollector.setDelegationTokenAndFutureForApp(timelineToken,\n        renewalOrRegenerationFuture, tokenId.getMaxDate(),\n        tokenId.getRenewer().toString());\n  }\n  LOG.info(\"Generated a new token {} for app {}\", timelineToken, appId);\n  return org.apache.hadoop.yarn.api.records.Token.newInstance(\n      timelineToken.getIdentifier(), timelineToken.getKind().toString(),\n      timelineToken.getPassword(), timelineToken.getService().toString());\n}",
        "reject_response": "private org.apache.hadoop.yarn.api.records.Token generateTokenAndSetTimer(\n    ApplicationId appId, AppLevelTimelineCollector appCollector)\n    throws IOException {\n  Token<TimelineDelegationTokenIdentifier> timelineToken =\n      generateTokenForAppCollector(appCollector.getAppUser());\n  TimelineDelegationTokenIdentifier tokenId =\n      timelineToken.decodeIdentifier();\n  long renewalDelay = getRenewalDelay(tokenRenewInterval);\n  long regenerationDelay = getRegenerationDelay(tokenId.getMaxDate());\n  if (renewalDelay > 0 || regenerationDelay > 0) {\n    boolean isTimerForRenewal = renewalDelay < regenerationDelay;\n    Future<?> renewalOrRegenerationFuture = tokenRenewalExecutor.schedule(\n        new CollectorTokenRenewer(appId, isTimerForRenewal),\n        isTimerForRenewal? renewalDelay : regenerationDelay,\n        TimeUnit.MILLISECONDS);\n    appCollector.setDelegationTokenAndFutureForApp(timelineToken,\n        renewalOrRegenerationFuture, tokenId.getMaxDate(),\n        tokenId.getRenewer().toString());\n  }\n  LOG.info(\"Generated a new token \" + timelineToken + \" for app \" + appId);\n  return org.apache.hadoop.yarn.api.records.Token.newInstance(\n      timelineToken.getIdentifier(), timelineToken.getKind().toString(),\n      timelineToken.getPassword(), timelineToken.getService().toString());\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2890,
        "instruction": "void expandDotCell(int i, int j) {\n\n  /*\n   * (1) If the dot is just to the left of a non-terminal variable, we look for theorems or axioms\n   * in the Chart that may apply and extend the dot position. We look for existing axioms over all\n   * spans (k,j), i < k < j.\n   */\n  for (int k = i + 1; k < j; k++) {\n    extendDotItemsWithProvedItems(i, k, j, false);\n  }\n\n  /*\n   * (2) If the the dot-item is looking for a source-side terminal symbol, we simply match against\n   * the input sentence and advance the dot.\n   */\n  Node<Token> node = input.getNode(j - 1);\n  for (Arc<Token> arc : node.getOutgoingArcs()) {\n\n    int last_word = arc.getLabel().getWord();\n    int arc_len = arc.getHead().getNumber() - arc.getTail().getNumber();\n\n    // int last_word=foreign_sent[j-1]; // input.getNode(j-1).getNumber(); //\n\n    if (null != dotcells.get(i, j - 1)) {\n      // dotitem in dot_bins[i][k]: looking for an item in the right to the dot\n\n\n      for (DotNode dotNode : dotcells.get(i, j - 1).getDotNodes()) {\n\n        // String arcWord = Vocabulary.word(last_word);\n        // Assert.assertFalse(arcWord.endsWith(\"]\"));\n        // Assert.assertFalse(arcWord.startsWith(\"[\"));\n        // logger.info(\"DotChart.expandDotCell: \" + arcWord);\n\n        // List<Trie> child_tnodes = ruleMatcher.produceMatchingChildTNodesTerminalevel(dotNode,\n        // last_word);\n\n        List<Trie> child_tnodes = null;\n\n        if (this.regexpMatching) {\n          child_tnodes = matchAll(dotNode, last_word);\n        } else {\n          Trie child_node = dotNode.trieNode.match(last_word);\n          child_tnodes = Arrays.asList(child_node);\n        }\n\n        if (!(child_tnodes == null || child_tnodes.isEmpty())) {\n          for (Trie child_tnode : child_tnodes) {\n            if (null != child_tnode) {\n              addDotItem(child_tnode, i, j - 1 + arc_len, dotNode.antSuperNodes, null,\n                  dotNode.srcPath.extend(arc));\n            }\n          }\n        }\n      }\n    }\n  }\n}",
        "accept_response": "void expandDotCell(int i, int j) {\n    LOG.debug(\"Expanding dot cell ({}, {})\", i, j);\n\n  /*\n   * (1) If the dot is just to the left of a non-terminal variable, we look for theorems or axioms\n   * in the Chart that may apply and extend the dot position. We look for existing axioms over all\n   * spans (k,j), i < k < j.\n   */\n  for (int k = i + 1; k < j; k++) {\n    extendDotItemsWithProvedItems(i, k, j, false);\n  }\n\n  /*\n   * (2) If the the dot-item is looking for a source-side terminal symbol, we simply match against\n   * the input sentence and advance the dot.\n   */\n  Node<Token> node = input.getNode(j - 1);\n  for (Arc<Token> arc : node.getOutgoingArcs()) {\n\n    int last_word = arc.getLabel().getWord();\n    int arc_len = arc.getHead().getNumber() - arc.getTail().getNumber();\n\n    // int last_word=foreign_sent[j-1]; // input.getNode(j-1).getNumber(); //\n\n    if (null != dotcells.get(i, j - 1)) {\n      // dotitem in dot_bins[i][k]: looking for an item in the right to the dot\n\n\n      for (DotNode dotNode : dotcells.get(i, j - 1).getDotNodes()) {\n\n        // String arcWord = Vocabulary.word(last_word);\n        // Assert.assertFalse(arcWord.endsWith(\"]\"));\n        // Assert.assertFalse(arcWord.startsWith(\"[\"));\n        // logger.info(\"DotChart.expandDotCell: \" + arcWord);\n\n        // List<Trie> child_tnodes = ruleMatcher.produceMatchingChildTNodesTerminalevel(dotNode,\n        // last_word);\n\n        List<Trie> child_tnodes = null;\n\n        if (this.regexpMatching) {\n          child_tnodes = matchAll(dotNode, last_word);\n        } else {\n          Trie child_node = dotNode.trieNode.match(last_word);\n          child_tnodes = Arrays.asList(child_node);\n        }\n\n        if (!(child_tnodes == null || child_tnodes.isEmpty())) {\n          for (Trie child_tnode : child_tnodes) {\n            if (null != child_tnode) {\n              addDotItem(child_tnode, i, j - 1 + arc_len, dotNode.antSuperNodes, null,\n                  dotNode.srcPath.extend(arc));\n            }\n          }\n        }\n      }\n    }\n  }\n}",
        "reject_response": "void expandDotCell(int i, int j) {\n  if (logger.isLoggable(Level.FINEST))\n    logger.finest(\"Expanding dot cell (\" + i + \",\" + j + \")\");\n\n  /*\n   * (1) If the dot is just to the left of a non-terminal variable, we look for theorems or axioms\n   * in the Chart that may apply and extend the dot position. We look for existing axioms over all\n   * spans (k,j), i < k < j.\n   */\n  for (int k = i + 1; k < j; k++) {\n    extendDotItemsWithProvedItems(i, k, j, false);\n  }\n\n  /*\n   * (2) If the the dot-item is looking for a source-side terminal symbol, we simply match against\n   * the input sentence and advance the dot.\n   */\n  Node<Token> node = input.getNode(j - 1);\n  for (Arc<Token> arc : node.getOutgoingArcs()) {\n\n    int last_word = arc.getLabel().getWord();\n    int arc_len = arc.getHead().getNumber() - arc.getTail().getNumber();\n\n    // int last_word=foreign_sent[j-1]; // input.getNode(j-1).getNumber(); //\n\n    if (null != dotcells.get(i, j - 1)) {\n      // dotitem in dot_bins[i][k]: looking for an item in the right to the dot\n\n\n      for (DotNode dotNode : dotcells.get(i, j - 1).getDotNodes()) {\n\n        // String arcWord = Vocabulary.word(last_word);\n        // Assert.assertFalse(arcWord.endsWith(\"]\"));\n        // Assert.assertFalse(arcWord.startsWith(\"[\"));\n        // logger.info(\"DotChart.expandDotCell: \" + arcWord);\n\n        // List<Trie> child_tnodes = ruleMatcher.produceMatchingChildTNodesTerminalevel(dotNode,\n        // last_word);\n\n        List<Trie> child_tnodes = null;\n\n        if (this.regexpMatching) {\n          child_tnodes = matchAll(dotNode, last_word);\n        } else {\n          Trie child_node = dotNode.trieNode.match(last_word);\n          child_tnodes = Arrays.asList(child_node);\n        }\n\n        if (!(child_tnodes == null || child_tnodes.isEmpty())) {\n          for (Trie child_tnode : child_tnodes) {\n            if (null != child_tnode) {\n              addDotItem(child_tnode, i, j - 1 + arc_len, dotNode.antSuperNodes, null,\n                  dotNode.srcPath.extend(arc));\n            }\n          }\n        }\n      }\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3216,
        "instruction": "public void process(WatchedEvent event) {\n    // lets either become the leader or watch the new/updated node\n    try {\n        lock();\n    } catch (Exception e) {\n        LOG.warn(\"Failed to acquire lock: \" + e, e);\n    }\n}",
        "accept_response": "public void process(WatchedEvent event) {\n    // lets either become the leader or watch the new/updated node\n    LOG.debug(\"Watcher fired: {}\", event);\n    try {\n        lock();\n    } catch (Exception e) {\n        LOG.warn(\"Failed to acquire lock: \" + e, e);\n    }\n}",
        "reject_response": "public void process(WatchedEvent event) {\n    // lets either become the leader or watch the new/updated node\n    LOG.debug(\"Watcher fired on path: \" + event.getPath() + \" state: \" + \n            event.getState() + \" type \" + event.getType());\n    try {\n        lock();\n    } catch (Exception e) {\n        LOG.warn(\"Failed to acquire lock: \" + e, e);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2897,
        "instruction": "private String matchMacro(String macro, MacroData macroData) throws PermErrorException, RequireClientDomainException {\n\n    String rValue = null;\n\n    String variable = macro.toLowerCase();\n    if (variable.equalsIgnoreCase(\"i\")) {\n        rValue = macroData.getMacroIpAddress();\n    } else if (variable.equalsIgnoreCase(\"s\")) {\n        rValue = macroData.getMailFrom();\n    } else if (variable.equalsIgnoreCase(\"h\")) {\n        rValue = macroData.getHostName();\n    } else if (variable.equalsIgnoreCase(\"l\")) {\n        rValue = macroData.getCurrentSenderPart();\n    } else if (variable.equalsIgnoreCase(\"d\")) {\n        rValue = macroData.getCurrentDomain();\n    } else if (variable.equalsIgnoreCase(\"v\")) {\n        rValue = macroData.getInAddress();\n    } else if (variable.equalsIgnoreCase(\"t\")) {\n        rValue = Long.toString(macroData.getTimeStamp());\n    } else if (variable.equalsIgnoreCase(\"c\")) {\n        rValue = macroData.getReadableIP();\n    } else if (variable.equalsIgnoreCase(\"p\")) {\n        rValue = macroData.getClientDomain();\n        if (rValue == null) {\n            throw new RequireClientDomainException();\n        }\n    } else if (variable.equalsIgnoreCase(\"o\")) {\n        rValue = macroData.getSenderDomain();\n    } else if (variable.equalsIgnoreCase(\"r\")) {\n        rValue = macroData.getReceivingDomain();\n        if (rValue == null) {\n            rValue = \"unknown\";\n            List<String> dNames = dnsProbe.getLocalDomainNames();\n\n            for (int i = 0; i < dNames.size(); i++) {\n                // check if the domainname is a FQDN\n                if (SPF1Utils.checkFQDN(dNames.get(i).toString())) {\n                    rValue = dNames.get(i).toString();\n                    if (macroData instanceof SPFSession) {\n                        ((SPFSession) macroData).setReceivingDomain(rValue);\n                    }\n                    break;\n                }\n            }\n        }\n    }\n\n    if (rValue == null) {\n        throw new PermErrorException(\"Unknown command : \" + variable);\n\n    } else {\n\n        return rValue;\n    }\n}",
        "accept_response": "private String matchMacro(String macro, MacroData macroData) throws PermErrorException, RequireClientDomainException {\n\n    String rValue = null;\n\n    String variable = macro.toLowerCase();\n    if (variable.equalsIgnoreCase(\"i\")) {\n        rValue = macroData.getMacroIpAddress();\n    } else if (variable.equalsIgnoreCase(\"s\")) {\n        rValue = macroData.getMailFrom();\n    } else if (variable.equalsIgnoreCase(\"h\")) {\n        rValue = macroData.getHostName();\n    } else if (variable.equalsIgnoreCase(\"l\")) {\n        rValue = macroData.getCurrentSenderPart();\n    } else if (variable.equalsIgnoreCase(\"d\")) {\n        rValue = macroData.getCurrentDomain();\n    } else if (variable.equalsIgnoreCase(\"v\")) {\n        rValue = macroData.getInAddress();\n    } else if (variable.equalsIgnoreCase(\"t\")) {\n        rValue = Long.toString(macroData.getTimeStamp());\n    } else if (variable.equalsIgnoreCase(\"c\")) {\n        rValue = macroData.getReadableIP();\n    } else if (variable.equalsIgnoreCase(\"p\")) {\n        rValue = macroData.getClientDomain();\n        if (rValue == null) {\n            throw new RequireClientDomainException();\n        }\n    } else if (variable.equalsIgnoreCase(\"o\")) {\n        rValue = macroData.getSenderDomain();\n    } else if (variable.equalsIgnoreCase(\"r\")) {\n        rValue = macroData.getReceivingDomain();\n        if (rValue == null) {\n            rValue = \"unknown\";\n            List<String> dNames = dnsProbe.getLocalDomainNames();\n\n            for (int i = 0; i < dNames.size(); i++) {\n                // check if the domainname is a FQDN\n                if (SPF1Utils.checkFQDN(dNames.get(i).toString())) {\n                    rValue = dNames.get(i).toString();\n                    if (macroData instanceof SPFSession) {\n                        ((SPFSession) macroData).setReceivingDomain(rValue);\n                    }\n                    break;\n                }\n            }\n        }\n    }\n\n    if (rValue == null) {\n        throw new PermErrorException(\"Unknown command : \" + variable);\n\n    } else {\n        LOGGER.debug(\"Used macro: {} replaced with: {}\", macro, rValue);\n\n        return rValue;\n    }\n}",
        "reject_response": "private String matchMacro(String macro, MacroData macroData) throws PermErrorException, RequireClientDomainException {\n\n    String rValue = null;\n\n    String variable = macro.toLowerCase();\n    if (variable.equalsIgnoreCase(\"i\")) {\n        rValue = macroData.getMacroIpAddress();\n    } else if (variable.equalsIgnoreCase(\"s\")) {\n        rValue = macroData.getMailFrom();\n    } else if (variable.equalsIgnoreCase(\"h\")) {\n        rValue = macroData.getHostName();\n    } else if (variable.equalsIgnoreCase(\"l\")) {\n        rValue = macroData.getCurrentSenderPart();\n    } else if (variable.equalsIgnoreCase(\"d\")) {\n        rValue = macroData.getCurrentDomain();\n    } else if (variable.equalsIgnoreCase(\"v\")) {\n        rValue = macroData.getInAddress();\n    } else if (variable.equalsIgnoreCase(\"t\")) {\n        rValue = Long.toString(macroData.getTimeStamp());\n    } else if (variable.equalsIgnoreCase(\"c\")) {\n        rValue = macroData.getReadableIP();\n    } else if (variable.equalsIgnoreCase(\"p\")) {\n        rValue = macroData.getClientDomain();\n        if (rValue == null) {\n            throw new RequireClientDomainException();\n        }\n    } else if (variable.equalsIgnoreCase(\"o\")) {\n        rValue = macroData.getSenderDomain();\n    } else if (variable.equalsIgnoreCase(\"r\")) {\n        rValue = macroData.getReceivingDomain();\n        if (rValue == null) {\n            rValue = \"unknown\";\n            List<String> dNames = dnsProbe.getLocalDomainNames();\n\n            for (int i = 0; i < dNames.size(); i++) {\n                // check if the domainname is a FQDN\n                if (SPF1Utils.checkFQDN(dNames.get(i).toString())) {\n                    rValue = dNames.get(i).toString();\n                    if (macroData instanceof SPFSession) {\n                        ((SPFSession) macroData).setReceivingDomain(rValue);\n                    }\n                    break;\n                }\n            }\n        }\n    }\n\n    if (rValue == null) {\n        throw new PermErrorException(\"Unknown command : \" + variable);\n\n    } else {\n        log.debug(\"Used macro: \" + macro + \" replaced with: \" + rValue);\n\n        return rValue;\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2895,
        "instruction": "public void evaluate(MailAdapter mail, Node startNode)\n        throws SieveException {\n    final SieveContext context = new BaseSieveContext(commandManager,\n            comparatorManager, testManager);\n    try {\n        // Ensure that the context is set on the mail\n        mail.setContext(context);\n\n        SieveParserVisitor visitor = new SieveParserVisitorImpl(context);\n        try {\n            // Evaluate the Nodes\n            startNode.jjtAccept(visitor, mail);\n\n        } catch (StopException ex) {\n            // Stop is OK\n        } catch (SieveException ex) {\n            LOGGER.error(\"Evaluation failed.\", ex);\n            throw ex;\n        }\n\n        // If after evaluating all of the nodes or stopping, implicitKeep is\n        // still\n        // in effect, add a Keep to the list of Actions.\n        if (context.getCommandStateManager().isImplicitKeep())\n            mail.addAction(new ActionKeep());\n\n        // Execute the List of Actions\n        try {\n            mail.executeActions();\n        } catch (SieveException ex) {\n            throw ex;\n        }\n    } finally {\n        // Tidy up by ensuring that a reference to the context is not held by the adapter.\n        // This prevents leaks when the adapter stores the context in a thread local variable.\n        mail.setContext(null);\n    }\n}",
        "accept_response": "public void evaluate(MailAdapter mail, Node startNode)\n        throws SieveException {\n    final SieveContext context = new BaseSieveContext(commandManager,\n            comparatorManager, testManager);\n    try {\n        // Ensure that the context is set on the mail\n        mail.setContext(context);\n\n        SieveParserVisitor visitor = new SieveParserVisitorImpl(context);\n        try {\n            // Evaluate the Nodes\n            startNode.jjtAccept(visitor, mail);\n\n        } catch (StopException ex) {\n            // Stop is OK\n        } catch (SieveException ex) {\n            LOGGER.error(\"Evaluation failed.\", ex);\n            throw ex;\n        }\n\n        // If after evaluating all of the nodes or stopping, implicitKeep is\n        // still\n        // in effect, add a Keep to the list of Actions.\n        if (context.getCommandStateManager().isImplicitKeep())\n            mail.addAction(new ActionKeep());\n\n        // Execute the List of Actions\n        try {\n            mail.executeActions();\n        } catch (SieveException ex) {\n            LOGGER.error(\"Evaluation failed.\", ex);\n            throw ex;\n        }\n    } finally {\n        // Tidy up by ensuring that a reference to the context is not held by the adapter.\n        // This prevents leaks when the adapter stores the context in a thread local variable.\n        mail.setContext(null);\n    }\n}",
        "reject_response": "public void evaluate(MailAdapter mail, Node startNode)\n        throws SieveException {\n    final SieveContext context = new BaseSieveContext(commandManager,\n            comparatorManager, testManager);\n    try {\n        // Ensure that the context is set on the mail\n        mail.setContext(context);\n\n        SieveParserVisitor visitor = new SieveParserVisitorImpl(context);\n        try {\n            // Evaluate the Nodes\n            startNode.jjtAccept(visitor, mail);\n\n        } catch (StopException ex) {\n            // Stop is OK\n        } catch (SieveException ex) {\n            LOGGER.error(\"Evaluation failed.\", ex);\n            throw ex;\n        }\n\n        // If after evaluating all of the nodes or stopping, implicitKeep is\n        // still\n        // in effect, add a Keep to the list of Actions.\n        if (context.getCommandStateManager().isImplicitKeep())\n            mail.addAction(new ActionKeep());\n\n        // Execute the List of Actions\n        try {\n            mail.executeActions();\n        } catch (SieveException ex) {\n            if (log.isErrorEnabled())\n                log.error(\"Evaluation failed. Reason: \" + ex.getMessage());\n            if (log.isDebugEnabled())\n                log.debug(\"Evaluation failed.\", ex);\n            throw ex;\n        }\n    } finally {\n        // Tidy up by ensuring that a reference to the context is not held by the adapter.\n        // This prevents leaks when the adapter stores the context in a thread local variable.\n        mail.setContext(null);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2598,
        "instruction": "public int run(String[] args) {\n\n\t//\n\t//\tCommand Line Options\n\t//\n\tOptions options = new Options();\n\tgetYARNSessionCLIOptions(options);\n\n\tCommandLineParser parser = new PosixParser();\n\tCommandLine cmd = null;\n\ttry {\n\t\tcmd = parser.parse(options, args);\n\t} catch(Exception e) {\n\t\tSystem.out.println(e.getMessage());\n\t\tprintUsage();\n\t\treturn 1;\n\t}\n\n\t// Query cluster for metrics\n\tif (cmd.hasOption(QUERY.getOpt())) {\n\t\tAbstractFlinkYarnClient flinkYarnClient = getFlinkYarnClient();\n\t\tString description = null;\n\t\ttry {\n\t\t\tdescription = flinkYarnClient.getClusterDescription();\n\t\t} catch (Exception e) {\n\t\t\tSystem.err.println(\"Error while querying the YARN cluster for available resources: \"+e.getMessage());\n\t\t\te.printStackTrace(System.err);\n\t\t\treturn 1;\n\t\t}\n\t\tSystem.out.println(description);\n\t\treturn 0;\n\t} else {\n\t\tAbstractFlinkYarnClient flinkYarnClient = createFlinkYarnClient(cmd);\n\n\t\tif (flinkYarnClient == null) {\n\t\t\tSystem.err.println(\"Error while starting the YARN Client. Please check log output!\");\n\t\t\treturn 1;\n\t\t}\n\n\n\t\ttry {\n\t\t\tyarnCluster = flinkYarnClient.deploy(null);\n\t\t\t// only connect to cluster if its not a detached session.\n\t\t\tif(!flinkYarnClient.isDetached()) {\n\t\t\t\tyarnCluster.connectToCluster();\n\t\t\t}\n\t\t} catch (Exception e) {\n\t\t\tSystem.err.println(\"Error while deploying YARN cluster: \"+e.getMessage());\n\t\t\te.printStackTrace(System.err);\n\t\t\treturn 1;\n\t\t}\n\t\t//------------------ Cluster deployed, handle connection details\n\t\tString jobManagerAddress = yarnCluster.getJobManagerAddress().getHostName() + \":\" + yarnCluster.getJobManagerAddress().getPort();\n\t\tSystem.out.println(\"Flink JobManager is now running on \" + jobManagerAddress);\n\t\tSystem.out.println(\"JobManager Web Interface: \" + yarnCluster.getWebInterfaceURL());\n\t\t// file that we write into the conf/ dir containing the jobManager address and the dop.\n\t\tString confDirPath = CliFrontend.getConfigurationDirectoryFromEnv();\n\t\tFile yarnPropertiesFile = new File(confDirPath + File.separator + CliFrontend.YARN_PROPERTIES_FILE);\n\n\t\tProperties yarnProps = new Properties();\n\t\tyarnProps.setProperty(CliFrontend.YARN_PROPERTIES_JOBMANAGER_KEY, jobManagerAddress);\n\t\tif (flinkYarnClient.getTaskManagerSlots() != -1) {\n\t\t\tString parallelism =\n\t\t\t\t\tInteger.toString(flinkYarnClient.getTaskManagerSlots() * flinkYarnClient.getTaskManagerCount());\n\t\t\tyarnProps.setProperty(CliFrontend.YARN_PROPERTIES_PARALLELISM, parallelism);\n\t\t}\n\t\t// add dynamic properties\n\t\tif (flinkYarnClient.getDynamicPropertiesEncoded() != null) {\n\t\t\tyarnProps.setProperty(CliFrontend.YARN_PROPERTIES_DYNAMIC_PROPERTIES_STRING,\n\t\t\t\t\tflinkYarnClient.getDynamicPropertiesEncoded());\n\t\t}\n\t\twriteYarnProperties(yarnProps, yarnPropertiesFile);\n\n\t\t//------------------ Cluster running, let user control it ------------\n\n\t\tif (detachedMode) {\n\t\t\t// print info and quit:\n\t\t} else {\n\t\t\trunInteractiveCli(yarnCluster);\n\n\t\t\tif (!yarnCluster.hasBeenStopped()) {\n\t\t\t\tLOG.info(\"Command Line Interface requested session shutdown\");\n\t\t\t\tyarnCluster.shutdown();\n\t\t\t}\n\n\t\t\ttry {\n\t\t\t\tyarnPropertiesFile.delete();\n\t\t\t} catch (Exception e) {\n\t\t\t\tLOG.warn(\"Exception while deleting the JobManager address file\", e);\n\t\t\t}\n\t\t}\n\t}\n\treturn 0;\n}",
        "accept_response": "public int run(String[] args) {\n\n\t//\n\t//\tCommand Line Options\n\t//\n\tOptions options = new Options();\n\tgetYARNSessionCLIOptions(options);\n\n\tCommandLineParser parser = new PosixParser();\n\tCommandLine cmd = null;\n\ttry {\n\t\tcmd = parser.parse(options, args);\n\t} catch(Exception e) {\n\t\tSystem.out.println(e.getMessage());\n\t\tprintUsage();\n\t\treturn 1;\n\t}\n\n\t// Query cluster for metrics\n\tif (cmd.hasOption(QUERY.getOpt())) {\n\t\tAbstractFlinkYarnClient flinkYarnClient = getFlinkYarnClient();\n\t\tString description = null;\n\t\ttry {\n\t\t\tdescription = flinkYarnClient.getClusterDescription();\n\t\t} catch (Exception e) {\n\t\t\tSystem.err.println(\"Error while querying the YARN cluster for available resources: \"+e.getMessage());\n\t\t\te.printStackTrace(System.err);\n\t\t\treturn 1;\n\t\t}\n\t\tSystem.out.println(description);\n\t\treturn 0;\n\t} else {\n\t\tAbstractFlinkYarnClient flinkYarnClient = createFlinkYarnClient(cmd);\n\n\t\tif (flinkYarnClient == null) {\n\t\t\tSystem.err.println(\"Error while starting the YARN Client. Please check log output!\");\n\t\t\treturn 1;\n\t\t}\n\n\n\t\ttry {\n\t\t\tyarnCluster = flinkYarnClient.deploy(null);\n\t\t\t// only connect to cluster if its not a detached session.\n\t\t\tif(!flinkYarnClient.isDetached()) {\n\t\t\t\tyarnCluster.connectToCluster();\n\t\t\t}\n\t\t} catch (Exception e) {\n\t\t\tSystem.err.println(\"Error while deploying YARN cluster: \"+e.getMessage());\n\t\t\te.printStackTrace(System.err);\n\t\t\treturn 1;\n\t\t}\n\t\t//------------------ Cluster deployed, handle connection details\n\t\tString jobManagerAddress = yarnCluster.getJobManagerAddress().getHostName() + \":\" + yarnCluster.getJobManagerAddress().getPort();\n\t\tSystem.out.println(\"Flink JobManager is now running on \" + jobManagerAddress);\n\t\tSystem.out.println(\"JobManager Web Interface: \" + yarnCluster.getWebInterfaceURL());\n\t\t// file that we write into the conf/ dir containing the jobManager address and the dop.\n\t\tString confDirPath = CliFrontend.getConfigurationDirectoryFromEnv();\n\t\tFile yarnPropertiesFile = new File(confDirPath + File.separator + CliFrontend.YARN_PROPERTIES_FILE);\n\n\t\tProperties yarnProps = new Properties();\n\t\tyarnProps.setProperty(CliFrontend.YARN_PROPERTIES_JOBMANAGER_KEY, jobManagerAddress);\n\t\tif (flinkYarnClient.getTaskManagerSlots() != -1) {\n\t\t\tString parallelism =\n\t\t\t\t\tInteger.toString(flinkYarnClient.getTaskManagerSlots() * flinkYarnClient.getTaskManagerCount());\n\t\t\tyarnProps.setProperty(CliFrontend.YARN_PROPERTIES_PARALLELISM, parallelism);\n\t\t}\n\t\t// add dynamic properties\n\t\tif (flinkYarnClient.getDynamicPropertiesEncoded() != null) {\n\t\t\tyarnProps.setProperty(CliFrontend.YARN_PROPERTIES_DYNAMIC_PROPERTIES_STRING,\n\t\t\t\t\tflinkYarnClient.getDynamicPropertiesEncoded());\n\t\t}\n\t\twriteYarnProperties(yarnProps, yarnPropertiesFile);\n\n\t\t//------------------ Cluster running, let user control it ------------\n\n\t\tif (detachedMode) {\n\t\t\t// print info and quit:\n\t\t\tLOG.info(\"The Flink YARN client has been started in detached mode. In order to stop \" +\n\t\t\t\t\t\"Flink on YARN, use the following command or a YARN web interface to stop it:\\n\" +\n\t\t\t\t\t\"yarn application -kill \"+yarnCluster.getApplicationId()+\"\\n\" +\n\t\t\t\t\t\"Please also note that the temporary files of the YARN session in {} will not be removed.\",\n\t\t\t\t\tflinkYarnClient.getSessionFilesDir());\n\t\t} else {\n\t\t\trunInteractiveCli(yarnCluster);\n\n\t\t\tif (!yarnCluster.hasBeenStopped()) {\n\t\t\t\tLOG.info(\"Command Line Interface requested session shutdown\");\n\t\t\t\tyarnCluster.shutdown();\n\t\t\t}\n\n\t\t\ttry {\n\t\t\t\tyarnPropertiesFile.delete();\n\t\t\t} catch (Exception e) {\n\t\t\t\tLOG.warn(\"Exception while deleting the JobManager address file\", e);\n\t\t\t}\n\t\t}\n\t}\n\treturn 0;\n}",
        "reject_response": "public int run(String[] args) {\n\n\t//\n\t//\tCommand Line Options\n\t//\n\tOptions options = new Options();\n\tgetYARNSessionCLIOptions(options);\n\n\tCommandLineParser parser = new PosixParser();\n\tCommandLine cmd = null;\n\ttry {\n\t\tcmd = parser.parse(options, args);\n\t} catch(Exception e) {\n\t\tSystem.out.println(e.getMessage());\n\t\tprintUsage();\n\t\treturn 1;\n\t}\n\n\t// Query cluster for metrics\n\tif (cmd.hasOption(QUERY.getOpt())) {\n\t\tAbstractFlinkYarnClient flinkYarnClient = getFlinkYarnClient();\n\t\tString description = null;\n\t\ttry {\n\t\t\tdescription = flinkYarnClient.getClusterDescription();\n\t\t} catch (Exception e) {\n\t\t\tSystem.err.println(\"Error while querying the YARN cluster for available resources: \"+e.getMessage());\n\t\t\te.printStackTrace(System.err);\n\t\t\treturn 1;\n\t\t}\n\t\tSystem.out.println(description);\n\t\treturn 0;\n\t} else {\n\t\tAbstractFlinkYarnClient flinkYarnClient = createFlinkYarnClient(cmd);\n\n\t\tif (flinkYarnClient == null) {\n\t\t\tSystem.err.println(\"Error while starting the YARN Client. Please check log output!\");\n\t\t\treturn 1;\n\t\t}\n\n\n\t\ttry {\n\t\t\tyarnCluster = flinkYarnClient.deploy(null);\n\t\t\t// only connect to cluster if its not a detached session.\n\t\t\tif(!flinkYarnClient.isDetached()) {\n\t\t\t\tyarnCluster.connectToCluster();\n\t\t\t}\n\t\t} catch (Exception e) {\n\t\t\tSystem.err.println(\"Error while deploying YARN cluster: \"+e.getMessage());\n\t\t\te.printStackTrace(System.err);\n\t\t\treturn 1;\n\t\t}\n\t\t//------------------ Cluster deployed, handle connection details\n\t\tString jobManagerAddress = yarnCluster.getJobManagerAddress().getHostName() + \":\" + yarnCluster.getJobManagerAddress().getPort();\n\t\tSystem.out.println(\"Flink JobManager is now running on \" + jobManagerAddress);\n\t\tSystem.out.println(\"JobManager Web Interface: \" + yarnCluster.getWebInterfaceURL());\n\t\t// file that we write into the conf/ dir containing the jobManager address and the dop.\n\t\tString confDirPath = CliFrontend.getConfigurationDirectoryFromEnv();\n\t\tFile yarnPropertiesFile = new File(confDirPath + File.separator + CliFrontend.YARN_PROPERTIES_FILE);\n\n\t\tProperties yarnProps = new Properties();\n\t\tyarnProps.setProperty(CliFrontend.YARN_PROPERTIES_JOBMANAGER_KEY, jobManagerAddress);\n\t\tif (flinkYarnClient.getTaskManagerSlots() != -1) {\n\t\t\tString parallelism =\n\t\t\t\t\tInteger.toString(flinkYarnClient.getTaskManagerSlots() * flinkYarnClient.getTaskManagerCount());\n\t\t\tyarnProps.setProperty(CliFrontend.YARN_PROPERTIES_PARALLELISM, parallelism);\n\t\t}\n\t\t// add dynamic properties\n\t\tif (flinkYarnClient.getDynamicPropertiesEncoded() != null) {\n\t\t\tyarnProps.setProperty(CliFrontend.YARN_PROPERTIES_DYNAMIC_PROPERTIES_STRING,\n\t\t\t\t\tflinkYarnClient.getDynamicPropertiesEncoded());\n\t\t}\n\t\twriteYarnProperties(yarnProps, yarnPropertiesFile);\n\n\t\t//------------------ Cluster running, let user control it ------------\n\n\t\tif (detachedMode) {\n\t\t\t// print info and quit:\n\t\t\tLOG.info(\"The Flink YARN client has been started in detached mode. In order to stop\" +\n\t\t} else {\n\t\t\trunInteractiveCli(yarnCluster);\n\n\t\t\tif (!yarnCluster.hasBeenStopped()) {\n\t\t\t\tLOG.info(\"Command Line Interface requested session shutdown\");\n\t\t\t\tyarnCluster.shutdown();\n\t\t\t}\n\n\t\t\ttry {\n\t\t\t\tyarnPropertiesFile.delete();\n\t\t\t} catch (Exception e) {\n\t\t\t\tLOG.warn(\"Exception while deleting the JobManager address file\", e);\n\t\t\t}\n\t\t}\n\t}\n\treturn 0;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2909,
        "instruction": "private SPFResult runSingleTest(String testName) {\n    Map<String, ?> currentTest = data.getTests().get(testName);\n\n    String ip = null;\n    String sender = null;\n    String helo = null;\n\n    if (currentTest.get(\"helo\") != null) {\n        helo = (String) currentTest.get(\"helo\");\n    }\n    if (currentTest.get(\"host\") != null) {\n        ip = (String) currentTest.get(\"host\");\n    }\n    if (currentTest.get(\"mailfrom\") != null) {\n        sender = (String) currentTest.get(\"mailfrom\");\n    } else {\n        sender = \"\";\n    }\n\n    SPFResult res = spf.checkSPF(ip, sender, helo);\n    return res;\n}",
        "accept_response": "private SPFResult runSingleTest(String testName) {\n    Map<String, ?> currentTest = data.getTests().get(testName);\n    LOGGER.info(\"TESTING {}: {}\", testName, currentTest.get(\"description\"));\n\n    String ip = null;\n    String sender = null;\n    String helo = null;\n\n    if (currentTest.get(\"helo\") != null) {\n        helo = (String) currentTest.get(\"helo\");\n    }\n    if (currentTest.get(\"host\") != null) {\n        ip = (String) currentTest.get(\"host\");\n    }\n    if (currentTest.get(\"mailfrom\") != null) {\n        sender = (String) currentTest.get(\"mailfrom\");\n    } else {\n        sender = \"\";\n    }\n\n    SPFResult res = spf.checkSPF(ip, sender, helo);\n    return res;\n}",
        "reject_response": "private SPFResult runSingleTest(String testName) {\n    Map<String, ?> currentTest = data.getTests().get(testName);\n    Logger testLogger = log.getChildLogger(testName);\n    testLogger.info(\"TESTING \"+testName+\": \"+currentTest.get(\"description\"));\n\n    String ip = null;\n    String sender = null;\n    String helo = null;\n\n    if (currentTest.get(\"helo\") != null) {\n        helo = (String) currentTest.get(\"helo\");\n    }\n    if (currentTest.get(\"host\") != null) {\n        ip = (String) currentTest.get(\"host\");\n    }\n    if (currentTest.get(\"mailfrom\") != null) {\n        sender = (String) currentTest.get(\"mailfrom\");\n    } else {\n        sender = \"\";\n    }\n\n    SPFResult res = spf.checkSPF(ip, sender, helo);\n    return res;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2638,
        "instruction": "private Long getLongAttribute(Object object, String name) {\n  if (object == null) {\n    return Long.valueOf(0);\n  }\n\n  try {\n    if (!(object.getClass().equals(Long.class))) {\n      return Long.valueOf(0);\n    } else {\n      return (Long) object;\n    }\n  } catch (Exception e) {\n    logger.info(\"Exception occurred: \", e);\n    return Long.valueOf(0);\n  }\n\n}",
        "accept_response": "private Long getLongAttribute(Object object, String name) {\n  if (object == null) {\n    return Long.valueOf(0);\n  }\n\n  try {\n    if (!(object.getClass().equals(Long.class))) {\n      logger.info(\n          \"************************Unexpected type for attribute: {}; Expected type: {}; Received type: {}************************\",\n          name, Long.class.getName(), object.getClass().getName());\n      return Long.valueOf(0);\n    } else {\n      return (Long) object;\n    }\n  } catch (Exception e) {\n    logger.info(\"Exception occurred: \", e);\n    return Long.valueOf(0);\n  }\n\n}",
        "reject_response": "private Long getLongAttribute(Object object, String name) {\n  if (object == null) {\n    return Long.valueOf(0);\n  }\n\n  try {\n    if (!(object.getClass().equals(Long.class))) {\n      if (LOGGER.infoEnabled()) {\n        LOGGER.info(\"************************Unexpected type for attribute: \" + name\n            + \" Expected type: \" + Long.class.getName() + \" Received type: \"\n            + object.getClass().getName() + \"************************\");\n      }\n      return Long.valueOf(0);\n    } else {\n      return (Long) object;\n    }\n  } catch (Exception e) {\n    logger.info(\"Exception occurred: \", e);\n    return Long.valueOf(0);\n  }\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2676,
        "instruction": "private MRClientProtocol getProxy() throws IOException {\n  if (realProxy != null) {\n    return realProxy;\n  }\n\n  // Possibly allow nulls through the PB tunnel, otherwise deal with an exception\n  // and redirect to the history server.\n  ApplicationReport application = null;\n  try {\n    application = rm.getApplicationReport(appId);\n  } catch (ApplicationNotFoundException e) {\n    application = null;\n  } catch (YarnException e2) {\n    throw new IOException(e2);\n  }\n  if (application != null) {\n    trackingUrl = application.getTrackingUrl();\n  }\n  InetSocketAddress serviceAddr = null;\n  while (application == null\n      || YarnApplicationState.RUNNING == application\n          .getYarnApplicationState()) {\n    if (application == null) {\n      LOG.info(\"Could not get Job info from RM for job \" + jobId\n          + \". Redirecting to job history server.\");\n      return checkAndGetHSProxy(null, JobState.NEW);\n    }\n    try {\n      if (application.getHost() == null || \"\".equals(application.getHost())) {\n        LOG.debug(\"AM not assigned to Job. Waiting to get the AM ...\");\n        Thread.sleep(2000);\n\n        LOG.debug(\"Application state is \" + application.getYarnApplicationState());\n        application = rm.getApplicationReport(appId);\n        continue;\n      } else if (UNAVAILABLE.equals(application.getHost())) {\n        if (!amAclDisabledStatusLogged) {\n          LOG.info(\"Job \" + jobId + \" is running, but the host is unknown.\"\n              + \" Verify user has VIEW_JOB access.\");\n          amAclDisabledStatusLogged = true;\n        }\n        return getNotRunningJob(application, JobState.RUNNING);\n      }\n      if(!conf.getBoolean(MRJobConfig.JOB_AM_ACCESS_DISABLED, false)) {\n        UserGroupInformation newUgi = UserGroupInformation.createRemoteUser(\n            UserGroupInformation.getCurrentUser().getUserName());\n        serviceAddr = NetUtils.createSocketAddrForHost(\n            application.getHost(), application.getRpcPort());\n        if (UserGroupInformation.isSecurityEnabled()) {\n          org.apache.hadoop.yarn.api.records.Token clientToAMToken =\n              application.getClientToAMToken();\n          Token<ClientToAMTokenIdentifier> token =\n              ConverterUtils.convertFromYarn(clientToAMToken, serviceAddr);\n          newUgi.addToken(token);\n        }\n        LOG.debug(\"Connecting to \" + serviceAddr);\n        final InetSocketAddress finalServiceAddr = serviceAddr;\n        realProxy = newUgi.doAs(new PrivilegedExceptionAction<MRClientProtocol>() {\n          @Override\n          public MRClientProtocol run() throws IOException {\n            return instantiateAMProxy(finalServiceAddr);\n          }\n        });\n      } else {\n        if (!amAclDisabledStatusLogged) {\n          LOG.info(\"Network ACL closed to AM for job \" + jobId\n              + \". Not going to try to reach the AM.\");\n          amAclDisabledStatusLogged = true;\n        }\n        return getNotRunningJob(null, JobState.RUNNING);\n      }\n      return realProxy;\n    } catch (IOException e) {\n      //possibly the AM has crashed\n      //there may be some time before AM is restarted\n      //keep retrying by getting the address from RM\n      LOG.info(\"Could not connect to \" + serviceAddr +\n      \". Waiting for getting the latest AM address...\");\n      try {\n        Thread.sleep(2000);\n      } catch (InterruptedException e1) {\n        LOG.warn(\"getProxy() call interrupted\", e1);\n        throw new YarnRuntimeException(e1);\n      }\n      try {\n        application = rm.getApplicationReport(appId);\n      } catch (YarnException e1) {\n        throw new IOException(e1);\n      }\n      if (application == null) {\n        LOG.info(\"Could not get Job info from RM for job \" + jobId\n            + \". Redirecting to job history server.\");\n        return checkAndGetHSProxy(null, JobState.RUNNING);\n      }\n    } catch (InterruptedException e) {\n      throw new YarnRuntimeException(e);\n    } catch (YarnException e) {\n      throw new IOException(e);\n    }\n  }\n\n  /** we just want to return if its allocating, so that we don't\n   * block on it. This is to be able to return job status\n   * on an allocating Application.\n   */\n  String user = application.getUser();\n  if (user == null) {\n    throw new IOException(\"User is not set in the application report\");\n  }\n  if (application.getYarnApplicationState() == YarnApplicationState.NEW\n      || application.getYarnApplicationState() ==\n          YarnApplicationState.NEW_SAVING\n      || application.getYarnApplicationState() == YarnApplicationState.SUBMITTED\n      || application.getYarnApplicationState() == YarnApplicationState.ACCEPTED) {\n    realProxy = null;\n    return getNotRunningJob(application, JobState.NEW);\n  }\n\n  if (application.getYarnApplicationState() == YarnApplicationState.FAILED) {\n    realProxy = null;\n    return getNotRunningJob(application, JobState.FAILED);\n  }\n\n  if (application.getYarnApplicationState() == YarnApplicationState.KILLED) {\n    realProxy = null;\n    return getNotRunningJob(application, JobState.KILLED);\n  }\n\n  //History server can serve a job only if application\n  //succeeded.\n  if (application.getYarnApplicationState() == YarnApplicationState.FINISHED) {\n    LOG.info(\"Application state is completed. FinalApplicationStatus=\"\n        + application.getFinalApplicationStatus().toString()\n        + \". Redirecting to job history server\");\n    realProxy = checkAndGetHSProxy(application, JobState.SUCCEEDED);\n  }\n  return realProxy;\n}",
        "accept_response": "private MRClientProtocol getProxy() throws IOException {\n  if (realProxy != null) {\n    return realProxy;\n  }\n\n  // Possibly allow nulls through the PB tunnel, otherwise deal with an exception\n  // and redirect to the history server.\n  ApplicationReport application = null;\n  try {\n    application = rm.getApplicationReport(appId);\n  } catch (ApplicationNotFoundException e) {\n    application = null;\n  } catch (YarnException e2) {\n    throw new IOException(e2);\n  }\n  if (application != null) {\n    trackingUrl = application.getTrackingUrl();\n  }\n  InetSocketAddress serviceAddr = null;\n  while (application == null\n      || YarnApplicationState.RUNNING == application\n          .getYarnApplicationState()) {\n    if (application == null) {\n      LOG.info(\"Could not get Job info from RM for job \" + jobId\n          + \". Redirecting to job history server.\");\n      return checkAndGetHSProxy(null, JobState.NEW);\n    }\n    try {\n      if (application.getHost() == null || \"\".equals(application.getHost())) {\n        LOG.debug(\"AM not assigned to Job. Waiting to get the AM ...\");\n        Thread.sleep(2000);\n\n        LOG.debug(\"Application state is \" + application.getYarnApplicationState());\n        application = rm.getApplicationReport(appId);\n        continue;\n      } else if (UNAVAILABLE.equals(application.getHost())) {\n        if (!amAclDisabledStatusLogged) {\n          LOG.info(\"Job \" + jobId + \" is running, but the host is unknown.\"\n              + \" Verify user has VIEW_JOB access.\");\n          amAclDisabledStatusLogged = true;\n        }\n        return getNotRunningJob(application, JobState.RUNNING);\n      }\n      if(!conf.getBoolean(MRJobConfig.JOB_AM_ACCESS_DISABLED, false)) {\n        UserGroupInformation newUgi = UserGroupInformation.createRemoteUser(\n            UserGroupInformation.getCurrentUser().getUserName());\n        serviceAddr = NetUtils.createSocketAddrForHost(\n            application.getHost(), application.getRpcPort());\n        if (UserGroupInformation.isSecurityEnabled()) {\n          org.apache.hadoop.yarn.api.records.Token clientToAMToken =\n              application.getClientToAMToken();\n          Token<ClientToAMTokenIdentifier> token =\n              ConverterUtils.convertFromYarn(clientToAMToken, serviceAddr);\n          newUgi.addToken(token);\n        }\n        LOG.debug(\"Connecting to \" + serviceAddr);\n        final InetSocketAddress finalServiceAddr = serviceAddr;\n        realProxy = newUgi.doAs(new PrivilegedExceptionAction<MRClientProtocol>() {\n          @Override\n          public MRClientProtocol run() throws IOException {\n            return instantiateAMProxy(finalServiceAddr);\n          }\n        });\n      } else {\n        if (!amAclDisabledStatusLogged) {\n          LOG.info(\"Network ACL closed to AM for job \" + jobId\n              + \". Not going to try to reach the AM.\");\n          amAclDisabledStatusLogged = true;\n        }\n        return getNotRunningJob(null, JobState.RUNNING);\n      }\n      return realProxy;\n    } catch (IOException e) {\n      //possibly the AM has crashed\n      //there may be some time before AM is restarted\n      //keep retrying by getting the address from RM\n      LOG.info(\"Could not connect to \" + serviceAddr +\n      \". Waiting for getting the latest AM address...\");\n      try {\n        Thread.sleep(2000);\n      } catch (InterruptedException e1) {\n        LOG.warn(\"getProxy() call interrupted\", e1);\n        throw new YarnRuntimeException(e1);\n      }\n      try {\n        application = rm.getApplicationReport(appId);\n      } catch (YarnException e1) {\n        throw new IOException(e1);\n      }\n      if (application == null) {\n        LOG.info(\"Could not get Job info from RM for job \" + jobId\n            + \". Redirecting to job history server.\");\n        return checkAndGetHSProxy(null, JobState.RUNNING);\n      }\n    } catch (InterruptedException e) {\n      LOG.warn(\"getProxy() call interrupted\", e);\n      throw new YarnRuntimeException(e);\n    } catch (YarnException e) {\n      throw new IOException(e);\n    }\n  }\n\n  /** we just want to return if its allocating, so that we don't\n   * block on it. This is to be able to return job status\n   * on an allocating Application.\n   */\n  String user = application.getUser();\n  if (user == null) {\n    throw new IOException(\"User is not set in the application report\");\n  }\n  if (application.getYarnApplicationState() == YarnApplicationState.NEW\n      || application.getYarnApplicationState() ==\n          YarnApplicationState.NEW_SAVING\n      || application.getYarnApplicationState() == YarnApplicationState.SUBMITTED\n      || application.getYarnApplicationState() == YarnApplicationState.ACCEPTED) {\n    realProxy = null;\n    return getNotRunningJob(application, JobState.NEW);\n  }\n\n  if (application.getYarnApplicationState() == YarnApplicationState.FAILED) {\n    realProxy = null;\n    return getNotRunningJob(application, JobState.FAILED);\n  }\n\n  if (application.getYarnApplicationState() == YarnApplicationState.KILLED) {\n    realProxy = null;\n    return getNotRunningJob(application, JobState.KILLED);\n  }\n\n  //History server can serve a job only if application\n  //succeeded.\n  if (application.getYarnApplicationState() == YarnApplicationState.FINISHED) {\n    LOG.info(\"Application state is completed. FinalApplicationStatus=\"\n        + application.getFinalApplicationStatus().toString()\n        + \". Redirecting to job history server\");\n    realProxy = checkAndGetHSProxy(application, JobState.SUCCEEDED);\n  }\n  return realProxy;\n}",
        "reject_response": "private MRClientProtocol getProxy() throws IOException {\n  if (realProxy != null) {\n    return realProxy;\n  }\n\n  // Possibly allow nulls through the PB tunnel, otherwise deal with an exception\n  // and redirect to the history server.\n  ApplicationReport application = null;\n  try {\n    application = rm.getApplicationReport(appId);\n  } catch (ApplicationNotFoundException e) {\n    application = null;\n  } catch (YarnException e2) {\n    throw new IOException(e2);\n  }\n  if (application != null) {\n    trackingUrl = application.getTrackingUrl();\n  }\n  InetSocketAddress serviceAddr = null;\n  while (application == null\n      || YarnApplicationState.RUNNING == application\n          .getYarnApplicationState()) {\n    if (application == null) {\n      LOG.info(\"Could not get Job info from RM for job \" + jobId\n          + \". Redirecting to job history server.\");\n      return checkAndGetHSProxy(null, JobState.NEW);\n    }\n    try {\n      if (application.getHost() == null || \"\".equals(application.getHost())) {\n        LOG.debug(\"AM not assigned to Job. Waiting to get the AM ...\");\n        Thread.sleep(2000);\n\n        LOG.debug(\"Application state is \" + application.getYarnApplicationState());\n        application = rm.getApplicationReport(appId);\n        continue;\n      } else if (UNAVAILABLE.equals(application.getHost())) {\n        if (!amAclDisabledStatusLogged) {\n          LOG.info(\"Job \" + jobId + \" is running, but the host is unknown.\"\n              + \" Verify user has VIEW_JOB access.\");\n          amAclDisabledStatusLogged = true;\n        }\n        return getNotRunningJob(application, JobState.RUNNING);\n      }\n      if(!conf.getBoolean(MRJobConfig.JOB_AM_ACCESS_DISABLED, false)) {\n        UserGroupInformation newUgi = UserGroupInformation.createRemoteUser(\n            UserGroupInformation.getCurrentUser().getUserName());\n        serviceAddr = NetUtils.createSocketAddrForHost(\n            application.getHost(), application.getRpcPort());\n        if (UserGroupInformation.isSecurityEnabled()) {\n          org.apache.hadoop.yarn.api.records.Token clientToAMToken =\n              application.getClientToAMToken();\n          Token<ClientToAMTokenIdentifier> token =\n              ConverterUtils.convertFromYarn(clientToAMToken, serviceAddr);\n          newUgi.addToken(token);\n        }\n        LOG.debug(\"Connecting to \" + serviceAddr);\n        final InetSocketAddress finalServiceAddr = serviceAddr;\n        realProxy = newUgi.doAs(new PrivilegedExceptionAction<MRClientProtocol>() {\n          @Override\n          public MRClientProtocol run() throws IOException {\n            return instantiateAMProxy(finalServiceAddr);\n          }\n        });\n      } else {\n        if (!amAclDisabledStatusLogged) {\n          LOG.info(\"Network ACL closed to AM for job \" + jobId\n              + \". Not going to try to reach the AM.\");\n          amAclDisabledStatusLogged = true;\n        }\n        return getNotRunningJob(null, JobState.RUNNING);\n      }\n      return realProxy;\n    } catch (IOException e) {\n      //possibly the AM has crashed\n      //there may be some time before AM is restarted\n      //keep retrying by getting the address from RM\n      LOG.info(\"Could not connect to \" + serviceAddr +\n      \". Waiting for getting the latest AM address...\");\n      try {\n        Thread.sleep(2000);\n      } catch (InterruptedException e1) {\n        LOG.warn(\"getProxy() call interrupted\", e1);\n        throw new YarnRuntimeException(e1);\n      }\n      try {\n        application = rm.getApplicationReport(appId);\n      } catch (YarnException e1) {\n        throw new IOException(e1);\n      }\n      if (application == null) {\n        LOG.info(\"Could not get Job info from RM for job \" + jobId\n            + \". Redirecting to job history server.\");\n        return checkAndGetHSProxy(null, JobState.RUNNING);\n      }\n    } catch (InterruptedException e) {\n      LOG.warn(\"getProxy() call interruped\", e);\n      throw new YarnRuntimeException(e);\n    } catch (YarnException e) {\n      throw new IOException(e);\n    }\n  }\n\n  /** we just want to return if its allocating, so that we don't\n   * block on it. This is to be able to return job status\n   * on an allocating Application.\n   */\n  String user = application.getUser();\n  if (user == null) {\n    throw new IOException(\"User is not set in the application report\");\n  }\n  if (application.getYarnApplicationState() == YarnApplicationState.NEW\n      || application.getYarnApplicationState() ==\n          YarnApplicationState.NEW_SAVING\n      || application.getYarnApplicationState() == YarnApplicationState.SUBMITTED\n      || application.getYarnApplicationState() == YarnApplicationState.ACCEPTED) {\n    realProxy = null;\n    return getNotRunningJob(application, JobState.NEW);\n  }\n\n  if (application.getYarnApplicationState() == YarnApplicationState.FAILED) {\n    realProxy = null;\n    return getNotRunningJob(application, JobState.FAILED);\n  }\n\n  if (application.getYarnApplicationState() == YarnApplicationState.KILLED) {\n    realProxy = null;\n    return getNotRunningJob(application, JobState.KILLED);\n  }\n\n  //History server can serve a job only if application\n  //succeeded.\n  if (application.getYarnApplicationState() == YarnApplicationState.FINISHED) {\n    LOG.info(\"Application state is completed. FinalApplicationStatus=\"\n        + application.getFinalApplicationStatus().toString()\n        + \". Redirecting to job history server\");\n    realProxy = checkAndGetHSProxy(application, JobState.SUCCEEDED);\n  }\n  return realProxy;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2765,
        "instruction": "@Override\npublic final void close(final CloseMode closeMode) {\n    ioReactor.initiateShutdown();\n    ioReactor.close(closeMode);\n    executorService.shutdownNow();\n    internalClose(closeMode);\n}",
        "accept_response": "@Override\npublic final void close(final CloseMode closeMode) {\n    if (log.isDebugEnabled()) {\n        log.debug(\"Shutdown {}\", closeMode);\n    }\n    ioReactor.initiateShutdown();\n    ioReactor.close(closeMode);\n    executorService.shutdownNow();\n    internalClose(closeMode);\n}",
        "reject_response": "@Override\npublic final void close(final CloseMode closeMode) {\n    if (log.isDebugEnabled()) {\n        log.debug(\"Shutdown \" + closeMode);\n    }\n    ioReactor.initiateShutdown();\n    ioReactor.close(closeMode);\n    executorService.shutdownNow();\n    internalClose(closeMode);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2767,
        "instruction": "@Override\npublic void execute(\n        final HttpRequest request,\n        final AsyncEntityProducer entityProducer,\n        final AsyncExecChain.Scope scope,\n        final AsyncExecChain chain,\n        final AsyncExecCallback asyncExecCallback) throws HttpException, IOException {\n    Args.notNull(request, \"HTTP request\");\n    Args.notNull(scope, \"Scope\");\n\n    final String exchangeId = scope.exchangeId;\n    final HttpRoute route = scope.route;\n    final CancellableDependency cancellableDependency = scope.cancellableDependency;\n    final HttpClientContext clientContext = scope.clientContext;\n    final AsyncExecRuntime execRuntime = scope.execRuntime;\n    final State state = new State(route);\n\n    if (!execRuntime.isEndpointAcquired()) {\n        final Object userToken = clientContext.getUserToken();\n        cancellableDependency.setDependency(execRuntime.acquireEndpoint(\n                exchangeId, route, userToken, clientContext, new FutureCallback<AsyncExecRuntime>() {\n\n                    @Override\n                    public void completed(final AsyncExecRuntime execRuntime) {\n                        if (execRuntime.isEndpointConnected()) {\n                            try {\n                                chain.proceed(request, entityProducer, scope, asyncExecCallback);\n                            } catch (final HttpException | IOException ex) {\n                                asyncExecCallback.failed(ex);\n                            }\n                        } else {\n                            proceedToNextHop(state, request, entityProducer, scope, chain, asyncExecCallback);\n                        }\n                    }\n\n                    @Override\n                    public void failed(final Exception ex) {\n                        asyncExecCallback.failed(ex);\n                    }\n\n                    @Override\n                    public void cancelled() {\n                        asyncExecCallback.failed(new InterruptedIOException());\n                    }\n\n                }));\n    } else {\n        if (execRuntime.isEndpointConnected()) {\n            try {\n                chain.proceed(request, entityProducer, scope, asyncExecCallback);\n            } catch (final HttpException | IOException ex) {\n                asyncExecCallback.failed(ex);\n            }\n        } else {\n            proceedToNextHop(state, request, entityProducer, scope, chain, asyncExecCallback);\n        }\n    }\n\n}",
        "accept_response": "@Override\npublic void execute(\n        final HttpRequest request,\n        final AsyncEntityProducer entityProducer,\n        final AsyncExecChain.Scope scope,\n        final AsyncExecChain chain,\n        final AsyncExecCallback asyncExecCallback) throws HttpException, IOException {\n    Args.notNull(request, \"HTTP request\");\n    Args.notNull(scope, \"Scope\");\n\n    final String exchangeId = scope.exchangeId;\n    final HttpRoute route = scope.route;\n    final CancellableDependency cancellableDependency = scope.cancellableDependency;\n    final HttpClientContext clientContext = scope.clientContext;\n    final AsyncExecRuntime execRuntime = scope.execRuntime;\n    final State state = new State(route);\n\n    if (!execRuntime.isEndpointAcquired()) {\n        final Object userToken = clientContext.getUserToken();\n        if (log.isDebugEnabled()) {\n            log.debug(\"{}: acquiring connection with route {}\", exchangeId, route);\n        }\n        cancellableDependency.setDependency(execRuntime.acquireEndpoint(\n                exchangeId, route, userToken, clientContext, new FutureCallback<AsyncExecRuntime>() {\n\n                    @Override\n                    public void completed(final AsyncExecRuntime execRuntime) {\n                        if (execRuntime.isEndpointConnected()) {\n                            try {\n                                chain.proceed(request, entityProducer, scope, asyncExecCallback);\n                            } catch (final HttpException | IOException ex) {\n                                asyncExecCallback.failed(ex);\n                            }\n                        } else {\n                            proceedToNextHop(state, request, entityProducer, scope, chain, asyncExecCallback);\n                        }\n                    }\n\n                    @Override\n                    public void failed(final Exception ex) {\n                        asyncExecCallback.failed(ex);\n                    }\n\n                    @Override\n                    public void cancelled() {\n                        asyncExecCallback.failed(new InterruptedIOException());\n                    }\n\n                }));\n    } else {\n        if (execRuntime.isEndpointConnected()) {\n            try {\n                chain.proceed(request, entityProducer, scope, asyncExecCallback);\n            } catch (final HttpException | IOException ex) {\n                asyncExecCallback.failed(ex);\n            }\n        } else {\n            proceedToNextHop(state, request, entityProducer, scope, chain, asyncExecCallback);\n        }\n    }\n\n}",
        "reject_response": "@Override\npublic void execute(\n        final HttpRequest request,\n        final AsyncEntityProducer entityProducer,\n        final AsyncExecChain.Scope scope,\n        final AsyncExecChain chain,\n        final AsyncExecCallback asyncExecCallback) throws HttpException, IOException {\n    Args.notNull(request, \"HTTP request\");\n    Args.notNull(scope, \"Scope\");\n\n    final String exchangeId = scope.exchangeId;\n    final HttpRoute route = scope.route;\n    final CancellableDependency cancellableDependency = scope.cancellableDependency;\n    final HttpClientContext clientContext = scope.clientContext;\n    final AsyncExecRuntime execRuntime = scope.execRuntime;\n    final State state = new State(route);\n\n    if (!execRuntime.isEndpointAcquired()) {\n        final Object userToken = clientContext.getUserToken();\n        if (log.isDebugEnabled()) {\n            log.debug(exchangeId + \": acquiring connection with route \" + route);\n        }\n        cancellableDependency.setDependency(execRuntime.acquireEndpoint(\n                exchangeId, route, userToken, clientContext, new FutureCallback<AsyncExecRuntime>() {\n\n                    @Override\n                    public void completed(final AsyncExecRuntime execRuntime) {\n                        if (execRuntime.isEndpointConnected()) {\n                            try {\n                                chain.proceed(request, entityProducer, scope, asyncExecCallback);\n                            } catch (final HttpException | IOException ex) {\n                                asyncExecCallback.failed(ex);\n                            }\n                        } else {\n                            proceedToNextHop(state, request, entityProducer, scope, chain, asyncExecCallback);\n                        }\n                    }\n\n                    @Override\n                    public void failed(final Exception ex) {\n                        asyncExecCallback.failed(ex);\n                    }\n\n                    @Override\n                    public void cancelled() {\n                        asyncExecCallback.failed(new InterruptedIOException());\n                    }\n\n                }));\n    } else {\n        if (execRuntime.isEndpointConnected()) {\n            try {\n                chain.proceed(request, entityProducer, scope, asyncExecCallback);\n            } catch (final HttpException | IOException ex) {\n                asyncExecCallback.failed(ex);\n            }\n        } else {\n            proceedToNextHop(state, request, entityProducer, scope, chain, asyncExecCallback);\n        }\n    }\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3237,
        "instruction": "synchronized void connectOne(long sid){\n    if (senderWorkerMap.get(sid) != null) {\n        return;\n    }\n    synchronized (self.QV_LOCK) {\n        boolean knownId = false;\n        // Resolve hostname for the remote server before attempting to\n        // connect in case the underlying ip address has changed.\n        self.recreateSocketAddresses(sid);\n        Map<Long, QuorumPeer.QuorumServer> lastCommittedView = self.getView();\n        QuorumVerifier lastSeenQV = self.getLastSeenQuorumVerifier();\n        Map<Long, QuorumPeer.QuorumServer> lastProposedView = lastSeenQV.getAllMembers();\n        if (lastCommittedView.containsKey(sid)) {\n            knownId = true;\n            if (connectOne(sid, lastCommittedView.get(sid).electionAddr))\n                return;\n        }\n        if (lastSeenQV != null && lastProposedView.containsKey(sid)\n                && (!knownId || (lastProposedView.get(sid).electionAddr !=\n                lastCommittedView.get(sid).electionAddr))) {\n            knownId = true;\n            if (connectOne(sid, lastProposedView.get(sid).electionAddr))\n                return;\n        }\n        if (!knownId) {\n            LOG.warn(\"Invalid server id: \" + sid);\n            return;\n        }\n    }\n}",
        "accept_response": "synchronized void connectOne(long sid){\n    if (senderWorkerMap.get(sid) != null) {\n        LOG.debug(\"There is a connection already for server {}\", sid);\n        return;\n    }\n    synchronized (self.QV_LOCK) {\n        boolean knownId = false;\n        // Resolve hostname for the remote server before attempting to\n        // connect in case the underlying ip address has changed.\n        self.recreateSocketAddresses(sid);\n        Map<Long, QuorumPeer.QuorumServer> lastCommittedView = self.getView();\n        QuorumVerifier lastSeenQV = self.getLastSeenQuorumVerifier();\n        Map<Long, QuorumPeer.QuorumServer> lastProposedView = lastSeenQV.getAllMembers();\n        if (lastCommittedView.containsKey(sid)) {\n            knownId = true;\n            if (connectOne(sid, lastCommittedView.get(sid).electionAddr))\n                return;\n        }\n        if (lastSeenQV != null && lastProposedView.containsKey(sid)\n                && (!knownId || (lastProposedView.get(sid).electionAddr !=\n                lastCommittedView.get(sid).electionAddr))) {\n            knownId = true;\n            if (connectOne(sid, lastProposedView.get(sid).electionAddr))\n                return;\n        }\n        if (!knownId) {\n            LOG.warn(\"Invalid server id: \" + sid);\n            return;\n        }\n    }\n}",
        "reject_response": "synchronized void connectOne(long sid){\n    if (senderWorkerMap.get(sid) != null) {\n        LOG.debug(\"There is a connection already for server \" + sid);\n        return;\n    }\n    synchronized (self.QV_LOCK) {\n        boolean knownId = false;\n        // Resolve hostname for the remote server before attempting to\n        // connect in case the underlying ip address has changed.\n        self.recreateSocketAddresses(sid);\n        Map<Long, QuorumPeer.QuorumServer> lastCommittedView = self.getView();\n        QuorumVerifier lastSeenQV = self.getLastSeenQuorumVerifier();\n        Map<Long, QuorumPeer.QuorumServer> lastProposedView = lastSeenQV.getAllMembers();\n        if (lastCommittedView.containsKey(sid)) {\n            knownId = true;\n            if (connectOne(sid, lastCommittedView.get(sid).electionAddr))\n                return;\n        }\n        if (lastSeenQV != null && lastProposedView.containsKey(sid)\n                && (!knownId || (lastProposedView.get(sid).electionAddr !=\n                lastCommittedView.get(sid).electionAddr))) {\n            knownId = true;\n            if (connectOne(sid, lastProposedView.get(sid).electionAddr))\n                return;\n        }\n        if (!knownId) {\n            LOG.warn(\"Invalid server id: \" + sid);\n            return;\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2547,
        "instruction": "public UserException build(final Logger logger) {\n  if (uex != null) {\n    return uex;\n  }\n\n  boolean isSystemError = errorType == DrillPBError.ErrorType.SYSTEM;\n\n  // make sure system errors use the root error message and display the root cause class name\n  if (isSystemError) {\n    message = ErrorHelper.getRootMessage(cause);\n  }\n\n  final UserException newException = new UserException(this);\n\n  // since we just created a new exception, we should log it for later reference. If this is a system error, this is\n  // an issue that the Drill admin should pay attention to and we should log as ERROR. However, if this is a user\n  // mistake or data read issue, the system admin should not be concerned about these and thus we'll log this\n  // as an INFO message.\n  if (isSystemError) {\n    logger.error(newException.getMessage(), newException);\n  } else {\n    StringBuilder buf = new StringBuilder();\n    buf.append(\"User Error Occurred\");\n    if (message != null) {\n      buf.append(\": \").append(message);\n    }\n    if (cause != null) {\n      buf.append(\" (\").append(cause.getMessage()).append(\")\");\n    }\n  }\n\n  return newException;\n}",
        "accept_response": "public UserException build(final Logger logger) {\n  if (uex != null) {\n    return uex;\n  }\n\n  boolean isSystemError = errorType == DrillPBError.ErrorType.SYSTEM;\n\n  // make sure system errors use the root error message and display the root cause class name\n  if (isSystemError) {\n    message = ErrorHelper.getRootMessage(cause);\n  }\n\n  final UserException newException = new UserException(this);\n\n  // since we just created a new exception, we should log it for later reference. If this is a system error, this is\n  // an issue that the Drill admin should pay attention to and we should log as ERROR. However, if this is a user\n  // mistake or data read issue, the system admin should not be concerned about these and thus we'll log this\n  // as an INFO message.\n  if (isSystemError) {\n    logger.error(newException.getMessage(), newException);\n  } else {\n    StringBuilder buf = new StringBuilder();\n    buf.append(\"User Error Occurred\");\n    if (message != null) {\n      buf.append(\": \").append(message);\n    }\n    if (cause != null) {\n      buf.append(\" (\").append(cause.getMessage()).append(\")\");\n    }\n    logger.info(buf.toString(), newException);\n  }\n\n  return newException;\n}",
        "reject_response": "public UserException build(final Logger logger) {\n  if (uex != null) {\n    return uex;\n  }\n\n  boolean isSystemError = errorType == DrillPBError.ErrorType.SYSTEM;\n\n  // make sure system errors use the root error message and display the root cause class name\n  if (isSystemError) {\n    message = ErrorHelper.getRootMessage(cause);\n  }\n\n  final UserException newException = new UserException(this);\n\n  // since we just created a new exception, we should log it for later reference. If this is a system error, this is\n  // an issue that the Drill admin should pay attention to and we should log as ERROR. However, if this is a user\n  // mistake or data read issue, the system admin should not be concerned about these and thus we'll log this\n  // as an INFO message.\n  if (isSystemError) {\n    logger.error(newException.getMessage(), newException);\n  } else {\n    StringBuilder buf = new StringBuilder();\n    buf.append(\"User Error Occurred\");\n    if (message != null) {\n      buf.append(\": \").append(message);\n    }\n    if (cause != null) {\n      buf.append(\" (\").append(cause.getMessage()).append(\")\");\n    }\n    logger.info(\"User Error Occurred\", newException);\n  }\n\n  return newException;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2991,
        "instruction": "@GET\n@Path(KMSRESTConstants.KEY_RESOURCE + \"/{name:.*}\")\npublic Response getKey(@PathParam(\"name\") String name, @Context HttpServletRequest request)\n    throws Exception {\n  try {\n    return getMetadata(name, request);\n  } catch (Exception e) {\n    LOG.error(\"Exception in getKey.\", e);\n    throw e;\n  }\n}",
        "accept_response": "@GET\n@Path(KMSRESTConstants.KEY_RESOURCE + \"/{name:.*}\")\npublic Response getKey(@PathParam(\"name\") String name, @Context HttpServletRequest request)\n    throws Exception {\n  try {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Getting key information for key with name {}.\", name);\n    }\n    return getMetadata(name, request);\n  } catch (Exception e) {\n    LOG.error(\"Exception in getKey.\", e);\n    throw e;\n  }\n}",
        "reject_response": "@GET\n@Path(KMSRESTConstants.KEY_RESOURCE + \"/{name:.*}\")\npublic Response getKey(@PathParam(\"name\") String name, @Context HttpServletRequest request)\n    throws Exception {\n  try {\n    if (LOG.isDebugEnabled()) {\n    LOG.info(\"Entering getKey method.\");\n    LOG.debug(\"Getting key information for key with name {}.\", name);\n    LOG.info(\"Exiting getKey method.\");\n    }\n    return getMetadata(name, request);\n  } catch (Exception e) {\n    LOG.error(\"Exception in getKey.\", e);\n    throw e;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3243,
        "instruction": "private void startConnect(InetSocketAddress addr) throws IOException {\n    // initializing it for new connection\n    saslLoginFailed = false;\n    if (!isFirstConnect) {\n        try {\n            Thread.sleep(r.nextInt(1000));\n        } catch (InterruptedException e) {\n            LOG.warn(\"Unexpected exception\", e);\n        }\n    }\n    state = States.CONNECTING;\n\n    String hostPort = addr.getHostString() + \":\" + addr.getPort();\n    MDC.put(\"myid\", hostPort);\n    setName(getName().replaceAll(\"\\\\(.*\\\\)\", \"(\" + hostPort + \")\"));\n    if (clientConfig.isSaslClientEnabled()) {\n        try {\n            if (zooKeeperSaslClient != null) {\n                zooKeeperSaslClient.shutdown();\n            }\n            zooKeeperSaslClient = new ZooKeeperSaslClient(SaslServerPrincipal.getServerPrincipal(addr, clientConfig), clientConfig);\n        } catch (LoginException e) {\n            // An authentication error occurred when the SASL client tried to initialize:\n            // for Kerberos this means that the client failed to authenticate with the KDC.\n            // This is different from an authentication error that occurs during communication\n            // with the Zookeeper server, which is handled below.\n            eventThread.queueEvent(new WatchedEvent(Watcher.Event.EventType.None, Watcher.Event.KeeperState.AuthFailed, null));\n            saslLoginFailed = true;\n        }\n    }\n    logStartConnect(addr);\n\n    clientCnxnSocket.connect(addr);\n}",
        "accept_response": "private void startConnect(InetSocketAddress addr) throws IOException {\n    // initializing it for new connection\n    saslLoginFailed = false;\n    if (!isFirstConnect) {\n        try {\n            Thread.sleep(r.nextInt(1000));\n        } catch (InterruptedException e) {\n            LOG.warn(\"Unexpected exception\", e);\n        }\n    }\n    state = States.CONNECTING;\n\n    String hostPort = addr.getHostString() + \":\" + addr.getPort();\n    MDC.put(\"myid\", hostPort);\n    setName(getName().replaceAll(\"\\\\(.*\\\\)\", \"(\" + hostPort + \")\"));\n    if (clientConfig.isSaslClientEnabled()) {\n        try {\n            if (zooKeeperSaslClient != null) {\n                zooKeeperSaslClient.shutdown();\n            }\n            zooKeeperSaslClient = new ZooKeeperSaslClient(SaslServerPrincipal.getServerPrincipal(addr, clientConfig), clientConfig);\n        } catch (LoginException e) {\n            // An authentication error occurred when the SASL client tried to initialize:\n            // for Kerberos this means that the client failed to authenticate with the KDC.\n            // This is different from an authentication error that occurs during communication\n            // with the Zookeeper server, which is handled below.\n            LOG.warn(\n                \"SASL configuration failed. \"\n                    + \"Will continue connection to Zookeeper server without \"\n                    + \"SASL authentication, if Zookeeper server allows it.\", e);\n            eventThread.queueEvent(new WatchedEvent(Watcher.Event.EventType.None, Watcher.Event.KeeperState.AuthFailed, null));\n            saslLoginFailed = true;\n        }\n    }\n    logStartConnect(addr);\n\n    clientCnxnSocket.connect(addr);\n}",
        "reject_response": "private void startConnect(InetSocketAddress addr) throws IOException {\n    // initializing it for new connection\n    saslLoginFailed = false;\n    if (!isFirstConnect) {\n        try {\n            Thread.sleep(r.nextInt(1000));\n        } catch (InterruptedException e) {\n            LOG.warn(\"Unexpected exception\", e);\n        }\n    }\n    state = States.CONNECTING;\n\n    String hostPort = addr.getHostString() + \":\" + addr.getPort();\n    MDC.put(\"myid\", hostPort);\n    setName(getName().replaceAll(\"\\\\(.*\\\\)\", \"(\" + hostPort + \")\"));\n    if (clientConfig.isSaslClientEnabled()) {\n        try {\n            if (zooKeeperSaslClient != null) {\n                zooKeeperSaslClient.shutdown();\n            }\n            zooKeeperSaslClient = new ZooKeeperSaslClient(SaslServerPrincipal.getServerPrincipal(addr, clientConfig), clientConfig);\n        } catch (LoginException e) {\n            // An authentication error occurred when the SASL client tried to initialize:\n            // for Kerberos this means that the client failed to authenticate with the KDC.\n            // This is different from an authentication error that occurs during communication\n            // with the Zookeeper server, which is handled below.\n            LOG.warn(\"SASL configuration failed: \"\n                     + e\n                     + \" Will continue connection to Zookeeper server without \"\n                     + \"SASL authentication, if Zookeeper server allows it.\");\n            eventThread.queueEvent(new WatchedEvent(Watcher.Event.EventType.None, Watcher.Event.KeeperState.AuthFailed, null));\n            saslLoginFailed = true;\n        }\n    }\n    logStartConnect(addr);\n\n    clientCnxnSocket.connect(addr);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2830,
        "instruction": "private SegmentArchiveResult archiveSegment(long absIdx) throws IgniteCheckedException {\n    long segIdx = absIdx % dsCfg.getWalSegments();\n\n    File origFile = new File(walWorkDir, FileDescriptor.fileName(segIdx));\n\n    String name = FileDescriptor.fileName(absIdx);\n\n    File dstTmpFile = new File(walArchiveDir, name + \".tmp\");\n\n    File dstFile = new File(walArchiveDir, name);\n\n    if (log.isInfoEnabled())\n        log.info(\"Starting to copy WAL segment [absIdx=\" + absIdx + \", segIdx=\" + segIdx +\n            \", origFile=\" + origFile.getAbsolutePath() + \", dstFile=\" + dstFile.getAbsolutePath() + ']');\n\n    try {\n        Files.deleteIfExists(dstTmpFile.toPath());\n\n        Files.copy(origFile.toPath(), dstTmpFile.toPath());\n\n        Files.move(dstTmpFile.toPath(), dstFile.toPath());\n\n        if (mode == WALMode.FSYNC) {\n            try (FileIO f0 = ioFactory.create(dstFile, CREATE, READ, WRITE)) {\n                f0.force();\n            }\n        }\n    }\n    catch (IOException e) {\n        throw new IgniteCheckedException(\"Failed to archive WAL segment [\" +\n            \"srcFile=\" + origFile.getAbsolutePath() +\n            \", dstFile=\" + dstTmpFile.getAbsolutePath() + ']', e);\n    }\n\n    if (log.isInfoEnabled())\n\n    return new SegmentArchiveResult(absIdx, origFile, dstFile);\n}",
        "accept_response": "private SegmentArchiveResult archiveSegment(long absIdx) throws IgniteCheckedException {\n    long segIdx = absIdx % dsCfg.getWalSegments();\n\n    File origFile = new File(walWorkDir, FileDescriptor.fileName(segIdx));\n\n    String name = FileDescriptor.fileName(absIdx);\n\n    File dstTmpFile = new File(walArchiveDir, name + \".tmp\");\n\n    File dstFile = new File(walArchiveDir, name);\n\n    if (log.isInfoEnabled())\n        log.info(\"Starting to copy WAL segment [absIdx=\" + absIdx + \", segIdx=\" + segIdx +\n            \", origFile=\" + origFile.getAbsolutePath() + \", dstFile=\" + dstFile.getAbsolutePath() + ']');\n\n    try {\n        Files.deleteIfExists(dstTmpFile.toPath());\n\n        Files.copy(origFile.toPath(), dstTmpFile.toPath());\n\n        Files.move(dstTmpFile.toPath(), dstFile.toPath());\n\n        if (mode == WALMode.FSYNC) {\n            try (FileIO f0 = ioFactory.create(dstFile, CREATE, READ, WRITE)) {\n                f0.force();\n            }\n        }\n    }\n    catch (IOException e) {\n        throw new IgniteCheckedException(\"Failed to archive WAL segment [\" +\n            \"srcFile=\" + origFile.getAbsolutePath() +\n            \", dstFile=\" + dstTmpFile.getAbsolutePath() + ']', e);\n    }\n\n    if (log.isInfoEnabled())\n        log.info(\"Copied file [src=\" + origFile.getAbsolutePath() +\n            \", dst=\" + dstFile.getAbsolutePath() + ']');\n\n    return new SegmentArchiveResult(absIdx, origFile, dstFile);\n}",
        "reject_response": "private SegmentArchiveResult archiveSegment(long absIdx) throws IgniteCheckedException {\n    long segIdx = absIdx % dsCfg.getWalSegments();\n\n    File origFile = new File(walWorkDir, FileDescriptor.fileName(segIdx));\n\n    String name = FileDescriptor.fileName(absIdx);\n\n    File dstTmpFile = new File(walArchiveDir, name + \".tmp\");\n\n    File dstFile = new File(walArchiveDir, name);\n\n    if (log.isInfoEnabled())\n        log.info(\"Starting to copy WAL segment [absIdx=\" + absIdx + \", segIdx=\" + segIdx +\n            \", origFile=\" + origFile.getAbsolutePath() + \", dstFile=\" + dstFile.getAbsolutePath() + ']');\n\n    try {\n        Files.deleteIfExists(dstTmpFile.toPath());\n\n        Files.copy(origFile.toPath(), dstTmpFile.toPath());\n\n        Files.move(dstTmpFile.toPath(), dstFile.toPath());\n\n        if (mode == WALMode.FSYNC) {\n            try (FileIO f0 = ioFactory.create(dstFile, CREATE, READ, WRITE)) {\n                f0.force();\n            }\n        }\n    }\n    catch (IOException e) {\n        throw new IgniteCheckedException(\"Failed to archive WAL segment [\" +\n            \"srcFile=\" + origFile.getAbsolutePath() +\n            \", dstFile=\" + dstTmpFile.getAbsolutePath() + ']', e);\n    }\n\n    if (log.isInfoEnabled())\n    if (log.isDebugEnabled())\n        log.debug(\"Copied file [src=\" + origFile.getAbsolutePath() +\n\n    return new SegmentArchiveResult(absIdx, origFile, dstFile);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2603,
        "instruction": "@Override\npublic void run() {\n\tLOG.debug(getName() + \": starting\");\n\tSERVER.set(Server.this);\n\tByteArrayOutputStream buf = new ByteArrayOutputStream(10240);\n\twhile (running) {\n\t\ttry {\n\t\t\tfinal Call call = callQueue.take(); // pop the queue; maybe blocked here\n\n\t\t\tString errorClass = null;\n\t\t\tString error = null;\n\t\t\tIOReadableWritable value = null;\n\n\t\t\tCurCall.set(call);\n\n\t\t\tvalue = call(call.connection.protocol, call.param, call.timestamp);\n\n\t\t\tCurCall.set(null);\n\n\t\t\tsetupResponse(buf, call, (error == null) ? Status.SUCCESS : Status.ERROR, value, errorClass, error);\n\t\t\tresponder.doRespond(call);\n\t\t} catch (InterruptedException e) {\n\t\t\tif (running) { // unexpected -- log it\n\t\t\t\tLOG.info(getName() + \" caught: \", e);\n\t\t\t}\n\t\t} catch (Exception e) {\n\t\t\tLOG.info(getName() + \" caught: \", e);\n\t\t}\n\t}\n\n\tthis.shutDown = true;\n}",
        "accept_response": "@Override\npublic void run() {\n\tLOG.debug(getName() + \": starting\");\n\tSERVER.set(Server.this);\n\tByteArrayOutputStream buf = new ByteArrayOutputStream(10240);\n\twhile (running) {\n\t\ttry {\n\t\t\tfinal Call call = callQueue.take(); // pop the queue; maybe blocked here\n\n\t\t\tString errorClass = null;\n\t\t\tString error = null;\n\t\t\tIOReadableWritable value = null;\n\n\t\t\tCurCall.set(call);\n\n\t\t\tvalue = call(call.connection.protocol, call.param, call.timestamp);\n\n\t\t\tCurCall.set(null);\n\n\t\t\tsetupResponse(buf, call, (error == null) ? Status.SUCCESS : Status.ERROR, value, errorClass, error);\n\t\t\tresponder.doRespond(call);\n\t\t} catch (InterruptedException e) {\n\t\t\tif (running) { // unexpected -- log it\n\t\t\t\tLOG.info(getName() + \" caught: \", e);\n\t\t\t}\n\t\t} catch (Exception e) {\n\t\t\tLOG.info(getName() + \" caught: \", e);\n\t\t}\n\t}\n\tLOG.debug(getName() + \": exiting\");\n\n\tthis.shutDown = true;\n}",
        "reject_response": "@Override\npublic void run() {\n\tLOG.debug(getName() + \": starting\");\n\tSERVER.set(Server.this);\n\tByteArrayOutputStream buf = new ByteArrayOutputStream(10240);\n\twhile (running) {\n\t\ttry {\n\t\t\tfinal Call call = callQueue.take(); // pop the queue; maybe blocked here\n\n\t\t\tString errorClass = null;\n\t\t\tString error = null;\n\t\t\tIOReadableWritable value = null;\n\n\t\t\tCurCall.set(call);\n\n\t\t\tvalue = call(call.connection.protocol, call.param, call.timestamp);\n\n\t\t\tCurCall.set(null);\n\n\t\t\tsetupResponse(buf, call, (error == null) ? Status.SUCCESS : Status.ERROR, value, errorClass, error);\n\t\t\tresponder.doRespond(call);\n\t\t} catch (InterruptedException e) {\n\t\t\tif (running) { // unexpected -- log it\n\t\t\t\tLOG.info(getName() + \" caught: \", e);\n\t\t\t}\n\t\t} catch (Exception e) {\n\t\t\tLOG.info(getName() + \" caught: \", e);\n\t\t}\n\t}\n\tLOG.info(getName() + \": exiting\");\n\n\tthis.shutDown = true;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2778,
        "instruction": "ClassicHttpResponse handleBackendResponse(\n        final HttpHost target,\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final Date requestDate,\n        final Date responseDate,\n        final ClassicHttpResponse backendResponse) throws IOException {\n\n    responseCompliance.ensureProtocolCompliance(scope.originalRequest, request, backendResponse);\n\n    responseCache.flushCacheEntriesInvalidatedByExchange(target, request, backendResponse);\n    final boolean cacheable = responseCachingPolicy.isResponseCacheable(request, backendResponse);\n    if (cacheable) {\n        storeRequestIfModifiedSinceFor304Response(request, backendResponse);\n        return cacheAndReturnResponse(target, request, backendResponse, scope, requestDate, responseDate);\n    }\n    responseCache.flushCacheEntriesFor(target, request);\n    return backendResponse;\n}",
        "accept_response": "ClassicHttpResponse handleBackendResponse(\n        final HttpHost target,\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final Date requestDate,\n        final Date responseDate,\n        final ClassicHttpResponse backendResponse) throws IOException {\n\n    responseCompliance.ensureProtocolCompliance(scope.originalRequest, request, backendResponse);\n\n    responseCache.flushCacheEntriesInvalidatedByExchange(target, request, backendResponse);\n    final boolean cacheable = responseCachingPolicy.isResponseCacheable(request, backendResponse);\n    if (cacheable) {\n        storeRequestIfModifiedSinceFor304Response(request, backendResponse);\n        return cacheAndReturnResponse(target, request, backendResponse, scope, requestDate, responseDate);\n    }\n    LOG.debug(\"Backend response is not cacheable\");\n    responseCache.flushCacheEntriesFor(target, request);\n    return backendResponse;\n}",
        "reject_response": "ClassicHttpResponse handleBackendResponse(\n        final HttpHost target,\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final Date requestDate,\n        final Date responseDate,\n        final ClassicHttpResponse backendResponse) throws IOException {\n\n    responseCompliance.ensureProtocolCompliance(scope.originalRequest, request, backendResponse);\n\n    responseCache.flushCacheEntriesInvalidatedByExchange(target, request, backendResponse);\n    final boolean cacheable = responseCachingPolicy.isResponseCacheable(request, backendResponse);\n    if (cacheable) {\n        storeRequestIfModifiedSinceFor304Response(request, backendResponse);\n        return cacheAndReturnResponse(target, request, backendResponse, scope, requestDate, responseDate);\n    }\n    log.debug(\"Backend response is not cacheable\");\n    responseCache.flushCacheEntriesFor(target, request);\n    return backendResponse;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2692,
        "instruction": "@GET\n@Produces(MediaType.APPLICATION_OCTET_STREAM)\npublic Response read(@Context final ServletContext servletContext,\n                     @Context HttpHeaders headers) throws Exception {\n    // Convert headers into a regular map\n    Map<String, String> params = convertToCaseInsensitiveMap(headers.getRequestHeaders());\n\n    LOG.debug(\"started with parameters: \" + params);\n\n    ProtocolData protData = new ProtocolData(params);\n    SecuredHDFS.verifyToken(protData, servletContext);\n    Bridge bridge;\n    float sampleRatio = protData.getStatsSampleRatio();\n    if (sampleRatio > 0) {\n        bridge = new ReadSamplingBridge(protData);\n    } else {\n        bridge = new ReadBridge(protData);\n    }\n    String dataDir = protData.getDataSource();\n    // THREAD-SAFE parameter has precedence\n    boolean isThreadSafe = protData.isThreadSafe() && bridge.isThreadSafe();\n\n    return readResponse(bridge, protData, isThreadSafe);\n}",
        "accept_response": "@GET\n@Produces(MediaType.APPLICATION_OCTET_STREAM)\npublic Response read(@Context final ServletContext servletContext,\n                     @Context HttpHeaders headers) throws Exception {\n    // Convert headers into a regular map\n    Map<String, String> params = convertToCaseInsensitiveMap(headers.getRequestHeaders());\n\n    LOG.debug(\"started with parameters: \" + params);\n\n    ProtocolData protData = new ProtocolData(params);\n    SecuredHDFS.verifyToken(protData, servletContext);\n    Bridge bridge;\n    float sampleRatio = protData.getStatsSampleRatio();\n    if (sampleRatio > 0) {\n        bridge = new ReadSamplingBridge(protData);\n    } else {\n        bridge = new ReadBridge(protData);\n    }\n    String dataDir = protData.getDataSource();\n    // THREAD-SAFE parameter has precedence\n    boolean isThreadSafe = protData.isThreadSafe() && bridge.isThreadSafe();\n    LOG.debug(\"Request for \" + dataDir + \" will be handled \"\n            + (isThreadSafe ? \"without\" : \"with\") + \" synchronization\");\n\n    return readResponse(bridge, protData, isThreadSafe);\n}",
        "reject_response": "@GET\n@Produces(MediaType.APPLICATION_OCTET_STREAM)\npublic Response read(@Context final ServletContext servletContext,\n                     @Context HttpHeaders headers) throws Exception {\n    // Convert headers into a regular map\n    Map<String, String> params = convertToCaseInsensitiveMap(headers.getRequestHeaders());\n\n    LOG.debug(\"started with parameters: \" + params);\n\n    ProtocolData protData = new ProtocolData(params);\n    SecuredHDFS.verifyToken(protData, servletContext);\n    Bridge bridge;\n    float sampleRatio = protData.getStatsSampleRatio();\n    if (sampleRatio > 0) {\n        bridge = new ReadSamplingBridge(protData);\n    } else {\n        bridge = new ReadBridge(protData);\n    }\n    String dataDir = protData.getDataSource();\n    // THREAD-SAFE parameter has precedence\n    boolean isThreadSafe = protData.isThreadSafe() && bridge.isThreadSafe();\n    Log.debug(\"Request for \" + dataDir + \" will be handled \"\n\n    return readResponse(bridge, protData, isThreadSafe);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2440,
        "instruction": "public static boolean removeAppender(Logger logger, String name)\n{\n  if (logger == null) {\n    logger = LogManager.getRootLogger();\n  }\n  try {\n    logger.removeAppender(name);\n  } catch (Exception ex) {\n    return false;\n  }\n  return true;\n}",
        "accept_response": "public static boolean removeAppender(Logger logger, String name)\n{\n  if (logger == null) {\n    logger = LogManager.getRootLogger();\n  }\n  try {\n    logger.removeAppender(name);\n  } catch (Exception ex) {\n    LoggerUtil.logger.error(\"Cannot remove the logger appender: {}\", name, ex);\n    return false;\n  }\n  return true;\n}",
        "reject_response": "public static boolean removeAppender(Logger logger, String name)\n{\n  if (logger == null) {\n    logger = LogManager.getRootLogger();\n  }\n  try {\n    logger.removeAppender(name);\n  } catch (Exception ex) {\n    logger.error(\"Cannot remove the logger appender: {}\", name, ex);\n    return false;\n  }\n  return true;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2474,
        "instruction": "private void request(int requestedCapacity) {\n    if (requestedCapacity <= this.buffer.size()) {\n        return;\n    }\n\n    int oldCapacity = this.buffer.size();\n    int newCapacity = oldCapacity + this.incrementCapacityBy;\n    this.buffer.ensureCapacity(newCapacity);\n    instantiateItems(oldCapacity, newCapacity);\n}",
        "accept_response": "private void request(int requestedCapacity) {\n    if (requestedCapacity <= this.buffer.size()) {\n        return;\n    }\n\n    int oldCapacity = this.buffer.size();\n    int newCapacity = oldCapacity + this.incrementCapacityBy;\n    this.buffer.ensureCapacity(newCapacity);\n    instantiateItems(oldCapacity, newCapacity);\n\n    if (LOG.isDebugEnabled()) {\n        LOG.debug(\"FixedBufferList: Requested: {} From: {} To:{}\", requestedCapacity, oldCapacity, newCapacity);\n    }\n}",
        "reject_response": "private void request(int requestedCapacity) {\n    if (requestedCapacity <= this.buffer.size()) {\n        return;\n    }\n\n    int oldCapacity = this.buffer.size();\n    int newCapacity = oldCapacity + this.incrementCapacityBy;\n    this.buffer.ensureCapacity(newCapacity);\n    instantiateItems(oldCapacity, newCapacity);\n\n    if (LOG.isDebugEnabled()) {\n    LOG.info(\"FixedBufferList: Requested: {} From: {} To:{}\", requestedCapacity, oldCapacity, newCapacity);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2883,
        "instruction": "public void reset() {\n  LOG.info(\"Resetting the JoshuaConfiguration to its defaults ...\");\n  LOG.info(\"\\n\\t...done\");\n  StatefulFF.resetGlobalStateIndex();\n  tms = new ArrayList<String>();\n  weights_file = \"\";\n  default_non_terminal = \"[X]\";\n  oovList = new ArrayList<OOVItem>();\n  oovList.add(new OOVItem(default_non_terminal, 1.0f));\n  goal_symbol = \"[GOAL]\";\n  amortized_sorting = true;\n  constrain_parse = false;\n  use_pos_labels = false;\n  true_oovs_only = false;\n  filter_grammar = false;\n  pop_limit = 100;\n  maxlen = 200;\n  use_unique_nbest = false;\n  include_align_index = false;\n  topN = 1;\n  outputFormat = \"%i ||| %s ||| %f ||| %c\";\n  num_parallel_decoders = 1;\n  hypergraphFilePattern = \"\";\n  mark_oovs = false;\n  // oracleFile = null;\n  parse = false; // perform synchronous parsing\n  features = new ArrayList<String>();\n  weights = new ArrayList<String>();\n  server_port = 0;\n\n  reordering_limit = 8;\n  num_translation_options = 20;\n  LOG.info(\"...done\");\n}",
        "accept_response": "public void reset() {\n  LOG.info(\"Resetting the JoshuaConfiguration to its defaults ...\");\n  LOG.info(\"\\n\\tResetting the StatefullFF global state index ...\");\n  LOG.info(\"\\n\\t...done\");\n  StatefulFF.resetGlobalStateIndex();\n  tms = new ArrayList<String>();\n  weights_file = \"\";\n  default_non_terminal = \"[X]\";\n  oovList = new ArrayList<OOVItem>();\n  oovList.add(new OOVItem(default_non_terminal, 1.0f));\n  goal_symbol = \"[GOAL]\";\n  amortized_sorting = true;\n  constrain_parse = false;\n  use_pos_labels = false;\n  true_oovs_only = false;\n  filter_grammar = false;\n  pop_limit = 100;\n  maxlen = 200;\n  use_unique_nbest = false;\n  include_align_index = false;\n  topN = 1;\n  outputFormat = \"%i ||| %s ||| %f ||| %c\";\n  num_parallel_decoders = 1;\n  hypergraphFilePattern = \"\";\n  mark_oovs = false;\n  // oracleFile = null;\n  parse = false; // perform synchronous parsing\n  features = new ArrayList<String>();\n  weights = new ArrayList<String>();\n  server_port = 0;\n\n  reordering_limit = 8;\n  num_translation_options = 20;\n  LOG.info(\"...done\");\n}",
        "reject_response": "public void reset() {\n  LOG.info(\"Resetting the JoshuaConfiguration to its defaults ...\");\n  logger.info(\"Resetting the JoshuaConfiguration to its defaults ...\");\n  logger.info(\"\\n\\tResetting the StatefullFF global state index ...\");\n  logger.info(\"\\n\\t...done\");\n  LOG.info(\"\\n\\t...done\");\n  StatefulFF.resetGlobalStateIndex();\n  tms = new ArrayList<String>();\n  weights_file = \"\";\n  default_non_terminal = \"[X]\";\n  oovList = new ArrayList<OOVItem>();\n  oovList.add(new OOVItem(default_non_terminal, 1.0f));\n  goal_symbol = \"[GOAL]\";\n  amortized_sorting = true;\n  constrain_parse = false;\n  use_pos_labels = false;\n  true_oovs_only = false;\n  filter_grammar = false;\n  pop_limit = 100;\n  maxlen = 200;\n  use_unique_nbest = false;\n  include_align_index = false;\n  topN = 1;\n  outputFormat = \"%i ||| %s ||| %f ||| %c\";\n  num_parallel_decoders = 1;\n  hypergraphFilePattern = \"\";\n  mark_oovs = false;\n  // oracleFile = null;\n  parse = false; // perform synchronous parsing\n  features = new ArrayList<String>();\n  weights = new ArrayList<String>();\n  server_port = 0;\n\n  reordering_limit = 8;\n  num_translation_options = 20;\n  LOG.info(\"...done\");\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2713,
        "instruction": "@edu.umd.cs.findbugs.annotations.SuppressWarnings(value=\"UL_UNRELEASED_LOCK_EXCEPTION_PATH\",\n    justification=\"I think FindBugs is confused\")\nprivate Map<byte[], List<StoreFile>> doClose(final boolean abort, MonitoredTask status)\n    throws IOException {\n  if (isClosed()) {\n    LOG.warn(\"Region \" + this + \" already closed\");\n    return null;\n  }\n\n  if (coprocessorHost != null) {\n    status.setStatus(\"Running coprocessor pre-close hooks\");\n    this.coprocessorHost.preClose(abort);\n  }\n\n  status.setStatus(\"Disabling compacts and flushes for region\");\n  boolean canFlush = true;\n  synchronized (writestate) {\n    // Disable compacting and flushing by background threads for this\n    // region.\n    canFlush = !writestate.readOnly;\n    writestate.writesEnabled = false;\n    LOG.debug(\"Closing \" + this + \": disabling compactions & flushes\");\n    waitForFlushesAndCompactions();\n  }\n  // If we were not just flushing, is it worth doing a preflush...one\n  // that will clear out of the bulk of the memstore before we put up\n  // the close flag?\n  if (!abort && worthPreFlushing() && canFlush) {\n    status.setStatus(\"Pre-flushing region before close\");\n    try {\n      internalFlushcache(status);\n    } catch (IOException ioe) {\n      // Failed to flush the region. Keep going.\n      status.setStatus(\"Failed pre-flush \" + this + \"; \" + ioe.getMessage());\n    }\n  }\n\n  // block waiting for the lock for closing\n  lock.writeLock().lock(); // FindBugs: Complains UL_UNRELEASED_LOCK_EXCEPTION_PATH but seems fine\n  this.closing.set(true);\n  status.setStatus(\"Disabling writes for close\");\n  try {\n    if (this.isClosed()) {\n      status.abort(\"Already got closed by another process\");\n      // SplitTransaction handles the null\n      return null;\n    }\n    LOG.debug(\"Updates disabled for region \" + this);\n    // Don't flush the cache if we are aborting\n    if (!abort && canFlush) {\n      int failedfFlushCount = 0;\n      int flushCount = 0;\n      long tmp = 0;\n      long remainingSize = this.memstoreSize.get();\n      while (remainingSize > 0) {\n        try {\n          internalFlushcache(status);\n          if(flushCount >0) {\n            LOG.info(\"Running extra flush, \" + flushCount +\n                \" (carrying snapshot?) \" + this);\n          }\n          flushCount++;\n          tmp = this.memstoreSize.get();\n          if (tmp >= remainingSize) {\n            failedfFlushCount++;\n          }\n          remainingSize = tmp;\n          if (failedfFlushCount > 5) {\n            // If we failed 5 times and are unable to clear memory, abort\n            // so we do not lose data\n            throw new DroppedSnapshotException(\"Failed clearing memory after \" +\n                flushCount + \" attempts on region: \" +\n                Bytes.toStringBinary(getRegionInfo().getRegionName()));\n          }\n        } catch (IOException ioe) {\n          status.setStatus(\"Failed flush \" + this + \", putting online again\");\n          synchronized (writestate) {\n            writestate.writesEnabled = true;\n          }\n          // Have to throw to upper layers.  I can't abort server from here.\n          throw ioe;\n        }\n      }\n    }\n\n    Map<byte[], List<StoreFile>> result =\n      new TreeMap<byte[], List<StoreFile>>(Bytes.BYTES_COMPARATOR);\n    if (!stores.isEmpty()) {\n      // initialize the thread pool for closing stores in parallel.\n      ThreadPoolExecutor storeCloserThreadPool =\n        getStoreOpenAndCloseThreadPool(\"StoreCloserThread-\" +\n          getRegionInfo().getRegionNameAsString());\n      CompletionService<Pair<byte[], Collection<StoreFile>>> completionService =\n        new ExecutorCompletionService<Pair<byte[], Collection<StoreFile>>>(storeCloserThreadPool);\n\n      // close each store in parallel\n      for (final Store store : stores.values()) {\n        long flushableSize = store.getFlushableSize();\n        if (!(abort || flushableSize == 0 || writestate.readOnly)) {\n          if (getRegionServerServices() != null) {\n            getRegionServerServices().abort(\"Assertion failed while closing store \"\n              + getRegionInfo().getRegionNameAsString() + \" \" + store\n              + \". flushableSize expected=0, actual= \" + flushableSize\n              + \". Current memstoreSize=\" + getMemstoreSize() + \". Maybe a coprocessor \"\n              + \"operation failed and left the memstore in a partially updated state.\", null);\n          }\n        }\n        completionService\n            .submit(new Callable<Pair<byte[], Collection<StoreFile>>>() {\n              @Override\n              public Pair<byte[], Collection<StoreFile>> call() throws IOException {\n                return new Pair<byte[], Collection<StoreFile>>(\n                  store.getFamily().getName(), store.close());\n              }\n            });\n      }\n      try {\n        for (int i = 0; i < stores.size(); i++) {\n          Future<Pair<byte[], Collection<StoreFile>>> future = completionService.take();\n          Pair<byte[], Collection<StoreFile>> storeFiles = future.get();\n          List<StoreFile> familyFiles = result.get(storeFiles.getFirst());\n          if (familyFiles == null) {\n            familyFiles = new ArrayList<StoreFile>();\n            result.put(storeFiles.getFirst(), familyFiles);\n          }\n          familyFiles.addAll(storeFiles.getSecond());\n        }\n      } catch (InterruptedException e) {\n        throw (InterruptedIOException)new InterruptedIOException().initCause(e);\n      } catch (ExecutionException e) {\n        throw new IOException(e.getCause());\n      } finally {\n        storeCloserThreadPool.shutdownNow();\n      }\n    }\n\n    status.setStatus(\"Writing region close event to WAL\");\n    if (!abort && wal != null && getRegionServerServices() != null && !writestate.readOnly) {\n      writeRegionCloseMarker(wal);\n    }\n\n    this.closed.set(true);\n    if (!canFlush) {\n      addAndGetGlobalMemstoreSize(-memstoreSize.get());\n    } else if (memstoreSize.get() != 0) {\n      LOG.error(\"Memstore size is \" + memstoreSize.get());\n    }\n    if (coprocessorHost != null) {\n      status.setStatus(\"Running coprocessor post-close hooks\");\n      this.coprocessorHost.postClose(abort);\n    }\n    if (this.metricsRegion != null) {\n      this.metricsRegion.close();\n    }\n    if (this.metricsRegionWrapper != null) {\n      Closeables.closeQuietly(this.metricsRegionWrapper);\n    }\n    // stop the Compacted hfile discharger\n    if (this.compactedFileDischarger != null) this.compactedFileDischarger.cancel(true);\n\n    status.markComplete(\"Closed\");\n    LOG.info(\"Closed \" + this);\n    return result;\n  } finally {\n    lock.writeLock().unlock();\n  }\n}",
        "accept_response": "@edu.umd.cs.findbugs.annotations.SuppressWarnings(value=\"UL_UNRELEASED_LOCK_EXCEPTION_PATH\",\n    justification=\"I think FindBugs is confused\")\nprivate Map<byte[], List<StoreFile>> doClose(final boolean abort, MonitoredTask status)\n    throws IOException {\n  if (isClosed()) {\n    LOG.warn(\"Region \" + this + \" already closed\");\n    return null;\n  }\n\n  if (coprocessorHost != null) {\n    status.setStatus(\"Running coprocessor pre-close hooks\");\n    this.coprocessorHost.preClose(abort);\n  }\n\n  status.setStatus(\"Disabling compacts and flushes for region\");\n  boolean canFlush = true;\n  synchronized (writestate) {\n    // Disable compacting and flushing by background threads for this\n    // region.\n    canFlush = !writestate.readOnly;\n    writestate.writesEnabled = false;\n    LOG.debug(\"Closing \" + this + \": disabling compactions & flushes\");\n    waitForFlushesAndCompactions();\n  }\n  // If we were not just flushing, is it worth doing a preflush...one\n  // that will clear out of the bulk of the memstore before we put up\n  // the close flag?\n  if (!abort && worthPreFlushing() && canFlush) {\n    status.setStatus(\"Pre-flushing region before close\");\n    LOG.info(\"Running close preflush of \" + this);\n    try {\n      internalFlushcache(status);\n    } catch (IOException ioe) {\n      // Failed to flush the region. Keep going.\n      status.setStatus(\"Failed pre-flush \" + this + \"; \" + ioe.getMessage());\n    }\n  }\n\n  // block waiting for the lock for closing\n  lock.writeLock().lock(); // FindBugs: Complains UL_UNRELEASED_LOCK_EXCEPTION_PATH but seems fine\n  this.closing.set(true);\n  status.setStatus(\"Disabling writes for close\");\n  try {\n    if (this.isClosed()) {\n      status.abort(\"Already got closed by another process\");\n      // SplitTransaction handles the null\n      return null;\n    }\n    LOG.debug(\"Updates disabled for region \" + this);\n    // Don't flush the cache if we are aborting\n    if (!abort && canFlush) {\n      int failedfFlushCount = 0;\n      int flushCount = 0;\n      long tmp = 0;\n      long remainingSize = this.memstoreSize.get();\n      while (remainingSize > 0) {\n        try {\n          internalFlushcache(status);\n          if(flushCount >0) {\n            LOG.info(\"Running extra flush, \" + flushCount +\n                \" (carrying snapshot?) \" + this);\n          }\n          flushCount++;\n          tmp = this.memstoreSize.get();\n          if (tmp >= remainingSize) {\n            failedfFlushCount++;\n          }\n          remainingSize = tmp;\n          if (failedfFlushCount > 5) {\n            // If we failed 5 times and are unable to clear memory, abort\n            // so we do not lose data\n            throw new DroppedSnapshotException(\"Failed clearing memory after \" +\n                flushCount + \" attempts on region: \" +\n                Bytes.toStringBinary(getRegionInfo().getRegionName()));\n          }\n        } catch (IOException ioe) {\n          status.setStatus(\"Failed flush \" + this + \", putting online again\");\n          synchronized (writestate) {\n            writestate.writesEnabled = true;\n          }\n          // Have to throw to upper layers.  I can't abort server from here.\n          throw ioe;\n        }\n      }\n    }\n\n    Map<byte[], List<StoreFile>> result =\n      new TreeMap<byte[], List<StoreFile>>(Bytes.BYTES_COMPARATOR);\n    if (!stores.isEmpty()) {\n      // initialize the thread pool for closing stores in parallel.\n      ThreadPoolExecutor storeCloserThreadPool =\n        getStoreOpenAndCloseThreadPool(\"StoreCloserThread-\" +\n          getRegionInfo().getRegionNameAsString());\n      CompletionService<Pair<byte[], Collection<StoreFile>>> completionService =\n        new ExecutorCompletionService<Pair<byte[], Collection<StoreFile>>>(storeCloserThreadPool);\n\n      // close each store in parallel\n      for (final Store store : stores.values()) {\n        long flushableSize = store.getFlushableSize();\n        if (!(abort || flushableSize == 0 || writestate.readOnly)) {\n          if (getRegionServerServices() != null) {\n            getRegionServerServices().abort(\"Assertion failed while closing store \"\n              + getRegionInfo().getRegionNameAsString() + \" \" + store\n              + \". flushableSize expected=0, actual= \" + flushableSize\n              + \". Current memstoreSize=\" + getMemstoreSize() + \". Maybe a coprocessor \"\n              + \"operation failed and left the memstore in a partially updated state.\", null);\n          }\n        }\n        completionService\n            .submit(new Callable<Pair<byte[], Collection<StoreFile>>>() {\n              @Override\n              public Pair<byte[], Collection<StoreFile>> call() throws IOException {\n                return new Pair<byte[], Collection<StoreFile>>(\n                  store.getFamily().getName(), store.close());\n              }\n            });\n      }\n      try {\n        for (int i = 0; i < stores.size(); i++) {\n          Future<Pair<byte[], Collection<StoreFile>>> future = completionService.take();\n          Pair<byte[], Collection<StoreFile>> storeFiles = future.get();\n          List<StoreFile> familyFiles = result.get(storeFiles.getFirst());\n          if (familyFiles == null) {\n            familyFiles = new ArrayList<StoreFile>();\n            result.put(storeFiles.getFirst(), familyFiles);\n          }\n          familyFiles.addAll(storeFiles.getSecond());\n        }\n      } catch (InterruptedException e) {\n        throw (InterruptedIOException)new InterruptedIOException().initCause(e);\n      } catch (ExecutionException e) {\n        throw new IOException(e.getCause());\n      } finally {\n        storeCloserThreadPool.shutdownNow();\n      }\n    }\n\n    status.setStatus(\"Writing region close event to WAL\");\n    if (!abort && wal != null && getRegionServerServices() != null && !writestate.readOnly) {\n      writeRegionCloseMarker(wal);\n    }\n\n    this.closed.set(true);\n    if (!canFlush) {\n      addAndGetGlobalMemstoreSize(-memstoreSize.get());\n    } else if (memstoreSize.get() != 0) {\n      LOG.error(\"Memstore size is \" + memstoreSize.get());\n    }\n    if (coprocessorHost != null) {\n      status.setStatus(\"Running coprocessor post-close hooks\");\n      this.coprocessorHost.postClose(abort);\n    }\n    if (this.metricsRegion != null) {\n      this.metricsRegion.close();\n    }\n    if (this.metricsRegionWrapper != null) {\n      Closeables.closeQuietly(this.metricsRegionWrapper);\n    }\n    // stop the Compacted hfile discharger\n    if (this.compactedFileDischarger != null) this.compactedFileDischarger.cancel(true);\n\n    status.markComplete(\"Closed\");\n    LOG.info(\"Closed \" + this);\n    return result;\n  } finally {\n    lock.writeLock().unlock();\n  }\n}",
        "reject_response": "@edu.umd.cs.findbugs.annotations.SuppressWarnings(value=\"UL_UNRELEASED_LOCK_EXCEPTION_PATH\",\n    justification=\"I think FindBugs is confused\")\nprivate Map<byte[], List<StoreFile>> doClose(final boolean abort, MonitoredTask status)\n    throws IOException {\n  if (isClosed()) {\n    LOG.warn(\"Region \" + this + \" already closed\");\n    return null;\n  }\n\n  if (coprocessorHost != null) {\n    status.setStatus(\"Running coprocessor pre-close hooks\");\n    this.coprocessorHost.preClose(abort);\n  }\n\n  status.setStatus(\"Disabling compacts and flushes for region\");\n  boolean canFlush = true;\n  synchronized (writestate) {\n    // Disable compacting and flushing by background threads for this\n    // region.\n    canFlush = !writestate.readOnly;\n    writestate.writesEnabled = false;\n    LOG.debug(\"Closing \" + this + \": disabling compactions & flushes\");\n    waitForFlushesAndCompactions();\n  }\n  // If we were not just flushing, is it worth doing a preflush...one\n  // that will clear out of the bulk of the memstore before we put up\n  // the close flag?\n  if (!abort && worthPreFlushing() && canFlush) {\n    status.setStatus(\"Pre-flushing region before close\");\n    LOG.info(\"Running close preflush of \" + getRegionInfo().getRegionNameAsString());\n    try {\n      internalFlushcache(status);\n    } catch (IOException ioe) {\n      // Failed to flush the region. Keep going.\n      status.setStatus(\"Failed pre-flush \" + this + \"; \" + ioe.getMessage());\n    }\n  }\n\n  // block waiting for the lock for closing\n  lock.writeLock().lock(); // FindBugs: Complains UL_UNRELEASED_LOCK_EXCEPTION_PATH but seems fine\n  this.closing.set(true);\n  status.setStatus(\"Disabling writes for close\");\n  try {\n    if (this.isClosed()) {\n      status.abort(\"Already got closed by another process\");\n      // SplitTransaction handles the null\n      return null;\n    }\n    LOG.debug(\"Updates disabled for region \" + this);\n    // Don't flush the cache if we are aborting\n    if (!abort && canFlush) {\n      int failedfFlushCount = 0;\n      int flushCount = 0;\n      long tmp = 0;\n      long remainingSize = this.memstoreSize.get();\n      while (remainingSize > 0) {\n        try {\n          internalFlushcache(status);\n          if(flushCount >0) {\n            LOG.info(\"Running extra flush, \" + flushCount +\n                \" (carrying snapshot?) \" + this);\n          }\n          flushCount++;\n          tmp = this.memstoreSize.get();\n          if (tmp >= remainingSize) {\n            failedfFlushCount++;\n          }\n          remainingSize = tmp;\n          if (failedfFlushCount > 5) {\n            // If we failed 5 times and are unable to clear memory, abort\n            // so we do not lose data\n            throw new DroppedSnapshotException(\"Failed clearing memory after \" +\n                flushCount + \" attempts on region: \" +\n                Bytes.toStringBinary(getRegionInfo().getRegionName()));\n          }\n        } catch (IOException ioe) {\n          status.setStatus(\"Failed flush \" + this + \", putting online again\");\n          synchronized (writestate) {\n            writestate.writesEnabled = true;\n          }\n          // Have to throw to upper layers.  I can't abort server from here.\n          throw ioe;\n        }\n      }\n    }\n\n    Map<byte[], List<StoreFile>> result =\n      new TreeMap<byte[], List<StoreFile>>(Bytes.BYTES_COMPARATOR);\n    if (!stores.isEmpty()) {\n      // initialize the thread pool for closing stores in parallel.\n      ThreadPoolExecutor storeCloserThreadPool =\n        getStoreOpenAndCloseThreadPool(\"StoreCloserThread-\" +\n          getRegionInfo().getRegionNameAsString());\n      CompletionService<Pair<byte[], Collection<StoreFile>>> completionService =\n        new ExecutorCompletionService<Pair<byte[], Collection<StoreFile>>>(storeCloserThreadPool);\n\n      // close each store in parallel\n      for (final Store store : stores.values()) {\n        long flushableSize = store.getFlushableSize();\n        if (!(abort || flushableSize == 0 || writestate.readOnly)) {\n          if (getRegionServerServices() != null) {\n            getRegionServerServices().abort(\"Assertion failed while closing store \"\n              + getRegionInfo().getRegionNameAsString() + \" \" + store\n              + \". flushableSize expected=0, actual= \" + flushableSize\n              + \". Current memstoreSize=\" + getMemstoreSize() + \". Maybe a coprocessor \"\n              + \"operation failed and left the memstore in a partially updated state.\", null);\n          }\n        }\n        completionService\n            .submit(new Callable<Pair<byte[], Collection<StoreFile>>>() {\n              @Override\n              public Pair<byte[], Collection<StoreFile>> call() throws IOException {\n                return new Pair<byte[], Collection<StoreFile>>(\n                  store.getFamily().getName(), store.close());\n              }\n            });\n      }\n      try {\n        for (int i = 0; i < stores.size(); i++) {\n          Future<Pair<byte[], Collection<StoreFile>>> future = completionService.take();\n          Pair<byte[], Collection<StoreFile>> storeFiles = future.get();\n          List<StoreFile> familyFiles = result.get(storeFiles.getFirst());\n          if (familyFiles == null) {\n            familyFiles = new ArrayList<StoreFile>();\n            result.put(storeFiles.getFirst(), familyFiles);\n          }\n          familyFiles.addAll(storeFiles.getSecond());\n        }\n      } catch (InterruptedException e) {\n        throw (InterruptedIOException)new InterruptedIOException().initCause(e);\n      } catch (ExecutionException e) {\n        throw new IOException(e.getCause());\n      } finally {\n        storeCloserThreadPool.shutdownNow();\n      }\n    }\n\n    status.setStatus(\"Writing region close event to WAL\");\n    if (!abort && wal != null && getRegionServerServices() != null && !writestate.readOnly) {\n      writeRegionCloseMarker(wal);\n    }\n\n    this.closed.set(true);\n    if (!canFlush) {\n      addAndGetGlobalMemstoreSize(-memstoreSize.get());\n    } else if (memstoreSize.get() != 0) {\n      LOG.error(\"Memstore size is \" + memstoreSize.get());\n    }\n    if (coprocessorHost != null) {\n      status.setStatus(\"Running coprocessor post-close hooks\");\n      this.coprocessorHost.postClose(abort);\n    }\n    if (this.metricsRegion != null) {\n      this.metricsRegion.close();\n    }\n    if (this.metricsRegionWrapper != null) {\n      Closeables.closeQuietly(this.metricsRegionWrapper);\n    }\n    // stop the Compacted hfile discharger\n    if (this.compactedFileDischarger != null) this.compactedFileDischarger.cancel(true);\n\n    status.markComplete(\"Closed\");\n    LOG.info(\"Closed \" + this);\n    return result;\n  } finally {\n    lock.writeLock().unlock();\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3029,
        "instruction": "@Override\npublic void handle(RoutingContext context) {\n  AccessLogParam<RoutingContext> accessLogParam = new AccessLogParam<>();\n  accessLogParam.setStartMillisecond(System.currentTimeMillis()).setContextData(context);\n\n\n  context.next();\n}",
        "accept_response": "@Override\npublic void handle(RoutingContext context) {\n  AccessLogParam<RoutingContext> accessLogParam = new AccessLogParam<>();\n  accessLogParam.setStartMillisecond(System.currentTimeMillis()).setContextData(context);\n\n  context.response().endHandler(event -> LOGGER.info(accessLogGenerator.generateLog(accessLogParam)));\n\n  context.next();\n}",
        "reject_response": "@Override\npublic void handle(RoutingContext context) {\n  AccessLogParam<RoutingContext> accessLogParam = new AccessLogParam<>();\n  accessLogParam.setStartMillisecond(System.currentTimeMillis()).setContextData(context);\n\n  context.addBodyEndHandler(v -> LOGGER.info(accessLogGenerator.generateLog(accessLogParam)));\n\n  context.next();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2506,
        "instruction": "private void changeDropletKernel(Exchange exchange) throws Exception {\n    if (ObjectHelper.isEmpty(exchange.getIn().getHeader(DigitalOceanHeaders.KERNEL_ID))) {\n        throw new IllegalArgumentException(DigitalOceanHeaders.KERNEL_ID + \" must be specified\");\n    }\n\n    Action action = getEndpoint().getDigitalOceanClient().changeDropletKernel(dropletId, exchange.getIn().getHeader(DigitalOceanHeaders.KERNEL_ID, Integer.class));\n    exchange.getOut().setBody(action);\n}",
        "accept_response": "private void changeDropletKernel(Exchange exchange) throws Exception {\n    if (ObjectHelper.isEmpty(exchange.getIn().getHeader(DigitalOceanHeaders.KERNEL_ID))) {\n        throw new IllegalArgumentException(DigitalOceanHeaders.KERNEL_ID + \" must be specified\");\n    }\n\n    Action action = getEndpoint().getDigitalOceanClient().changeDropletKernel(dropletId, exchange.getIn().getHeader(DigitalOceanHeaders.KERNEL_ID, Integer.class));\n    LOG.trace(\"Change Droplet {} : [{}] \", dropletId, action);\n    exchange.getOut().setBody(action);\n}",
        "reject_response": "private void changeDropletKernel(Exchange exchange) throws Exception {\n    if (ObjectHelper.isEmpty(exchange.getIn().getHeader(DigitalOceanHeaders.KERNEL_ID))) {\n        throw new IllegalArgumentException(DigitalOceanHeaders.KERNEL_ID + \" must be specified\");\n    }\n\n    Action action = getEndpoint().getDigitalOceanClient().changeDropletKernel(dropletId, exchange.getIn().getHeader(DigitalOceanHeaders.KERNEL_ID, Integer.class));\n    LOG.trace(\"Rename Droplet {} : [{}] \", dropletId, action);\n    exchange.getOut().setBody(action);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3035,
        "instruction": "public void importPackage(PackageMessage pkgMsg, long offset, long createdTime) throws DistributionException {\n    String firstPath = getFirstPath(pkgMsg);\n    addPackageMDC(pkgMsg);\n    try (Timer.Context context = distributionMetricsService.getImportedPackageDuration().time();\n            ResourceResolver importerResolver = getServiceResolver(SUBSERVICE_IMPORTER)) {\n        packageHandler.apply(importerResolver, pkgMsg);\n        if (config.isEditable()) {\n            storeStatus(importerResolver, new PackageStatus(PackageStatusMessage.Status.IMPORTED, offset, pkgMsg.getPubAgentName()));\n        }\n        storeOffset(importerResolver, offset);\n        importerResolver.commit();\n        distributionMetricsService.getImportedPackageSize().update(pkgMsg.getPkgLength());\n        distributionMetricsService.getPackageDistributedDuration().update((currentTimeMillis() - createdTime), TimeUnit.MILLISECONDS);\n        packageRetries.clear(pkgMsg.getPubAgentName());\n        Event event = new ImportedEvent(pkgMsg, config.getSubAgentName()).toEvent();\n        eventAdmin.postEvent(event);\n    } catch (DistributionException | LoginException | IOException | RuntimeException e) {\n        failure(pkgMsg, offset, e);\n    } finally {\n        MDC.clear();\n    }\n}",
        "accept_response": "public void importPackage(PackageMessage pkgMsg, long offset, long createdTime) throws DistributionException {\n    String firstPath = getFirstPath(pkgMsg);\n    log.info(\"Importing distribution package {} of type {} at offset {} first path {} url {}\",\n            pkgMsg.getPkgId(), pkgMsg.getReqType(), offset, firstPath, pkgMsg.getPkgBinaryRef());\n    addPackageMDC(pkgMsg);\n    try (Timer.Context context = distributionMetricsService.getImportedPackageDuration().time();\n            ResourceResolver importerResolver = getServiceResolver(SUBSERVICE_IMPORTER)) {\n        packageHandler.apply(importerResolver, pkgMsg);\n        if (config.isEditable()) {\n            storeStatus(importerResolver, new PackageStatus(PackageStatusMessage.Status.IMPORTED, offset, pkgMsg.getPubAgentName()));\n        }\n        storeOffset(importerResolver, offset);\n        importerResolver.commit();\n        distributionMetricsService.getImportedPackageSize().update(pkgMsg.getPkgLength());\n        distributionMetricsService.getPackageDistributedDuration().update((currentTimeMillis() - createdTime), TimeUnit.MILLISECONDS);\n        packageRetries.clear(pkgMsg.getPubAgentName());\n        Event event = new ImportedEvent(pkgMsg, config.getSubAgentName()).toEvent();\n        eventAdmin.postEvent(event);\n    } catch (DistributionException | LoginException | IOException | RuntimeException e) {\n        failure(pkgMsg, offset, e);\n    } finally {\n        MDC.clear();\n    }\n}",
        "reject_response": "public void importPackage(PackageMessage pkgMsg, long offset, long createdTime) throws DistributionException {\n    String firstPath = getFirstPath(pkgMsg);\n    log.info(\"Importing distribution package {} of type {} at offset {}\", \n            pkgMsg.getPkgId(), pkgMsg.getReqType(), offset);\n    addPackageMDC(pkgMsg);\n    try (Timer.Context context = distributionMetricsService.getImportedPackageDuration().time();\n            ResourceResolver importerResolver = getServiceResolver(SUBSERVICE_IMPORTER)) {\n        packageHandler.apply(importerResolver, pkgMsg);\n        if (config.isEditable()) {\n            storeStatus(importerResolver, new PackageStatus(PackageStatusMessage.Status.IMPORTED, offset, pkgMsg.getPubAgentName()));\n        }\n        storeOffset(importerResolver, offset);\n        importerResolver.commit();\n        distributionMetricsService.getImportedPackageSize().update(pkgMsg.getPkgLength());\n        distributionMetricsService.getPackageDistributedDuration().update((currentTimeMillis() - createdTime), TimeUnit.MILLISECONDS);\n        packageRetries.clear(pkgMsg.getPubAgentName());\n        Event event = new ImportedEvent(pkgMsg, config.getSubAgentName()).toEvent();\n        eventAdmin.postEvent(event);\n    } catch (DistributionException | LoginException | IOException | RuntimeException e) {\n        failure(pkgMsg, offset, e);\n    } finally {\n        MDC.clear();\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2619,
        "instruction": "@Override\npublic void contextInitialized(ServletContextEvent event) {\n\n  messagesToBeLogged = messagesToBeLogged\n      .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_CONTEXT_INITIALIZED\")));\n\n  // Load Pulse version details\n  loadPulseVersionDetails();\n\n  // Load Pulse Properties\n  pulseProperties = loadProperties(PulseConstants.PULSE_PROPERTIES_FILE);\n\n  if (pulseProperties.isEmpty()) {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_NOT_FOUND\")));\n  } else {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_FOUND\")));\n\n    // set Pulse product support into the Pulse controller for access from\n    // client side\n    // to display the appropriate ui depending on which product is supported\n    // in present deployment\n    String pulseProduct =\n        pulseProperties.getProperty(PulseConstants.APPLICATION_PROPERTY_PULSE_PRODUCTSUPPORT);\n  }\n\n  pulseSecurityProperties = loadProperties(PulseConstants.PULSE_SECURITY_PROPERTIES_FILE);\n\n  // Initialize logger\n  initializeLogger();\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n  logger.info(resourceBundle.getString(\"LOG_MSG_CHECK_APP_RUNNING_MODE\"));\n\n  boolean sysIsEmbedded = Boolean.getBoolean(PulseConstants.SYSTEM_PROPERTY_PULSE_EMBEDDED);\n\n  if (sysIsEmbedded) {\n    // Application Pulse is running in Embedded Mode\n    repository.setIsEmbeddedMode(true);\n\n    sysPulseUseLocator = Boolean.FALSE;\n    try {\n      // Get host name of machine running pulse in embedded mode\n      sysPulseHost = InetAddress.getLocalHost().getCanonicalHostName();\n    } catch (Exception e) {\n      logger.debug(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_UNKNOWN_HOST\"), e);\n      // Set default host name\n      sysPulseHost = PulseConstants.GEMFIRE_DEFAULT_HOST;\n    }\n    sysPulsePort = PulseConstants.GEMFIRE_DEFAULT_PORT;\n\n  } else {\n    // Application Pulse is running in Non-Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_NONEMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(false);\n\n    // Load JMX User Details\n    loadJMXUserDetails();\n    // Load locator and/or manager details\n    loadLocatorManagerDetails();\n\n    useGemFireCredentials = areWeUsingGemFireSecurityProfile(event);\n  }\n\n  // Set user details in repository\n  repository.setJmxUserName(jmxUserName);\n  repository.setJmxUserPassword(jmxUserPassword);\n\n  // Set locator/Manager details in repository\n  repository.setJmxUseLocator(sysPulseUseLocator);\n  repository.setJmxHost(sysPulseHost);\n  repository.setJmxPort(sysPulsePort);\n\n  // set SSL info\n  initializeSSL();\n  repository.setUseSSLLocator(sysPulseUseSSLLocator);\n  repository.setUseSSLManager(sysPulseUseSSLManager);\n\n  repository.setUseGemFireCredentials(useGemFireCredentials);\n\n}",
        "accept_response": "@Override\npublic void contextInitialized(ServletContextEvent event) {\n\n  messagesToBeLogged = messagesToBeLogged\n      .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_CONTEXT_INITIALIZED\")));\n\n  // Load Pulse version details\n  loadPulseVersionDetails();\n\n  // Load Pulse Properties\n  pulseProperties = loadProperties(PulseConstants.PULSE_PROPERTIES_FILE);\n\n  if (pulseProperties.isEmpty()) {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_NOT_FOUND\")));\n  } else {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_FOUND\")));\n\n    // set Pulse product support into the Pulse controller for access from\n    // client side\n    // to display the appropriate ui depending on which product is supported\n    // in present deployment\n    String pulseProduct =\n        pulseProperties.getProperty(PulseConstants.APPLICATION_PROPERTY_PULSE_PRODUCTSUPPORT);\n  }\n\n  pulseSecurityProperties = loadProperties(PulseConstants.PULSE_SECURITY_PROPERTIES_FILE);\n\n  // Initialize logger\n  initializeLogger();\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n  logger.info(resourceBundle.getString(\"LOG_MSG_CHECK_APP_RUNNING_MODE\"));\n\n  boolean sysIsEmbedded = Boolean.getBoolean(PulseConstants.SYSTEM_PROPERTY_PULSE_EMBEDDED);\n\n  if (sysIsEmbedded) {\n    // Application Pulse is running in Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_EMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(true);\n\n    sysPulseUseLocator = Boolean.FALSE;\n    try {\n      // Get host name of machine running pulse in embedded mode\n      sysPulseHost = InetAddress.getLocalHost().getCanonicalHostName();\n    } catch (Exception e) {\n      logger.debug(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_UNKNOWN_HOST\"), e);\n      // Set default host name\n      sysPulseHost = PulseConstants.GEMFIRE_DEFAULT_HOST;\n    }\n    sysPulsePort = PulseConstants.GEMFIRE_DEFAULT_PORT;\n\n  } else {\n    // Application Pulse is running in Non-Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_NONEMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(false);\n\n    // Load JMX User Details\n    loadJMXUserDetails();\n    // Load locator and/or manager details\n    loadLocatorManagerDetails();\n\n    useGemFireCredentials = areWeUsingGemFireSecurityProfile(event);\n  }\n\n  // Set user details in repository\n  repository.setJmxUserName(jmxUserName);\n  repository.setJmxUserPassword(jmxUserPassword);\n\n  // Set locator/Manager details in repository\n  repository.setJmxUseLocator(sysPulseUseLocator);\n  repository.setJmxHost(sysPulseHost);\n  repository.setJmxPort(sysPulsePort);\n\n  // set SSL info\n  initializeSSL();\n  repository.setUseSSLLocator(sysPulseUseSSLLocator);\n  repository.setUseSSLManager(sysPulseUseSSLManager);\n\n  repository.setUseGemFireCredentials(useGemFireCredentials);\n\n}",
        "reject_response": "@Override\npublic void contextInitialized(ServletContextEvent event) {\n\n  messagesToBeLogged = messagesToBeLogged\n      .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_CONTEXT_INITIALIZED\")));\n\n  // Load Pulse version details\n  loadPulseVersionDetails();\n\n  // Load Pulse Properties\n  pulseProperties = loadProperties(PulseConstants.PULSE_PROPERTIES_FILE);\n\n  if (pulseProperties.isEmpty()) {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_NOT_FOUND\")));\n  } else {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_FOUND\")));\n\n    // set Pulse product support into the Pulse controller for access from\n    // client side\n    // to display the appropriate ui depending on which product is supported\n    // in present deployment\n    String pulseProduct =\n        pulseProperties.getProperty(PulseConstants.APPLICATION_PROPERTY_PULSE_PRODUCTSUPPORT);\n  }\n\n  pulseSecurityProperties = loadProperties(PulseConstants.PULSE_SECURITY_PROPERTIES_FILE);\n\n  // Initialize logger\n  initializeLogger();\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n  logger.info(resourceBundle.getString(\"LOG_MSG_CHECK_APP_RUNNING_MODE\"));\n\n  boolean sysIsEmbedded = Boolean.getBoolean(PulseConstants.SYSTEM_PROPERTY_PULSE_EMBEDDED);\n\n  if (sysIsEmbedded) {\n    // Application Pulse is running in Embedded Mode\n    if (LOGGER.infoEnabled()) {\n      LOGGER.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_EMBEDDED_MODE\"));\n    }\n    repository.setIsEmbeddedMode(true);\n\n    sysPulseUseLocator = Boolean.FALSE;\n    try {\n      // Get host name of machine running pulse in embedded mode\n      sysPulseHost = InetAddress.getLocalHost().getCanonicalHostName();\n    } catch (Exception e) {\n      logger.debug(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_UNKNOWN_HOST\"), e);\n      // Set default host name\n      sysPulseHost = PulseConstants.GEMFIRE_DEFAULT_HOST;\n    }\n    sysPulsePort = PulseConstants.GEMFIRE_DEFAULT_PORT;\n\n  } else {\n    // Application Pulse is running in Non-Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_NONEMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(false);\n\n    // Load JMX User Details\n    loadJMXUserDetails();\n    // Load locator and/or manager details\n    loadLocatorManagerDetails();\n\n    useGemFireCredentials = areWeUsingGemFireSecurityProfile(event);\n  }\n\n  // Set user details in repository\n  repository.setJmxUserName(jmxUserName);\n  repository.setJmxUserPassword(jmxUserPassword);\n\n  // Set locator/Manager details in repository\n  repository.setJmxUseLocator(sysPulseUseLocator);\n  repository.setJmxHost(sysPulseHost);\n  repository.setJmxPort(sysPulsePort);\n\n  // set SSL info\n  initializeSSL();\n  repository.setUseSSLLocator(sysPulseUseSSLLocator);\n  repository.setUseSSLManager(sysPulseUseSSLManager);\n\n  repository.setUseGemFireCredentials(useGemFireCredentials);\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3165,
        "instruction": "protected IRequestHandler mapExpectedExceptions(Exception e, final Application application)\n{\n\tif (e instanceof StalePageException)\n\t{\n\t\t// If the page was stale, just re-render it\n\t\t// (the url should always be updated by an redirect in that case)\n\t\treturn new RenderPageRequestHandler(new PageProvider(((StalePageException)e).getPage()));\n\t}\n\telse if (e instanceof PageExpiredException)\n\t{\n\t\treturn createPageRequestHandler(new PageProvider(Application.get()\n\t\t\t.getApplicationSettings()\n\t\t\t.getPageExpiredErrorPage()));\n\t}\n\telse if (e instanceof AuthorizationException ||\n\t\te instanceof ListenerInvocationNotAllowedException)\n\t{\n\t\treturn createPageRequestHandler(new PageProvider(Application.get()\n\t\t\t.getApplicationSettings()\n\t\t\t.getAccessDeniedPage()));\n\t}\n\telse if (e instanceof ResponseIOException)\n\t{\n\t\treturn new EmptyRequestHandler();\n\t}\n\telse if (e instanceof PackageResource.PackageResourceBlockedException && application.usesDeploymentConfig())\n\t{\n\t\tlogger.debug(e.getMessage(), e);\n\t\treturn new ErrorCodeRequestHandler(404);\n\t}\n\n\treturn null;\n}",
        "accept_response": "protected IRequestHandler mapExpectedExceptions(Exception e, final Application application)\n{\n\tif (e instanceof StalePageException)\n\t{\n\t\t// If the page was stale, just re-render it\n\t\t// (the url should always be updated by an redirect in that case)\n\t\treturn new RenderPageRequestHandler(new PageProvider(((StalePageException)e).getPage()));\n\t}\n\telse if (e instanceof PageExpiredException)\n\t{\n\t\treturn createPageRequestHandler(new PageProvider(Application.get()\n\t\t\t.getApplicationSettings()\n\t\t\t.getPageExpiredErrorPage()));\n\t}\n\telse if (e instanceof AuthorizationException ||\n\t\te instanceof ListenerInvocationNotAllowedException)\n\t{\n\t\treturn createPageRequestHandler(new PageProvider(Application.get()\n\t\t\t.getApplicationSettings()\n\t\t\t.getAccessDeniedPage()));\n\t}\n\telse if (e instanceof ResponseIOException)\n\t{\n\t\tlogger.debug(\"Connection lost, give up responding.\", e);\n\t\treturn new EmptyRequestHandler();\n\t}\n\telse if (e instanceof PackageResource.PackageResourceBlockedException && application.usesDeploymentConfig())\n\t{\n\t\tlogger.debug(e.getMessage(), e);\n\t\treturn new ErrorCodeRequestHandler(404);\n\t}\n\n\treturn null;\n}",
        "reject_response": "protected IRequestHandler mapExpectedExceptions(Exception e, final Application application)\n{\n\tif (e instanceof StalePageException)\n\t{\n\t\t// If the page was stale, just re-render it\n\t\t// (the url should always be updated by an redirect in that case)\n\t\treturn new RenderPageRequestHandler(new PageProvider(((StalePageException)e).getPage()));\n\t}\n\telse if (e instanceof PageExpiredException)\n\t{\n\t\treturn createPageRequestHandler(new PageProvider(Application.get()\n\t\t\t.getApplicationSettings()\n\t\t\t.getPageExpiredErrorPage()));\n\t}\n\telse if (e instanceof AuthorizationException ||\n\t\te instanceof ListenerInvocationNotAllowedException)\n\t{\n\t\treturn createPageRequestHandler(new PageProvider(Application.get()\n\t\t\t.getApplicationSettings()\n\t\t\t.getAccessDeniedPage()));\n\t}\n\telse if (e instanceof ResponseIOException)\n\t{\n\t\tlogger.error(\"Connection lost, give up responding.\", e);\n\t\treturn new EmptyRequestHandler();\n\t}\n\telse if (e instanceof PackageResource.PackageResourceBlockedException && application.usesDeploymentConfig())\n\t{\n\t\tlogger.debug(e.getMessage(), e);\n\t\treturn new ErrorCodeRequestHandler(404);\n\t}\n\n\treturn null;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2453,
        "instruction": "@Override\npublic synchronized void flushConfirmations() {\n   if (resendCache != null && receivedBytes != 0) {\n      receivedBytes = 0;\n\n      final Packet confirmed = new PacketsConfirmedMessage(lastConfirmedCommandID.get());\n\n      confirmed.setChannelID(id);\n\n      doWrite(confirmed);\n   }\n}",
        "accept_response": "@Override\npublic synchronized void flushConfirmations() {\n   if (resendCache != null && receivedBytes != 0) {\n      receivedBytes = 0;\n\n      final Packet confirmed = new PacketsConfirmedMessage(lastConfirmedCommandID.get());\n\n      confirmed.setChannelID(id);\n\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" ChannelImpl::flushConfirmation flushing confirmation \" + confirmed);\n      }\n\n      doWrite(confirmed);\n   }\n}",
        "reject_response": "@Override\npublic synchronized void flushConfirmations() {\n   if (resendCache != null && receivedBytes != 0) {\n      receivedBytes = 0;\n\n      final Packet confirmed = new PacketsConfirmedMessage(lastConfirmedCommandID.get());\n\n      confirmed.setChannelID(id);\n\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"ChannelImpl::flushConfirmation flushing confirmation \" + confirmed);\n      }\n\n      doWrite(confirmed);\n   }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2760,
        "instruction": "private InputSplit[] getCombineSplits(JobConf job, int numSplits,\n    Map<Path, PartitionDesc> pathToPartitionInfo)\n    throws IOException {\n  init(job);\n  Map<Path, ArrayList<String>> pathToAliases = mrwork.getPathToAliases();\n  Map<String, Operator<? extends OperatorDesc>> aliasToWork =\n    mrwork.getAliasToWork();\n  CombineFileInputFormatShim combine = ShimLoader.getHadoopShims()\n      .getCombineFileInputFormat();\n\n  InputSplit[] splits = null;\n  if (combine == null) {\n    splits = super.getSplits(job, numSplits);\n    return splits;\n  }\n\n  if (combine.getInputPathsShim(job).length == 0) {\n    throw new IOException(\"No input paths specified in job\");\n  }\n  ArrayList<InputSplit> result = new ArrayList<InputSplit>();\n\n  // combine splits only from same tables and same partitions. Do not combine splits from multiple\n  // tables or multiple partitions.\n  Path[] paths = StringInternUtils.internUriStringsInPathArray(combine.getInputPathsShim(job));\n\n  List<Path> inpDirs = new ArrayList<Path>();\n  List<Path> inpFiles = new ArrayList<Path>();\n  Map<CombinePathInputFormat, CombineFilter> poolMap =\n    new HashMap<CombinePathInputFormat, CombineFilter>();\n  Set<Path> poolSet = new HashSet<Path>();\n  LockedDriverState lDrvStat = LockedDriverState.getLockedDriverState();\n\n  for (Path path : paths) {\n    if (lDrvStat != null && lDrvStat.isAborted()) {\n      throw new IOException(\"Operation is Canceled. \");\n    }\n\n    PartitionDesc part = HiveFileFormatUtils.getFromPathRecursively(\n        pathToPartitionInfo, path, IOPrepareCache.get().allocatePartitionDescMap());\n    TableDesc tableDesc = part.getTableDesc();\n    if ((tableDesc != null) && tableDesc.isNonNative()) {\n      return super.getSplits(job, numSplits);\n    }\n\n    // Use HiveInputFormat if any of the paths is not splittable\n    Class inputFormatClass = part.getInputFileFormatClass();\n    String inputFormatClassName = inputFormatClass.getName();\n    InputFormat inputFormat = getInputFormatFromCache(inputFormatClass, job);\n    String deserializerClassName = null;\n    try {\n      deserializerClassName = part.getDeserializer(job).getClass().getName();\n    } catch (Exception e) {\n      // ignore\n    }\n    FileSystem inpFs = path.getFileSystem(job);\n\n    //don't combine if inputformat is a SymlinkTextInputFormat\n    if (inputFormat instanceof SymlinkTextInputFormat) {\n      splits = super.getSplits(job, numSplits);\n      return splits;\n    }\n\n    Path filterPath = path;\n\n    // Does a pool exist for this path already\n    CombineFilter f = null;\n    List<Operator<? extends OperatorDesc>> opList = null;\n\n    if (!mrwork.isMapperCannotSpanPartns()) {\n      //if mapper can span partitions, make sure a splits does not contain multiple\n      // opList + inputFormatClassName + deserializerClassName combination\n      // This is done using the Map of CombinePathInputFormat to PathFilter\n\n      opList = HiveFileFormatUtils.doGetWorksFromPath(\n                 pathToAliases, aliasToWork, filterPath);\n      CombinePathInputFormat combinePathInputFormat =\n          new CombinePathInputFormat(opList, inputFormatClassName, deserializerClassName);\n      f = poolMap.get(combinePathInputFormat);\n      if (f == null) {\n        f = new CombineFilter(filterPath);\n        LOG.info(\"CombineHiveInputSplit creating pool for \" + path +\n                 \"; using filter path \" + filterPath);\n        combine.createPool(job, f);\n        poolMap.put(combinePathInputFormat, f);\n      } else {\n        f.addPath(filterPath);\n      }\n    } else {\n      // In the case of tablesample, the input paths are pointing to files rather than directories.\n      // We need to get the parent directory as the filtering path so that all files in the same\n      // parent directory will be grouped into one pool but not files from different parent\n      // directories. This guarantees that a split will combine all files in the same partition\n      // but won't cross multiple partitions if the user has asked so.\n      if (!path.getFileSystem(job).getFileStatus(path).isDir()) { // path is not directory\n        filterPath = path.getParent();\n        inpFiles.add(path);\n        poolSet.add(filterPath);\n      } else {\n        inpDirs.add(path);\n      }\n    }\n  }\n\n  // Processing directories\n  List<CombineFileSplit> iss = new ArrayList<CombineFileSplit>();\n  if (!mrwork.isMapperCannotSpanPartns()) {\n    //mapper can span partitions\n    //combine into as few as one split, subject to the PathFilters set\n    // using combine.createPool.\n    iss = Arrays.asList(combine.getSplits(job, 1));\n  } else {\n    for (Path path : inpDirs) {\n      processPaths(job, combine, iss, path);\n    }\n\n    if (inpFiles.size() > 0) {\n      // Processing files\n      for (Path filterPath : poolSet) {\n        combine.createPool(job, new CombineFilter(filterPath));\n      }\n      processPaths(job, combine, iss, inpFiles.toArray(new Path[0]));\n    }\n  }\n\n  if (mrwork.getNameToSplitSample() != null && !mrwork.getNameToSplitSample().isEmpty()) {\n    iss = sampleSplits(iss);\n  }\n\n  for (CombineFileSplit is : iss) {\n    CombineHiveInputSplit csplit = new CombineHiveInputSplit(job, is, pathToPartitionInfo);\n    result.add(csplit);\n  }\n  LOG.info(\"number of splits \" + result.size());\n  return result.toArray(new InputSplit[result.size()]);\n}",
        "accept_response": "private InputSplit[] getCombineSplits(JobConf job, int numSplits,\n    Map<Path, PartitionDesc> pathToPartitionInfo)\n    throws IOException {\n  init(job);\n  Map<Path, ArrayList<String>> pathToAliases = mrwork.getPathToAliases();\n  Map<String, Operator<? extends OperatorDesc>> aliasToWork =\n    mrwork.getAliasToWork();\n  CombineFileInputFormatShim combine = ShimLoader.getHadoopShims()\n      .getCombineFileInputFormat();\n\n  InputSplit[] splits = null;\n  if (combine == null) {\n    splits = super.getSplits(job, numSplits);\n    return splits;\n  }\n\n  if (combine.getInputPathsShim(job).length == 0) {\n    throw new IOException(\"No input paths specified in job\");\n  }\n  ArrayList<InputSplit> result = new ArrayList<InputSplit>();\n\n  // combine splits only from same tables and same partitions. Do not combine splits from multiple\n  // tables or multiple partitions.\n  Path[] paths = StringInternUtils.internUriStringsInPathArray(combine.getInputPathsShim(job));\n\n  List<Path> inpDirs = new ArrayList<Path>();\n  List<Path> inpFiles = new ArrayList<Path>();\n  Map<CombinePathInputFormat, CombineFilter> poolMap =\n    new HashMap<CombinePathInputFormat, CombineFilter>();\n  Set<Path> poolSet = new HashSet<Path>();\n  LockedDriverState lDrvStat = LockedDriverState.getLockedDriverState();\n\n  for (Path path : paths) {\n    if (lDrvStat != null && lDrvStat.isAborted()) {\n      throw new IOException(\"Operation is Canceled. \");\n    }\n\n    PartitionDesc part = HiveFileFormatUtils.getFromPathRecursively(\n        pathToPartitionInfo, path, IOPrepareCache.get().allocatePartitionDescMap());\n    TableDesc tableDesc = part.getTableDesc();\n    if ((tableDesc != null) && tableDesc.isNonNative()) {\n      return super.getSplits(job, numSplits);\n    }\n\n    // Use HiveInputFormat if any of the paths is not splittable\n    Class inputFormatClass = part.getInputFileFormatClass();\n    String inputFormatClassName = inputFormatClass.getName();\n    InputFormat inputFormat = getInputFormatFromCache(inputFormatClass, job);\n    String deserializerClassName = null;\n    try {\n      deserializerClassName = part.getDeserializer(job).getClass().getName();\n    } catch (Exception e) {\n      // ignore\n    }\n    FileSystem inpFs = path.getFileSystem(job);\n\n    //don't combine if inputformat is a SymlinkTextInputFormat\n    if (inputFormat instanceof SymlinkTextInputFormat) {\n      splits = super.getSplits(job, numSplits);\n      return splits;\n    }\n\n    Path filterPath = path;\n\n    // Does a pool exist for this path already\n    CombineFilter f = null;\n    List<Operator<? extends OperatorDesc>> opList = null;\n\n    if (!mrwork.isMapperCannotSpanPartns()) {\n      //if mapper can span partitions, make sure a splits does not contain multiple\n      // opList + inputFormatClassName + deserializerClassName combination\n      // This is done using the Map of CombinePathInputFormat to PathFilter\n\n      opList = HiveFileFormatUtils.doGetWorksFromPath(\n                 pathToAliases, aliasToWork, filterPath);\n      CombinePathInputFormat combinePathInputFormat =\n          new CombinePathInputFormat(opList, inputFormatClassName, deserializerClassName);\n      f = poolMap.get(combinePathInputFormat);\n      if (f == null) {\n        f = new CombineFilter(filterPath);\n        LOG.info(\"CombineHiveInputSplit creating pool for \" + path +\n                 \"; using filter path \" + filterPath);\n        combine.createPool(job, f);\n        poolMap.put(combinePathInputFormat, f);\n      } else {\n        LOG.debug(\"CombineHiveInputSplit: pool is already created for \" + path +\n                 \"; using filter path \" + filterPath);\n        f.addPath(filterPath);\n      }\n    } else {\n      // In the case of tablesample, the input paths are pointing to files rather than directories.\n      // We need to get the parent directory as the filtering path so that all files in the same\n      // parent directory will be grouped into one pool but not files from different parent\n      // directories. This guarantees that a split will combine all files in the same partition\n      // but won't cross multiple partitions if the user has asked so.\n      if (!path.getFileSystem(job).getFileStatus(path).isDir()) { // path is not directory\n        filterPath = path.getParent();\n        inpFiles.add(path);\n        poolSet.add(filterPath);\n      } else {\n        inpDirs.add(path);\n      }\n    }\n  }\n\n  // Processing directories\n  List<CombineFileSplit> iss = new ArrayList<CombineFileSplit>();\n  if (!mrwork.isMapperCannotSpanPartns()) {\n    //mapper can span partitions\n    //combine into as few as one split, subject to the PathFilters set\n    // using combine.createPool.\n    iss = Arrays.asList(combine.getSplits(job, 1));\n  } else {\n    for (Path path : inpDirs) {\n      processPaths(job, combine, iss, path);\n    }\n\n    if (inpFiles.size() > 0) {\n      // Processing files\n      for (Path filterPath : poolSet) {\n        combine.createPool(job, new CombineFilter(filterPath));\n      }\n      processPaths(job, combine, iss, inpFiles.toArray(new Path[0]));\n    }\n  }\n\n  if (mrwork.getNameToSplitSample() != null && !mrwork.getNameToSplitSample().isEmpty()) {\n    iss = sampleSplits(iss);\n  }\n\n  for (CombineFileSplit is : iss) {\n    CombineHiveInputSplit csplit = new CombineHiveInputSplit(job, is, pathToPartitionInfo);\n    result.add(csplit);\n  }\n  LOG.info(\"number of splits \" + result.size());\n  return result.toArray(new InputSplit[result.size()]);\n}",
        "reject_response": "private InputSplit[] getCombineSplits(JobConf job, int numSplits,\n    Map<Path, PartitionDesc> pathToPartitionInfo)\n    throws IOException {\n  init(job);\n  Map<Path, ArrayList<String>> pathToAliases = mrwork.getPathToAliases();\n  Map<String, Operator<? extends OperatorDesc>> aliasToWork =\n    mrwork.getAliasToWork();\n  CombineFileInputFormatShim combine = ShimLoader.getHadoopShims()\n      .getCombineFileInputFormat();\n\n  InputSplit[] splits = null;\n  if (combine == null) {\n    splits = super.getSplits(job, numSplits);\n    return splits;\n  }\n\n  if (combine.getInputPathsShim(job).length == 0) {\n    throw new IOException(\"No input paths specified in job\");\n  }\n  ArrayList<InputSplit> result = new ArrayList<InputSplit>();\n\n  // combine splits only from same tables and same partitions. Do not combine splits from multiple\n  // tables or multiple partitions.\n  Path[] paths = StringInternUtils.internUriStringsInPathArray(combine.getInputPathsShim(job));\n\n  List<Path> inpDirs = new ArrayList<Path>();\n  List<Path> inpFiles = new ArrayList<Path>();\n  Map<CombinePathInputFormat, CombineFilter> poolMap =\n    new HashMap<CombinePathInputFormat, CombineFilter>();\n  Set<Path> poolSet = new HashSet<Path>();\n  LockedDriverState lDrvStat = LockedDriverState.getLockedDriverState();\n\n  for (Path path : paths) {\n    if (lDrvStat != null && lDrvStat.isAborted()) {\n      throw new IOException(\"Operation is Canceled. \");\n    }\n\n    PartitionDesc part = HiveFileFormatUtils.getFromPathRecursively(\n        pathToPartitionInfo, path, IOPrepareCache.get().allocatePartitionDescMap());\n    TableDesc tableDesc = part.getTableDesc();\n    if ((tableDesc != null) && tableDesc.isNonNative()) {\n      return super.getSplits(job, numSplits);\n    }\n\n    // Use HiveInputFormat if any of the paths is not splittable\n    Class inputFormatClass = part.getInputFileFormatClass();\n    String inputFormatClassName = inputFormatClass.getName();\n    InputFormat inputFormat = getInputFormatFromCache(inputFormatClass, job);\n    String deserializerClassName = null;\n    try {\n      deserializerClassName = part.getDeserializer(job).getClass().getName();\n    } catch (Exception e) {\n      // ignore\n    }\n    FileSystem inpFs = path.getFileSystem(job);\n\n    //don't combine if inputformat is a SymlinkTextInputFormat\n    if (inputFormat instanceof SymlinkTextInputFormat) {\n      splits = super.getSplits(job, numSplits);\n      return splits;\n    }\n\n    Path filterPath = path;\n\n    // Does a pool exist for this path already\n    CombineFilter f = null;\n    List<Operator<? extends OperatorDesc>> opList = null;\n\n    if (!mrwork.isMapperCannotSpanPartns()) {\n      //if mapper can span partitions, make sure a splits does not contain multiple\n      // opList + inputFormatClassName + deserializerClassName combination\n      // This is done using the Map of CombinePathInputFormat to PathFilter\n\n      opList = HiveFileFormatUtils.doGetWorksFromPath(\n                 pathToAliases, aliasToWork, filterPath);\n      CombinePathInputFormat combinePathInputFormat =\n          new CombinePathInputFormat(opList, inputFormatClassName, deserializerClassName);\n      f = poolMap.get(combinePathInputFormat);\n      if (f == null) {\n        f = new CombineFilter(filterPath);\n        LOG.info(\"CombineHiveInputSplit creating pool for \" + path +\n                 \"; using filter path \" + filterPath);\n        combine.createPool(job, f);\n        poolMap.put(combinePathInputFormat, f);\n      } else {\n        LOG.info(\"CombineHiveInputSplit: pool is already created for \" + path +\n        f.addPath(filterPath);\n      }\n    } else {\n      // In the case of tablesample, the input paths are pointing to files rather than directories.\n      // We need to get the parent directory as the filtering path so that all files in the same\n      // parent directory will be grouped into one pool but not files from different parent\n      // directories. This guarantees that a split will combine all files in the same partition\n      // but won't cross multiple partitions if the user has asked so.\n      if (!path.getFileSystem(job).getFileStatus(path).isDir()) { // path is not directory\n        filterPath = path.getParent();\n        inpFiles.add(path);\n        poolSet.add(filterPath);\n      } else {\n        inpDirs.add(path);\n      }\n    }\n  }\n\n  // Processing directories\n  List<CombineFileSplit> iss = new ArrayList<CombineFileSplit>();\n  if (!mrwork.isMapperCannotSpanPartns()) {\n    //mapper can span partitions\n    //combine into as few as one split, subject to the PathFilters set\n    // using combine.createPool.\n    iss = Arrays.asList(combine.getSplits(job, 1));\n  } else {\n    for (Path path : inpDirs) {\n      processPaths(job, combine, iss, path);\n    }\n\n    if (inpFiles.size() > 0) {\n      // Processing files\n      for (Path filterPath : poolSet) {\n        combine.createPool(job, new CombineFilter(filterPath));\n      }\n      processPaths(job, combine, iss, inpFiles.toArray(new Path[0]));\n    }\n  }\n\n  if (mrwork.getNameToSplitSample() != null && !mrwork.getNameToSplitSample().isEmpty()) {\n    iss = sampleSplits(iss);\n  }\n\n  for (CombineFileSplit is : iss) {\n    CombineHiveInputSplit csplit = new CombineHiveInputSplit(job, is, pathToPartitionInfo);\n    result.add(csplit);\n  }\n  LOG.info(\"number of splits \" + result.size());\n  return result.toArray(new InputSplit[result.size()]);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2806,
        "instruction": "private void loadEntry(KeyCacheObject key,\n    Object val,\n    GridCacheVersion ver,\n    @Nullable IgniteBiPredicate<K, V> p,\n    AffinityTopologyVersion topVer,\n    boolean replicate,\n    @Nullable ExpiryPolicy plc) {\n    if (p != null && !p.apply(key.<K>value(ctx.cacheObjectContext(), false), (V)val))\n        return;\n\n    try {\n        GridDhtLocalPartition part = top.localPartition(ctx.affinity().partition(key),\n            AffinityTopologyVersion.NONE, true);\n\n        // Reserve to make sure that partition does not get unloaded.\n        if (part.reserve()) {\n            GridCacheEntryEx entry = null;\n\n            try {\n                long ttl = CU.ttlForLoad(plc);\n\n                if (ttl == CU.TTL_ZERO)\n                    return;\n\n                CacheObject cacheVal = ctx.toCacheObject(val);\n\n                entry = entryEx(key);\n\n                entry.initialValue(cacheVal,\n                    ver,\n                    ttl,\n                    CU.EXPIRE_TIME_CALCULATE,\n                    false,\n                    topVer,\n                    replicate ? DR_LOAD : DR_NONE,\n                    false);\n            }\n            catch (IgniteCheckedException e) {\n                throw new IgniteException(\"Failed to put cache value: \" + entry, e);\n            }\n            catch (GridCacheEntryRemovedException ignore) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Got removed entry during loadCache (will ignore): \" + entry);\n            }\n            finally {\n                if (entry != null)\n                    entry.context().evicts().touch(entry, topVer);\n\n                part.release();\n            }\n        }\n        else if (log.isDebugEnabled())\n            log.debug(\"Will node load entry into cache (partition is invalid): \" + part);\n    }\n    catch (GridDhtInvalidPartitionException e) {\n        if (log.isDebugEnabled())\n    }\n}",
        "accept_response": "private void loadEntry(KeyCacheObject key,\n    Object val,\n    GridCacheVersion ver,\n    @Nullable IgniteBiPredicate<K, V> p,\n    AffinityTopologyVersion topVer,\n    boolean replicate,\n    @Nullable ExpiryPolicy plc) {\n    if (p != null && !p.apply(key.<K>value(ctx.cacheObjectContext(), false), (V)val))\n        return;\n\n    try {\n        GridDhtLocalPartition part = top.localPartition(ctx.affinity().partition(key),\n            AffinityTopologyVersion.NONE, true);\n\n        // Reserve to make sure that partition does not get unloaded.\n        if (part.reserve()) {\n            GridCacheEntryEx entry = null;\n\n            try {\n                long ttl = CU.ttlForLoad(plc);\n\n                if (ttl == CU.TTL_ZERO)\n                    return;\n\n                CacheObject cacheVal = ctx.toCacheObject(val);\n\n                entry = entryEx(key);\n\n                entry.initialValue(cacheVal,\n                    ver,\n                    ttl,\n                    CU.EXPIRE_TIME_CALCULATE,\n                    false,\n                    topVer,\n                    replicate ? DR_LOAD : DR_NONE,\n                    false);\n            }\n            catch (IgniteCheckedException e) {\n                throw new IgniteException(\"Failed to put cache value: \" + entry, e);\n            }\n            catch (GridCacheEntryRemovedException ignore) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Got removed entry during loadCache (will ignore): \" + entry);\n            }\n            finally {\n                if (entry != null)\n                    entry.context().evicts().touch(entry, topVer);\n\n                part.release();\n            }\n        }\n        else if (log.isDebugEnabled())\n            log.debug(\"Will node load entry into cache (partition is invalid): \" + part);\n    }\n    catch (GridDhtInvalidPartitionException e) {\n        if (log.isDebugEnabled())\n            log.debug(S.toString(\"Ignoring entry for partition that does not belong\",\n                \"key\", key, true,\n                \"val\", val, true,\n                \"err\", e, false));\n    }\n}",
        "reject_response": "private void loadEntry(KeyCacheObject key,\n    Object val,\n    GridCacheVersion ver,\n    @Nullable IgniteBiPredicate<K, V> p,\n    AffinityTopologyVersion topVer,\n    boolean replicate,\n    @Nullable ExpiryPolicy plc) {\n    if (p != null && !p.apply(key.<K>value(ctx.cacheObjectContext(), false), (V)val))\n        return;\n\n    try {\n        GridDhtLocalPartition part = top.localPartition(ctx.affinity().partition(key),\n            AffinityTopologyVersion.NONE, true);\n\n        // Reserve to make sure that partition does not get unloaded.\n        if (part.reserve()) {\n            GridCacheEntryEx entry = null;\n\n            try {\n                long ttl = CU.ttlForLoad(plc);\n\n                if (ttl == CU.TTL_ZERO)\n                    return;\n\n                CacheObject cacheVal = ctx.toCacheObject(val);\n\n                entry = entryEx(key);\n\n                entry.initialValue(cacheVal,\n                    ver,\n                    ttl,\n                    CU.EXPIRE_TIME_CALCULATE,\n                    false,\n                    topVer,\n                    replicate ? DR_LOAD : DR_NONE,\n                    false);\n            }\n            catch (IgniteCheckedException e) {\n                throw new IgniteException(\"Failed to put cache value: \" + entry, e);\n            }\n            catch (GridCacheEntryRemovedException ignore) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Got removed entry during loadCache (will ignore): \" + entry);\n            }\n            finally {\n                if (entry != null)\n                    entry.context().evicts().touch(entry, topVer);\n\n                part.release();\n            }\n        }\n        else if (log.isDebugEnabled())\n            log.debug(\"Will node load entry into cache (partition is invalid): \" + part);\n    }\n    catch (GridDhtInvalidPartitionException e) {\n        if (log.isDebugEnabled())\n            log.debug(\"Ignoring entry for partition that does not belong [key=\" + key + \", val=\" + val +\n                \", err=\" + e + ']');\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2787,
        "instruction": "ClassicHttpResponse cacheAndReturnResponse(\n        final HttpHost target,\n        final HttpRequest request,\n        final ClassicHttpResponse backendResponse,\n        final ExecChain.Scope scope,\n        final Date requestSent,\n        final Date responseReceived) throws IOException {\n    LOG.debug(\"Caching backend response\");\n    final ByteArrayBuffer buf;\n    final HttpEntity entity = backendResponse.getEntity();\n    if (entity != null) {\n        buf = new ByteArrayBuffer(1024);\n        final InputStream inStream = entity.getContent();\n        final byte[] tmp = new byte[2048];\n        long total = 0;\n        int l;\n        while ((l = inStream.read(tmp)) != -1) {\n            buf.append(tmp, 0, l);\n            total += l;\n            if (total > cacheConfig.getMaxObjectSize()) {\n                LOG.debug(\"Backend response content length exceeds maximum\");\n                backendResponse.setEntity(new CombinedEntity(entity, buf));\n                return backendResponse;\n            }\n        }\n    } else {\n        buf = null;\n    }\n    backendResponse.close();\n\n    final HttpCacheEntry cacheEntry;\n    if (cacheConfig.isFreshnessCheckEnabled()) {\n        final HttpCacheEntry existingEntry = responseCache.getCacheEntry(target, request);\n        if (DateUtils.isAfter(existingEntry, backendResponse, HttpHeaders.DATE)) {\n            LOG.debug(\"Backend already contains fresher cache entry\");\n            cacheEntry = existingEntry;\n        } else {\n            cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n            LOG.debug(\"Backend response successfully cached\");\n        }\n    } else {\n        cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n    }\n    return convert(responseGenerator.generateResponse(request, cacheEntry), scope);\n}",
        "accept_response": "ClassicHttpResponse cacheAndReturnResponse(\n        final HttpHost target,\n        final HttpRequest request,\n        final ClassicHttpResponse backendResponse,\n        final ExecChain.Scope scope,\n        final Date requestSent,\n        final Date responseReceived) throws IOException {\n    LOG.debug(\"Caching backend response\");\n    final ByteArrayBuffer buf;\n    final HttpEntity entity = backendResponse.getEntity();\n    if (entity != null) {\n        buf = new ByteArrayBuffer(1024);\n        final InputStream inStream = entity.getContent();\n        final byte[] tmp = new byte[2048];\n        long total = 0;\n        int l;\n        while ((l = inStream.read(tmp)) != -1) {\n            buf.append(tmp, 0, l);\n            total += l;\n            if (total > cacheConfig.getMaxObjectSize()) {\n                LOG.debug(\"Backend response content length exceeds maximum\");\n                backendResponse.setEntity(new CombinedEntity(entity, buf));\n                return backendResponse;\n            }\n        }\n    } else {\n        buf = null;\n    }\n    backendResponse.close();\n\n    final HttpCacheEntry cacheEntry;\n    if (cacheConfig.isFreshnessCheckEnabled()) {\n        final HttpCacheEntry existingEntry = responseCache.getCacheEntry(target, request);\n        if (DateUtils.isAfter(existingEntry, backendResponse, HttpHeaders.DATE)) {\n            LOG.debug(\"Backend already contains fresher cache entry\");\n            cacheEntry = existingEntry;\n        } else {\n            cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n            LOG.debug(\"Backend response successfully cached\");\n        }\n    } else {\n        cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n        LOG.debug(\"Backend response successfully cached (freshness check skipped)\");\n    }\n    return convert(responseGenerator.generateResponse(request, cacheEntry), scope);\n}",
        "reject_response": "ClassicHttpResponse cacheAndReturnResponse(\n        final HttpHost target,\n        final HttpRequest request,\n        final ClassicHttpResponse backendResponse,\n        final ExecChain.Scope scope,\n        final Date requestSent,\n        final Date responseReceived) throws IOException {\n    LOG.debug(\"Caching backend response\");\n    final ByteArrayBuffer buf;\n    final HttpEntity entity = backendResponse.getEntity();\n    if (entity != null) {\n        buf = new ByteArrayBuffer(1024);\n        final InputStream inStream = entity.getContent();\n        final byte[] tmp = new byte[2048];\n        long total = 0;\n        int l;\n        while ((l = inStream.read(tmp)) != -1) {\n            buf.append(tmp, 0, l);\n            total += l;\n            if (total > cacheConfig.getMaxObjectSize()) {\n                LOG.debug(\"Backend response content length exceeds maximum\");\n                backendResponse.setEntity(new CombinedEntity(entity, buf));\n                return backendResponse;\n            }\n        }\n    } else {\n        buf = null;\n    }\n    backendResponse.close();\n\n    final HttpCacheEntry cacheEntry;\n    if (cacheConfig.isFreshnessCheckEnabled()) {\n        final HttpCacheEntry existingEntry = responseCache.getCacheEntry(target, request);\n        if (DateUtils.isAfter(existingEntry, backendResponse, HttpHeaders.DATE)) {\n            LOG.debug(\"Backend already contains fresher cache entry\");\n            cacheEntry = existingEntry;\n        } else {\n            cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n            LOG.debug(\"Backend response successfully cached\");\n        }\n    } else {\n        cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n        log.debug(\"Backend response successfully cached (freshness check skipped)\");\n    }\n    return convert(responseGenerator.generateResponse(request, cacheEntry), scope);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3014,
        "instruction": "void consumeQueue() {\n  try {\n    RemoteEvent<T> event;\n    while ((event = queue.poll(0, TimeUnit.MICROSECONDS)) != null) {\n      linkRef.get().write(encoder.encode(event));\n    }\n  } catch (final InterruptedException ex) {\n    LOG.log(Level.SEVERE, \"Interrupted\", ex);\n    throw new RemoteRuntimeException(ex);\n  }\n}",
        "accept_response": "void consumeQueue() {\n  try {\n    RemoteEvent<T> event;\n    while ((event = queue.poll(0, TimeUnit.MICROSECONDS)) != null) {\n      LOG.log(Level.FINEST, \"Event: {0}\", event);\n      linkRef.get().write(encoder.encode(event));\n    }\n  } catch (final InterruptedException ex) {\n    LOG.log(Level.SEVERE, \"Interrupted\", ex);\n    throw new RemoteRuntimeException(ex);\n  }\n}",
        "reject_response": "void consumeQueue() {\n  try {\n    RemoteEvent<T> event;\n    while ((event = queue.poll(0, TimeUnit.MICROSECONDS)) != null) {\n      LOG.log(Level.FINEST, \"{0}\", event);\n      linkRef.get().write(encoder.encode(event));\n    }\n  } catch (final InterruptedException ex) {\n    LOG.log(Level.SEVERE, \"Interrupted\", ex);\n    throw new RemoteRuntimeException(ex);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3244,
        "instruction": "private void pingRwServer() throws RWServerFoundException {\n    String result = null;\n    InetSocketAddress addr = hostProvider.next(0);\n\n\n    Socket sock = null;\n    BufferedReader br = null;\n    try {\n        sock = new Socket(addr.getHostString(), addr.getPort());\n        sock.setSoLinger(false, -1);\n        sock.setSoTimeout(1000);\n        sock.setTcpNoDelay(true);\n        sock.getOutputStream().write(\"isro\".getBytes());\n        sock.getOutputStream().flush();\n        sock.shutdownOutput();\n        br = new BufferedReader(new InputStreamReader(sock.getInputStream()));\n        result = br.readLine();\n    } catch (ConnectException e) {\n        // ignore, this just means server is not up\n    } catch (IOException e) {\n        // some unexpected error, warn about it\n        LOG.warn(\"Exception while seeking for r/w server.\", e);\n    } finally {\n        if (sock != null) {\n            try {\n                sock.close();\n            } catch (IOException e) {\n                LOG.warn(\"Unexpected exception\", e);\n            }\n        }\n        if (br != null) {\n            try {\n                br.close();\n            } catch (IOException e) {\n                LOG.warn(\"Unexpected exception\", e);\n            }\n        }\n    }\n\n    if (\"rw\".equals(result)) {\n        pingRwTimeout = minPingRwTimeout;\n        // save the found address so that it's used during the next\n        // connection attempt\n        rwServerAddress = addr;\n        throw new RWServerFoundException(\"Majority server found at \"\n                                         + addr.getHostString() + \":\" + addr.getPort());\n    }\n}",
        "accept_response": "private void pingRwServer() throws RWServerFoundException {\n    String result = null;\n    InetSocketAddress addr = hostProvider.next(0);\n\n    LOG.info(\"Checking server {} for being r/w. Timeout {}\", addr, pingRwTimeout);\n\n    Socket sock = null;\n    BufferedReader br = null;\n    try {\n        sock = new Socket(addr.getHostString(), addr.getPort());\n        sock.setSoLinger(false, -1);\n        sock.setSoTimeout(1000);\n        sock.setTcpNoDelay(true);\n        sock.getOutputStream().write(\"isro\".getBytes());\n        sock.getOutputStream().flush();\n        sock.shutdownOutput();\n        br = new BufferedReader(new InputStreamReader(sock.getInputStream()));\n        result = br.readLine();\n    } catch (ConnectException e) {\n        // ignore, this just means server is not up\n    } catch (IOException e) {\n        // some unexpected error, warn about it\n        LOG.warn(\"Exception while seeking for r/w server.\", e);\n    } finally {\n        if (sock != null) {\n            try {\n                sock.close();\n            } catch (IOException e) {\n                LOG.warn(\"Unexpected exception\", e);\n            }\n        }\n        if (br != null) {\n            try {\n                br.close();\n            } catch (IOException e) {\n                LOG.warn(\"Unexpected exception\", e);\n            }\n        }\n    }\n\n    if (\"rw\".equals(result)) {\n        pingRwTimeout = minPingRwTimeout;\n        // save the found address so that it's used during the next\n        // connection attempt\n        rwServerAddress = addr;\n        throw new RWServerFoundException(\"Majority server found at \"\n                                         + addr.getHostString() + \":\" + addr.getPort());\n    }\n}",
        "reject_response": "private void pingRwServer() throws RWServerFoundException {\n    String result = null;\n    InetSocketAddress addr = hostProvider.next(0);\n\n    LOG.info(\"Checking server \" + addr + \" for being r/w.\" + \" Timeout \" + pingRwTimeout);\n\n    Socket sock = null;\n    BufferedReader br = null;\n    try {\n        sock = new Socket(addr.getHostString(), addr.getPort());\n        sock.setSoLinger(false, -1);\n        sock.setSoTimeout(1000);\n        sock.setTcpNoDelay(true);\n        sock.getOutputStream().write(\"isro\".getBytes());\n        sock.getOutputStream().flush();\n        sock.shutdownOutput();\n        br = new BufferedReader(new InputStreamReader(sock.getInputStream()));\n        result = br.readLine();\n    } catch (ConnectException e) {\n        // ignore, this just means server is not up\n    } catch (IOException e) {\n        // some unexpected error, warn about it\n        LOG.warn(\"Exception while seeking for r/w server.\", e);\n    } finally {\n        if (sock != null) {\n            try {\n                sock.close();\n            } catch (IOException e) {\n                LOG.warn(\"Unexpected exception\", e);\n            }\n        }\n        if (br != null) {\n            try {\n                br.close();\n            } catch (IOException e) {\n                LOG.warn(\"Unexpected exception\", e);\n            }\n        }\n    }\n\n    if (\"rw\".equals(result)) {\n        pingRwTimeout = minPingRwTimeout;\n        // save the found address so that it's used during the next\n        // connection attempt\n        rwServerAddress = addr;\n        throw new RWServerFoundException(\"Majority server found at \"\n                                         + addr.getHostString() + \":\" + addr.getPort());\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2709,
        "instruction": "@GET\n@Path(\"getTableMetadata\")\n@Produces(\"application/json\")\npublic Response read(@Context final ServletContext servletContext,\n                     @Context final HttpHeaders headers,\n                     @QueryParam(\"table\") final String table)\n        throws Exception {\n    LOG.debug(\"getTableMetadata started\");\n    String jsonOutput;\n    try {\n        // 1. start MetadataFetcher\n        MetadataFetcher metadataFetcher = MetadataFetcherFactory.create(\"org.apache.hawq.pxf.plugins.hive.HiveMetadataFetcher\"); // TODO:\n                                                                                                                                 // nhorn\n                                                                                                                                 // -\n                                                                                                                                 // 09-03-15\n                                                                                                                                 // -\n                                                                                                                                 // pass\n                                                                                                                                 // as\n                                                                                                                                 // param\n\n        // 2. get Metadata\n        Metadata metadata = metadataFetcher.getTableMetadata(table);\n\n        // 3. serialize to JSON\n        jsonOutput = MetadataResponseFormatter.formatResponseString(metadata);\n\n        LOG.debug(\"getTableMetadata output: \" + jsonOutput);\n\n    } catch (ClientAbortException e) {\n        throw e;\n    } catch (java.io.IOException e) {\n        LOG.error(\"Unhandled exception thrown\", e);\n        throw e;\n    }\n\n    return Response.ok(jsonOutput, MediaType.APPLICATION_JSON_TYPE).build();\n}",
        "accept_response": "@GET\n@Path(\"getTableMetadata\")\n@Produces(\"application/json\")\npublic Response read(@Context final ServletContext servletContext,\n                     @Context final HttpHeaders headers,\n                     @QueryParam(\"table\") final String table)\n        throws Exception {\n    LOG.debug(\"getTableMetadata started\");\n    String jsonOutput;\n    try {\n        // 1. start MetadataFetcher\n        MetadataFetcher metadataFetcher = MetadataFetcherFactory.create(\"org.apache.hawq.pxf.plugins.hive.HiveMetadataFetcher\"); // TODO:\n                                                                                                                                 // nhorn\n                                                                                                                                 // -\n                                                                                                                                 // 09-03-15\n                                                                                                                                 // -\n                                                                                                                                 // pass\n                                                                                                                                 // as\n                                                                                                                                 // param\n\n        // 2. get Metadata\n        Metadata metadata = metadataFetcher.getTableMetadata(table);\n\n        // 3. serialize to JSON\n        jsonOutput = MetadataResponseFormatter.formatResponseString(metadata);\n\n        LOG.debug(\"getTableMetadata output: \" + jsonOutput);\n\n    } catch (ClientAbortException e) {\n        LOG.error(\"Remote connection closed by HAWQ\", e);\n        throw e;\n    } catch (java.io.IOException e) {\n        LOG.error(\"Unhandled exception thrown\", e);\n        throw e;\n    }\n\n    return Response.ok(jsonOutput, MediaType.APPLICATION_JSON_TYPE).build();\n}",
        "reject_response": "@GET\n@Path(\"getTableMetadata\")\n@Produces(\"application/json\")\npublic Response read(@Context final ServletContext servletContext,\n                     @Context final HttpHeaders headers,\n                     @QueryParam(\"table\") final String table)\n        throws Exception {\n    LOG.debug(\"getTableMetadata started\");\n    String jsonOutput;\n    try {\n        // 1. start MetadataFetcher\n        MetadataFetcher metadataFetcher = MetadataFetcherFactory.create(\"org.apache.hawq.pxf.plugins.hive.HiveMetadataFetcher\"); // TODO:\n                                                                                                                                 // nhorn\n                                                                                                                                 // -\n                                                                                                                                 // 09-03-15\n                                                                                                                                 // -\n                                                                                                                                 // pass\n                                                                                                                                 // as\n                                                                                                                                 // param\n\n        // 2. get Metadata\n        Metadata metadata = metadataFetcher.getTableMetadata(table);\n\n        // 3. serialize to JSON\n        jsonOutput = MetadataResponseFormatter.formatResponseString(metadata);\n\n        LOG.debug(\"getTableMetadata output: \" + jsonOutput);\n\n    } catch (ClientAbortException e) {\n        Log.error(\"Remote connection closed by HAWQ\", e);\n        throw e;\n    } catch (java.io.IOException e) {\n        LOG.error(\"Unhandled exception thrown\", e);\n        throw e;\n    }\n\n    return Response.ok(jsonOutput, MediaType.APPLICATION_JSON_TYPE).build();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3003,
        "instruction": "public VXPortalUser changeEmailAddress(XXPortalUser gjUser, VXPasswordChange changeEmail) {\n\tcheckAccessForUpdate(gjUser);\n\trangerBizUtil.blockAuditorRoleUser();\n\tif (StringUtils.isEmpty(changeEmail.getEmailAddress())) {\n\t\tchangeEmail.setEmailAddress(null);\n\t}\n\n\tif (!StringUtils.isEmpty(changeEmail.getEmailAddress()) && !stringUtil.validateEmail(changeEmail.getEmailAddress())) {\n\t\tlogger.info(\"Invalid email address.\" + changeEmail);\n\t\tthrow restErrorUtil.createRESTException(\"serverMsg.userMgrInvalidEmail\",\n\t\t\t\tMessageEnums.INVALID_INPUT_DATA, changeEmail.getId(), \"emailAddress\", changeEmail.toString());\n\t}\n\n\tif (this.isFipsEnabled) {\n\t\tif (!isPasswordValid(changeEmail.getLoginId(), gjUser.getPassword(), changeEmail.getOldPassword())) {\n\t\t\tthrow restErrorUtil.createRESTException(\"serverMsg.userMgrWrongPassword\",\n\t\t\t\t\t\t\t\t\t\t\tMessageEnums.OPER_NO_PERMISSION, null, null, \"\" + changeEmail);\n\t\t}\n\t} else {\n\t\tString encryptedOldPwd = encrypt(gjUser.getLoginId(), changeEmail.getOldPassword());\n\t\tif (!stringUtil.equals(encryptedOldPwd, gjUser.getPassword())) {\n\t\t\tlogger.info(\"changeEmailAddress(). Invalid  password. changeEmail=\" + changeEmail);\n\t\t\tthrow restErrorUtil.createRESTException(\"serverMsg.userMgrWrongPassword\",\n\t\t\t\t\tMessageEnums.OPER_NO_PERMISSION, null, null, \"\" + changeEmail);\n\t\t}\n\t}\n\n\t// Normalize email. Make it lower case\n\tgjUser.setEmailAddress(stringUtil.normalizeEmail(changeEmail.getEmailAddress()));\n\n\tString saltEncodedpasswd = encrypt(gjUser.getLoginId(), changeEmail.getOldPassword());\n\tif (gjUser.getUserSource() == RangerCommonEnums.USER_APP) {\n\t\tgjUser.setPassword(saltEncodedpasswd);\n\t} else if (gjUser.getUserSource() == RangerCommonEnums.USER_EXTERNAL) {\n\t\tgjUser.setPassword(gjUser.getPassword());\n\t}\n\tdaoManager.getXXPortalUser().update(gjUser);\n\treturn mapXXPortalUserVXPortalUser(gjUser);\n}",
        "accept_response": "public VXPortalUser changeEmailAddress(XXPortalUser gjUser, VXPasswordChange changeEmail) {\n\tcheckAccessForUpdate(gjUser);\n\trangerBizUtil.blockAuditorRoleUser();\n\tif (StringUtils.isEmpty(changeEmail.getEmailAddress())) {\n\t\tchangeEmail.setEmailAddress(null);\n\t}\n\n\tif (!StringUtils.isEmpty(changeEmail.getEmailAddress()) && !stringUtil.validateEmail(changeEmail.getEmailAddress())) {\n\t\tlogger.info(\"Invalid email address.\" + changeEmail);\n\t\tthrow restErrorUtil.createRESTException(\"serverMsg.userMgrInvalidEmail\",\n\t\t\t\tMessageEnums.INVALID_INPUT_DATA, changeEmail.getId(), \"emailAddress\", changeEmail.toString());\n\t}\n\n\tif (this.isFipsEnabled) {\n\t\tif (!isPasswordValid(changeEmail.getLoginId(), gjUser.getPassword(), changeEmail.getOldPassword())) {\n\t\t\tlogger.info(\"changeEmailAddress(). Invalid  password. changeEmail=\" + changeEmail);\n\t\t\tthrow restErrorUtil.createRESTException(\"serverMsg.userMgrWrongPassword\",\n\t\t\t\t\t\t\t\t\t\t\tMessageEnums.OPER_NO_PERMISSION, null, null, \"\" + changeEmail);\n\t\t}\n\t} else {\n\t\tString encryptedOldPwd = encrypt(gjUser.getLoginId(), changeEmail.getOldPassword());\n\t\tif (!stringUtil.equals(encryptedOldPwd, gjUser.getPassword())) {\n\t\t\tlogger.info(\"changeEmailAddress(). Invalid  password. changeEmail=\" + changeEmail);\n\t\t\tthrow restErrorUtil.createRESTException(\"serverMsg.userMgrWrongPassword\",\n\t\t\t\t\tMessageEnums.OPER_NO_PERMISSION, null, null, \"\" + changeEmail);\n\t\t}\n\t}\n\n\t// Normalize email. Make it lower case\n\tgjUser.setEmailAddress(stringUtil.normalizeEmail(changeEmail.getEmailAddress()));\n\n\tString saltEncodedpasswd = encrypt(gjUser.getLoginId(), changeEmail.getOldPassword());\n\tif (gjUser.getUserSource() == RangerCommonEnums.USER_APP) {\n\t\tgjUser.setPassword(saltEncodedpasswd);\n\t} else if (gjUser.getUserSource() == RangerCommonEnums.USER_EXTERNAL) {\n\t\tgjUser.setPassword(gjUser.getPassword());\n\t}\n\tdaoManager.getXXPortalUser().update(gjUser);\n\treturn mapXXPortalUserVXPortalUser(gjUser);\n}",
        "reject_response": "public VXPortalUser changeEmailAddress(XXPortalUser gjUser, VXPasswordChange changeEmail) {\n\tcheckAccessForUpdate(gjUser);\n\trangerBizUtil.blockAuditorRoleUser();\n\tif (StringUtils.isEmpty(changeEmail.getEmailAddress())) {\n\t\tchangeEmail.setEmailAddress(null);\n\t}\n\n\tif (!StringUtils.isEmpty(changeEmail.getEmailAddress()) && !stringUtil.validateEmail(changeEmail.getEmailAddress())) {\n\t\tlogger.info(\"Invalid email address.\" + changeEmail);\n\t\tthrow restErrorUtil.createRESTException(\"serverMsg.userMgrInvalidEmail\",\n\t\t\t\tMessageEnums.INVALID_INPUT_DATA, changeEmail.getId(), \"emailAddress\", changeEmail.toString());\n\t}\n\n\tif (this.isFipsEnabled) {\n\t\tif (!isPasswordValid(changeEmail.getLoginId(), gjUser.getPassword(), changeEmail.getOldPassword())) {\n\t\t\tlogger.info(\"changeEmailAddress(). Invalid  password. changeEmail=\"\n\t\t\t\t\t\t\t+ changeEmail);\n\t\t\t\t\t\t\tthrow restErrorUtil.createRESTException(\n\t\t\t\t\t\t\t\t\t\t\"serverMsg.userMgrWrongPassword\",\n\t\t\t\t\t\t\t\t\t\t\tMessageEnums.OPER_NO_PERMISSION, null, null, \"\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t+ changeEmail);\n\t\t\t\t}\n\t\t} else {\n\t\t\tString encryptedOldPwd = encrypt(gjUser.getLoginId(), changeEmail.getOldPassword());\n\t\t\tif (!stringUtil.equals(encryptedOldPwd, gjUser.getPassword())) {\n\t\t\t\tlogger.info(\"changeEmailAddress(). Invalid  password. changeEmail=\"\n\t\t\t\t\t\t+ changeEmail);\n\t\t\t\tthrow restErrorUtil.createRESTException(\n\t\t\t\t\t\t\"serverMsg.userMgrWrongPassword\",\n\t\t\t\t\t\tMessageEnums.OPER_NO_PERMISSION, null, null, \"\"\n\t\t\t\t\t\t\t\t+ changeEmail);\n\t\t\t}\n\t\t\tthrow restErrorUtil.createRESTException(\"serverMsg.userMgrWrongPassword\",\n\t\t\t\t\t\t\t\t\t\t\tMessageEnums.OPER_NO_PERMISSION, null, null, \"\" + changeEmail);\n\t\t}\n\t} else {\n\t\tString encryptedOldPwd = encrypt(gjUser.getLoginId(), changeEmail.getOldPassword());\n\t\tif (!stringUtil.equals(encryptedOldPwd, gjUser.getPassword())) {\n\t\t\tlogger.info(\"changeEmailAddress(). Invalid  password. changeEmail=\" + changeEmail);\n\t\t\tthrow restErrorUtil.createRESTException(\"serverMsg.userMgrWrongPassword\",\n\t\t\t\t\tMessageEnums.OPER_NO_PERMISSION, null, null, \"\" + changeEmail);\n\t\t}\n\t}\n\n\t// Normalize email. Make it lower case\n\tgjUser.setEmailAddress(stringUtil.normalizeEmail(changeEmail.getEmailAddress()));\n\n\tString saltEncodedpasswd = encrypt(gjUser.getLoginId(), changeEmail.getOldPassword());\n\tif (gjUser.getUserSource() == RangerCommonEnums.USER_APP) {\n\t\tgjUser.setPassword(saltEncodedpasswd);\n\t} else if (gjUser.getUserSource() == RangerCommonEnums.USER_EXTERNAL) {\n\t\tgjUser.setPassword(gjUser.getPassword());\n\t}\n\tdaoManager.getXXPortalUser().update(gjUser);\n\treturn mapXXPortalUserVXPortalUser(gjUser);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2689,
        "instruction": "private void calculateBitSetSize() {\n\n    sampleSize = (int) (sampleRatio * 10000);\n    bitSetSize = 10000;\n\n    while ((bitSetSize > 100) && (sampleSize % 10 == 0)) {\n        bitSetSize /= 10;\n        sampleSize /= 10;\n    }\n}",
        "accept_response": "private void calculateBitSetSize() {\n\n    sampleSize = (int) (sampleRatio * 10000);\n    bitSetSize = 10000;\n\n    while ((bitSetSize > 100) && (sampleSize % 10 == 0)) {\n        bitSetSize /= 10;\n        sampleSize /= 10;\n    }\n    LOG.debug(\"bit set size = \" + bitSetSize + \" sample size = \"\n            + sampleSize);\n}",
        "reject_response": "private void calculateBitSetSize() {\n\n    sampleSize = (int) (sampleRatio * 10000);\n    bitSetSize = 10000;\n\n    while ((bitSetSize > 100) && (sampleSize % 10 == 0)) {\n        bitSetSize /= 10;\n        sampleSize /= 10;\n    }\n    Log.debug(\"bit set size = \" + bitSetSize + \" sample size = \"\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2379,
        "instruction": "private void getMonitorLock() throws KeeperException, InterruptedException {\n  final String zRoot = ZooUtil.getRoot(instance);\n  final String monitorPath = zRoot + Constants.ZMONITOR;\n  final String monitorLockPath = zRoot + Constants.ZMONITOR_LOCK;\n\n  // Ensure that everything is kosher with ZK as this has changed.\n  ZooReaderWriter zoo = ZooReaderWriter.getInstance();\n  if (zoo.exists(monitorPath)) {\n    byte[] data = zoo.getData(monitorPath, null);\n    // If the node isn't empty, it's from a previous install (has hostname:port for HTTP server)\n    if (0 != data.length) {\n      // Recursively delete from that parent node\n      zoo.recursiveDelete(monitorPath, NodeMissingPolicy.SKIP);\n\n      // And then make the nodes that we expect for the incoming ephemeral nodes\n      zoo.putPersistentData(monitorPath, new byte[0], NodeExistsPolicy.FAIL);\n      zoo.putPersistentData(monitorLockPath, new byte[0], NodeExistsPolicy.FAIL);\n    } else if (!zoo.exists(monitorLockPath)) {\n      // monitor node in ZK exists and is empty as we expect\n      // but the monitor/lock node does not\n      zoo.putPersistentData(monitorLockPath, new byte[0], NodeExistsPolicy.FAIL);\n    }\n  } else {\n    // 1.5.0 and earlier\n    zoo.putPersistentData(zRoot + Constants.ZMONITOR, new byte[0], NodeExistsPolicy.FAIL);\n    if (!zoo.exists(monitorLockPath)) {\n      // Somehow the monitor node exists but not monitor/lock\n      zoo.putPersistentData(monitorLockPath, new byte[0], NodeExistsPolicy.FAIL);\n    }\n  }\n\n  log.info(\"Attempting to acquire Monitor Lock\");\n  // Get a ZooLock for the monitor\n  while (true) {\n    MoniterLockWatcher monitorLockWatcher = new MoniterLockWatcher();\n    monitorLock = new ZooLock(monitorLockPath);\n    monitorLock.lockAsync(monitorLockWatcher, new byte[0]);\n\n    monitorLockWatcher.waitForChange();\n\n    if (monitorLockWatcher.acquiredLock) {\n      break;\n    }\n\n    if (!monitorLockWatcher.failedToAcquireLock) {\n      throw new IllegalStateException(\"monitor lock in unknown state\");\n    }\n\n    monitorLock.tryToCancelAsyncLockOrUnlock();\n\n    sleepUninterruptibly(getContext().getConfiguration().getTimeInMillis(Property.MONITOR_LOCK_CHECK_INTERVAL), TimeUnit.MILLISECONDS);\n  }\n\n}",
        "accept_response": "private void getMonitorLock() throws KeeperException, InterruptedException {\n  final String zRoot = ZooUtil.getRoot(instance);\n  final String monitorPath = zRoot + Constants.ZMONITOR;\n  final String monitorLockPath = zRoot + Constants.ZMONITOR_LOCK;\n\n  // Ensure that everything is kosher with ZK as this has changed.\n  ZooReaderWriter zoo = ZooReaderWriter.getInstance();\n  if (zoo.exists(monitorPath)) {\n    byte[] data = zoo.getData(monitorPath, null);\n    // If the node isn't empty, it's from a previous install (has hostname:port for HTTP server)\n    if (0 != data.length) {\n      // Recursively delete from that parent node\n      zoo.recursiveDelete(monitorPath, NodeMissingPolicy.SKIP);\n\n      // And then make the nodes that we expect for the incoming ephemeral nodes\n      zoo.putPersistentData(monitorPath, new byte[0], NodeExistsPolicy.FAIL);\n      zoo.putPersistentData(monitorLockPath, new byte[0], NodeExistsPolicy.FAIL);\n    } else if (!zoo.exists(monitorLockPath)) {\n      // monitor node in ZK exists and is empty as we expect\n      // but the monitor/lock node does not\n      zoo.putPersistentData(monitorLockPath, new byte[0], NodeExistsPolicy.FAIL);\n    }\n  } else {\n    // 1.5.0 and earlier\n    zoo.putPersistentData(zRoot + Constants.ZMONITOR, new byte[0], NodeExistsPolicy.FAIL);\n    if (!zoo.exists(monitorLockPath)) {\n      // Somehow the monitor node exists but not monitor/lock\n      zoo.putPersistentData(monitorLockPath, new byte[0], NodeExistsPolicy.FAIL);\n    }\n  }\n\n  log.info(\"Attempting to acquire Monitor Lock\");\n  // Get a ZooLock for the monitor\n  while (true) {\n    MoniterLockWatcher monitorLockWatcher = new MoniterLockWatcher();\n    monitorLock = new ZooLock(monitorLockPath);\n    monitorLock.lockAsync(monitorLockWatcher, new byte[0]);\n\n    monitorLockWatcher.waitForChange();\n\n    if (monitorLockWatcher.acquiredLock) {\n      break;\n    }\n\n    if (!monitorLockWatcher.failedToAcquireLock) {\n      throw new IllegalStateException(\"monitor lock in unknown state\");\n    }\n\n    monitorLock.tryToCancelAsyncLockOrUnlock();\n\n    sleepUninterruptibly(getContext().getConfiguration().getTimeInMillis(Property.MONITOR_LOCK_CHECK_INTERVAL), TimeUnit.MILLISECONDS);\n  }\n\n  log.info(\"Acquired Monitor Lock \" + monitorLock.getLockPath());\n}",
        "reject_response": "private void getMonitorLock() throws KeeperException, InterruptedException {\n  final String zRoot = ZooUtil.getRoot(instance);\n  final String monitorPath = zRoot + Constants.ZMONITOR;\n  final String monitorLockPath = zRoot + Constants.ZMONITOR_LOCK;\n\n  // Ensure that everything is kosher with ZK as this has changed.\n  ZooReaderWriter zoo = ZooReaderWriter.getInstance();\n  if (zoo.exists(monitorPath)) {\n    byte[] data = zoo.getData(monitorPath, null);\n    // If the node isn't empty, it's from a previous install (has hostname:port for HTTP server)\n    if (0 != data.length) {\n      // Recursively delete from that parent node\n      zoo.recursiveDelete(monitorPath, NodeMissingPolicy.SKIP);\n\n      // And then make the nodes that we expect for the incoming ephemeral nodes\n      zoo.putPersistentData(monitorPath, new byte[0], NodeExistsPolicy.FAIL);\n      zoo.putPersistentData(monitorLockPath, new byte[0], NodeExistsPolicy.FAIL);\n    } else if (!zoo.exists(monitorLockPath)) {\n      // monitor node in ZK exists and is empty as we expect\n      // but the monitor/lock node does not\n      zoo.putPersistentData(monitorLockPath, new byte[0], NodeExistsPolicy.FAIL);\n    }\n  } else {\n    // 1.5.0 and earlier\n    zoo.putPersistentData(zRoot + Constants.ZMONITOR, new byte[0], NodeExistsPolicy.FAIL);\n    if (!zoo.exists(monitorLockPath)) {\n      // Somehow the monitor node exists but not monitor/lock\n      zoo.putPersistentData(monitorLockPath, new byte[0], NodeExistsPolicy.FAIL);\n    }\n  }\n\n  log.info(\"Attempting to acquire Monitor Lock\");\n  // Get a ZooLock for the monitor\n  while (true) {\n    MoniterLockWatcher monitorLockWatcher = new MoniterLockWatcher();\n    monitorLock = new ZooLock(monitorLockPath);\n    monitorLock.lockAsync(monitorLockWatcher, new byte[0]);\n\n    monitorLockWatcher.waitForChange();\n\n    if (monitorLockWatcher.acquiredLock) {\n      break;\n    }\n\n    if (!monitorLockWatcher.failedToAcquireLock) {\n      throw new IllegalStateException(\"monitor lock in unknown state\");\n    }\n\n    monitorLock.tryToCancelAsyncLockOrUnlock();\n\n    sleepUninterruptibly(getContext().getConfiguration().getTimeInMillis(Property.MONITOR_LOCK_CHECK_INTERVAL), TimeUnit.MILLISECONDS);\n  }\n\n  log.info(\"Got Monitor lock.\");\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2550,
        "instruction": "@Override\npublic void update() {\n  // Get sizing information for the batch.\n  setRecordBatchSizer(new RecordBatchSizer(incoming));\n\n  final TypedFieldId typedFieldId = incoming.getValueVectorId(popConfig.getColumn());\n  final MaterializedField field = incoming.getSchema().getColumn(typedFieldId.getFieldIds()[0]);\n\n  // Get column size of flatten column.\n  RecordBatchSizer.ColumnSize columnSize = getRecordBatchSizer().getColumn(field.getName());\n\n  // Average rowWidth of flatten column\n  final int avgRowWidthFlattenColumn = columnSize.getNetSizePerEntry();\n\n  // Average rowWidth excluding the flatten column.\n  final int avgRowWidthWithOutFlattenColumn = getRecordBatchSizer().netRowWidth() - avgRowWidthFlattenColumn;\n\n  // Average rowWidth of single element in the flatten list.\n  // subtract the offset vector size from column data size.\n  final int avgRowWidthSingleFlattenEntry =\n    RecordBatchSizer.safeDivide(columnSize.getTotalNetSize() - (getOffsetVectorWidth() * columnSize.getValueCount()),\n      columnSize.getElementCount());\n\n  // Average rowWidth of outgoing batch.\n  final int avgOutgoingRowWidth = avgRowWidthWithOutFlattenColumn + avgRowWidthSingleFlattenEntry;\n\n  final int outputBatchSize = getOutputBatchSize();\n  // Number of rows in outgoing batch\n  setOutputRowCount(outputBatchSize, avgOutgoingRowWidth);\n\n  setOutgoingRowWidth(avgOutgoingRowWidth);\n\n  // Limit to lower bound of total number of rows possible for this batch\n  // i.e. all rows fit within memory budget.\n  setOutputRowCount(Math.min(columnSize.getElementCount(), getOutputRowCount()));\n\n  updateIncomingStats();\n}",
        "accept_response": "@Override\npublic void update() {\n  // Get sizing information for the batch.\n  setRecordBatchSizer(new RecordBatchSizer(incoming));\n\n  final TypedFieldId typedFieldId = incoming.getValueVectorId(popConfig.getColumn());\n  final MaterializedField field = incoming.getSchema().getColumn(typedFieldId.getFieldIds()[0]);\n\n  // Get column size of flatten column.\n  RecordBatchSizer.ColumnSize columnSize = getRecordBatchSizer().getColumn(field.getName());\n\n  // Average rowWidth of flatten column\n  final int avgRowWidthFlattenColumn = columnSize.getNetSizePerEntry();\n\n  // Average rowWidth excluding the flatten column.\n  final int avgRowWidthWithOutFlattenColumn = getRecordBatchSizer().netRowWidth() - avgRowWidthFlattenColumn;\n\n  // Average rowWidth of single element in the flatten list.\n  // subtract the offset vector size from column data size.\n  final int avgRowWidthSingleFlattenEntry =\n    RecordBatchSizer.safeDivide(columnSize.getTotalNetSize() - (getOffsetVectorWidth() * columnSize.getValueCount()),\n      columnSize.getElementCount());\n\n  // Average rowWidth of outgoing batch.\n  final int avgOutgoingRowWidth = avgRowWidthWithOutFlattenColumn + avgRowWidthSingleFlattenEntry;\n\n  final int outputBatchSize = getOutputBatchSize();\n  // Number of rows in outgoing batch\n  setOutputRowCount(outputBatchSize, avgOutgoingRowWidth);\n\n  setOutgoingRowWidth(avgOutgoingRowWidth);\n\n  // Limit to lower bound of total number of rows possible for this batch\n  // i.e. all rows fit within memory budget.\n  setOutputRowCount(Math.min(columnSize.getElementCount(), getOutputRowCount()));\n\n  if (logger.isDebugEnabled()) {\n    logger.debug(\"BATCH_STATS, incoming:\\n {}\", getRecordBatchSizer());\n  }\n\n  updateIncomingStats();\n}",
        "reject_response": "@Override\npublic void update() {\n  // Get sizing information for the batch.\n  setRecordBatchSizer(new RecordBatchSizer(incoming));\n\n  final TypedFieldId typedFieldId = incoming.getValueVectorId(popConfig.getColumn());\n  final MaterializedField field = incoming.getSchema().getColumn(typedFieldId.getFieldIds()[0]);\n\n  // Get column size of flatten column.\n  RecordBatchSizer.ColumnSize columnSize = getRecordBatchSizer().getColumn(field.getName());\n\n  // Average rowWidth of flatten column\n  final int avgRowWidthFlattenColumn = columnSize.getNetSizePerEntry();\n\n  // Average rowWidth excluding the flatten column.\n  final int avgRowWidthWithOutFlattenColumn = getRecordBatchSizer().netRowWidth() - avgRowWidthFlattenColumn;\n\n  // Average rowWidth of single element in the flatten list.\n  // subtract the offset vector size from column data size.\n  final int avgRowWidthSingleFlattenEntry =\n    RecordBatchSizer.safeDivide(columnSize.getTotalNetSize() - (getOffsetVectorWidth() * columnSize.getValueCount()),\n      columnSize.getElementCount());\n\n  // Average rowWidth of outgoing batch.\n  final int avgOutgoingRowWidth = avgRowWidthWithOutFlattenColumn + avgRowWidthSingleFlattenEntry;\n\n  final int outputBatchSize = getOutputBatchSize();\n  // Number of rows in outgoing batch\n  setOutputRowCount(outputBatchSize, avgOutgoingRowWidth);\n\n  setOutgoingRowWidth(avgOutgoingRowWidth);\n\n  // Limit to lower bound of total number of rows possible for this batch\n  // i.e. all rows fit within memory budget.\n  setOutputRowCount(Math.min(columnSize.getElementCount(), getOutputRowCount()));\n\n  if (logger.isDebugEnabled()) {\n  logger.debug(\"incoming batch size : {}\", getRecordBatchSizer());\n\n  logger.debug(\"output batch size : {}, avg outgoing rowWidth : {}, output rowCount : {}\",\n    outputBatchSize, avgOutgoingRowWidth, getOutputRowCount());\n  }\n\n  updateIncomingStats();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2399,
        "instruction": "@SuppressWarnings(\"serial\")\npublic ExecutionCommand getExecutionCommand() {\n  if (executionCommand != null) {\n    return executionCommand;\n  } else if (jsonExecutionCommand != null) {\n    executionCommand = StageUtils.getGson().fromJson(jsonExecutionCommand, ExecutionCommand.class);\n\n    if (injector == null) {\n      throw new RuntimeException(\"Injector not found, configuration cannot be restored\");\n    } else if (executionCommand.getConfigurationTags() != null &&\n        !executionCommand.getConfigurationTags().isEmpty()) {\n\n      // For a configuration type, both tag and an actual configuration can be stored\n      // Configurations from the tag is always expanded and then over-written by the actual\n      // global:version1:{a1:A1,b1:B1,d1:D1} + global:{a1:A2,c1:C1,DELETED_d1:x} ==>\n      // global:{a1:A2,b1:B1,c1:C1}\n      Clusters clusters = injector.getInstance(Clusters.class);\n      HostRoleCommandDAO hostRoleCommandDAO = injector.getInstance(HostRoleCommandDAO.class);\n      Long clusterId = hostRoleCommandDAO.findByPK(\n          executionCommand.getTaskId()).getStage().getClusterId();\n\n      try {\n        Cluster cluster = clusters.getClusterById(clusterId);\n        ConfigHelper configHelper = injector.getInstance(ConfigHelper.class);\n        Map<String, Map<String, String>> configurationTags = executionCommand.getConfigurationTags();\n\n        // Execution commands have config-tags already set during their creation. However, these\n        // tags become stale at runtime when other ExecutionCommands run and change the desired\n        // configs (like ConfigureAction). Hence an ExecutionCommand can specify which config-types\n        // should be refreshed at runtime. Specifying <code>*</code> will result in all config-type\n        // tags to be refreshed to the latest cluster desired-configs.\n        Set<String> refreshConfigTagsBeforeExecution = executionCommand.getForceRefreshConfigTagsBeforeExecution();\n        if (refreshConfigTagsBeforeExecution != null && !refreshConfigTagsBeforeExecution.isEmpty()) {\n          Map<String, DesiredConfig> desiredConfigs = cluster.getDesiredConfigs();\n          for (String refreshConfigTag : refreshConfigTagsBeforeExecution) {\n            if (\"*\".equals(refreshConfigTag)) {\n              // if forcing a refresh of *, then clear out any existing\n              // configurations so that all of the new configurations are\n              // forcefully applied\n              executionCommand.getConfigurations().clear();\n\n              for (final Entry<String, DesiredConfig> desiredConfig : desiredConfigs.entrySet()) {\n                configurationTags.put(desiredConfig.getKey(), new HashMap<String, String>() {{\n                  put(\"tag\", desiredConfig.getValue().getTag());\n                }});\n              }\n              break;\n            } else if (configurationTags.containsKey(refreshConfigTag) && desiredConfigs.containsKey(refreshConfigTag)) {\n              configurationTags.get(refreshConfigTag).put(\"tag\", desiredConfigs.get(refreshConfigTag).getTag());\n            }\n          }\n        }\n\n        Map<String, Map<String, String>> configProperties = configHelper\n          .getEffectiveConfigProperties(cluster, configurationTags);\n\n        // Apply the configurations saved with the Execution Cmd on top of\n        // derived configs - This will take care of all the hacks\n        for (Map.Entry<String, Map<String, String>> entry : configProperties.entrySet()) {\n          String type = entry.getKey();\n          Map<String, String> allLevelMergedConfig = entry.getValue();\n\n          if (executionCommand.getConfigurations().containsKey(type)) {\n            Map<String, String> mergedConfig =\n              configHelper.getMergedConfig(allLevelMergedConfig,\n                executionCommand.getConfigurations().get(type));\n\n            executionCommand.getConfigurations().get(type).clear();\n            executionCommand.getConfigurations().get(type).putAll(mergedConfig);\n\n          } else {\n            executionCommand.getConfigurations().put(type, new HashMap<String, String>());\n            executionCommand.getConfigurations().get(type).putAll(allLevelMergedConfig);\n          }\n        }\n\n        Map<String, Map<String, Map<String, String>>> configAttributes = configHelper.getEffectiveConfigAttributes(cluster,\n            executionCommand.getConfigurationTags());\n\n        for (Map.Entry<String, Map<String, Map<String, String>>> attributesOccurrence : configAttributes.entrySet()) {\n          String type = attributesOccurrence.getKey();\n          Map<String, Map<String, String>> attributes = attributesOccurrence.getValue();\n\n          if (executionCommand.getConfigurationAttributes() != null) {\n            if (!executionCommand.getConfigurationAttributes().containsKey(type)) {\n              executionCommand.getConfigurationAttributes().put(type, new TreeMap<String, Map<String, String>>());\n            }\n            configHelper.cloneAttributesMap(attributes, executionCommand.getConfigurationAttributes().get(type));\n          }\n        }\n\n      } catch (AmbariException e) {\n        throw new RuntimeException(e);\n      }\n    }\n\n    return executionCommand;\n  } else {\n    throw new RuntimeException(\n        \"Invalid ExecutionCommandWrapper, both object and string\"\n            + \" representations are null\");\n  }\n}",
        "accept_response": "@SuppressWarnings(\"serial\")\npublic ExecutionCommand getExecutionCommand() {\n  if (executionCommand != null) {\n    return executionCommand;\n  } else if (jsonExecutionCommand != null) {\n    executionCommand = StageUtils.getGson().fromJson(jsonExecutionCommand, ExecutionCommand.class);\n\n    if (injector == null) {\n      throw new RuntimeException(\"Injector not found, configuration cannot be restored\");\n    } else if (executionCommand.getConfigurationTags() != null &&\n        !executionCommand.getConfigurationTags().isEmpty()) {\n\n      // For a configuration type, both tag and an actual configuration can be stored\n      // Configurations from the tag is always expanded and then over-written by the actual\n      // global:version1:{a1:A1,b1:B1,d1:D1} + global:{a1:A2,c1:C1,DELETED_d1:x} ==>\n      // global:{a1:A2,b1:B1,c1:C1}\n      Clusters clusters = injector.getInstance(Clusters.class);\n      HostRoleCommandDAO hostRoleCommandDAO = injector.getInstance(HostRoleCommandDAO.class);\n      Long clusterId = hostRoleCommandDAO.findByPK(\n          executionCommand.getTaskId()).getStage().getClusterId();\n\n      try {\n        Cluster cluster = clusters.getClusterById(clusterId);\n        ConfigHelper configHelper = injector.getInstance(ConfigHelper.class);\n        Map<String, Map<String, String>> configurationTags = executionCommand.getConfigurationTags();\n\n        // Execution commands have config-tags already set during their creation. However, these\n        // tags become stale at runtime when other ExecutionCommands run and change the desired\n        // configs (like ConfigureAction). Hence an ExecutionCommand can specify which config-types\n        // should be refreshed at runtime. Specifying <code>*</code> will result in all config-type\n        // tags to be refreshed to the latest cluster desired-configs.\n        Set<String> refreshConfigTagsBeforeExecution = executionCommand.getForceRefreshConfigTagsBeforeExecution();\n        if (refreshConfigTagsBeforeExecution != null && !refreshConfigTagsBeforeExecution.isEmpty()) {\n          Map<String, DesiredConfig> desiredConfigs = cluster.getDesiredConfigs();\n          for (String refreshConfigTag : refreshConfigTagsBeforeExecution) {\n            if (\"*\".equals(refreshConfigTag)) {\n              // if forcing a refresh of *, then clear out any existing\n              // configurations so that all of the new configurations are\n              // forcefully applied\n              LOG.debug(\"ExecutionCommandWrapper.getExecutionCommand: refreshConfigTag set to {}, so clearing config for full refresh.\", refreshConfigTag);\n              executionCommand.getConfigurations().clear();\n\n              for (final Entry<String, DesiredConfig> desiredConfig : desiredConfigs.entrySet()) {\n                configurationTags.put(desiredConfig.getKey(), new HashMap<String, String>() {{\n                  put(\"tag\", desiredConfig.getValue().getTag());\n                }});\n              }\n              break;\n            } else if (configurationTags.containsKey(refreshConfigTag) && desiredConfigs.containsKey(refreshConfigTag)) {\n              configurationTags.get(refreshConfigTag).put(\"tag\", desiredConfigs.get(refreshConfigTag).getTag());\n            }\n          }\n        }\n\n        Map<String, Map<String, String>> configProperties = configHelper\n          .getEffectiveConfigProperties(cluster, configurationTags);\n\n        // Apply the configurations saved with the Execution Cmd on top of\n        // derived configs - This will take care of all the hacks\n        for (Map.Entry<String, Map<String, String>> entry : configProperties.entrySet()) {\n          String type = entry.getKey();\n          Map<String, String> allLevelMergedConfig = entry.getValue();\n\n          if (executionCommand.getConfigurations().containsKey(type)) {\n            Map<String, String> mergedConfig =\n              configHelper.getMergedConfig(allLevelMergedConfig,\n                executionCommand.getConfigurations().get(type));\n\n            executionCommand.getConfigurations().get(type).clear();\n            executionCommand.getConfigurations().get(type).putAll(mergedConfig);\n\n          } else {\n            executionCommand.getConfigurations().put(type, new HashMap<String, String>());\n            executionCommand.getConfigurations().get(type).putAll(allLevelMergedConfig);\n          }\n        }\n\n        Map<String, Map<String, Map<String, String>>> configAttributes = configHelper.getEffectiveConfigAttributes(cluster,\n            executionCommand.getConfigurationTags());\n\n        for (Map.Entry<String, Map<String, Map<String, String>>> attributesOccurrence : configAttributes.entrySet()) {\n          String type = attributesOccurrence.getKey();\n          Map<String, Map<String, String>> attributes = attributesOccurrence.getValue();\n\n          if (executionCommand.getConfigurationAttributes() != null) {\n            if (!executionCommand.getConfigurationAttributes().containsKey(type)) {\n              executionCommand.getConfigurationAttributes().put(type, new TreeMap<String, Map<String, String>>());\n            }\n            configHelper.cloneAttributesMap(attributes, executionCommand.getConfigurationAttributes().get(type));\n          }\n        }\n\n      } catch (AmbariException e) {\n        throw new RuntimeException(e);\n      }\n    }\n\n    return executionCommand;\n  } else {\n    throw new RuntimeException(\n        \"Invalid ExecutionCommandWrapper, both object and string\"\n            + \" representations are null\");\n  }\n}",
        "reject_response": "@SuppressWarnings(\"serial\")\npublic ExecutionCommand getExecutionCommand() {\n  if (executionCommand != null) {\n    return executionCommand;\n  } else if (jsonExecutionCommand != null) {\n    executionCommand = StageUtils.getGson().fromJson(jsonExecutionCommand, ExecutionCommand.class);\n\n    if (injector == null) {\n      throw new RuntimeException(\"Injector not found, configuration cannot be restored\");\n    } else if (executionCommand.getConfigurationTags() != null &&\n        !executionCommand.getConfigurationTags().isEmpty()) {\n\n      // For a configuration type, both tag and an actual configuration can be stored\n      // Configurations from the tag is always expanded and then over-written by the actual\n      // global:version1:{a1:A1,b1:B1,d1:D1} + global:{a1:A2,c1:C1,DELETED_d1:x} ==>\n      // global:{a1:A2,b1:B1,c1:C1}\n      Clusters clusters = injector.getInstance(Clusters.class);\n      HostRoleCommandDAO hostRoleCommandDAO = injector.getInstance(HostRoleCommandDAO.class);\n      Long clusterId = hostRoleCommandDAO.findByPK(\n          executionCommand.getTaskId()).getStage().getClusterId();\n\n      try {\n        Cluster cluster = clusters.getClusterById(clusterId);\n        ConfigHelper configHelper = injector.getInstance(ConfigHelper.class);\n        Map<String, Map<String, String>> configurationTags = executionCommand.getConfigurationTags();\n\n        // Execution commands have config-tags already set during their creation. However, these\n        // tags become stale at runtime when other ExecutionCommands run and change the desired\n        // configs (like ConfigureAction). Hence an ExecutionCommand can specify which config-types\n        // should be refreshed at runtime. Specifying <code>*</code> will result in all config-type\n        // tags to be refreshed to the latest cluster desired-configs.\n        Set<String> refreshConfigTagsBeforeExecution = executionCommand.getForceRefreshConfigTagsBeforeExecution();\n        if (refreshConfigTagsBeforeExecution != null && !refreshConfigTagsBeforeExecution.isEmpty()) {\n          Map<String, DesiredConfig> desiredConfigs = cluster.getDesiredConfigs();\n          for (String refreshConfigTag : refreshConfigTagsBeforeExecution) {\n            if (\"*\".equals(refreshConfigTag)) {\n              // if forcing a refresh of *, then clear out any existing\n              // configurations so that all of the new configurations are\n              // forcefully applied\n              LOG.info(\"ExecutionCommandWrapper.getExecutionCommand: refreshConfigTag set to {}, so clearing config for full refresh.\", refreshConfigTag);\n              executionCommand.getConfigurations().clear();\n\n              for (final Entry<String, DesiredConfig> desiredConfig : desiredConfigs.entrySet()) {\n                configurationTags.put(desiredConfig.getKey(), new HashMap<String, String>() {{\n                  put(\"tag\", desiredConfig.getValue().getTag());\n                }});\n              }\n              break;\n            } else if (configurationTags.containsKey(refreshConfigTag) && desiredConfigs.containsKey(refreshConfigTag)) {\n              configurationTags.get(refreshConfigTag).put(\"tag\", desiredConfigs.get(refreshConfigTag).getTag());\n            }\n          }\n        }\n\n        Map<String, Map<String, String>> configProperties = configHelper\n          .getEffectiveConfigProperties(cluster, configurationTags);\n\n        // Apply the configurations saved with the Execution Cmd on top of\n        // derived configs - This will take care of all the hacks\n        for (Map.Entry<String, Map<String, String>> entry : configProperties.entrySet()) {\n          String type = entry.getKey();\n          Map<String, String> allLevelMergedConfig = entry.getValue();\n\n          if (executionCommand.getConfigurations().containsKey(type)) {\n            Map<String, String> mergedConfig =\n              configHelper.getMergedConfig(allLevelMergedConfig,\n                executionCommand.getConfigurations().get(type));\n\n            executionCommand.getConfigurations().get(type).clear();\n            executionCommand.getConfigurations().get(type).putAll(mergedConfig);\n\n          } else {\n            executionCommand.getConfigurations().put(type, new HashMap<String, String>());\n            executionCommand.getConfigurations().get(type).putAll(allLevelMergedConfig);\n          }\n        }\n\n        Map<String, Map<String, Map<String, String>>> configAttributes = configHelper.getEffectiveConfigAttributes(cluster,\n            executionCommand.getConfigurationTags());\n\n        for (Map.Entry<String, Map<String, Map<String, String>>> attributesOccurrence : configAttributes.entrySet()) {\n          String type = attributesOccurrence.getKey();\n          Map<String, Map<String, String>> attributes = attributesOccurrence.getValue();\n\n          if (executionCommand.getConfigurationAttributes() != null) {\n            if (!executionCommand.getConfigurationAttributes().containsKey(type)) {\n              executionCommand.getConfigurationAttributes().put(type, new TreeMap<String, Map<String, String>>());\n            }\n            configHelper.cloneAttributesMap(attributes, executionCommand.getConfigurationAttributes().get(type));\n          }\n        }\n\n      } catch (AmbariException e) {\n        throw new RuntimeException(e);\n      }\n    }\n\n    return executionCommand;\n  } else {\n    throw new RuntimeException(\n        \"Invalid ExecutionCommandWrapper, both object and string\"\n            + \" representations are null\");\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3096,
        "instruction": "@Override\npublic void close()\n    throws IOException\n{\n\ttry\n\t{\n\t\t//write output if required (matrix indexed write)\n\t\tRemoteParForUtils.exportResultVariables( _workerID, _ec.getVariables(), _resultVars, _out );\n\n\t\t//statistic maintenance (after final export)\n\t\tRemoteParForUtils.incrementParForMRCounters(_report, 0, 0);\n\n\t\t//print heaver hitter per task\n\t\tJobConf job = ConfigurationManager.getCachedJobConf();\n\t\tif( DMLScript.STATISTICS && !InfrastructureAnalyzer.isLocalMode(job) )\n\t}\n\tcatch(Exception ex)\n\t{\n\t\tthrow new IOException( ex );\n\t}\n\n\t//cleanup cache and local tmp dir\n\tRemoteParForUtils.cleanupWorkingDirectories();\n\n\t//ensure caching is not disabled for CP in local mode\n\tCacheableData.enableCaching();\n}",
        "accept_response": "@Override\npublic void close()\n    throws IOException\n{\n\ttry\n\t{\n\t\t//write output if required (matrix indexed write)\n\t\tRemoteParForUtils.exportResultVariables( _workerID, _ec.getVariables(), _resultVars, _out );\n\n\t\t//statistic maintenance (after final export)\n\t\tRemoteParForUtils.incrementParForMRCounters(_report, 0, 0);\n\n\t\t//print heaver hitter per task\n\t\tJobConf job = ConfigurationManager.getCachedJobConf();\n\t\tif( DMLScript.STATISTICS && !InfrastructureAnalyzer.isLocalMode(job) )\n\t\t\tLOG.info(\"\\nSystemML Statistics:\\nHeavy hitter instructions (name, time, count):\\n\" + Statistics.getHeavyHitters(DMLScript.STATISTICS_COUNT));\n\t}\n\tcatch(Exception ex)\n\t{\n\t\tthrow new IOException( ex );\n\t}\n\n\t//cleanup cache and local tmp dir\n\tRemoteParForUtils.cleanupWorkingDirectories();\n\n\t//ensure caching is not disabled for CP in local mode\n\tCacheableData.enableCaching();\n}",
        "reject_response": "@Override\npublic void close()\n    throws IOException\n{\n\ttry\n\t{\n\t\t//write output if required (matrix indexed write)\n\t\tRemoteParForUtils.exportResultVariables( _workerID, _ec.getVariables(), _resultVars, _out );\n\n\t\t//statistic maintenance (after final export)\n\t\tRemoteParForUtils.incrementParForMRCounters(_report, 0, 0);\n\n\t\t//print heaver hitter per task\n\t\tJobConf job = ConfigurationManager.getCachedJobConf();\n\t\tif( DMLScript.STATISTICS && !InfrastructureAnalyzer.isLocalMode(job) )\n\t\t\tLOG.info(\"\\nSystemML Statistics:\\nHeavy hitter instructions (name, time, count):\\n\" + Statistics.getHeavyHitters(10));\n\t}\n\tcatch(Exception ex)\n\t{\n\t\tthrow new IOException( ex );\n\t}\n\n\t//cleanup cache and local tmp dir\n\tRemoteParForUtils.cleanupWorkingDirectories();\n\n\t//ensure caching is not disabled for CP in local mode\n\tCacheableData.enableCaching();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3073,
        "instruction": "private void deleteExecutorDirs(String[] dirs) {\n  for (String localDir : dirs) {\n    try {\n      JavaUtils.deleteRecursively(new File(localDir));\n    } catch (Exception e) {\n      logger.error(\"Failed to delete directory: \" + localDir, e);\n    }\n  }\n}",
        "accept_response": "private void deleteExecutorDirs(String[] dirs) {\n  for (String localDir : dirs) {\n    try {\n      JavaUtils.deleteRecursively(new File(localDir));\n      logger.debug(\"Successfully cleaned up directory: {}\", localDir);\n    } catch (Exception e) {\n      logger.error(\"Failed to delete directory: \" + localDir, e);\n    }\n  }\n}",
        "reject_response": "private void deleteExecutorDirs(String[] dirs) {\n  for (String localDir : dirs) {\n    try {\n      JavaUtils.deleteRecursively(new File(localDir));\n      logger.debug(\"Successfully cleaned up directory: \" + localDir);\n    } catch (Exception e) {\n      logger.error(\"Failed to delete directory: \" + localDir, e);\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2869,
        "instruction": "public static SPARQLResult result(String filenameOrURI, ResultsFormat format) {\n    if (format == null)\n        format = ResultsFormat.guessSyntax(filenameOrURI);\n\n    if (format == null) {\n        Log.warn(ResultSet.class, \"Null format - defaulting to XML\");\n        format = ResultsFormat.FMT_RS_XML;\n    }\n\n    if (format.equals(ResultsFormat.FMT_TEXT)) {\n        Log.error(ResultSet.class, \"Can't read a text result set\");\n        throw new ResultSetException(\"Can't read a text result set\");\n    }\n\n    if (format.equals(ResultsFormat.FMT_RS_XML) || format.equals(ResultsFormat.FMT_RS_JSON)\n        || format.equals(ResultsFormat.FMT_RS_TSV) || format.equals(ResultsFormat.FMT_RS_CSV)) {\n        InputStream in = null;\n        try {\n            in = FileManager.get().open(filenameOrURI);\n            if (in == null)\n                throw new NotFoundException(filenameOrURI);\n        } catch (NotFoundException ex) {\n            throw new NotFoundException(\"File not found: \" + filenameOrURI);\n        }\n\n        SPARQLResult x = null;\n\n        if (format.equals(ResultsFormat.FMT_RS_JSON))\n            return JSONInput.make(in, GraphFactory.makeDefaultModel());\n        else if (format.equals(ResultsFormat.FMT_RS_XML))\n            return XMLInput.make(in, GraphFactory.makeDefaultModel());\n        else if (format.equals(ResultsFormat.FMT_RS_TSV)) {\n            ResultSet rs = TSVInput.fromTSV(in);\n            return new SPARQLResult(rs);\n        } else if (format.equals(ResultsFormat.FMT_RS_CSV)) {\n            ResultSet rs = CSVInput.fromCSV(in);\n            return new SPARQLResult(rs);\n        } else if (format.equals(ResultsFormat.FMT_RS_BIO)) {\n            ResultSet rs = BIOInput.fromBIO(in);\n            return new SPARQLResult(rs);\n        }\n    }\n\n    if (ResultsFormat.isRDFGraphSyntax(format)) {\n        Model model = FileManager.get().loadModel(filenameOrURI);\n        return new SPARQLResult(model);\n    }\n\n    return null;\n}",
        "accept_response": "public static SPARQLResult result(String filenameOrURI, ResultsFormat format) {\n    if (format == null)\n        format = ResultsFormat.guessSyntax(filenameOrURI);\n\n    if (format == null) {\n        Log.warn(ResultSet.class, \"Null format - defaulting to XML\");\n        format = ResultsFormat.FMT_RS_XML;\n    }\n\n    if (format.equals(ResultsFormat.FMT_TEXT)) {\n        Log.error(ResultSet.class, \"Can't read a text result set\");\n        throw new ResultSetException(\"Can't read a text result set\");\n    }\n\n    if (format.equals(ResultsFormat.FMT_RS_XML) || format.equals(ResultsFormat.FMT_RS_JSON)\n        || format.equals(ResultsFormat.FMT_RS_TSV) || format.equals(ResultsFormat.FMT_RS_CSV)) {\n        InputStream in = null;\n        try {\n            in = FileManager.get().open(filenameOrURI);\n            if (in == null)\n                throw new NotFoundException(filenameOrURI);\n        } catch (NotFoundException ex) {\n            throw new NotFoundException(\"File not found: \" + filenameOrURI);\n        }\n\n        SPARQLResult x = null;\n\n        if (format.equals(ResultsFormat.FMT_RS_JSON))\n            return JSONInput.make(in, GraphFactory.makeDefaultModel());\n        else if (format.equals(ResultsFormat.FMT_RS_XML))\n            return XMLInput.make(in, GraphFactory.makeDefaultModel());\n        else if (format.equals(ResultsFormat.FMT_RS_TSV)) {\n            ResultSet rs = TSVInput.fromTSV(in);\n            return new SPARQLResult(rs);\n        } else if (format.equals(ResultsFormat.FMT_RS_CSV)) {\n            ResultSet rs = CSVInput.fromCSV(in);\n            return new SPARQLResult(rs);\n        } else if (format.equals(ResultsFormat.FMT_RS_BIO)) {\n            ResultSet rs = BIOInput.fromBIO(in);\n            return new SPARQLResult(rs);\n        }\n    }\n\n    if (ResultsFormat.isRDFGraphSyntax(format)) {\n        Model model = FileManager.get().loadModel(filenameOrURI);\n        return new SPARQLResult(model);\n    }\n\n    Log.error(ResultSet.class, \"Unknown result set syntax: \" + format);\n    return null;\n}",
        "reject_response": "public static SPARQLResult result(String filenameOrURI, ResultsFormat format) {\n    if (format == null)\n        format = ResultsFormat.guessSyntax(filenameOrURI);\n\n    if (format == null) {\n        Log.warn(ResultSet.class, \"Null format - defaulting to XML\");\n        format = ResultsFormat.FMT_RS_XML;\n    }\n\n    if (format.equals(ResultsFormat.FMT_TEXT)) {\n        Log.error(ResultSet.class, \"Can't read a text result set\");\n        throw new ResultSetException(\"Can't read a text result set\");\n    }\n\n    if (format.equals(ResultsFormat.FMT_RS_XML) || format.equals(ResultsFormat.FMT_RS_JSON)\n        || format.equals(ResultsFormat.FMT_RS_TSV) || format.equals(ResultsFormat.FMT_RS_CSV)) {\n        InputStream in = null;\n        try {\n            in = FileManager.get().open(filenameOrURI);\n            if (in == null)\n                throw new NotFoundException(filenameOrURI);\n        } catch (NotFoundException ex) {\n            throw new NotFoundException(\"File not found: \" + filenameOrURI);\n        }\n\n        SPARQLResult x = null;\n\n        if (format.equals(ResultsFormat.FMT_RS_JSON))\n            return JSONInput.make(in, GraphFactory.makeDefaultModel());\n        else if (format.equals(ResultsFormat.FMT_RS_XML))\n            return XMLInput.make(in, GraphFactory.makeDefaultModel());\n        else if (format.equals(ResultsFormat.FMT_RS_TSV)) {\n            ResultSet rs = TSVInput.fromTSV(in);\n            return new SPARQLResult(rs);\n        } else if (format.equals(ResultsFormat.FMT_RS_CSV)) {\n            ResultSet rs = CSVInput.fromCSV(in);\n            return new SPARQLResult(rs);\n        } else if (format.equals(ResultsFormat.FMT_RS_BIO)) {\n            ResultSet rs = BIOInput.fromBIO(in);\n            return new SPARQLResult(rs);\n        }\n    }\n\n    if (ResultsFormat.isRDFGraphSyntax(format)) {\n        Model model = FileManager.get().loadModel(filenameOrURI);\n        return new SPARQLResult(model);\n    }\n\n    Log.fatal(ResultSet.class, \"Unknown result set syntax: \" + format);\n    return null;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2505,
        "instruction": "private void rebootBroker(AmazonMQ mqClient, Exchange exchange) {\n    String brokerId;\n    RebootBrokerRequest request = new RebootBrokerRequest();\n    if (ObjectHelper.isNotEmpty(exchange.getIn().getHeader(MQConstants.BROKER_ID))) {\n        brokerId = exchange.getIn().getHeader(MQConstants.BROKER_ID, String.class);\n        request.withBrokerId(brokerId);\n    } else {\n        throw new IllegalArgumentException(\"Broker Name must be specified\");\n    }\n    RebootBrokerResult result;\n    try {\n        result = mqClient.rebootBroker(request);\n    } catch (AmazonServiceException ase) {\n        throw ase;\n    }\n    Message message = getMessageForResponse(exchange);\n    message.setBody(result);\n}",
        "accept_response": "private void rebootBroker(AmazonMQ mqClient, Exchange exchange) {\n    String brokerId;\n    RebootBrokerRequest request = new RebootBrokerRequest();\n    if (ObjectHelper.isNotEmpty(exchange.getIn().getHeader(MQConstants.BROKER_ID))) {\n        brokerId = exchange.getIn().getHeader(MQConstants.BROKER_ID, String.class);\n        request.withBrokerId(brokerId);\n    } else {\n        throw new IllegalArgumentException(\"Broker Name must be specified\");\n    }\n    RebootBrokerResult result;\n    try {\n        result = mqClient.rebootBroker(request);\n    } catch (AmazonServiceException ase) {\n        LOG.trace(\"Reboot Broker command returned the error code {}\", ase.getErrorCode());\n        throw ase;\n    }\n    Message message = getMessageForResponse(exchange);\n    message.setBody(result);\n}",
        "reject_response": "private void rebootBroker(AmazonMQ mqClient, Exchange exchange) {\n    String brokerId;\n    RebootBrokerRequest request = new RebootBrokerRequest();\n    if (ObjectHelper.isNotEmpty(exchange.getIn().getHeader(MQConstants.BROKER_ID))) {\n        brokerId = exchange.getIn().getHeader(MQConstants.BROKER_ID, String.class);\n        request.withBrokerId(brokerId);\n    } else {\n        throw new IllegalArgumentException(\"Broker Name must be specified\");\n    }\n    RebootBrokerResult result;\n    try {\n        result = mqClient.rebootBroker(request);\n    } catch (AmazonServiceException ase) {\n        LOG.trace(\"Delete Broker command returned the error code {}\", ase.getErrorCode());\n        throw ase;\n    }\n    Message message = getMessageForResponse(exchange);\n    message.setBody(result);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2845,
        "instruction": "private void processSinglePartitionUpdate(final ClusterNode node, final GridDhtPartitionsSingleMessage msg) {\n    if (!enterBusy())\n        return;\n\n    try {\n        if (msg.exchangeId() == null) {\n            if (log.isTraceEnabled())\n\n            boolean updated = false;\n\n            for (Map.Entry<Integer, GridDhtPartitionMap> entry : msg.partitions().entrySet()) {\n                Integer grpId = entry.getKey();\n\n                CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                if (grp != null &&\n                    grp.localStartVersion().compareTo(entry.getValue().topologyVersion()) > 0)\n                    continue;\n\n                GridDhtPartitionTopology top = null;\n\n                if (grp == null)\n                    top = clientTops.get(grpId);\n                else if (!grp.isLocal())\n                    top = grp.topology();\n\n                if (top != null) {\n                    updated |= top.update(null, entry.getValue(), false);\n\n                    cctx.affinity().checkRebalanceState(top, grpId);\n                }\n            }\n\n            if (updated) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Partitions have been scheduled to resend [reason=Single update from \" + node.id() + \"]\");\n\n                scheduleResendPartitions();\n            }\n        }\n        else {\n            GridDhtPartitionsExchangeFuture exchFut = exchangeFuture(msg.exchangeId());\n\n            if (log.isTraceEnabled())\n                log.trace(\"Notifying exchange future about single message: \" + exchFut);\n\n            if (msg.client()) {\n                AffinityTopologyVersion initVer = exchFut.initialVersion();\n                AffinityTopologyVersion readyVer = readyAffinityVersion();\n\n                if (initVer.compareTo(readyVer) < 0 && !exchFut.isDone()) {\n                    U.warn(log, \"Client node tries to connect but its exchange \" +\n                        \"info is cleaned up from exchange history. \" +\n                        \"Consider increasing 'IGNITE_EXCHANGE_HISTORY_SIZE' property \" +\n                        \"or start clients in  smaller batches. \" +\n                        \"Current settings and versions: \" +\n                        \"[IGNITE_EXCHANGE_HISTORY_SIZE=\" + EXCHANGE_HISTORY_SIZE + \", \" +\n                        \"initVer=\" + initVer + \", \" +\n                        \"readyVer=\" + readyVer + \"].\"\n                    );\n\n                    exchFut.forceClientReconnect(node, msg);\n\n                    return;\n                }\n            }\n\n            exchFut.onReceiveSingleMessage(node, msg);\n        }\n    }\n    finally {\n        leaveBusy();\n    }\n}",
        "accept_response": "private void processSinglePartitionUpdate(final ClusterNode node, final GridDhtPartitionsSingleMessage msg) {\n    if (!enterBusy())\n        return;\n\n    try {\n        if (msg.exchangeId() == null) {\n            if (log.isTraceEnabled())\n                log.trace(\"Received local partition update [nodeId=\" + node.id() + \", parts=\" +\n                    msg + ']');\n\n            boolean updated = false;\n\n            for (Map.Entry<Integer, GridDhtPartitionMap> entry : msg.partitions().entrySet()) {\n                Integer grpId = entry.getKey();\n\n                CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                if (grp != null &&\n                    grp.localStartVersion().compareTo(entry.getValue().topologyVersion()) > 0)\n                    continue;\n\n                GridDhtPartitionTopology top = null;\n\n                if (grp == null)\n                    top = clientTops.get(grpId);\n                else if (!grp.isLocal())\n                    top = grp.topology();\n\n                if (top != null) {\n                    updated |= top.update(null, entry.getValue(), false);\n\n                    cctx.affinity().checkRebalanceState(top, grpId);\n                }\n            }\n\n            if (updated) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Partitions have been scheduled to resend [reason=Single update from \" + node.id() + \"]\");\n\n                scheduleResendPartitions();\n            }\n        }\n        else {\n            GridDhtPartitionsExchangeFuture exchFut = exchangeFuture(msg.exchangeId());\n\n            if (log.isTraceEnabled())\n                log.trace(\"Notifying exchange future about single message: \" + exchFut);\n\n            if (msg.client()) {\n                AffinityTopologyVersion initVer = exchFut.initialVersion();\n                AffinityTopologyVersion readyVer = readyAffinityVersion();\n\n                if (initVer.compareTo(readyVer) < 0 && !exchFut.isDone()) {\n                    U.warn(log, \"Client node tries to connect but its exchange \" +\n                        \"info is cleaned up from exchange history. \" +\n                        \"Consider increasing 'IGNITE_EXCHANGE_HISTORY_SIZE' property \" +\n                        \"or start clients in  smaller batches. \" +\n                        \"Current settings and versions: \" +\n                        \"[IGNITE_EXCHANGE_HISTORY_SIZE=\" + EXCHANGE_HISTORY_SIZE + \", \" +\n                        \"initVer=\" + initVer + \", \" +\n                        \"readyVer=\" + readyVer + \"].\"\n                    );\n\n                    exchFut.forceClientReconnect(node, msg);\n\n                    return;\n                }\n            }\n\n            exchFut.onReceiveSingleMessage(node, msg);\n        }\n    }\n    finally {\n        leaveBusy();\n    }\n}",
        "reject_response": "private void processSinglePartitionUpdate(final ClusterNode node, final GridDhtPartitionsSingleMessage msg) {\n    if (!enterBusy())\n        return;\n\n    try {\n        if (msg.exchangeId() == null) {\n            if (log.isTraceEnabled())\n            if (log.isDebugEnabled())\n                log.debug(\"Received local partition update [nodeId=\" + node.id() + \", parts=\" +\n\n            boolean updated = false;\n\n            for (Map.Entry<Integer, GridDhtPartitionMap> entry : msg.partitions().entrySet()) {\n                Integer grpId = entry.getKey();\n\n                CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                if (grp != null &&\n                    grp.localStartVersion().compareTo(entry.getValue().topologyVersion()) > 0)\n                    continue;\n\n                GridDhtPartitionTopology top = null;\n\n                if (grp == null)\n                    top = clientTops.get(grpId);\n                else if (!grp.isLocal())\n                    top = grp.topology();\n\n                if (top != null) {\n                    updated |= top.update(null, entry.getValue(), false);\n\n                    cctx.affinity().checkRebalanceState(top, grpId);\n                }\n            }\n\n            if (updated) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Partitions have been scheduled to resend [reason=Single update from \" + node.id() + \"]\");\n\n                scheduleResendPartitions();\n            }\n        }\n        else {\n            GridDhtPartitionsExchangeFuture exchFut = exchangeFuture(msg.exchangeId());\n\n            if (log.isTraceEnabled())\n                log.trace(\"Notifying exchange future about single message: \" + exchFut);\n\n            if (msg.client()) {\n                AffinityTopologyVersion initVer = exchFut.initialVersion();\n                AffinityTopologyVersion readyVer = readyAffinityVersion();\n\n                if (initVer.compareTo(readyVer) < 0 && !exchFut.isDone()) {\n                    U.warn(log, \"Client node tries to connect but its exchange \" +\n                        \"info is cleaned up from exchange history. \" +\n                        \"Consider increasing 'IGNITE_EXCHANGE_HISTORY_SIZE' property \" +\n                        \"or start clients in  smaller batches. \" +\n                        \"Current settings and versions: \" +\n                        \"[IGNITE_EXCHANGE_HISTORY_SIZE=\" + EXCHANGE_HISTORY_SIZE + \", \" +\n                        \"initVer=\" + initVer + \", \" +\n                        \"readyVer=\" + readyVer + \"].\"\n                    );\n\n                    exchFut.forceClientReconnect(node, msg);\n\n                    return;\n                }\n            }\n\n            exchFut.onReceiveSingleMessage(node, msg);\n        }\n    }\n    finally {\n        leaveBusy();\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2646,
        "instruction": "public void onLogoutSuccess(HttpServletRequest request, HttpServletResponse response,\n    Authentication authentication) throws IOException, ServletException {\n  logger.debug(\"Invoked #LogoutHandler ...\");\n  if (Repository.get().isUseGemFireCredentials()) {\n    GemFireAuthentication gemauthentication = (GemFireAuthentication) authentication;\n    if (gemauthentication != null) {\n      gemauthentication.getJmxc().close();\n    }\n  }\n  super.onLogoutSuccess(request, response, authentication);\n}",
        "accept_response": "public void onLogoutSuccess(HttpServletRequest request, HttpServletResponse response,\n    Authentication authentication) throws IOException, ServletException {\n  logger.debug(\"Invoked #LogoutHandler ...\");\n  if (Repository.get().isUseGemFireCredentials()) {\n    GemFireAuthentication gemauthentication = (GemFireAuthentication) authentication;\n    if (gemauthentication != null) {\n      gemauthentication.getJmxc().close();\n      logger.info(\"#LogoutHandler : Closing GemFireAuthentication JMX Connection...\");\n    }\n  }\n  super.onLogoutSuccess(request, response, authentication);\n}",
        "reject_response": "public void onLogoutSuccess(HttpServletRequest request, HttpServletResponse response,\n    Authentication authentication) throws IOException, ServletException {\n  logger.debug(\"Invoked #LogoutHandler ...\");\n  if (Repository.get().isUseGemFireCredentials()) {\n    GemFireAuthentication gemauthentication = (GemFireAuthentication) authentication;\n    if (gemauthentication != null) {\n      gemauthentication.getJmxc().close();\n      LOGGER.info(\"#LogoutHandler : Closing GemFireAuthentication JMX Connection...\");\n    }\n  }\n  super.onLogoutSuccess(request, response, authentication);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2766,
        "instruction": "@Override\npublic void releaseEndpoint() {\n    final AsyncConnectionEndpoint endpoint = endpointRef.getAndSet(null);\n    if (endpoint != null) {\n        if (reusable) {\n            manager.release(endpoint, state, validDuration);\n        } else {\n            discardEndpoint(endpoint);\n        }\n    }\n}",
        "accept_response": "@Override\npublic void releaseEndpoint() {\n    final AsyncConnectionEndpoint endpoint = endpointRef.getAndSet(null);\n    if (endpoint != null) {\n        if (reusable) {\n            if (log.isDebugEnabled()) {\n                log.debug(\"{}: releasing valid endpoint\", ConnPoolSupport.getId(endpoint));\n            }\n            manager.release(endpoint, state, validDuration);\n        } else {\n            discardEndpoint(endpoint);\n        }\n    }\n}",
        "reject_response": "@Override\npublic void releaseEndpoint() {\n    final AsyncConnectionEndpoint endpoint = endpointRef.getAndSet(null);\n    if (endpoint != null) {\n        if (reusable) {\n            if (log.isDebugEnabled()) {\n                log.debug(ConnPoolSupport.getId(endpoint) + \": releasing valid endpoint\");\n            }\n            manager.release(endpoint, state, validDuration);\n        } else {\n            discardEndpoint(endpoint);\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2835,
        "instruction": "private SegmentArchiveResult archiveSegment(long absIdx) throws IgniteCheckedException {\n    long segIdx = absIdx % dsCfg.getWalSegments();\n\n    File origFile = new File(walWorkDir, FileDescriptor.fileName(segIdx));\n\n    String name = FileDescriptor.fileName(absIdx);\n\n    File dstTmpFile = new File(walArchiveDir, name + \".tmp\");\n\n    File dstFile = new File(walArchiveDir, name);\n\n    if (log.isInfoEnabled())\n        log.info(\"Starting to copy WAL segment [absIdx=\" + absIdx + \", segIdx=\" + segIdx +\n            \", origFile=\" + origFile.getAbsolutePath() + \", dstFile=\" + dstFile.getAbsolutePath() + ']');\n\n    try {\n        Files.deleteIfExists(dstTmpFile.toPath());\n\n        Files.copy(origFile.toPath(), dstTmpFile.toPath());\n\n        Files.move(dstTmpFile.toPath(), dstFile.toPath());\n\n        if (mode == WALMode.FSYNC) {\n            try (FileIO f0 = ioFactory.create(dstFile, CREATE, READ, WRITE)) {\n                f0.force();\n            }\n        }\n    }\n    catch (IOException e) {\n        throw new IgniteCheckedException(\"Failed to archive WAL segment [\" +\n            \"srcFile=\" + origFile.getAbsolutePath() +\n            \", dstFile=\" + dstTmpFile.getAbsolutePath() + ']', e);\n    }\n\n    if (log.isInfoEnabled())\n\n    return new SegmentArchiveResult(absIdx, origFile, dstFile);\n}",
        "accept_response": "private SegmentArchiveResult archiveSegment(long absIdx) throws IgniteCheckedException {\n    long segIdx = absIdx % dsCfg.getWalSegments();\n\n    File origFile = new File(walWorkDir, FileDescriptor.fileName(segIdx));\n\n    String name = FileDescriptor.fileName(absIdx);\n\n    File dstTmpFile = new File(walArchiveDir, name + \".tmp\");\n\n    File dstFile = new File(walArchiveDir, name);\n\n    if (log.isInfoEnabled())\n        log.info(\"Starting to copy WAL segment [absIdx=\" + absIdx + \", segIdx=\" + segIdx +\n            \", origFile=\" + origFile.getAbsolutePath() + \", dstFile=\" + dstFile.getAbsolutePath() + ']');\n\n    try {\n        Files.deleteIfExists(dstTmpFile.toPath());\n\n        Files.copy(origFile.toPath(), dstTmpFile.toPath());\n\n        Files.move(dstTmpFile.toPath(), dstFile.toPath());\n\n        if (mode == WALMode.FSYNC) {\n            try (FileIO f0 = ioFactory.create(dstFile, CREATE, READ, WRITE)) {\n                f0.force();\n            }\n        }\n    }\n    catch (IOException e) {\n        throw new IgniteCheckedException(\"Failed to archive WAL segment [\" +\n            \"srcFile=\" + origFile.getAbsolutePath() +\n            \", dstFile=\" + dstTmpFile.getAbsolutePath() + ']', e);\n    }\n\n    if (log.isInfoEnabled())\n        log.info(\"Copied file [src=\" + origFile.getAbsolutePath() +\n            \", dst=\" + dstFile.getAbsolutePath() + ']');\n\n    return new SegmentArchiveResult(absIdx, origFile, dstFile);\n}",
        "reject_response": "private SegmentArchiveResult archiveSegment(long absIdx) throws IgniteCheckedException {\n    long segIdx = absIdx % dsCfg.getWalSegments();\n\n    File origFile = new File(walWorkDir, FileDescriptor.fileName(segIdx));\n\n    String name = FileDescriptor.fileName(absIdx);\n\n    File dstTmpFile = new File(walArchiveDir, name + \".tmp\");\n\n    File dstFile = new File(walArchiveDir, name);\n\n    if (log.isInfoEnabled())\n        log.info(\"Starting to copy WAL segment [absIdx=\" + absIdx + \", segIdx=\" + segIdx +\n            \", origFile=\" + origFile.getAbsolutePath() + \", dstFile=\" + dstFile.getAbsolutePath() + ']');\n\n    try {\n        Files.deleteIfExists(dstTmpFile.toPath());\n\n        Files.copy(origFile.toPath(), dstTmpFile.toPath());\n\n        Files.move(dstTmpFile.toPath(), dstFile.toPath());\n\n        if (mode == WALMode.FSYNC) {\n            try (FileIO f0 = ioFactory.create(dstFile, CREATE, READ, WRITE)) {\n                f0.force();\n            }\n        }\n    }\n    catch (IOException e) {\n        throw new IgniteCheckedException(\"Failed to archive WAL segment [\" +\n            \"srcFile=\" + origFile.getAbsolutePath() +\n            \", dstFile=\" + dstTmpFile.getAbsolutePath() + ']', e);\n    }\n\n    if (log.isInfoEnabled())\n    if (log.isDebugEnabled())\n        log.debug(\"Copied file [src=\" + origFile.getAbsolutePath() +\n\n    return new SegmentArchiveResult(absIdx, origFile, dstFile);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2392,
        "instruction": "@Override\npublic void invalidateCache() {\n  int invalidatedCount;\n  wLock.lock();\n  try {\n    invalidatedCount = metaCache.size();\n    metaCache.clear();\n  } finally {\n    wLock.unlock();\n  }\n  if (log.isTraceEnabled())\n}",
        "accept_response": "@Override\npublic void invalidateCache() {\n  int invalidatedCount;\n  wLock.lock();\n  try {\n    invalidatedCount = metaCache.size();\n    metaCache.clear();\n  } finally {\n    wLock.unlock();\n  }\n  if (log.isTraceEnabled())\n    log.trace(\"invalidated all {} cache entries for table={}\", invalidatedCount, tableId);\n}",
        "reject_response": "@Override\npublic void invalidateCache() {\n  int invalidatedCount;\n  wLock.lock();\n  try {\n    invalidatedCount = metaCache.size();\n    metaCache.clear();\n  } finally {\n    wLock.unlock();\n  }\n  if (log.isTraceEnabled())\n    log.trace(\"invalidated all \" + invalidatedCount + \" cache entries for table=\" + tableId);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3163,
        "instruction": "public final void internalRenderComponent()\n{\n\tfinal IMarkupFragment markup = getMarkup();\n\tif (markup == null)\n\t{\n\t\tthrow new MarkupException(\"Markup not found. Component: \" + toString());\n\t}\n\n\tfinal MarkupStream markupStream = new MarkupStream(markup);\n\n\t// Get mutable copy of next tag\n\tfinal ComponentTag openTag = markupStream.getTag();\n\tfinal ComponentTag tag = openTag.mutable();\n\n\t// Call any tag handler\n\tonComponentTag(tag);\n\n\t// If we're an openclose tag\n\tif (!tag.isOpenClose() && !tag.isOpen())\n\t{\n\t\t// We were something other than <tag> or <tag/>\n\t\tmarkupStream.throwMarkupException(\"Method renderComponent called on bad markup element: \" +\n\t\t\ttag);\n\t}\n\n\tif (tag.isOpenClose() && openTag.isOpen())\n\t{\n\t\tmarkupStream.throwMarkupException(\"You can not modify a open tag to open-close: \" + tag);\n\t}\n\n\ttry\n\t{\n\t\t// Render open tag\n\t\tboolean renderBodyOnly = getRenderBodyOnly();\n\t\tif (renderBodyOnly)\n\t\t{\n\t\t\tExceptionSettings.NotRenderableErrorStrategy notRenderableErrorStrategy = ExceptionSettings.NotRenderableErrorStrategy.LOG_WARNING;\n\t\t\tif (Application.exists())\n\t\t\t{\n\t\t\t\tnotRenderableErrorStrategy = getApplication().getExceptionSettings().getNotRenderableErrorStrategy();\n\t\t\t}\n\n\t\t\tif (getFlag(FLAG_OUTPUT_MARKUP_ID))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Markup id set on a component that renders its body only. \" +\n\t\t\t\t                               \"Markup id: %s, component id: %s.\", getMarkupId(), getId());\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t\tif (getFlag(FLAG_PLACEHOLDER))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Placeholder tag set on a component that renders its body only. \" +\n\t\t\t\t                               \"Component id: %s.\", getId());\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\telse\n\t\t{\n\t\t\trenderComponentTag(tag);\n\t\t}\n\t\tmarkupStream.next();\n\n\t\t// Render the body only if open-body-close. Do not render if open-close.\n\t\tif (tag.isOpen())\n\t\t{\n\t\t\t// Render the body. The default strategy will simply call the component's\n\t\t\t// onComponentTagBody() implementation.\n\t\t\tgetMarkupSourcingStrategy().onComponentTagBody(this, markupStream, tag);\n\n\t\t\t// Render close tag\n\t\t\tif (openTag.isOpen())\n\t\t\t{\n\t\t\t\trenderClosingComponentTag(markupStream, tag, renderBodyOnly);\n\t\t\t}\n\t\t\telse if (renderBodyOnly == false)\n\t\t\t{\n\t\t\t\tif (needToRenderTag(openTag))\n\t\t\t\t{\n\t\t\t\t\t// Close the manually opened tag. And since the user might have changed the\n\t\t\t\t\t// tag name ...\n\t\t\t\t\tgetResponse().write(tag.syntheticCloseTagString());\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tcatch (WicketRuntimeException wre)\n\t{\n\t\tthrow wre;\n\t}\n\tcatch (RuntimeException re)\n\t{\n\t\tthrow new WicketRuntimeException(\"Exception in rendering component: \" + this, re);\n\t}\n}",
        "accept_response": "public final void internalRenderComponent()\n{\n\tfinal IMarkupFragment markup = getMarkup();\n\tif (markup == null)\n\t{\n\t\tthrow new MarkupException(\"Markup not found. Component: \" + toString());\n\t}\n\n\tfinal MarkupStream markupStream = new MarkupStream(markup);\n\n\t// Get mutable copy of next tag\n\tfinal ComponentTag openTag = markupStream.getTag();\n\tfinal ComponentTag tag = openTag.mutable();\n\n\t// Call any tag handler\n\tonComponentTag(tag);\n\n\t// If we're an openclose tag\n\tif (!tag.isOpenClose() && !tag.isOpen())\n\t{\n\t\t// We were something other than <tag> or <tag/>\n\t\tmarkupStream.throwMarkupException(\"Method renderComponent called on bad markup element: \" +\n\t\t\ttag);\n\t}\n\n\tif (tag.isOpenClose() && openTag.isOpen())\n\t{\n\t\tmarkupStream.throwMarkupException(\"You can not modify a open tag to open-close: \" + tag);\n\t}\n\n\ttry\n\t{\n\t\t// Render open tag\n\t\tboolean renderBodyOnly = getRenderBodyOnly();\n\t\tif (renderBodyOnly)\n\t\t{\n\t\t\tExceptionSettings.NotRenderableErrorStrategy notRenderableErrorStrategy = ExceptionSettings.NotRenderableErrorStrategy.LOG_WARNING;\n\t\t\tif (Application.exists())\n\t\t\t{\n\t\t\t\tnotRenderableErrorStrategy = getApplication().getExceptionSettings().getNotRenderableErrorStrategy();\n\t\t\t}\n\n\t\t\tif (getFlag(FLAG_OUTPUT_MARKUP_ID))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Markup id set on a component that renders its body only. \" +\n\t\t\t\t                               \"Markup id: %s, component id: %s.\", getMarkupId(), getId());\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t\tif (getFlag(FLAG_PLACEHOLDER))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Placeholder tag set on a component that renders its body only. \" +\n\t\t\t\t                               \"Component id: %s.\", getId());\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t}\n\t\telse\n\t\t{\n\t\t\trenderComponentTag(tag);\n\t\t}\n\t\tmarkupStream.next();\n\n\t\t// Render the body only if open-body-close. Do not render if open-close.\n\t\tif (tag.isOpen())\n\t\t{\n\t\t\t// Render the body. The default strategy will simply call the component's\n\t\t\t// onComponentTagBody() implementation.\n\t\t\tgetMarkupSourcingStrategy().onComponentTagBody(this, markupStream, tag);\n\n\t\t\t// Render close tag\n\t\t\tif (openTag.isOpen())\n\t\t\t{\n\t\t\t\trenderClosingComponentTag(markupStream, tag, renderBodyOnly);\n\t\t\t}\n\t\t\telse if (renderBodyOnly == false)\n\t\t\t{\n\t\t\t\tif (needToRenderTag(openTag))\n\t\t\t\t{\n\t\t\t\t\t// Close the manually opened tag. And since the user might have changed the\n\t\t\t\t\t// tag name ...\n\t\t\t\t\tgetResponse().write(tag.syntheticCloseTagString());\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tcatch (WicketRuntimeException wre)\n\t{\n\t\tthrow wre;\n\t}\n\tcatch (RuntimeException re)\n\t{\n\t\tthrow new WicketRuntimeException(\"Exception in rendering component: \" + this, re);\n\t}\n}",
        "reject_response": "public final void internalRenderComponent()\n{\n\tfinal IMarkupFragment markup = getMarkup();\n\tif (markup == null)\n\t{\n\t\tthrow new MarkupException(\"Markup not found. Component: \" + toString());\n\t}\n\n\tfinal MarkupStream markupStream = new MarkupStream(markup);\n\n\t// Get mutable copy of next tag\n\tfinal ComponentTag openTag = markupStream.getTag();\n\tfinal ComponentTag tag = openTag.mutable();\n\n\t// Call any tag handler\n\tonComponentTag(tag);\n\n\t// If we're an openclose tag\n\tif (!tag.isOpenClose() && !tag.isOpen())\n\t{\n\t\t// We were something other than <tag> or <tag/>\n\t\tmarkupStream.throwMarkupException(\"Method renderComponent called on bad markup element: \" +\n\t\t\ttag);\n\t}\n\n\tif (tag.isOpenClose() && openTag.isOpen())\n\t{\n\t\tmarkupStream.throwMarkupException(\"You can not modify a open tag to open-close: \" + tag);\n\t}\n\n\ttry\n\t{\n\t\t// Render open tag\n\t\tboolean renderBodyOnly = getRenderBodyOnly();\n\t\tif (renderBodyOnly)\n\t\t{\n\t\t\tExceptionSettings.NotRenderableErrorStrategy notRenderableErrorStrategy = ExceptionSettings.NotRenderableErrorStrategy.LOG_WARNING;\n\t\t\tif (Application.exists())\n\t\t\t{\n\t\t\t\tnotRenderableErrorStrategy = getApplication().getExceptionSettings().getNotRenderableErrorStrategy();\n\t\t\t}\n\n\t\t\tif (getFlag(FLAG_OUTPUT_MARKUP_ID))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Markup id set on a component that renders its body only. \" +\n\t\t\t\t                               \"Markup id: %s, component id: %s.\", getMarkupId(), getId());\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t\tif (getFlag(FLAG_PLACEHOLDER))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Placeholder tag set on a component that renders its body only. \" +\n\t\t\t\t                               \"Component id: %s.\", getId());\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(String.format(\n\t\t\t\t\t\"Placeholder tag set on a component that renders its body only. \"\n\t\t\t\t\t\t+ \"Component id: %s.\", getId()));\n\t\t\t}\n\t\t}\n\t\telse\n\t\t{\n\t\t\trenderComponentTag(tag);\n\t\t}\n\t\tmarkupStream.next();\n\n\t\t// Render the body only if open-body-close. Do not render if open-close.\n\t\tif (tag.isOpen())\n\t\t{\n\t\t\t// Render the body. The default strategy will simply call the component's\n\t\t\t// onComponentTagBody() implementation.\n\t\t\tgetMarkupSourcingStrategy().onComponentTagBody(this, markupStream, tag);\n\n\t\t\t// Render close tag\n\t\t\tif (openTag.isOpen())\n\t\t\t{\n\t\t\t\trenderClosingComponentTag(markupStream, tag, renderBodyOnly);\n\t\t\t}\n\t\t\telse if (renderBodyOnly == false)\n\t\t\t{\n\t\t\t\tif (needToRenderTag(openTag))\n\t\t\t\t{\n\t\t\t\t\t// Close the manually opened tag. And since the user might have changed the\n\t\t\t\t\t// tag name ...\n\t\t\t\t\tgetResponse().write(tag.syntheticCloseTagString());\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tcatch (WicketRuntimeException wre)\n\t{\n\t\tthrow wre;\n\t}\n\tcatch (RuntimeException re)\n\t{\n\t\tthrow new WicketRuntimeException(\"Exception in rendering component: \" + this, re);\n\t}\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3226,
        "instruction": "private void sendSaslPacket(ClientCnxn cnxn) throws SaslException {\n\n    GetSASLRequest request = new GetSASLRequest();\n    request.setToken(createSaslToken());\n    SetSASLResponse response = new SetSASLResponse();\n    ServerSaslResponseCallback cb = new ServerSaslResponseCallback();\n    try {\n        cnxn.sendPacket(request,response,cb, ZooDefs.OpCode.sasl);\n    } catch (IOException e) {\n        throw new SaslException(\"Failed to send SASL packet to server due \" +\n          \"to IOException:\", e);\n    }\n}",
        "accept_response": "private void sendSaslPacket(ClientCnxn cnxn) throws SaslException {\n    LOG.debug(\"ClientCnxn:sendSaslPacket:length={}\", saslToken.length);\n\n    GetSASLRequest request = new GetSASLRequest();\n    request.setToken(createSaslToken());\n    SetSASLResponse response = new SetSASLResponse();\n    ServerSaslResponseCallback cb = new ServerSaslResponseCallback();\n    try {\n        cnxn.sendPacket(request,response,cb, ZooDefs.OpCode.sasl);\n    } catch (IOException e) {\n        throw new SaslException(\"Failed to send SASL packet to server due \" +\n          \"to IOException:\", e);\n    }\n}",
        "reject_response": "private void sendSaslPacket(ClientCnxn cnxn) throws SaslException {\n    if (LOG.isDebugEnabled()) {\n        LOG.debug(\"ClientCnxn:sendSaslPacket:length=\"+saslToken.length);\n    }\n\n    GetSASLRequest request = new GetSASLRequest();\n    request.setToken(createSaslToken());\n    SetSASLResponse response = new SetSASLResponse();\n    ServerSaslResponseCallback cb = new ServerSaslResponseCallback();\n    try {\n        cnxn.sendPacket(request,response,cb, ZooDefs.OpCode.sasl);\n    } catch (IOException e) {\n        throw new SaslException(\"Failed to send SASL packet to server due \" +\n          \"to IOException:\", e);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2742,
        "instruction": "@Override\npublic void selectInputStreams(Collection<EditLogInputStream> streams,\n    long fromTxnId, boolean inProgressOk,\n    boolean onlyDurableTxns) throws IOException {\n  // Some calls will use inProgressOK to get in-progress edits even if\n  // the cache used for RPC calls is not enabled; fall back to using the\n  // streaming mechanism to serve such requests\n  if (inProgressOk && inProgressTailingEnabled) {\n    try {\n      Collection<EditLogInputStream> rpcStreams = new ArrayList<>();\n      selectRpcInputStreams(rpcStreams, fromTxnId, onlyDurableTxns);\n      streams.addAll(rpcStreams);\n      return;\n    } catch (IOException ioe) {\n      LOG.warn(\"Encountered exception while tailing edits >= \" + fromTxnId +\n          \" via RPC; falling back to streaming.\", ioe);\n    }\n  }\n  selectStreamingInputStreams(streams, fromTxnId, inProgressOk,\n      onlyDurableTxns);\n}",
        "accept_response": "@Override\npublic void selectInputStreams(Collection<EditLogInputStream> streams,\n    long fromTxnId, boolean inProgressOk,\n    boolean onlyDurableTxns) throws IOException {\n  // Some calls will use inProgressOK to get in-progress edits even if\n  // the cache used for RPC calls is not enabled; fall back to using the\n  // streaming mechanism to serve such requests\n  if (inProgressOk && inProgressTailingEnabled) {\n    LOG.debug(\"Tailing edits starting from txn ID {} via RPC mechanism\", fromTxnId);\n    try {\n      Collection<EditLogInputStream> rpcStreams = new ArrayList<>();\n      selectRpcInputStreams(rpcStreams, fromTxnId, onlyDurableTxns);\n      streams.addAll(rpcStreams);\n      return;\n    } catch (IOException ioe) {\n      LOG.warn(\"Encountered exception while tailing edits >= \" + fromTxnId +\n          \" via RPC; falling back to streaming.\", ioe);\n    }\n  }\n  selectStreamingInputStreams(streams, fromTxnId, inProgressOk,\n      onlyDurableTxns);\n}",
        "reject_response": "@Override\npublic void selectInputStreams(Collection<EditLogInputStream> streams,\n    long fromTxnId, boolean inProgressOk,\n    boolean onlyDurableTxns) throws IOException {\n  // Some calls will use inProgressOK to get in-progress edits even if\n  // the cache used for RPC calls is not enabled; fall back to using the\n  // streaming mechanism to serve such requests\n  if (inProgressOk && inProgressTailingEnabled) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Tailing edits starting from txn ID \" + fromTxnId +\n          \" via RPC mechanism\");\n    }\n    try {\n      Collection<EditLogInputStream> rpcStreams = new ArrayList<>();\n      selectRpcInputStreams(rpcStreams, fromTxnId, onlyDurableTxns);\n      streams.addAll(rpcStreams);\n      return;\n    } catch (IOException ioe) {\n      LOG.warn(\"Encountered exception while tailing edits >= \" + fromTxnId +\n          \" via RPC; falling back to streaming.\", ioe);\n    }\n  }\n  selectStreamingInputStreams(streams, fromTxnId, inProgressOk,\n      onlyDurableTxns);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2860,
        "instruction": "private void provisionInbox(ImapSession session, MailboxManager mailboxManager, MailboxSession mailboxSession) throws MailboxException {\n    final MailboxPath inboxPath = PathConverter.forSession(session).buildFullPath(MailboxConstants.INBOX);\n    if (mailboxManager.mailboxExists(inboxPath, mailboxSession)) {\n        LOGGER.debug(\"INBOX exists. No need to create it.\");\n    } else {\n        try {\n            Optional<MailboxId> mailboxId = mailboxManager.createMailbox(inboxPath, mailboxSession);\n        } catch (MailboxExistsException e) {\n            LOGGER.warn(\"Mailbox INBOX created by concurrent call. Safe to ignore this exception.\");\n        }\n    }\n}",
        "accept_response": "private void provisionInbox(ImapSession session, MailboxManager mailboxManager, MailboxSession mailboxSession) throws MailboxException {\n    final MailboxPath inboxPath = PathConverter.forSession(session).buildFullPath(MailboxConstants.INBOX);\n    if (mailboxManager.mailboxExists(inboxPath, mailboxSession)) {\n        LOGGER.debug(\"INBOX exists. No need to create it.\");\n    } else {\n        try {\n            Optional<MailboxId> mailboxId = mailboxManager.createMailbox(inboxPath, mailboxSession);\n            OptionalUtils.executeIfEmpty(mailboxId, () -> LOGGER.warn(\"Provisioning INBOX successful. But no MailboxId have been returned.\"))\n                .ifPresent(id -> LOGGER.info(\"Provisioning INBOX. {} created.\", id));\n        } catch (MailboxExistsException e) {\n            LOGGER.warn(\"Mailbox INBOX created by concurrent call. Safe to ignore this exception.\");\n        }\n    }\n}",
        "reject_response": "private void provisionInbox(ImapSession session, MailboxManager mailboxManager, MailboxSession mailboxSession) throws MailboxException {\n    final MailboxPath inboxPath = PathConverter.forSession(session).buildFullPath(MailboxConstants.INBOX);\n    if (mailboxManager.mailboxExists(inboxPath, mailboxSession)) {\n        LOGGER.debug(\"INBOX exists. No need to create it.\");\n    } else {\n        try {\n            Optional<MailboxId> mailboxId = mailboxManager.createMailbox(inboxPath, mailboxSession);\n            LOGGER.info(\"Provisioning INBOX. {} created.\", mailboxId);\n        } catch (MailboxExistsException e) {\n            LOGGER.warn(\"Mailbox INBOX created by concurrent call. Safe to ignore this exception.\");\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2530,
        "instruction": "private void keepAliveListener(Future<? super Void> future)\n{\n    if (future.isSuccess() || future.isCancelled())\n        return;\n\n    if (logger.isDebugEnabled())\n}",
        "accept_response": "private void keepAliveListener(Future<? super Void> future)\n{\n    if (future.isSuccess() || future.isCancelled())\n        return;\n\n    if (logger.isDebugEnabled())\n    \tlogger.debug(\"{} Could not send keep-alive message (perhaps stream session is finished?).\",\n                 createLogTag(session, channel), future.cause());\n}",
        "reject_response": "private void keepAliveListener(Future<? super Void> future)\n{\n    if (future.isSuccess() || future.isCancelled())\n        return;\n\n    if (logger.isDebugEnabled())\n    logger.debug(\"{} Could not send keep-alive message (perhaps stream session is finished?).\",\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3062,
        "instruction": "@Override\npublic void handleRequestBody(SolrQueryRequest req, SolrQueryResponse rsp) throws Exception {\n  // Make sure the cores is enabled\n  CoreContainer cores = checkErrors();\n\n  // Pick the action\n  SolrParams params = req.getParams();\n  String a = params.get(CoreAdminParams.ACTION);\n  if (a != null) {\n    CollectionAction action = CollectionAction.get(a);\n    if (action == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown action: \" + a);\n    }\n    CollectionOperation operation = CollectionOperation.get(action);\n    MDCLoggingContext.setCollection(req.getParams().get(COLLECTION));\n    invokeAction(req, rsp, cores, action, operation);\n  } else {\n    throw new SolrException(ErrorCode.BAD_REQUEST, \"action is a required param\");\n  }\n  rsp.setHttpCaching(false);\n}",
        "accept_response": "@Override\npublic void handleRequestBody(SolrQueryRequest req, SolrQueryResponse rsp) throws Exception {\n  // Make sure the cores is enabled\n  CoreContainer cores = checkErrors();\n\n  // Pick the action\n  SolrParams params = req.getParams();\n  String a = params.get(CoreAdminParams.ACTION);\n  if (a != null) {\n    CollectionAction action = CollectionAction.get(a);\n    if (action == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown action: \" + a);\n    }\n    CollectionOperation operation = CollectionOperation.get(action);\n    if (log.isDebugEnabled()) {\n      log.debug(\"Invoked Collection Action :{} with params {} and sendToOCPQueue={}\"\n          , action.toLower(), req.getParamString(), operation.sendToOCPQueue);\n    }\n    MDCLoggingContext.setCollection(req.getParams().get(COLLECTION));\n    invokeAction(req, rsp, cores, action, operation);\n  } else {\n    throw new SolrException(ErrorCode.BAD_REQUEST, \"action is a required param\");\n  }\n  rsp.setHttpCaching(false);\n}",
        "reject_response": "@Override\npublic void handleRequestBody(SolrQueryRequest req, SolrQueryResponse rsp) throws Exception {\n  // Make sure the cores is enabled\n  CoreContainer cores = checkErrors();\n\n  // Pick the action\n  SolrParams params = req.getParams();\n  String a = params.get(CoreAdminParams.ACTION);\n  if (a != null) {\n    CollectionAction action = CollectionAction.get(a);\n    if (action == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown action: \" + a);\n    }\n    CollectionOperation operation = CollectionOperation.get(action);\n    if (log.isDebugEnabled()) {\n    if (log.isInfoEnabled()) {\n      log.info(\"Invoked Collection Action :{} with params {} and sendToOCPQueue={}\"\n    }\n    MDCLoggingContext.setCollection(req.getParams().get(COLLECTION));\n    invokeAction(req, rsp, cores, action, operation);\n  } else {\n    throw new SolrException(ErrorCode.BAD_REQUEST, \"action is a required param\");\n  }\n  rsp.setHttpCaching(false);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2614,
        "instruction": "private void discardCheckpoint(PendingCheckpoint pendingCheckpoint, @Nullable Throwable cause) {\n\tassert(Thread.holdsLock(lock));\n\tPreconditions.checkNotNull(pendingCheckpoint);\n\n\tfinal long checkpointId = pendingCheckpoint.getCheckpointId();\n\n\tfinal String reason = (cause != null) ? cause.getMessage() : \"\";\n\n\n\tpendingCheckpoint.abortDeclined();\n\trememberRecentCheckpointId(checkpointId);\n\n\t// we don't have to schedule another \"dissolving\" checkpoint any more because the\n\t// cancellation barriers take care of breaking downstream alignments\n\t// we only need to make sure that suspended queued requests are resumed\n\n\tboolean haveMoreRecentPending = false;\n\tfor (PendingCheckpoint p : pendingCheckpoints.values()) {\n\t\tif (!p.isDiscarded() && p.getCheckpointId() >= pendingCheckpoint.getCheckpointId()) {\n\t\t\thaveMoreRecentPending = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!haveMoreRecentPending) {\n\t\ttriggerQueuedRequests();\n\t}\n}",
        "accept_response": "private void discardCheckpoint(PendingCheckpoint pendingCheckpoint, @Nullable Throwable cause) {\n\tassert(Thread.holdsLock(lock));\n\tPreconditions.checkNotNull(pendingCheckpoint);\n\n\tfinal long checkpointId = pendingCheckpoint.getCheckpointId();\n\n\tfinal String reason = (cause != null) ? cause.getMessage() : \"\";\n\n\tLOG.info(\"Discarding checkpoint {} of job {} because: {}\", checkpointId, job, reason);\n\n\tpendingCheckpoint.abortDeclined();\n\trememberRecentCheckpointId(checkpointId);\n\n\t// we don't have to schedule another \"dissolving\" checkpoint any more because the\n\t// cancellation barriers take care of breaking downstream alignments\n\t// we only need to make sure that suspended queued requests are resumed\n\n\tboolean haveMoreRecentPending = false;\n\tfor (PendingCheckpoint p : pendingCheckpoints.values()) {\n\t\tif (!p.isDiscarded() && p.getCheckpointId() >= pendingCheckpoint.getCheckpointId()) {\n\t\t\thaveMoreRecentPending = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!haveMoreRecentPending) {\n\t\ttriggerQueuedRequests();\n\t}\n}",
        "reject_response": "private void discardCheckpoint(PendingCheckpoint pendingCheckpoint, @Nullable Throwable cause) {\n\tassert(Thread.holdsLock(lock));\n\tPreconditions.checkNotNull(pendingCheckpoint);\n\n\tfinal long checkpointId = pendingCheckpoint.getCheckpointId();\n\n\tfinal String reason = (cause != null) ? cause.getMessage() : \"\";\n\n\tLOG.info(\"Discarding checkpoint {} because: {}\", checkpointId, reason);\n\n\tpendingCheckpoint.abortDeclined();\n\trememberRecentCheckpointId(checkpointId);\n\n\t// we don't have to schedule another \"dissolving\" checkpoint any more because the\n\t// cancellation barriers take care of breaking downstream alignments\n\t// we only need to make sure that suspended queued requests are resumed\n\n\tboolean haveMoreRecentPending = false;\n\tfor (PendingCheckpoint p : pendingCheckpoints.values()) {\n\t\tif (!p.isDiscarded() && p.getCheckpointId() >= pendingCheckpoint.getCheckpointId()) {\n\t\t\thaveMoreRecentPending = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!haveMoreRecentPending) {\n\t\ttriggerQueuedRequests();\n\t}\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2648,
        "instruction": "@Override\npublic ObjectNode execute(final HttpServletRequest request) throws Exception {\n  String userName = request.getUserPrincipal().getName();\n  String pulseData = request.getParameter(\"pulseData\");\n  JsonNode parameterMap = mapper.readTree(pulseData);\n  String selectedRegionFullPath =\n      parameterMap.get(\"ClusterSelectedRegionsMember\").get(\"regionFullPath\").textValue();\n\n  // get cluster object\n  Cluster cluster = Repository.get().getCluster();\n\n  // json object to be sent as response\n  ObjectNode responseJSON = mapper.createObjectNode();\n\n  // getting cluster's Regions\n  responseJSON.put(\"clusterName\", cluster.getServerName());\n  responseJSON.put(\"userName\", userName);\n  responseJSON.put(\"selectedRegionsMembers\",\n      getSelectedRegionsMembersJson(cluster, selectedRegionFullPath));\n  // Send json response\n  return responseJSON;\n}",
        "accept_response": "@Override\npublic ObjectNode execute(final HttpServletRequest request) throws Exception {\n  String userName = request.getUserPrincipal().getName();\n  String pulseData = request.getParameter(\"pulseData\");\n  JsonNode parameterMap = mapper.readTree(pulseData);\n  String selectedRegionFullPath =\n      parameterMap.get(\"ClusterSelectedRegionsMember\").get(\"regionFullPath\").textValue();\n  logger.trace(\"ClusterSelectedRegionsMemberService selectedRegionFullPath = {}\",\n      selectedRegionFullPath);\n\n  // get cluster object\n  Cluster cluster = Repository.get().getCluster();\n\n  // json object to be sent as response\n  ObjectNode responseJSON = mapper.createObjectNode();\n\n  // getting cluster's Regions\n  responseJSON.put(\"clusterName\", cluster.getServerName());\n  responseJSON.put(\"userName\", userName);\n  responseJSON.put(\"selectedRegionsMembers\",\n      getSelectedRegionsMembersJson(cluster, selectedRegionFullPath));\n  // Send json response\n  return responseJSON;\n}",
        "reject_response": "@Override\npublic ObjectNode execute(final HttpServletRequest request) throws Exception {\n  String userName = request.getUserPrincipal().getName();\n  String pulseData = request.getParameter(\"pulseData\");\n  JsonNode parameterMap = mapper.readTree(pulseData);\n  String selectedRegionFullPath =\n      parameterMap.get(\"ClusterSelectedRegionsMember\").get(\"regionFullPath\").textValue();\n  LOGGER.finest(\n      \"ClusterSelectedRegionsMemberService selectedRegionFullPath = \" + selectedRegionFullPath);\n\n  // get cluster object\n  Cluster cluster = Repository.get().getCluster();\n\n  // json object to be sent as response\n  ObjectNode responseJSON = mapper.createObjectNode();\n\n  // getting cluster's Regions\n  responseJSON.put(\"clusterName\", cluster.getServerName());\n  responseJSON.put(\"userName\", userName);\n  responseJSON.put(\"selectedRegionsMembers\",\n      getSelectedRegionsMembersJson(cluster, selectedRegionFullPath));\n  // Send json response\n  return responseJSON;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2716,
        "instruction": "static List<TableCoprocessorAttribute> getTableCoprocessorAttrsFromSchema(Configuration conf,\n    HTableDescriptor htd) {\n  List<TableCoprocessorAttribute> result = Lists.newArrayList();\n  for (Map.Entry<ImmutableBytesWritable, ImmutableBytesWritable> e: htd.getValues().entrySet()) {\n    String key = Bytes.toString(e.getKey().get()).trim();\n    if (HConstants.CP_HTD_ATTR_KEY_PATTERN.matcher(key).matches()) {\n      String spec = Bytes.toString(e.getValue().get()).trim();\n      // found one\n      try {\n        Matcher matcher = HConstants.CP_HTD_ATTR_VALUE_PATTERN.matcher(spec);\n        if (matcher.matches()) {\n          // jar file path can be empty if the cp class can be loaded\n          // from class loader.\n          Path path = matcher.group(1).trim().isEmpty() ?\n              null : new Path(matcher.group(1).trim());\n          String className = matcher.group(2).trim();\n          if (className.isEmpty()) {\n            continue;\n          }\n          int priority = matcher.group(3).trim().isEmpty() ?\n              Coprocessor.PRIORITY_USER : Integer.parseInt(matcher.group(3));\n          String cfgSpec = null;\n          try {\n            cfgSpec = matcher.group(4);\n          } catch (IndexOutOfBoundsException ex) {\n            // ignore\n          }\n          Configuration ourConf;\n          if (cfgSpec != null && !cfgSpec.trim().equals(\"|\")) {\n            cfgSpec = cfgSpec.substring(cfgSpec.indexOf('|') + 1);\n            // do an explicit deep copy of the passed configuration\n            ourConf = new Configuration(false);\n            HBaseConfiguration.merge(ourConf, conf);\n            Matcher m = HConstants.CP_HTD_ATTR_VALUE_PARAM_PATTERN.matcher(cfgSpec);\n            while (m.find()) {\n              ourConf.set(m.group(1), m.group(2));\n            }\n          } else {\n            ourConf = conf;\n          }\n          result.add(new TableCoprocessorAttribute(path, className, priority, ourConf));\n        } else {\n          LOG.error(\"Malformed table coprocessor specification: key=\" + key +\n            \", spec: \" + spec);\n        }\n      } catch (Exception ioe) {\n        LOG.error(\"Malformed table coprocessor specification: key=\" + key + \", spec: \" + spec,\n            ioe);\n      }\n    }\n  }\n  return result;\n}",
        "accept_response": "static List<TableCoprocessorAttribute> getTableCoprocessorAttrsFromSchema(Configuration conf,\n    HTableDescriptor htd) {\n  List<TableCoprocessorAttribute> result = Lists.newArrayList();\n  for (Map.Entry<ImmutableBytesWritable, ImmutableBytesWritable> e: htd.getValues().entrySet()) {\n    String key = Bytes.toString(e.getKey().get()).trim();\n    if (HConstants.CP_HTD_ATTR_KEY_PATTERN.matcher(key).matches()) {\n      String spec = Bytes.toString(e.getValue().get()).trim();\n      // found one\n      try {\n        Matcher matcher = HConstants.CP_HTD_ATTR_VALUE_PATTERN.matcher(spec);\n        if (matcher.matches()) {\n          // jar file path can be empty if the cp class can be loaded\n          // from class loader.\n          Path path = matcher.group(1).trim().isEmpty() ?\n              null : new Path(matcher.group(1).trim());\n          String className = matcher.group(2).trim();\n          if (className.isEmpty()) {\n            LOG.error(\"Malformed table coprocessor specification, Class name is empty: key=\" + key\n                + \", spec: \" + spec);\n            continue;\n          }\n          int priority = matcher.group(3).trim().isEmpty() ?\n              Coprocessor.PRIORITY_USER : Integer.parseInt(matcher.group(3));\n          String cfgSpec = null;\n          try {\n            cfgSpec = matcher.group(4);\n          } catch (IndexOutOfBoundsException ex) {\n            // ignore\n          }\n          Configuration ourConf;\n          if (cfgSpec != null && !cfgSpec.trim().equals(\"|\")) {\n            cfgSpec = cfgSpec.substring(cfgSpec.indexOf('|') + 1);\n            // do an explicit deep copy of the passed configuration\n            ourConf = new Configuration(false);\n            HBaseConfiguration.merge(ourConf, conf);\n            Matcher m = HConstants.CP_HTD_ATTR_VALUE_PARAM_PATTERN.matcher(cfgSpec);\n            while (m.find()) {\n              ourConf.set(m.group(1), m.group(2));\n            }\n          } else {\n            ourConf = conf;\n          }\n          result.add(new TableCoprocessorAttribute(path, className, priority, ourConf));\n        } else {\n          LOG.error(\"Malformed table coprocessor specification: key=\" + key +\n            \", spec: \" + spec);\n        }\n      } catch (Exception ioe) {\n        LOG.error(\"Malformed table coprocessor specification: key=\" + key + \", spec: \" + spec,\n            ioe);\n      }\n    }\n  }\n  return result;\n}",
        "reject_response": "static List<TableCoprocessorAttribute> getTableCoprocessorAttrsFromSchema(Configuration conf,\n    HTableDescriptor htd) {\n  List<TableCoprocessorAttribute> result = Lists.newArrayList();\n  for (Map.Entry<ImmutableBytesWritable, ImmutableBytesWritable> e: htd.getValues().entrySet()) {\n    String key = Bytes.toString(e.getKey().get()).trim();\n    if (HConstants.CP_HTD_ATTR_KEY_PATTERN.matcher(key).matches()) {\n      String spec = Bytes.toString(e.getValue().get()).trim();\n      // found one\n      try {\n        Matcher matcher = HConstants.CP_HTD_ATTR_VALUE_PATTERN.matcher(spec);\n        if (matcher.matches()) {\n          // jar file path can be empty if the cp class can be loaded\n          // from class loader.\n          Path path = matcher.group(1).trim().isEmpty() ?\n              null : new Path(matcher.group(1).trim());\n          String className = matcher.group(2).trim();\n          if (className.isEmpty()) {\n            LOG.error(\"Malformed table coprocessor specification: key=\" +\n              key + \", spec: \" + spec);\n            continue;\n          }\n          int priority = matcher.group(3).trim().isEmpty() ?\n              Coprocessor.PRIORITY_USER : Integer.parseInt(matcher.group(3));\n          String cfgSpec = null;\n          try {\n            cfgSpec = matcher.group(4);\n          } catch (IndexOutOfBoundsException ex) {\n            // ignore\n          }\n          Configuration ourConf;\n          if (cfgSpec != null && !cfgSpec.trim().equals(\"|\")) {\n            cfgSpec = cfgSpec.substring(cfgSpec.indexOf('|') + 1);\n            // do an explicit deep copy of the passed configuration\n            ourConf = new Configuration(false);\n            HBaseConfiguration.merge(ourConf, conf);\n            Matcher m = HConstants.CP_HTD_ATTR_VALUE_PARAM_PATTERN.matcher(cfgSpec);\n            while (m.find()) {\n              ourConf.set(m.group(1), m.group(2));\n            }\n          } else {\n            ourConf = conf;\n          }\n          result.add(new TableCoprocessorAttribute(path, className, priority, ourConf));\n        } else {\n          LOG.error(\"Malformed table coprocessor specification: key=\" + key +\n            \", spec: \" + spec);\n        }\n      } catch (Exception ioe) {\n        LOG.error(\"Malformed table coprocessor specification: key=\" + key + \", spec: \" + spec,\n            ioe);\n      }\n    }\n  }\n  return result;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2480,
        "instruction": "private void addExposedTypes(TypeVariable type, Class<?> cause) {\n  if (done(type)) {\n    return;\n  }\n  visit(type);\n  for (Type bound : type.getBounds()) {\n    addExposedTypes(bound, cause);\n  }\n}",
        "accept_response": "private void addExposedTypes(TypeVariable type, Class<?> cause) {\n  if (done(type)) {\n    return;\n  }\n  visit(type);\n  for (Type bound : type.getBounds()) {\n    LOG.debug(\"Adding exposed types from {}, which is a type bound on {}\", bound, type);\n    addExposedTypes(bound, cause);\n  }\n}",
        "reject_response": "private void addExposedTypes(TypeVariable type, Class<?> cause) {\n  if (done(type)) {\n    return;\n  }\n  visit(type);\n  for (Type bound : type.getBounds()) {\n    logger.debug(\"Adding exposed types from {}, which is a type bound on {}\", bound, type);\n    addExposedTypes(bound, cause);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3242,
        "instruction": "public QuorumVerifier setQuorumVerifier(QuorumVerifier qv, boolean writeToDisk){\n    synchronized (QV_LOCK) {\n        if ((quorumVerifier != null) && (quorumVerifier.getVersion() >= qv.getVersion())) {\n            // this is normal. For example - server found out about new config through FastLeaderElection gossiping\n            // and then got the same config in UPTODATE message so its already known\n            return quorumVerifier;\n        }\n        QuorumVerifier prevQV = quorumVerifier;\n        quorumVerifier = qv;\n        if (lastSeenQuorumVerifier == null || (qv.getVersion() > lastSeenQuorumVerifier.getVersion()))\n            lastSeenQuorumVerifier = qv;\n\n        if (writeToDisk) {\n            // some tests initialize QuorumPeer without a static config file\n            if (configFilename != null) {\n                try {\n                    String dynamicConfigFilename = makeDynamicConfigFilename(\n                            qv.getVersion());\n                    QuorumPeerConfig.writeDynamicConfig(\n                            dynamicConfigFilename, qv, false);\n                    QuorumPeerConfig.editStaticConfig(configFilename,\n                            dynamicConfigFilename,\n                            needEraseClientInfoFromStaticConfig());\n                } catch (IOException e) {\n                    LOG.error(\"Error closing file: \", e.getMessage());\n                }\n            } else {\n                LOG.info(\"writeToDisk == true but configFilename == null\");\n            }\n        }\n\n        if (qv.getVersion() == lastSeenQuorumVerifier.getVersion()) {\n            QuorumPeerConfig.deleteFile(getNextDynamicConfigFilename());\n        }\n        QuorumServer qs = qv.getAllMembers().get(getId());\n        if (qs != null) {\n            setAddrs(qs.addr, qs.electionAddr, qs.clientAddr);\n        }\n        updateObserverMasterList();\n        return prevQV;\n    }\n}",
        "accept_response": "public QuorumVerifier setQuorumVerifier(QuorumVerifier qv, boolean writeToDisk){\n    synchronized (QV_LOCK) {\n        if ((quorumVerifier != null) && (quorumVerifier.getVersion() >= qv.getVersion())) {\n            // this is normal. For example - server found out about new config through FastLeaderElection gossiping\n            // and then got the same config in UPTODATE message so its already known\n              LOG.debug(\"{} setQuorumVerifier called with known or old config {}.\"\n                      + \" Current version: {}\", getId(), qv.getVersion(),\n                      quorumVerifier.getVersion());\n            return quorumVerifier;\n        }\n        QuorumVerifier prevQV = quorumVerifier;\n        quorumVerifier = qv;\n        if (lastSeenQuorumVerifier == null || (qv.getVersion() > lastSeenQuorumVerifier.getVersion()))\n            lastSeenQuorumVerifier = qv;\n\n        if (writeToDisk) {\n            // some tests initialize QuorumPeer without a static config file\n            if (configFilename != null) {\n                try {\n                    String dynamicConfigFilename = makeDynamicConfigFilename(\n                            qv.getVersion());\n                    QuorumPeerConfig.writeDynamicConfig(\n                            dynamicConfigFilename, qv, false);\n                    QuorumPeerConfig.editStaticConfig(configFilename,\n                            dynamicConfigFilename,\n                            needEraseClientInfoFromStaticConfig());\n                } catch (IOException e) {\n                    LOG.error(\"Error closing file: \", e.getMessage());\n                }\n            } else {\n                LOG.info(\"writeToDisk == true but configFilename == null\");\n            }\n        }\n\n        if (qv.getVersion() == lastSeenQuorumVerifier.getVersion()) {\n            QuorumPeerConfig.deleteFile(getNextDynamicConfigFilename());\n        }\n        QuorumServer qs = qv.getAllMembers().get(getId());\n        if (qs != null) {\n            setAddrs(qs.addr, qs.electionAddr, qs.clientAddr);\n        }\n        updateObserverMasterList();\n        return prevQV;\n    }\n}",
        "reject_response": "public QuorumVerifier setQuorumVerifier(QuorumVerifier qv, boolean writeToDisk){\n    synchronized (QV_LOCK) {\n        if ((quorumVerifier != null) && (quorumVerifier.getVersion() >= qv.getVersion())) {\n            // this is normal. For example - server found out about new config through FastLeaderElection gossiping\n            // and then got the same config in UPTODATE message so its already known\n            LOG.debug(getId() + \" setQuorumVerifier called with known or old config \" + qv.getVersion() +\n                    \". Current version: \" + quorumVerifier.getVersion());\n            return quorumVerifier;\n        }\n        QuorumVerifier prevQV = quorumVerifier;\n        quorumVerifier = qv;\n        if (lastSeenQuorumVerifier == null || (qv.getVersion() > lastSeenQuorumVerifier.getVersion()))\n            lastSeenQuorumVerifier = qv;\n\n        if (writeToDisk) {\n            // some tests initialize QuorumPeer without a static config file\n            if (configFilename != null) {\n                try {\n                    String dynamicConfigFilename = makeDynamicConfigFilename(\n                            qv.getVersion());\n                    QuorumPeerConfig.writeDynamicConfig(\n                            dynamicConfigFilename, qv, false);\n                    QuorumPeerConfig.editStaticConfig(configFilename,\n                            dynamicConfigFilename,\n                            needEraseClientInfoFromStaticConfig());\n                } catch (IOException e) {\n                    LOG.error(\"Error closing file: \", e.getMessage());\n                }\n            } else {\n                LOG.info(\"writeToDisk == true but configFilename == null\");\n            }\n        }\n\n        if (qv.getVersion() == lastSeenQuorumVerifier.getVersion()) {\n            QuorumPeerConfig.deleteFile(getNextDynamicConfigFilename());\n        }\n        QuorumServer qs = qv.getAllMembers().get(getId());\n        if (qs != null) {\n            setAddrs(qs.addr, qs.electionAddr, qs.clientAddr);\n        }\n        updateObserverMasterList();\n        return prevQV;\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2400,
        "instruction": "private void configureViewLogging(ViewEntity viewDefinition,ClassLoader cl) {\n  URL resourceURL = cl.getResource(VIEW_LOG_FILE);\n  if( null != resourceURL ){\n    PropertyConfigurator.configure(resourceURL);\n  }\n}",
        "accept_response": "private void configureViewLogging(ViewEntity viewDefinition,ClassLoader cl) {\n  URL resourceURL = cl.getResource(VIEW_LOG_FILE);\n  if( null != resourceURL ){\n    LOG.info(\"setting up logging for view {} as per property file {}\",viewDefinition.getName(), resourceURL);\n    PropertyConfigurator.configure(resourceURL);\n  }\n}",
        "reject_response": "private void configureViewLogging(ViewEntity viewDefinition,ClassLoader cl) {\n  URL resourceURL = cl.getResource(VIEW_LOG_FILE);\n  if( null != resourceURL ){\n    LOG.debug(\"setting up logging for view {} as per property file {}\",viewDefinition.getName(), resourceURL);\n    PropertyConfigurator.configure(resourceURL);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2668,
        "instruction": "protected void log(Response.Status status, Throwable ex) {\n  UserGroupInformation ugi = KMSMDCFilter.getUgi();\n  String method = KMSMDCFilter.getMethod();\n  String url = KMSMDCFilter.getURL();\n  String remoteClientAddress = KMSMDCFilter.getRemoteClientAddress();\n  String msg = getOneLineMessage(ex);\n}",
        "accept_response": "protected void log(Response.Status status, Throwable ex) {\n  UserGroupInformation ugi = KMSMDCFilter.getUgi();\n  String method = KMSMDCFilter.getMethod();\n  String url = KMSMDCFilter.getURL();\n  String remoteClientAddress = KMSMDCFilter.getRemoteClientAddress();\n  String msg = getOneLineMessage(ex);\n  LOG.warn(\"User:'{}' Method:{} URL:{} From:{} Response:{}-{}\", ugi, method,\n      url, remoteClientAddress, status, msg, ex);\n}",
        "reject_response": "protected void log(Response.Status status, Throwable ex) {\n  UserGroupInformation ugi = KMSMDCFilter.getUgi();\n  String method = KMSMDCFilter.getMethod();\n  String url = KMSMDCFilter.getURL();\n  String remoteClientAddress = KMSMDCFilter.getRemoteClientAddress();\n  String msg = getOneLineMessage(ex);\n  LOG.warn(\"User:'{}' Method:{} URL:{} Response:{}-{}\", ugi, method, url,\n      status, msg, ex);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2522,
        "instruction": "public void mutateRepaired(Descriptor descriptor, long newRepairedAt, UUID newPendingRepair) throws IOException\n{\n\tif (logger.isTraceEnabled())\n    Map<MetadataType, MetadataComponent> currentComponents = deserialize(descriptor, EnumSet.allOf(MetadataType.class));\n    StatsMetadata stats = (StatsMetadata) currentComponents.remove(MetadataType.STATS);\n    // mutate time & id\n    currentComponents.put(MetadataType.STATS, stats.mutateRepairedAt(newRepairedAt).mutatePendingRepair(newPendingRepair));\n    rewriteSSTableMetadata(descriptor, currentComponents);\n}",
        "accept_response": "public void mutateRepaired(Descriptor descriptor, long newRepairedAt, UUID newPendingRepair) throws IOException\n{\n\tif (logger.isTraceEnabled())\n\t\tlogger.trace(\"Mutating {} to repairedAt time {} and pendingRepair {}\",\n                 descriptor.filenameFor(Component.STATS), newRepairedAt, newPendingRepair);\n    Map<MetadataType, MetadataComponent> currentComponents = deserialize(descriptor, EnumSet.allOf(MetadataType.class));\n    StatsMetadata stats = (StatsMetadata) currentComponents.remove(MetadataType.STATS);\n    // mutate time & id\n    currentComponents.put(MetadataType.STATS, stats.mutateRepairedAt(newRepairedAt).mutatePendingRepair(newPendingRepair));\n    rewriteSSTableMetadata(descriptor, currentComponents);\n}",
        "reject_response": "public void mutateRepaired(Descriptor descriptor, long newRepairedAt, UUID newPendingRepair) throws IOException\n{\n\tif (logger.isTraceEnabled())\n    logger.trace(\"Mutating {} to repairedAt time {} and pendingRepair {}\",\n    Map<MetadataType, MetadataComponent> currentComponents = deserialize(descriptor, EnumSet.allOf(MetadataType.class));\n    StatsMetadata stats = (StatsMetadata) currentComponents.remove(MetadataType.STATS);\n    // mutate time & id\n    currentComponents.put(MetadataType.STATS, stats.mutateRepairedAt(newRepairedAt).mutatePendingRepair(newPendingRepair));\n    rewriteSSTableMetadata(descriptor, currentComponents);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2621,
        "instruction": "private void initializeLogger() {\n\n  // Override default log configuration by properties which are provided in\n  // properties file.\n  loadLogDetailsFromPropertyFile();\n\n  // Override log configuration by properties which are provided in\n  // through system properties.\n  loadLogDetailsFromSystemProperties();\n\n  // Log messages stored in messagesToBeLogged\n  messagesToBeLogged = \"\";\n}",
        "accept_response": "private void initializeLogger() {\n\n  // Override default log configuration by properties which are provided in\n  // properties file.\n  loadLogDetailsFromPropertyFile();\n\n  // Override log configuration by properties which are provided in\n  // through system properties.\n  loadLogDetailsFromSystemProperties();\n\n  // Log messages stored in messagesToBeLogged\n  logger.info(messagesToBeLogged);\n  messagesToBeLogged = \"\";\n}",
        "reject_response": "private void initializeLogger() {\n\n  // Override default log configuration by properties which are provided in\n  // properties file.\n  loadLogDetailsFromPropertyFile();\n\n  // Override log configuration by properties which are provided in\n  // through system properties.\n  loadLogDetailsFromSystemProperties();\n\n  // Log messages stored in messagesToBeLogged\n  if (LOGGER.infoEnabled()) {\n    LOGGER.info(messagesToBeLogged);\n    messagesToBeLogged = \"\";\n  }\n  messagesToBeLogged = \"\";\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3141,
        "instruction": "@Override\npublic void onVertexStarted(List<TaskAttemptIdentifier> completions)\n    throws Exception {\n  // apply the ReconfigureDoneEvent and then schedule all the tasks.\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"onVertexStarted is invoked in NoOpVertexManager, vertex=\" + getContext().getVertexName());\n  }\n  if (!setParallelismInInitializing && configurationDoneEvent.isSetParallelismCalled()) {\n    reconfigureVertex();\n  }\n  getContext().doneReconfiguringVertex();\n  int numTasks = getContext().getVertexNumTasks(getContext().getVertexName());\n  List<ScheduleTaskRequest> tasks = new ArrayList<ScheduleTaskRequest>();\n  for (int i=0;i<numTasks;++i) {\n    tasks.add(ScheduleTaskRequest.create(i, null));\n  }\n  getContext().scheduleTasks(tasks);\n}",
        "accept_response": "@Override\npublic void onVertexStarted(List<TaskAttemptIdentifier> completions)\n    throws Exception {\n  // apply the ReconfigureDoneEvent and then schedule all the tasks.\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"onVertexStarted is invoked in NoOpVertexManager, vertex=\" + getContext().getVertexName());\n  }\n  if (!setParallelismInInitializing && configurationDoneEvent.isSetParallelismCalled()) {\n    reconfigureVertex();\n  }\n  getContext().doneReconfiguringVertex();\n  int numTasks = getContext().getVertexNumTasks(getContext().getVertexName());\n  LOG.debug(\"Schedule all the tasks, numTask={}\", numTasks);\n  List<ScheduleTaskRequest> tasks = new ArrayList<ScheduleTaskRequest>();\n  for (int i=0;i<numTasks;++i) {\n    tasks.add(ScheduleTaskRequest.create(i, null));\n  }\n  getContext().scheduleTasks(tasks);\n}",
        "reject_response": "@Override\npublic void onVertexStarted(List<TaskAttemptIdentifier> completions)\n    throws Exception {\n  // apply the ReconfigureDoneEvent and then schedule all the tasks.\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"onVertexStarted is invoked in NoOpVertexManager, vertex=\" + getContext().getVertexName());\n  }\n  if (!setParallelismInInitializing && configurationDoneEvent.isSetParallelismCalled()) {\n    reconfigureVertex();\n  }\n  getContext().doneReconfiguringVertex();\n  int numTasks = getContext().getVertexNumTasks(getContext().getVertexName());\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"Schedule all the tasks, numTask=\" + numTasks);\n  }\n  List<ScheduleTaskRequest> tasks = new ArrayList<ScheduleTaskRequest>();\n  for (int i=0;i<numTasks;++i) {\n    tasks.add(ScheduleTaskRequest.create(i, null));\n  }\n  getContext().scheduleTasks(tasks);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3196,
        "instruction": "private void renewToken(AppLevelTimelineCollector appCollector)\n    throws IOException {\n  long newExpirationTime = renewTokenForAppCollector(appCollector);\n  // Set renewal or regeneration timer based on delay.\n  long renewalDelay = 0;\n  if (newExpirationTime > 0) {\n    renewalDelay = getRenewalDelay(newExpirationTime - Time.now());\n  }\n  long regenerationDelay =\n      getRegenerationDelay(appCollector.getAppDelegationTokenMaxDate());\n  if (renewalDelay > 0 || regenerationDelay > 0) {\n    this.timerForRenewal = renewalDelay < regenerationDelay;\n    Future<?> renewalOrRegenerationFuture = tokenRenewalExecutor.schedule(\n        this, timerForRenewal ? renewalDelay : regenerationDelay,\n        TimeUnit.MILLISECONDS);\n    appCollector.setRenewalOrRegenerationFutureForApp(\n        renewalOrRegenerationFuture);\n  }\n}",
        "accept_response": "private void renewToken(AppLevelTimelineCollector appCollector)\n    throws IOException {\n  long newExpirationTime = renewTokenForAppCollector(appCollector);\n  // Set renewal or regeneration timer based on delay.\n  long renewalDelay = 0;\n  if (newExpirationTime > 0) {\n    LOG.info(\"Renewed token for {} with new expiration \" +\n        \"timestamp = {}\", appId, newExpirationTime);\n    renewalDelay = getRenewalDelay(newExpirationTime - Time.now());\n  }\n  long regenerationDelay =\n      getRegenerationDelay(appCollector.getAppDelegationTokenMaxDate());\n  if (renewalDelay > 0 || regenerationDelay > 0) {\n    this.timerForRenewal = renewalDelay < regenerationDelay;\n    Future<?> renewalOrRegenerationFuture = tokenRenewalExecutor.schedule(\n        this, timerForRenewal ? renewalDelay : regenerationDelay,\n        TimeUnit.MILLISECONDS);\n    appCollector.setRenewalOrRegenerationFutureForApp(\n        renewalOrRegenerationFuture);\n  }\n}",
        "reject_response": "private void renewToken(AppLevelTimelineCollector appCollector)\n    throws IOException {\n  long newExpirationTime = renewTokenForAppCollector(appCollector);\n  // Set renewal or regeneration timer based on delay.\n  long renewalDelay = 0;\n  if (newExpirationTime > 0) {\n    LOG.info(\"Renewed token for \" + appId + \" with new expiration \" +\n        \"timestamp = \" + newExpirationTime);\n    renewalDelay = getRenewalDelay(newExpirationTime - Time.now());\n  }\n  long regenerationDelay =\n      getRegenerationDelay(appCollector.getAppDelegationTokenMaxDate());\n  if (renewalDelay > 0 || regenerationDelay > 0) {\n    this.timerForRenewal = renewalDelay < regenerationDelay;\n    Future<?> renewalOrRegenerationFuture = tokenRenewalExecutor.schedule(\n        this, timerForRenewal ? renewalDelay : regenerationDelay,\n        TimeUnit.MILLISECONDS);\n    appCollector.setRenewalOrRegenerationFutureForApp(\n        renewalOrRegenerationFuture);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2634,
        "instruction": "@Override\npublic boolean updateData() {\n  setConnectedFlag(true);\n  Random r = new Random(System.currentTimeMillis());\n  totalHeapSize = (long) Math.abs(r.nextInt(3200 - 2048) + 2048);\n  usedHeapSize = (long) Math.abs(r.nextInt(2048));\n  writePerSec = Math.abs(r.nextInt(100));\n  subscriptionCount = Math.abs(r.nextInt(100));\n  registeredCQCount = (long) Math.abs(r.nextInt(100));\n  txnCommittedCount = Math.abs(r.nextInt(100));\n  txnRollbackCount = Math.abs(r.nextInt(100));\n  runningFunctionCount = Math.abs(r.nextInt(100));\n  clusterId = Math.abs(r.nextInt(100));\n  writePerSecTrend.add(writePerSec);\n  diskWritesRate = writePerSec;\n  garbageCollectionCount = (long) Math.abs(r.nextInt(100));\n  garbageCollectionTrend.add(garbageCollectionCount);\n\n  readPerSec = Math.abs(r.nextInt(100));\n  readPerSecTrend.add(readPerSec);\n\n  diskReadsRate = readPerSec;\n  queriesPerSec = Math.abs(r.nextInt(100));\n  queriesPerSecTrend.add(queriesPerSec);\n\n  loadPerSec = Math.abs(r.nextInt(100));\n  totalHeapSize = totalHeapSize;\n  totalBytesOnDisk = totalHeapSize;\n\n  totalBytesOnDiskTrend.add(totalBytesOnDisk);\n\n  memoryUsageTrend.add(usedHeapSize);\n  throughoutWritesTrend.add(writePerSec);\n  throughoutReadsTrend.add(readPerSec);\n\n  memberCount = 0;\n\n  // Create 3 members first time around\n  if (membersHMap.size() == 0) {\n\n    membersHMap.put(\"pnq-visitor1\",\n        initializeMember(\"pnq-visitor1(Launcher_Manager-1099-13-40-24-5368)-24357\",\n            \"pnq-visitor1\", true, true, true, true));\n\n    for (int i = 2; i <= 8; i++) {\n      if ((i % 2) == 0) {\n        membersHMap.put(\"pnq-visitor\" + i,\n            initializeMember(\"pnq-visitor\" + i + \"(Launcher_Server-1099-13-40-24-5368)-24357\",\n                \"pnq-visitor\" + i, false, false, true, false));\n      } else {\n        if ((i % 3) == 0) {\n          membersHMap.put(\"pnq-visitor\" + i,\n              initializeMember(\"pnq-visitor\" + i + \"(Launcher_Server-1099-13-40-24-5368)-24357\",\n                  \"pnq-visitor\" + i, false, false, false, false));\n        } else {\n          membersHMap.put(\"pnq-visitor\" + i,\n              initializeMember(\"pnq-visitor\" + i + \"(Launcher_Server-1099-13-40-24-5368)-24357\",\n                  \"pnq-visitor\" + i, false, true, true, true));\n        }\n      }\n    }\n\n    for (Entry<String, Member> memberSet : membersHMap.entrySet()) {\n      HashMap<String, Cluster.Region> memberRegions = new HashMap<String, Cluster.Region>();\n      HashMap<String, Cluster.Client> memberClientsHM = new HashMap<String, Cluster.Client>();\n\n      Random randomGenerator = new Random();\n      int randomInt = (randomGenerator.nextInt(15)) + 10;\n      int regionExists = 0;\n      for (int y = 0; y < randomInt; y++) {\n        Region region = initMemberRegion(y, memberSet.getValue().getName());\n        if (clusterRegionMap.entrySet().size() > 0) {\n          for (Region clusterRegion : clusterRegionMap.values()) {\n            if ((region.name).equals(clusterRegion.name)) {\n              clusterRegion.memberName.add(memberSet.getValue().getName());\n              clusterRegion.memberCount = clusterRegion.memberCount + 1;\n              regionExists = 1;\n              break;\n            }\n          }\n          if (regionExists == 0) {\n            addClusterRegion(region.getFullPath(), region);\n          }\n        } else {\n          addClusterRegion(region.getFullPath(), region);\n        }\n        memberRegions.put(region.getFullPath(), region);\n        totalRegionCount = clusterRegionMap.values().size();\n      }\n      membersHMap.get(memberSet.getKey()).setMemberRegions(memberRegions);\n\n      if (memberSet.getValue().isCache) {\n        Client client = initMemberClient(0, memberSet.getValue().getHost());\n        memberClientsHM.put(client.getId(), client);\n        randomInt = randomGenerator.nextInt(10);\n        for (int y = 1; y < randomInt; y++) {\n          Client newClient = initMemberClient(y, memberSet.getValue().getHost());\n          memberClientsHM.put(newClient.getId(), newClient);\n        }\n        membersHMap.get(memberSet.getKey()).updateMemberClientsHMap(memberClientsHM);\n        clientConnectionCount = clientConnectionCount\n            + membersHMap.get(memberSet.getKey()).getMemberClientsHMap().size();\n      }\n\n    }\n  }\n\n  // add additional regions to members\n  for (Entry<String, Member> memberSet : membersHMap.entrySet()) {\n    HashMap<String, Cluster.Region> memberRegions = new HashMap<String, Cluster.Region>();\n\n    Random randomGenerator = new Random();\n    int randomInt = (randomGenerator.nextInt(5)) + 5;\n    int regionExists = 0;\n    for (int y = 0; y < randomInt; y++) {\n      Region region = initMemberRegion(y, memberSet.getValue().getName());\n      if (clusterRegionMap.entrySet().size() > 0) {\n        for (Region clusterRegion : clusterRegionMap.values()) {\n          if ((region.name).equals(clusterRegion.name)) {\n            clusterRegion.memberName.add(memberSet.getValue().getName());\n            clusterRegion.memberCount = clusterRegion.memberCount + 1;\n            regionExists = 1;\n            break;\n          }\n        }\n        if (regionExists == 0) {\n          addClusterRegion(region.getFullPath(), region);\n        }\n      } else {\n        addClusterRegion(region.getFullPath(), region);\n      }\n      memberRegions.put(region.getFullPath(), region);\n      totalRegionCount = clusterRegionMap.values().size();\n    }\n    membersHMap.get(memberSet.getKey()).setMemberRegions(memberRegions);\n\n  }\n\n  wanInformation.clear();\n  int wanInfoSize = Math.abs(r.nextInt(10));\n  wanInfoSize++;\n  for (int i = 0; i < wanInfoSize; i++) {\n    String name = \"Mock Cluster\" + i;\n    Boolean value = false;\n    if (i % 2 == 0) {\n      value = true;\n    }\n    wanInformation.put(name, value);\n  }\n  memberCount = membersHMap.size();\n\n  totalHeapSize = (long) 0;\n  for (Entry<String, Member> memberSet : membersHMap.entrySet()) {\n    refresh(membersHMap.get(memberSet.getKey()));\n    Member member = membersHMap.get(memberSet.getKey());\n    totalHeapSize += member.currentHeapSize;\n  }\n\n  for (Region region : clusterRegionMap.values()) {\n    // Memory reads and writes\n    region.getsRate = (Math.abs(r.nextInt(100))) + 1;\n    region.putsRate = (Math.abs(r.nextInt(100))) + 1;\n    region.getsPerSecTrend.add(region.getsRate);\n    region.putsPerSecTrend.add(region.putsRate);\n\n    // Disk reads and writes\n    region.diskReadsRate = (Math.abs(r.nextInt(100))) + 1;\n    region.diskWritesRate = (Math.abs(r.nextInt(100))) + 1;\n    region.diskReadsPerSecTrend.add(region.diskReadsRate);\n    region.diskWritesPerSecTrend.add(region.diskWritesRate);\n  }\n\n  if (clusterStatementMap.size() < 500) {\n    for (int i = 1; i <= 500; ++i) {\n\n      updateClusterStatement(i);\n    }\n  } else if (clusterStatementMap.size() == 510) {\n    for (Iterator itSt = clusterStatementMap.values().iterator(); itSt.hasNext();) {\n      Cluster.Statement statement = (Cluster.Statement) itSt.next();\n      statement.setNumTimesCompiled((statement.getNumTimesCompiled() + 5));\n      statement.setNumExecution((statement.getNumExecution() + 5));\n      statement.setNumExecutionsInProgress((statement.getNumExecutionsInProgress() + 5));\n      statement.setNumTimesGlobalIndexLookup((statement.getNumTimesGlobalIndexLookup() + 5));\n      statement.setNumRowsModified((statement.getNumRowsModified() + 5));\n    }\n  } else if (clusterStatementMap.size() < 510) {\n    Cluster.Statement statement = new Cluster.Statement();\n    Random randomGenerator = new Random();\n    String statementDefinition = \"select * from member where member_name = member-510\"\n        + \" and lastUpdatedTime = '\" + new Date().toString() + \"'\";\n    Integer intVal = randomGenerator.nextInt(5);\n    statement.setQueryDefinition(statementDefinition);\n    statement.setNumTimesCompiled(intVal.longValue());\n    statement.setNumExecution(intVal.longValue());\n    statement.setNumExecutionsInProgress(intVal.longValue());\n    statement.setNumTimesGlobalIndexLookup(intVal.longValue());\n    statement.setNumRowsModified(intVal.longValue());\n    statement.setParseTime(randomGenerator.nextLong());\n    statement.setBindTime(randomGenerator.nextLong());\n    statement.setOptimizeTime(randomGenerator.nextLong());\n    statement.setRoutingInfoTime(randomGenerator.nextLong());\n    statement.setGenerateTime(randomGenerator.nextLong());\n    statement.setTotalCompilationTime(randomGenerator.nextLong());\n    statement.setExecutionTime(randomGenerator.nextLong());\n    statement.setProjectionTime(randomGenerator.nextLong());\n    statement.setTotalExecutionTime(randomGenerator.nextLong());\n    statement.setRowsModificationTime(randomGenerator.nextLong());\n    statement.setqNNumRowsSeen(intVal.longValue());\n    statement.setqNMsgSendTime(randomGenerator.nextLong());\n    statement.setqNMsgSerTime(randomGenerator.nextLong());\n    statement.setqNRespDeSerTime(randomGenerator.nextLong());\n    addClusterStatement(statementDefinition, statement);\n  }\n\n  return true;\n}",
        "accept_response": "@Override\npublic boolean updateData() {\n  setConnectedFlag(true);\n  Random r = new Random(System.currentTimeMillis());\n  totalHeapSize = (long) Math.abs(r.nextInt(3200 - 2048) + 2048);\n  usedHeapSize = (long) Math.abs(r.nextInt(2048));\n  writePerSec = Math.abs(r.nextInt(100));\n  subscriptionCount = Math.abs(r.nextInt(100));\n  registeredCQCount = (long) Math.abs(r.nextInt(100));\n  txnCommittedCount = Math.abs(r.nextInt(100));\n  txnRollbackCount = Math.abs(r.nextInt(100));\n  runningFunctionCount = Math.abs(r.nextInt(100));\n  clusterId = Math.abs(r.nextInt(100));\n  writePerSecTrend.add(writePerSec);\n  diskWritesRate = writePerSec;\n  garbageCollectionCount = (long) Math.abs(r.nextInt(100));\n  garbageCollectionTrend.add(garbageCollectionCount);\n\n  readPerSec = Math.abs(r.nextInt(100));\n  readPerSecTrend.add(readPerSec);\n\n  diskReadsRate = readPerSec;\n  queriesPerSec = Math.abs(r.nextInt(100));\n  queriesPerSecTrend.add(queriesPerSec);\n\n  loadPerSec = Math.abs(r.nextInt(100));\n  totalHeapSize = totalHeapSize;\n  totalBytesOnDisk = totalHeapSize;\n\n  totalBytesOnDiskTrend.add(totalBytesOnDisk);\n\n  memoryUsageTrend.add(usedHeapSize);\n  throughoutWritesTrend.add(writePerSec);\n  throughoutReadsTrend.add(readPerSec);\n\n  memberCount = 0;\n\n  // Create 3 members first time around\n  if (membersHMap.size() == 0) {\n\n    membersHMap.put(\"pnq-visitor1\",\n        initializeMember(\"pnq-visitor1(Launcher_Manager-1099-13-40-24-5368)-24357\",\n            \"pnq-visitor1\", true, true, true, true));\n\n    for (int i = 2; i <= 8; i++) {\n      if ((i % 2) == 0) {\n        membersHMap.put(\"pnq-visitor\" + i,\n            initializeMember(\"pnq-visitor\" + i + \"(Launcher_Server-1099-13-40-24-5368)-24357\",\n                \"pnq-visitor\" + i, false, false, true, false));\n      } else {\n        if ((i % 3) == 0) {\n          membersHMap.put(\"pnq-visitor\" + i,\n              initializeMember(\"pnq-visitor\" + i + \"(Launcher_Server-1099-13-40-24-5368)-24357\",\n                  \"pnq-visitor\" + i, false, false, false, false));\n        } else {\n          membersHMap.put(\"pnq-visitor\" + i,\n              initializeMember(\"pnq-visitor\" + i + \"(Launcher_Server-1099-13-40-24-5368)-24357\",\n                  \"pnq-visitor\" + i, false, true, true, true));\n        }\n      }\n    }\n\n    for (Entry<String, Member> memberSet : membersHMap.entrySet()) {\n      HashMap<String, Cluster.Region> memberRegions = new HashMap<String, Cluster.Region>();\n      HashMap<String, Cluster.Client> memberClientsHM = new HashMap<String, Cluster.Client>();\n\n      Random randomGenerator = new Random();\n      int randomInt = (randomGenerator.nextInt(15)) + 10;\n      int regionExists = 0;\n      for (int y = 0; y < randomInt; y++) {\n        Region region = initMemberRegion(y, memberSet.getValue().getName());\n        if (clusterRegionMap.entrySet().size() > 0) {\n          for (Region clusterRegion : clusterRegionMap.values()) {\n            if ((region.name).equals(clusterRegion.name)) {\n              clusterRegion.memberName.add(memberSet.getValue().getName());\n              clusterRegion.memberCount = clusterRegion.memberCount + 1;\n              regionExists = 1;\n              break;\n            }\n          }\n          if (regionExists == 0) {\n            addClusterRegion(region.getFullPath(), region);\n          }\n        } else {\n          addClusterRegion(region.getFullPath(), region);\n        }\n        memberRegions.put(region.getFullPath(), region);\n        totalRegionCount = clusterRegionMap.values().size();\n      }\n      membersHMap.get(memberSet.getKey()).setMemberRegions(memberRegions);\n\n      if (memberSet.getValue().isCache) {\n        Client client = initMemberClient(0, memberSet.getValue().getHost());\n        memberClientsHM.put(client.getId(), client);\n        randomInt = randomGenerator.nextInt(10);\n        for (int y = 1; y < randomInt; y++) {\n          Client newClient = initMemberClient(y, memberSet.getValue().getHost());\n          memberClientsHM.put(newClient.getId(), newClient);\n        }\n        membersHMap.get(memberSet.getKey()).updateMemberClientsHMap(memberClientsHM);\n        clientConnectionCount = clientConnectionCount\n            + membersHMap.get(memberSet.getKey()).getMemberClientsHMap().size();\n      }\n\n    }\n  }\n\n  // add additional regions to members\n  for (Entry<String, Member> memberSet : membersHMap.entrySet()) {\n    HashMap<String, Cluster.Region> memberRegions = new HashMap<String, Cluster.Region>();\n\n    Random randomGenerator = new Random();\n    int randomInt = (randomGenerator.nextInt(5)) + 5;\n    int regionExists = 0;\n    for (int y = 0; y < randomInt; y++) {\n      Region region = initMemberRegion(y, memberSet.getValue().getName());\n      if (clusterRegionMap.entrySet().size() > 0) {\n        for (Region clusterRegion : clusterRegionMap.values()) {\n          if ((region.name).equals(clusterRegion.name)) {\n            clusterRegion.memberName.add(memberSet.getValue().getName());\n            clusterRegion.memberCount = clusterRegion.memberCount + 1;\n            regionExists = 1;\n            break;\n          }\n        }\n        if (regionExists == 0) {\n          addClusterRegion(region.getFullPath(), region);\n        }\n      } else {\n        addClusterRegion(region.getFullPath(), region);\n      }\n      memberRegions.put(region.getFullPath(), region);\n      totalRegionCount = clusterRegionMap.values().size();\n    }\n    membersHMap.get(memberSet.getKey()).setMemberRegions(memberRegions);\n\n  }\n\n  wanInformation.clear();\n  int wanInfoSize = Math.abs(r.nextInt(10));\n  wanInfoSize++;\n  for (int i = 0; i < wanInfoSize; i++) {\n    String name = \"Mock Cluster\" + i;\n    Boolean value = false;\n    if (i % 2 == 0) {\n      value = true;\n    }\n    wanInformation.put(name, value);\n  }\n  memberCount = membersHMap.size();\n\n  totalHeapSize = (long) 0;\n  for (Entry<String, Member> memberSet : membersHMap.entrySet()) {\n    refresh(membersHMap.get(memberSet.getKey()));\n    Member member = membersHMap.get(memberSet.getKey());\n    totalHeapSize += member.currentHeapSize;\n  }\n\n  for (Region region : clusterRegionMap.values()) {\n    // Memory reads and writes\n    region.getsRate = (Math.abs(r.nextInt(100))) + 1;\n    region.putsRate = (Math.abs(r.nextInt(100))) + 1;\n    region.getsPerSecTrend.add(region.getsRate);\n    region.putsPerSecTrend.add(region.putsRate);\n\n    // Disk reads and writes\n    region.diskReadsRate = (Math.abs(r.nextInt(100))) + 1;\n    region.diskWritesRate = (Math.abs(r.nextInt(100))) + 1;\n    region.diskReadsPerSecTrend.add(region.diskReadsRate);\n    region.diskWritesPerSecTrend.add(region.diskWritesRate);\n  }\n\n  if (clusterStatementMap.size() < 500) {\n    for (int i = 1; i <= 500; ++i) {\n      logger.info(\"Adding statement = {}\", i);\n\n      updateClusterStatement(i);\n    }\n  } else if (clusterStatementMap.size() == 510) {\n    for (Iterator itSt = clusterStatementMap.values().iterator(); itSt.hasNext();) {\n      Cluster.Statement statement = (Cluster.Statement) itSt.next();\n      statement.setNumTimesCompiled((statement.getNumTimesCompiled() + 5));\n      statement.setNumExecution((statement.getNumExecution() + 5));\n      statement.setNumExecutionsInProgress((statement.getNumExecutionsInProgress() + 5));\n      statement.setNumTimesGlobalIndexLookup((statement.getNumTimesGlobalIndexLookup() + 5));\n      statement.setNumRowsModified((statement.getNumRowsModified() + 5));\n    }\n  } else if (clusterStatementMap.size() < 510) {\n    Cluster.Statement statement = new Cluster.Statement();\n    Random randomGenerator = new Random();\n    String statementDefinition = \"select * from member where member_name = member-510\"\n        + \" and lastUpdatedTime = '\" + new Date().toString() + \"'\";\n    Integer intVal = randomGenerator.nextInt(5);\n    statement.setQueryDefinition(statementDefinition);\n    statement.setNumTimesCompiled(intVal.longValue());\n    statement.setNumExecution(intVal.longValue());\n    statement.setNumExecutionsInProgress(intVal.longValue());\n    statement.setNumTimesGlobalIndexLookup(intVal.longValue());\n    statement.setNumRowsModified(intVal.longValue());\n    statement.setParseTime(randomGenerator.nextLong());\n    statement.setBindTime(randomGenerator.nextLong());\n    statement.setOptimizeTime(randomGenerator.nextLong());\n    statement.setRoutingInfoTime(randomGenerator.nextLong());\n    statement.setGenerateTime(randomGenerator.nextLong());\n    statement.setTotalCompilationTime(randomGenerator.nextLong());\n    statement.setExecutionTime(randomGenerator.nextLong());\n    statement.setProjectionTime(randomGenerator.nextLong());\n    statement.setTotalExecutionTime(randomGenerator.nextLong());\n    statement.setRowsModificationTime(randomGenerator.nextLong());\n    statement.setqNNumRowsSeen(intVal.longValue());\n    statement.setqNMsgSendTime(randomGenerator.nextLong());\n    statement.setqNMsgSerTime(randomGenerator.nextLong());\n    statement.setqNRespDeSerTime(randomGenerator.nextLong());\n    addClusterStatement(statementDefinition, statement);\n  }\n\n  return true;\n}",
        "reject_response": "@Override\npublic boolean updateData() {\n  setConnectedFlag(true);\n  Random r = new Random(System.currentTimeMillis());\n  totalHeapSize = (long) Math.abs(r.nextInt(3200 - 2048) + 2048);\n  usedHeapSize = (long) Math.abs(r.nextInt(2048));\n  writePerSec = Math.abs(r.nextInt(100));\n  subscriptionCount = Math.abs(r.nextInt(100));\n  registeredCQCount = (long) Math.abs(r.nextInt(100));\n  txnCommittedCount = Math.abs(r.nextInt(100));\n  txnRollbackCount = Math.abs(r.nextInt(100));\n  runningFunctionCount = Math.abs(r.nextInt(100));\n  clusterId = Math.abs(r.nextInt(100));\n  writePerSecTrend.add(writePerSec);\n  diskWritesRate = writePerSec;\n  garbageCollectionCount = (long) Math.abs(r.nextInt(100));\n  garbageCollectionTrend.add(garbageCollectionCount);\n\n  readPerSec = Math.abs(r.nextInt(100));\n  readPerSecTrend.add(readPerSec);\n\n  diskReadsRate = readPerSec;\n  queriesPerSec = Math.abs(r.nextInt(100));\n  queriesPerSecTrend.add(queriesPerSec);\n\n  loadPerSec = Math.abs(r.nextInt(100));\n  totalHeapSize = totalHeapSize;\n  totalBytesOnDisk = totalHeapSize;\n\n  totalBytesOnDiskTrend.add(totalBytesOnDisk);\n\n  memoryUsageTrend.add(usedHeapSize);\n  throughoutWritesTrend.add(writePerSec);\n  throughoutReadsTrend.add(readPerSec);\n\n  memberCount = 0;\n\n  // Create 3 members first time around\n  if (membersHMap.size() == 0) {\n\n    membersHMap.put(\"pnq-visitor1\",\n        initializeMember(\"pnq-visitor1(Launcher_Manager-1099-13-40-24-5368)-24357\",\n            \"pnq-visitor1\", true, true, true, true));\n\n    for (int i = 2; i <= 8; i++) {\n      if ((i % 2) == 0) {\n        membersHMap.put(\"pnq-visitor\" + i,\n            initializeMember(\"pnq-visitor\" + i + \"(Launcher_Server-1099-13-40-24-5368)-24357\",\n                \"pnq-visitor\" + i, false, false, true, false));\n      } else {\n        if ((i % 3) == 0) {\n          membersHMap.put(\"pnq-visitor\" + i,\n              initializeMember(\"pnq-visitor\" + i + \"(Launcher_Server-1099-13-40-24-5368)-24357\",\n                  \"pnq-visitor\" + i, false, false, false, false));\n        } else {\n          membersHMap.put(\"pnq-visitor\" + i,\n              initializeMember(\"pnq-visitor\" + i + \"(Launcher_Server-1099-13-40-24-5368)-24357\",\n                  \"pnq-visitor\" + i, false, true, true, true));\n        }\n      }\n    }\n\n    for (Entry<String, Member> memberSet : membersHMap.entrySet()) {\n      HashMap<String, Cluster.Region> memberRegions = new HashMap<String, Cluster.Region>();\n      HashMap<String, Cluster.Client> memberClientsHM = new HashMap<String, Cluster.Client>();\n\n      Random randomGenerator = new Random();\n      int randomInt = (randomGenerator.nextInt(15)) + 10;\n      int regionExists = 0;\n      for (int y = 0; y < randomInt; y++) {\n        Region region = initMemberRegion(y, memberSet.getValue().getName());\n        if (clusterRegionMap.entrySet().size() > 0) {\n          for (Region clusterRegion : clusterRegionMap.values()) {\n            if ((region.name).equals(clusterRegion.name)) {\n              clusterRegion.memberName.add(memberSet.getValue().getName());\n              clusterRegion.memberCount = clusterRegion.memberCount + 1;\n              regionExists = 1;\n              break;\n            }\n          }\n          if (regionExists == 0) {\n            addClusterRegion(region.getFullPath(), region);\n          }\n        } else {\n          addClusterRegion(region.getFullPath(), region);\n        }\n        memberRegions.put(region.getFullPath(), region);\n        totalRegionCount = clusterRegionMap.values().size();\n      }\n      membersHMap.get(memberSet.getKey()).setMemberRegions(memberRegions);\n\n      if (memberSet.getValue().isCache) {\n        Client client = initMemberClient(0, memberSet.getValue().getHost());\n        memberClientsHM.put(client.getId(), client);\n        randomInt = randomGenerator.nextInt(10);\n        for (int y = 1; y < randomInt; y++) {\n          Client newClient = initMemberClient(y, memberSet.getValue().getHost());\n          memberClientsHM.put(newClient.getId(), newClient);\n        }\n        membersHMap.get(memberSet.getKey()).updateMemberClientsHMap(memberClientsHM);\n        clientConnectionCount = clientConnectionCount\n            + membersHMap.get(memberSet.getKey()).getMemberClientsHMap().size();\n      }\n\n    }\n  }\n\n  // add additional regions to members\n  for (Entry<String, Member> memberSet : membersHMap.entrySet()) {\n    HashMap<String, Cluster.Region> memberRegions = new HashMap<String, Cluster.Region>();\n\n    Random randomGenerator = new Random();\n    int randomInt = (randomGenerator.nextInt(5)) + 5;\n    int regionExists = 0;\n    for (int y = 0; y < randomInt; y++) {\n      Region region = initMemberRegion(y, memberSet.getValue().getName());\n      if (clusterRegionMap.entrySet().size() > 0) {\n        for (Region clusterRegion : clusterRegionMap.values()) {\n          if ((region.name).equals(clusterRegion.name)) {\n            clusterRegion.memberName.add(memberSet.getValue().getName());\n            clusterRegion.memberCount = clusterRegion.memberCount + 1;\n            regionExists = 1;\n            break;\n          }\n        }\n        if (regionExists == 0) {\n          addClusterRegion(region.getFullPath(), region);\n        }\n      } else {\n        addClusterRegion(region.getFullPath(), region);\n      }\n      memberRegions.put(region.getFullPath(), region);\n      totalRegionCount = clusterRegionMap.values().size();\n    }\n    membersHMap.get(memberSet.getKey()).setMemberRegions(memberRegions);\n\n  }\n\n  wanInformation.clear();\n  int wanInfoSize = Math.abs(r.nextInt(10));\n  wanInfoSize++;\n  for (int i = 0; i < wanInfoSize; i++) {\n    String name = \"Mock Cluster\" + i;\n    Boolean value = false;\n    if (i % 2 == 0) {\n      value = true;\n    }\n    wanInformation.put(name, value);\n  }\n  memberCount = membersHMap.size();\n\n  totalHeapSize = (long) 0;\n  for (Entry<String, Member> memberSet : membersHMap.entrySet()) {\n    refresh(membersHMap.get(memberSet.getKey()));\n    Member member = membersHMap.get(memberSet.getKey());\n    totalHeapSize += member.currentHeapSize;\n  }\n\n  for (Region region : clusterRegionMap.values()) {\n    // Memory reads and writes\n    region.getsRate = (Math.abs(r.nextInt(100))) + 1;\n    region.putsRate = (Math.abs(r.nextInt(100))) + 1;\n    region.getsPerSecTrend.add(region.getsRate);\n    region.putsPerSecTrend.add(region.putsRate);\n\n    // Disk reads and writes\n    region.diskReadsRate = (Math.abs(r.nextInt(100))) + 1;\n    region.diskWritesRate = (Math.abs(r.nextInt(100))) + 1;\n    region.diskReadsPerSecTrend.add(region.diskReadsRate);\n    region.diskWritesPerSecTrend.add(region.diskWritesRate);\n  }\n\n  if (clusterStatementMap.size() < 500) {\n    for (int i = 1; i <= 500; ++i) {\n      if (LOGGER.infoEnabled()) {\n        LOGGER.info(\"Adding statement = \" + i);\n      }\n\n      updateClusterStatement(i);\n    }\n  } else if (clusterStatementMap.size() == 510) {\n    for (Iterator itSt = clusterStatementMap.values().iterator(); itSt.hasNext();) {\n      Cluster.Statement statement = (Cluster.Statement) itSt.next();\n      statement.setNumTimesCompiled((statement.getNumTimesCompiled() + 5));\n      statement.setNumExecution((statement.getNumExecution() + 5));\n      statement.setNumExecutionsInProgress((statement.getNumExecutionsInProgress() + 5));\n      statement.setNumTimesGlobalIndexLookup((statement.getNumTimesGlobalIndexLookup() + 5));\n      statement.setNumRowsModified((statement.getNumRowsModified() + 5));\n    }\n  } else if (clusterStatementMap.size() < 510) {\n    Cluster.Statement statement = new Cluster.Statement();\n    Random randomGenerator = new Random();\n    String statementDefinition = \"select * from member where member_name = member-510\"\n        + \" and lastUpdatedTime = '\" + new Date().toString() + \"'\";\n    Integer intVal = randomGenerator.nextInt(5);\n    statement.setQueryDefinition(statementDefinition);\n    statement.setNumTimesCompiled(intVal.longValue());\n    statement.setNumExecution(intVal.longValue());\n    statement.setNumExecutionsInProgress(intVal.longValue());\n    statement.setNumTimesGlobalIndexLookup(intVal.longValue());\n    statement.setNumRowsModified(intVal.longValue());\n    statement.setParseTime(randomGenerator.nextLong());\n    statement.setBindTime(randomGenerator.nextLong());\n    statement.setOptimizeTime(randomGenerator.nextLong());\n    statement.setRoutingInfoTime(randomGenerator.nextLong());\n    statement.setGenerateTime(randomGenerator.nextLong());\n    statement.setTotalCompilationTime(randomGenerator.nextLong());\n    statement.setExecutionTime(randomGenerator.nextLong());\n    statement.setProjectionTime(randomGenerator.nextLong());\n    statement.setTotalExecutionTime(randomGenerator.nextLong());\n    statement.setRowsModificationTime(randomGenerator.nextLong());\n    statement.setqNNumRowsSeen(intVal.longValue());\n    statement.setqNMsgSendTime(randomGenerator.nextLong());\n    statement.setqNMsgSerTime(randomGenerator.nextLong());\n    statement.setqNRespDeSerTime(randomGenerator.nextLong());\n    addClusterStatement(statementDefinition, statement);\n  }\n\n  return true;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2707,
        "instruction": "@GET\n@Path(\"getTableMetadata\")\n@Produces(\"application/json\")\npublic Response read(@Context final ServletContext servletContext,\n                     @Context final HttpHeaders headers,\n                     @QueryParam(\"table\") final String table)\n        throws Exception {\n    LOG.debug(\"getTableMetadata started\");\n    String jsonOutput;\n    try {\n        // 1. start MetadataFetcher\n        MetadataFetcher metadataFetcher = MetadataFetcherFactory.create(\"org.apache.hawq.pxf.plugins.hive.HiveMetadataFetcher\"); // TODO:\n                                                                                                                                 // nhorn\n                                                                                                                                 // -\n                                                                                                                                 // 09-03-15\n                                                                                                                                 // -\n                                                                                                                                 // pass\n                                                                                                                                 // as\n                                                                                                                                 // param\n\n        // 2. get Metadata\n        Metadata metadata = metadataFetcher.getTableMetadata(table);\n\n        // 3. serialize to JSON\n        jsonOutput = MetadataResponseFormatter.formatResponseString(metadata);\n\n\n    } catch (ClientAbortException e) {\n        LOG.error(\"Remote connection closed by HAWQ\", e);\n        throw e;\n    } catch (java.io.IOException e) {\n        LOG.error(\"Unhandled exception thrown\", e);\n        throw e;\n    }\n\n    return Response.ok(jsonOutput, MediaType.APPLICATION_JSON_TYPE).build();\n}",
        "accept_response": "@GET\n@Path(\"getTableMetadata\")\n@Produces(\"application/json\")\npublic Response read(@Context final ServletContext servletContext,\n                     @Context final HttpHeaders headers,\n                     @QueryParam(\"table\") final String table)\n        throws Exception {\n    LOG.debug(\"getTableMetadata started\");\n    String jsonOutput;\n    try {\n        // 1. start MetadataFetcher\n        MetadataFetcher metadataFetcher = MetadataFetcherFactory.create(\"org.apache.hawq.pxf.plugins.hive.HiveMetadataFetcher\"); // TODO:\n                                                                                                                                 // nhorn\n                                                                                                                                 // -\n                                                                                                                                 // 09-03-15\n                                                                                                                                 // -\n                                                                                                                                 // pass\n                                                                                                                                 // as\n                                                                                                                                 // param\n\n        // 2. get Metadata\n        Metadata metadata = metadataFetcher.getTableMetadata(table);\n\n        // 3. serialize to JSON\n        jsonOutput = MetadataResponseFormatter.formatResponseString(metadata);\n\n        LOG.debug(\"getTableMetadata output: \" + jsonOutput);\n\n    } catch (ClientAbortException e) {\n        LOG.error(\"Remote connection closed by HAWQ\", e);\n        throw e;\n    } catch (java.io.IOException e) {\n        LOG.error(\"Unhandled exception thrown\", e);\n        throw e;\n    }\n\n    return Response.ok(jsonOutput, MediaType.APPLICATION_JSON_TYPE).build();\n}",
        "reject_response": "@GET\n@Path(\"getTableMetadata\")\n@Produces(\"application/json\")\npublic Response read(@Context final ServletContext servletContext,\n                     @Context final HttpHeaders headers,\n                     @QueryParam(\"table\") final String table)\n        throws Exception {\n    LOG.debug(\"getTableMetadata started\");\n    String jsonOutput;\n    try {\n        // 1. start MetadataFetcher\n        MetadataFetcher metadataFetcher = MetadataFetcherFactory.create(\"org.apache.hawq.pxf.plugins.hive.HiveMetadataFetcher\"); // TODO:\n                                                                                                                                 // nhorn\n                                                                                                                                 // -\n                                                                                                                                 // 09-03-15\n                                                                                                                                 // -\n                                                                                                                                 // pass\n                                                                                                                                 // as\n                                                                                                                                 // param\n\n        // 2. get Metadata\n        Metadata metadata = metadataFetcher.getTableMetadata(table);\n\n        // 3. serialize to JSON\n        jsonOutput = MetadataResponseFormatter.formatResponseString(metadata);\n\n        Log.debug(\"getTableMetadata output: \" + jsonOutput);\n\n    } catch (ClientAbortException e) {\n        LOG.error(\"Remote connection closed by HAWQ\", e);\n        throw e;\n    } catch (java.io.IOException e) {\n        LOG.error(\"Unhandled exception thrown\", e);\n        throw e;\n    }\n\n    return Response.ok(jsonOutput, MediaType.APPLICATION_JSON_TYPE).build();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2532,
        "instruction": "private void logSsl(ChannelHandlerContext ctx)\n{\n    SslHandler sslHandler = ctx.pipeline().get(SslHandler.class);\n    if (sslHandler != null)\n    {\n        SSLSession session = sslHandler.engine().getSession();\n    }\n}",
        "accept_response": "private void logSsl(ChannelHandlerContext ctx)\n{\n    SslHandler sslHandler = ctx.pipeline().get(SslHandler.class);\n    if (sslHandler != null)\n    {\n        SSLSession session = sslHandler.engine().getSession();\n        logger.info(\"connection from peer {} to {}, protocol = {}\",\n                    ctx.channel().remoteAddress(), ctx.channel().localAddress(), session.getProtocol());\n    }\n}",
        "reject_response": "private void logSsl(ChannelHandlerContext ctx)\n{\n    SslHandler sslHandler = ctx.pipeline().get(SslHandler.class);\n    if (sslHandler != null)\n    {\n        SSLSession session = sslHandler.engine().getSession();\n        logger.info(\"connection from peer {} to {}, protocol = {}, cipher suite = {}\",\n                    ctx.channel().remoteAddress(), ctx.channel().localAddress(), session.getProtocol(), session.getCipherSuite());\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3149,
        "instruction": "@Override\npublic JobStatus submitJob(JobID jobId, String jobSubmitDir, Credentials ts)\nthrows IOException, InterruptedException {\n\n  ApplicationId appId = resMgrDelegate.getApplicationId();\n\n  FileSystem fs = FileSystem.get(conf);\n  // Loads the job.xml written by the user.\n  JobConf jobConf = new JobConf(new TezConfiguration(conf));\n\n  // Extract individual raw MR configs.\n  Configuration[] stageConfs = MultiStageMRConfToTezTranslator.getStageConfs(jobConf);\n\n  // Transform all confs to use Tez keys\n  for (int i = 0; i < stageConfs.length; i++) {\n    MRHelpers.translateMRConfToTez(stageConfs[i], false);\n  }\n\n  // create inputs to tezClient.submit()\n\n  // FIXME set up job resources\n  Map<String, LocalResource> jobLocalResources =\n      createJobLocalResources(stageConfs[0], jobSubmitDir);\n\n  // FIXME createDAG should take the tezConf as a parameter, instead of using\n  // MR keys.\n  DAG dag = createDAG(fs, jobId, stageConfs, jobSubmitDir, ts,\n      jobLocalResources);\n\n  List<String> vargs = new LinkedList<String>();\n  // admin command opts and user command opts\n  String mrAppMasterAdminOptions = conf.get(MRJobConfig.MR_AM_ADMIN_COMMAND_OPTS,\n      MRJobConfig.DEFAULT_MR_AM_ADMIN_COMMAND_OPTS);\n  warnForJavaLibPath(mrAppMasterAdminOptions, \"app master\",\n      MRJobConfig.MR_AM_ADMIN_COMMAND_OPTS, MRJobConfig.MR_AM_ADMIN_USER_ENV);\n  vargs.add(mrAppMasterAdminOptions);\n\n  // Add AM user command opts\n  String mrAppMasterUserOptions = conf.get(MRJobConfig.MR_AM_COMMAND_OPTS,\n      MRJobConfig.DEFAULT_MR_AM_COMMAND_OPTS);\n  warnForJavaLibPath(mrAppMasterUserOptions, \"app master\",\n      MRJobConfig.MR_AM_COMMAND_OPTS, MRJobConfig.MR_AM_ENV);\n  vargs.add(mrAppMasterUserOptions);\n\n  StringBuilder javaOpts = new StringBuilder();\n  for (String varg : vargs) {\n    javaOpts.append(varg).append(\" \");\n  }\n\n  // Setup the CLASSPATH in environment\n  // i.e. add { Hadoop jars, job jar, CWD } to classpath.\n  Map<String, String> environment = new HashMap<String, String>();\n\n  // Setup the environment variables for AM\n  MRHelpers.updateEnvBasedOnMRAMEnv(conf, environment);\n  StringBuilder envStrBuilder = new StringBuilder();\n  boolean first = true;\n  for (Entry<String, String> entry : environment.entrySet()) {\n    if (!first) {\n      envStrBuilder.append(\",\");\n    } else {\n      first = false;\n    }\n    envStrBuilder.append(entry.getKey()).append(\"=\").append(entry.getValue());\n  }\n  String envStr = envStrBuilder.toString();\n\n  TezConfiguration dagAMConf = getDAGAMConfFromMRConf();\n  dagAMConf.set(TezConfiguration.TEZ_AM_LAUNCH_CMD_OPTS, javaOpts.toString());\n  if (envStr.length() > 0) {\n    dagAMConf.set(TezConfiguration.TEZ_AM_LAUNCH_ENV, envStr);\n  }\n\n  // Submit to ResourceManager\n  try {\n    dagAMConf.set(TezConfiguration.TEZ_AM_STAGING_DIR,\n        jobSubmitDir);\n\n    // Set Tez parameters based on MR parameters.\n    String queueName = jobConf.get(JobContext.QUEUE_NAME,\n        YarnConfiguration.DEFAULT_QUEUE_NAME);\n    dagAMConf.set(TezConfiguration.TEZ_QUEUE_NAME, queueName);\n\n    int amMemMB = jobConf.getInt(MRJobConfig.MR_AM_VMEM_MB, MRJobConfig.DEFAULT_MR_AM_VMEM_MB);\n    int amCores = jobConf.getInt(MRJobConfig.MR_AM_CPU_VCORES, MRJobConfig.DEFAULT_MR_AM_CPU_VCORES);\n    dagAMConf.setInt(TezConfiguration.TEZ_AM_RESOURCE_MEMORY_MB, amMemMB);\n    dagAMConf.setInt(TezConfiguration.TEZ_AM_RESOURCE_CPU_VCORES, amCores);\n\n    dagAMConf.setInt(TezConfiguration.TEZ_AM_MAX_APP_ATTEMPTS,\n        jobConf.getInt(MRJobConfig.MR_AM_MAX_ATTEMPTS, MRJobConfig.DEFAULT_MR_AM_MAX_ATTEMPTS));\n\n    tezClient = new MRTezClient(\"MapReduce\", dagAMConf, false, jobLocalResources, ts);\n    tezClient.start();\n    tezClient.submitDAGApplication(appId, dag);\n    tezClient.stop();\n  } catch (TezException e) {\n    throw new IOException(e);\n  }\n\n  return getJobStatus(jobId);\n}",
        "accept_response": "@Override\npublic JobStatus submitJob(JobID jobId, String jobSubmitDir, Credentials ts)\nthrows IOException, InterruptedException {\n\n  ApplicationId appId = resMgrDelegate.getApplicationId();\n\n  FileSystem fs = FileSystem.get(conf);\n  // Loads the job.xml written by the user.\n  JobConf jobConf = new JobConf(new TezConfiguration(conf));\n\n  // Extract individual raw MR configs.\n  Configuration[] stageConfs = MultiStageMRConfToTezTranslator.getStageConfs(jobConf);\n\n  // Transform all confs to use Tez keys\n  for (int i = 0; i < stageConfs.length; i++) {\n    MRHelpers.translateMRConfToTez(stageConfs[i], false);\n  }\n\n  // create inputs to tezClient.submit()\n\n  // FIXME set up job resources\n  Map<String, LocalResource> jobLocalResources =\n      createJobLocalResources(stageConfs[0], jobSubmitDir);\n\n  // FIXME createDAG should take the tezConf as a parameter, instead of using\n  // MR keys.\n  DAG dag = createDAG(fs, jobId, stageConfs, jobSubmitDir, ts,\n      jobLocalResources);\n\n  List<String> vargs = new LinkedList<String>();\n  // admin command opts and user command opts\n  String mrAppMasterAdminOptions = conf.get(MRJobConfig.MR_AM_ADMIN_COMMAND_OPTS,\n      MRJobConfig.DEFAULT_MR_AM_ADMIN_COMMAND_OPTS);\n  warnForJavaLibPath(mrAppMasterAdminOptions, \"app master\",\n      MRJobConfig.MR_AM_ADMIN_COMMAND_OPTS, MRJobConfig.MR_AM_ADMIN_USER_ENV);\n  vargs.add(mrAppMasterAdminOptions);\n\n  // Add AM user command opts\n  String mrAppMasterUserOptions = conf.get(MRJobConfig.MR_AM_COMMAND_OPTS,\n      MRJobConfig.DEFAULT_MR_AM_COMMAND_OPTS);\n  warnForJavaLibPath(mrAppMasterUserOptions, \"app master\",\n      MRJobConfig.MR_AM_COMMAND_OPTS, MRJobConfig.MR_AM_ENV);\n  vargs.add(mrAppMasterUserOptions);\n\n  StringBuilder javaOpts = new StringBuilder();\n  for (String varg : vargs) {\n    javaOpts.append(varg).append(\" \");\n  }\n\n  // Setup the CLASSPATH in environment\n  // i.e. add { Hadoop jars, job jar, CWD } to classpath.\n  Map<String, String> environment = new HashMap<String, String>();\n\n  // Setup the environment variables for AM\n  MRHelpers.updateEnvBasedOnMRAMEnv(conf, environment);\n  StringBuilder envStrBuilder = new StringBuilder();\n  boolean first = true;\n  for (Entry<String, String> entry : environment.entrySet()) {\n    if (!first) {\n      envStrBuilder.append(\",\");\n    } else {\n      first = false;\n    }\n    envStrBuilder.append(entry.getKey()).append(\"=\").append(entry.getValue());\n  }\n  String envStr = envStrBuilder.toString();\n\n  TezConfiguration dagAMConf = getDAGAMConfFromMRConf();\n  dagAMConf.set(TezConfiguration.TEZ_AM_LAUNCH_CMD_OPTS, javaOpts.toString());\n  if (envStr.length() > 0) {\n    dagAMConf.set(TezConfiguration.TEZ_AM_LAUNCH_ENV, envStr);\n    LOG.debug(\"Setting MR AM env to : {}\", envStr);\n  }\n\n  // Submit to ResourceManager\n  try {\n    dagAMConf.set(TezConfiguration.TEZ_AM_STAGING_DIR,\n        jobSubmitDir);\n\n    // Set Tez parameters based on MR parameters.\n    String queueName = jobConf.get(JobContext.QUEUE_NAME,\n        YarnConfiguration.DEFAULT_QUEUE_NAME);\n    dagAMConf.set(TezConfiguration.TEZ_QUEUE_NAME, queueName);\n\n    int amMemMB = jobConf.getInt(MRJobConfig.MR_AM_VMEM_MB, MRJobConfig.DEFAULT_MR_AM_VMEM_MB);\n    int amCores = jobConf.getInt(MRJobConfig.MR_AM_CPU_VCORES, MRJobConfig.DEFAULT_MR_AM_CPU_VCORES);\n    dagAMConf.setInt(TezConfiguration.TEZ_AM_RESOURCE_MEMORY_MB, amMemMB);\n    dagAMConf.setInt(TezConfiguration.TEZ_AM_RESOURCE_CPU_VCORES, amCores);\n\n    dagAMConf.setInt(TezConfiguration.TEZ_AM_MAX_APP_ATTEMPTS,\n        jobConf.getInt(MRJobConfig.MR_AM_MAX_ATTEMPTS, MRJobConfig.DEFAULT_MR_AM_MAX_ATTEMPTS));\n\n    tezClient = new MRTezClient(\"MapReduce\", dagAMConf, false, jobLocalResources, ts);\n    tezClient.start();\n    tezClient.submitDAGApplication(appId, dag);\n    tezClient.stop();\n  } catch (TezException e) {\n    throw new IOException(e);\n  }\n\n  return getJobStatus(jobId);\n}",
        "reject_response": "@Override\npublic JobStatus submitJob(JobID jobId, String jobSubmitDir, Credentials ts)\nthrows IOException, InterruptedException {\n\n  ApplicationId appId = resMgrDelegate.getApplicationId();\n\n  FileSystem fs = FileSystem.get(conf);\n  // Loads the job.xml written by the user.\n  JobConf jobConf = new JobConf(new TezConfiguration(conf));\n\n  // Extract individual raw MR configs.\n  Configuration[] stageConfs = MultiStageMRConfToTezTranslator.getStageConfs(jobConf);\n\n  // Transform all confs to use Tez keys\n  for (int i = 0; i < stageConfs.length; i++) {\n    MRHelpers.translateMRConfToTez(stageConfs[i], false);\n  }\n\n  // create inputs to tezClient.submit()\n\n  // FIXME set up job resources\n  Map<String, LocalResource> jobLocalResources =\n      createJobLocalResources(stageConfs[0], jobSubmitDir);\n\n  // FIXME createDAG should take the tezConf as a parameter, instead of using\n  // MR keys.\n  DAG dag = createDAG(fs, jobId, stageConfs, jobSubmitDir, ts,\n      jobLocalResources);\n\n  List<String> vargs = new LinkedList<String>();\n  // admin command opts and user command opts\n  String mrAppMasterAdminOptions = conf.get(MRJobConfig.MR_AM_ADMIN_COMMAND_OPTS,\n      MRJobConfig.DEFAULT_MR_AM_ADMIN_COMMAND_OPTS);\n  warnForJavaLibPath(mrAppMasterAdminOptions, \"app master\",\n      MRJobConfig.MR_AM_ADMIN_COMMAND_OPTS, MRJobConfig.MR_AM_ADMIN_USER_ENV);\n  vargs.add(mrAppMasterAdminOptions);\n\n  // Add AM user command opts\n  String mrAppMasterUserOptions = conf.get(MRJobConfig.MR_AM_COMMAND_OPTS,\n      MRJobConfig.DEFAULT_MR_AM_COMMAND_OPTS);\n  warnForJavaLibPath(mrAppMasterUserOptions, \"app master\",\n      MRJobConfig.MR_AM_COMMAND_OPTS, MRJobConfig.MR_AM_ENV);\n  vargs.add(mrAppMasterUserOptions);\n\n  StringBuilder javaOpts = new StringBuilder();\n  for (String varg : vargs) {\n    javaOpts.append(varg).append(\" \");\n  }\n\n  // Setup the CLASSPATH in environment\n  // i.e. add { Hadoop jars, job jar, CWD } to classpath.\n  Map<String, String> environment = new HashMap<String, String>();\n\n  // Setup the environment variables for AM\n  MRHelpers.updateEnvBasedOnMRAMEnv(conf, environment);\n  StringBuilder envStrBuilder = new StringBuilder();\n  boolean first = true;\n  for (Entry<String, String> entry : environment.entrySet()) {\n    if (!first) {\n      envStrBuilder.append(\",\");\n    } else {\n      first = false;\n    }\n    envStrBuilder.append(entry.getKey()).append(\"=\").append(entry.getValue());\n  }\n  String envStr = envStrBuilder.toString();\n\n  TezConfiguration dagAMConf = getDAGAMConfFromMRConf();\n  dagAMConf.set(TezConfiguration.TEZ_AM_LAUNCH_CMD_OPTS, javaOpts.toString());\n  if (envStr.length() > 0) {\n    dagAMConf.set(TezConfiguration.TEZ_AM_LAUNCH_ENV, envStr);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Setting MR AM env to : \" + envStr);\n    }\n  }\n\n  // Submit to ResourceManager\n  try {\n    dagAMConf.set(TezConfiguration.TEZ_AM_STAGING_DIR,\n        jobSubmitDir);\n\n    // Set Tez parameters based on MR parameters.\n    String queueName = jobConf.get(JobContext.QUEUE_NAME,\n        YarnConfiguration.DEFAULT_QUEUE_NAME);\n    dagAMConf.set(TezConfiguration.TEZ_QUEUE_NAME, queueName);\n\n    int amMemMB = jobConf.getInt(MRJobConfig.MR_AM_VMEM_MB, MRJobConfig.DEFAULT_MR_AM_VMEM_MB);\n    int amCores = jobConf.getInt(MRJobConfig.MR_AM_CPU_VCORES, MRJobConfig.DEFAULT_MR_AM_CPU_VCORES);\n    dagAMConf.setInt(TezConfiguration.TEZ_AM_RESOURCE_MEMORY_MB, amMemMB);\n    dagAMConf.setInt(TezConfiguration.TEZ_AM_RESOURCE_CPU_VCORES, amCores);\n\n    dagAMConf.setInt(TezConfiguration.TEZ_AM_MAX_APP_ATTEMPTS,\n        jobConf.getInt(MRJobConfig.MR_AM_MAX_ATTEMPTS, MRJobConfig.DEFAULT_MR_AM_MAX_ATTEMPTS));\n\n    tezClient = new MRTezClient(\"MapReduce\", dagAMConf, false, jobLocalResources, ts);\n    tezClient.start();\n    tezClient.submitDAGApplication(appId, dag);\n    tezClient.stop();\n  } catch (TezException e) {\n    throw new IOException(e);\n  }\n\n  return getJobStatus(jobId);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2782,
        "instruction": "private ClassicHttpResponse handleCacheHit(\n        final HttpHost target,\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain,\n        final HttpCacheEntry entry) throws IOException, HttpException {\n    final HttpClientContext context  = scope.clientContext;\n    context.setAttribute(HttpCoreContext.HTTP_REQUEST, request);\n    recordCacheHit(target, request);\n    final Date now = getCurrentDate();\n    if (suitabilityChecker.canCachedResponseBeUsed(target, request, entry, now)) {\n        LOG.debug(\"Cache hit\");\n        try {\n            return convert(generateCachedResponse(request, context, entry, now), scope);\n        } catch (final ResourceIOException ex) {\n            recordCacheFailure(target, request);\n            if (!mayCallBackend(request)) {\n                return convert(generateGatewayTimeout(context), scope);\n            }\n            setResponseStatus(scope.clientContext, CacheResponseStatus.FAILURE);\n            return chain.proceed(request, scope);\n        }\n    } else if (!mayCallBackend(request)) {\n        LOG.debug(\"Cache entry not suitable but only-if-cached requested\");\n        return convert(generateGatewayTimeout(context), scope);\n    } else if (!(entry.getStatus() == HttpStatus.SC_NOT_MODIFIED && !suitabilityChecker.isConditional(request))) {\n        LOG.debug(\"Revalidating cache entry\");\n        try {\n            if (cacheRevalidator != null\n                    && !staleResponseNotAllowed(request, entry, now)\n                    && validityPolicy.mayReturnStaleWhileRevalidating(entry, now)) {\n                LOG.debug(\"Serving stale with asynchronous revalidation\");\n                final String exchangeId = ExecSupport.getNextExchangeId();\n                final ExecChain.Scope fork = new ExecChain.Scope(\n                        exchangeId,\n                        scope.route,\n                        scope.originalRequest,\n                        scope.execRuntime.fork(null),\n                        HttpClientContext.create());\n                final SimpleHttpResponse response = generateCachedResponse(request, context, entry, now);\n                cacheRevalidator.revalidateCacheEntry(\n                        responseCache.generateKey(target, request, entry),\n                        new DefaultCacheRevalidator.RevalidationCall() {\n\n                    @Override\n                    public ClassicHttpResponse execute() throws HttpException, IOException {\n                        return revalidateCacheEntry(target, request, fork, chain, entry);\n                    }\n\n                });\n                return convert(response, scope);\n            }\n            return revalidateCacheEntry(target, request, scope, chain, entry);\n        } catch (final IOException ioex) {\n            return convert(handleRevalidationFailure(request, context, entry, now), scope);\n        }\n    } else {\n        return callBackend(target, request, scope, chain);\n    }\n}",
        "accept_response": "private ClassicHttpResponse handleCacheHit(\n        final HttpHost target,\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain,\n        final HttpCacheEntry entry) throws IOException, HttpException {\n    final HttpClientContext context  = scope.clientContext;\n    context.setAttribute(HttpCoreContext.HTTP_REQUEST, request);\n    recordCacheHit(target, request);\n    final Date now = getCurrentDate();\n    if (suitabilityChecker.canCachedResponseBeUsed(target, request, entry, now)) {\n        LOG.debug(\"Cache hit\");\n        try {\n            return convert(generateCachedResponse(request, context, entry, now), scope);\n        } catch (final ResourceIOException ex) {\n            recordCacheFailure(target, request);\n            if (!mayCallBackend(request)) {\n                return convert(generateGatewayTimeout(context), scope);\n            }\n            setResponseStatus(scope.clientContext, CacheResponseStatus.FAILURE);\n            return chain.proceed(request, scope);\n        }\n    } else if (!mayCallBackend(request)) {\n        LOG.debug(\"Cache entry not suitable but only-if-cached requested\");\n        return convert(generateGatewayTimeout(context), scope);\n    } else if (!(entry.getStatus() == HttpStatus.SC_NOT_MODIFIED && !suitabilityChecker.isConditional(request))) {\n        LOG.debug(\"Revalidating cache entry\");\n        try {\n            if (cacheRevalidator != null\n                    && !staleResponseNotAllowed(request, entry, now)\n                    && validityPolicy.mayReturnStaleWhileRevalidating(entry, now)) {\n                LOG.debug(\"Serving stale with asynchronous revalidation\");\n                final String exchangeId = ExecSupport.getNextExchangeId();\n                final ExecChain.Scope fork = new ExecChain.Scope(\n                        exchangeId,\n                        scope.route,\n                        scope.originalRequest,\n                        scope.execRuntime.fork(null),\n                        HttpClientContext.create());\n                final SimpleHttpResponse response = generateCachedResponse(request, context, entry, now);\n                cacheRevalidator.revalidateCacheEntry(\n                        responseCache.generateKey(target, request, entry),\n                        new DefaultCacheRevalidator.RevalidationCall() {\n\n                    @Override\n                    public ClassicHttpResponse execute() throws HttpException, IOException {\n                        return revalidateCacheEntry(target, request, fork, chain, entry);\n                    }\n\n                });\n                return convert(response, scope);\n            }\n            return revalidateCacheEntry(target, request, scope, chain, entry);\n        } catch (final IOException ioex) {\n            return convert(handleRevalidationFailure(request, context, entry, now), scope);\n        }\n    } else {\n        LOG.debug(\"Cache entry not usable; calling backend\");\n        return callBackend(target, request, scope, chain);\n    }\n}",
        "reject_response": "private ClassicHttpResponse handleCacheHit(\n        final HttpHost target,\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain,\n        final HttpCacheEntry entry) throws IOException, HttpException {\n    final HttpClientContext context  = scope.clientContext;\n    context.setAttribute(HttpCoreContext.HTTP_REQUEST, request);\n    recordCacheHit(target, request);\n    final Date now = getCurrentDate();\n    if (suitabilityChecker.canCachedResponseBeUsed(target, request, entry, now)) {\n        LOG.debug(\"Cache hit\");\n        try {\n            return convert(generateCachedResponse(request, context, entry, now), scope);\n        } catch (final ResourceIOException ex) {\n            recordCacheFailure(target, request);\n            if (!mayCallBackend(request)) {\n                return convert(generateGatewayTimeout(context), scope);\n            }\n            setResponseStatus(scope.clientContext, CacheResponseStatus.FAILURE);\n            return chain.proceed(request, scope);\n        }\n    } else if (!mayCallBackend(request)) {\n        LOG.debug(\"Cache entry not suitable but only-if-cached requested\");\n        return convert(generateGatewayTimeout(context), scope);\n    } else if (!(entry.getStatus() == HttpStatus.SC_NOT_MODIFIED && !suitabilityChecker.isConditional(request))) {\n        LOG.debug(\"Revalidating cache entry\");\n        try {\n            if (cacheRevalidator != null\n                    && !staleResponseNotAllowed(request, entry, now)\n                    && validityPolicy.mayReturnStaleWhileRevalidating(entry, now)) {\n                LOG.debug(\"Serving stale with asynchronous revalidation\");\n                final String exchangeId = ExecSupport.getNextExchangeId();\n                final ExecChain.Scope fork = new ExecChain.Scope(\n                        exchangeId,\n                        scope.route,\n                        scope.originalRequest,\n                        scope.execRuntime.fork(null),\n                        HttpClientContext.create());\n                final SimpleHttpResponse response = generateCachedResponse(request, context, entry, now);\n                cacheRevalidator.revalidateCacheEntry(\n                        responseCache.generateKey(target, request, entry),\n                        new DefaultCacheRevalidator.RevalidationCall() {\n\n                    @Override\n                    public ClassicHttpResponse execute() throws HttpException, IOException {\n                        return revalidateCacheEntry(target, request, fork, chain, entry);\n                    }\n\n                });\n                return convert(response, scope);\n            }\n            return revalidateCacheEntry(target, request, scope, chain, entry);\n        } catch (final IOException ioex) {\n            return convert(handleRevalidationFailure(request, context, entry, now), scope);\n        }\n    } else {\n        log.debug(\"Cache entry not usable; calling backend\");\n        return callBackend(target, request, scope, chain);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2753,
        "instruction": "@Deprecated\npublic long renewDelegationToken(Token<DelegationTokenIdentifier> token)\n    throws IOException {\n  try {\n    return token.renew(conf);\n  } catch (InterruptedException ie) {\n    throw new RuntimeException(\"caught interrupted\", ie);\n  } catch (RemoteException re) {\n    throw re.unwrapRemoteException(InvalidToken.class,\n        AccessControlException.class);\n  }\n}",
        "accept_response": "@Deprecated\npublic long renewDelegationToken(Token<DelegationTokenIdentifier> token)\n    throws IOException {\n  LOG.info(\"Renewing {}\", DelegationTokenIdentifier.stringifyToken(token));\n  try {\n    return token.renew(conf);\n  } catch (InterruptedException ie) {\n    throw new RuntimeException(\"caught interrupted\", ie);\n  } catch (RemoteException re) {\n    throw re.unwrapRemoteException(InvalidToken.class,\n        AccessControlException.class);\n  }\n}",
        "reject_response": "@Deprecated\npublic long renewDelegationToken(Token<DelegationTokenIdentifier> token)\n    throws IOException {\n  LOG.info(\"Renewing \" + DelegationTokenIdentifier.stringifyToken(token));\n  try {\n    return token.renew(conf);\n  } catch (InterruptedException ie) {\n    throw new RuntimeException(\"caught interrupted\", ie);\n  } catch (RemoteException re) {\n    throw re.unwrapRemoteException(InvalidToken.class,\n        AccessControlException.class);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2491,
        "instruction": "@Override\nint runCmd(CommandLine cmdLine) throws Exception {\n    boolean disable = cmdLine.hasOption(\"d\");\n    boolean enable = cmdLine.hasOption(\"e\");\n\n    if (enable && disable) {\n        printUsage();\n        return 1;\n    }\n    ZooKeeper zk = null;\n    try {\n        zk = ZooKeeperClient.newBuilder()\n                .connectString(bkConf.getZkServers())\n                .sessionTimeoutMs(bkConf.getZkTimeout())\n                .build();\n        LedgerManagerFactory mFactory = LedgerManagerFactory.newLedgerManagerFactory(bkConf, zk);\n        LedgerUnderreplicationManager underreplicationManager = mFactory.newLedgerUnderreplicationManager();\n        if (!enable && !disable) {\n            boolean enabled = underreplicationManager.isLedgerReplicationEnabled();\n            System.out.println(\"Autorecovery is \" + (enabled ? \"enabled.\" : \"disabled.\"));\n        } else if (enable) {\n            if (underreplicationManager.isLedgerReplicationEnabled()) {\n                LOG.warn(\"Autorecovery already enabled. Doing nothing\");\n            } else {\n                LOG.info(\"Enabling autorecovery\");\n                underreplicationManager.enableLedgerReplication();\n            }\n        } else {\n            if (!underreplicationManager.isLedgerReplicationEnabled()) {\n                LOG.warn(\"Autorecovery already disabled. Doing nothing\");\n            } else {\n                LOG.info(\"Disabling autorecovery\");\n                underreplicationManager.disableLedgerReplication();\n            }\n        }\n    } finally {\n        if (zk != null) {\n            zk.close();\n        }\n    }\n\n    return 0;\n}",
        "accept_response": "@Override\nint runCmd(CommandLine cmdLine) throws Exception {\n    boolean disable = cmdLine.hasOption(\"d\");\n    boolean enable = cmdLine.hasOption(\"e\");\n\n    if (enable && disable) {\n        LOG.error(\"Only one of -enable and -disable can be specified\");\n        printUsage();\n        return 1;\n    }\n    ZooKeeper zk = null;\n    try {\n        zk = ZooKeeperClient.newBuilder()\n                .connectString(bkConf.getZkServers())\n                .sessionTimeoutMs(bkConf.getZkTimeout())\n                .build();\n        LedgerManagerFactory mFactory = LedgerManagerFactory.newLedgerManagerFactory(bkConf, zk);\n        LedgerUnderreplicationManager underreplicationManager = mFactory.newLedgerUnderreplicationManager();\n        if (!enable && !disable) {\n            boolean enabled = underreplicationManager.isLedgerReplicationEnabled();\n            System.out.println(\"Autorecovery is \" + (enabled ? \"enabled.\" : \"disabled.\"));\n        } else if (enable) {\n            if (underreplicationManager.isLedgerReplicationEnabled()) {\n                LOG.warn(\"Autorecovery already enabled. Doing nothing\");\n            } else {\n                LOG.info(\"Enabling autorecovery\");\n                underreplicationManager.enableLedgerReplication();\n            }\n        } else {\n            if (!underreplicationManager.isLedgerReplicationEnabled()) {\n                LOG.warn(\"Autorecovery already disabled. Doing nothing\");\n            } else {\n                LOG.info(\"Disabling autorecovery\");\n                underreplicationManager.disableLedgerReplication();\n            }\n        }\n    } finally {\n        if (zk != null) {\n            zk.close();\n        }\n    }\n\n    return 0;\n}",
        "reject_response": "@Override\nint runCmd(CommandLine cmdLine) throws Exception {\n    boolean disable = cmdLine.hasOption(\"d\");\n    boolean enable = cmdLine.hasOption(\"e\");\n\n    if (enable && disable) {\n    if ((!disable && !enable)\n        || (enable && disable)) {\n        LOG.error(\"One and only one of -enable and -disable must be specified\");\n        printUsage();\n        return 1;\n    }\n    ZooKeeper zk = null;\n    try {\n        zk = ZooKeeperClient.newBuilder()\n                .connectString(bkConf.getZkServers())\n                .sessionTimeoutMs(bkConf.getZkTimeout())\n                .build();\n        LedgerManagerFactory mFactory = LedgerManagerFactory.newLedgerManagerFactory(bkConf, zk);\n        LedgerUnderreplicationManager underreplicationManager = mFactory.newLedgerUnderreplicationManager();\n        if (!enable && !disable) {\n            boolean enabled = underreplicationManager.isLedgerReplicationEnabled();\n            System.out.println(\"Autorecovery is \" + (enabled ? \"enabled.\" : \"disabled.\"));\n        } else if (enable) {\n            if (underreplicationManager.isLedgerReplicationEnabled()) {\n                LOG.warn(\"Autorecovery already enabled. Doing nothing\");\n            } else {\n                LOG.info(\"Enabling autorecovery\");\n                underreplicationManager.enableLedgerReplication();\n            }\n        } else {\n            if (!underreplicationManager.isLedgerReplicationEnabled()) {\n                LOG.warn(\"Autorecovery already disabled. Doing nothing\");\n            } else {\n                LOG.info(\"Disabling autorecovery\");\n                underreplicationManager.disableLedgerReplication();\n            }\n        }\n    } finally {\n        if (zk != null) {\n            zk.close();\n        }\n    }\n\n    return 0;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2886,
        "instruction": "public void reset() {\n  LOG.info(\"\\n\\tResetting the StatefullFF global state index ...\");\n  LOG.info(\"\\n\\t...done\");\n  StatefulFF.resetGlobalStateIndex();\n  tms = new ArrayList<String>();\n  weights_file = \"\";\n  default_non_terminal = \"[X]\";\n  oovList = new ArrayList<OOVItem>();\n  oovList.add(new OOVItem(default_non_terminal, 1.0f));\n  goal_symbol = \"[GOAL]\";\n  amortized_sorting = true;\n  constrain_parse = false;\n  use_pos_labels = false;\n  true_oovs_only = false;\n  filter_grammar = false;\n  pop_limit = 100;\n  maxlen = 200;\n  use_unique_nbest = false;\n  include_align_index = false;\n  topN = 1;\n  outputFormat = \"%i ||| %s ||| %f ||| %c\";\n  num_parallel_decoders = 1;\n  hypergraphFilePattern = \"\";\n  mark_oovs = false;\n  // oracleFile = null;\n  parse = false; // perform synchronous parsing\n  features = new ArrayList<String>();\n  weights = new ArrayList<String>();\n  server_port = 0;\n\n  reordering_limit = 8;\n  num_translation_options = 20;\n  LOG.info(\"...done\");\n}",
        "accept_response": "public void reset() {\n  LOG.info(\"Resetting the JoshuaConfiguration to its defaults ...\");\n  LOG.info(\"\\n\\tResetting the StatefullFF global state index ...\");\n  LOG.info(\"\\n\\t...done\");\n  StatefulFF.resetGlobalStateIndex();\n  tms = new ArrayList<String>();\n  weights_file = \"\";\n  default_non_terminal = \"[X]\";\n  oovList = new ArrayList<OOVItem>();\n  oovList.add(new OOVItem(default_non_terminal, 1.0f));\n  goal_symbol = \"[GOAL]\";\n  amortized_sorting = true;\n  constrain_parse = false;\n  use_pos_labels = false;\n  true_oovs_only = false;\n  filter_grammar = false;\n  pop_limit = 100;\n  maxlen = 200;\n  use_unique_nbest = false;\n  include_align_index = false;\n  topN = 1;\n  outputFormat = \"%i ||| %s ||| %f ||| %c\";\n  num_parallel_decoders = 1;\n  hypergraphFilePattern = \"\";\n  mark_oovs = false;\n  // oracleFile = null;\n  parse = false; // perform synchronous parsing\n  features = new ArrayList<String>();\n  weights = new ArrayList<String>();\n  server_port = 0;\n\n  reordering_limit = 8;\n  num_translation_options = 20;\n  LOG.info(\"...done\");\n}",
        "reject_response": "public void reset() {\n  logger.info(\"Resetting the JoshuaConfiguration to its defaults ...\");\n  logger.info(\"\\n\\tResetting the StatefullFF global state index ...\");\n  logger.info(\"\\n\\t...done\");\n  LOG.info(\"\\n\\tResetting the StatefullFF global state index ...\");\n  LOG.info(\"\\n\\t...done\");\n  StatefulFF.resetGlobalStateIndex();\n  tms = new ArrayList<String>();\n  weights_file = \"\";\n  default_non_terminal = \"[X]\";\n  oovList = new ArrayList<OOVItem>();\n  oovList.add(new OOVItem(default_non_terminal, 1.0f));\n  goal_symbol = \"[GOAL]\";\n  amortized_sorting = true;\n  constrain_parse = false;\n  use_pos_labels = false;\n  true_oovs_only = false;\n  filter_grammar = false;\n  pop_limit = 100;\n  maxlen = 200;\n  use_unique_nbest = false;\n  include_align_index = false;\n  topN = 1;\n  outputFormat = \"%i ||| %s ||| %f ||| %c\";\n  num_parallel_decoders = 1;\n  hypergraphFilePattern = \"\";\n  mark_oovs = false;\n  // oracleFile = null;\n  parse = false; // perform synchronous parsing\n  features = new ArrayList<String>();\n  weights = new ArrayList<String>();\n  server_port = 0;\n\n  reordering_limit = 8;\n  num_translation_options = 20;\n  LOG.info(\"...done\");\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2780,
        "instruction": "private ClassicHttpResponse handleCacheHit(\n        final HttpHost target,\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain,\n        final HttpCacheEntry entry) throws IOException, HttpException {\n    final HttpClientContext context  = scope.clientContext;\n    context.setAttribute(HttpCoreContext.HTTP_REQUEST, request);\n    recordCacheHit(target, request);\n    final Date now = getCurrentDate();\n    if (suitabilityChecker.canCachedResponseBeUsed(target, request, entry, now)) {\n        LOG.debug(\"Cache hit\");\n        try {\n            return convert(generateCachedResponse(request, context, entry, now), scope);\n        } catch (final ResourceIOException ex) {\n            recordCacheFailure(target, request);\n            if (!mayCallBackend(request)) {\n                return convert(generateGatewayTimeout(context), scope);\n            }\n            setResponseStatus(scope.clientContext, CacheResponseStatus.FAILURE);\n            return chain.proceed(request, scope);\n        }\n    } else if (!mayCallBackend(request)) {\n        LOG.debug(\"Cache entry not suitable but only-if-cached requested\");\n        return convert(generateGatewayTimeout(context), scope);\n    } else if (!(entry.getStatus() == HttpStatus.SC_NOT_MODIFIED && !suitabilityChecker.isConditional(request))) {\n        try {\n            if (cacheRevalidator != null\n                    && !staleResponseNotAllowed(request, entry, now)\n                    && validityPolicy.mayReturnStaleWhileRevalidating(entry, now)) {\n                LOG.debug(\"Serving stale with asynchronous revalidation\");\n                final String exchangeId = ExecSupport.getNextExchangeId();\n                final ExecChain.Scope fork = new ExecChain.Scope(\n                        exchangeId,\n                        scope.route,\n                        scope.originalRequest,\n                        scope.execRuntime.fork(null),\n                        HttpClientContext.create());\n                final SimpleHttpResponse response = generateCachedResponse(request, context, entry, now);\n                cacheRevalidator.revalidateCacheEntry(\n                        responseCache.generateKey(target, request, entry),\n                        new DefaultCacheRevalidator.RevalidationCall() {\n\n                    @Override\n                    public ClassicHttpResponse execute() throws HttpException, IOException {\n                        return revalidateCacheEntry(target, request, fork, chain, entry);\n                    }\n\n                });\n                return convert(response, scope);\n            }\n            return revalidateCacheEntry(target, request, scope, chain, entry);\n        } catch (final IOException ioex) {\n            return convert(handleRevalidationFailure(request, context, entry, now), scope);\n        }\n    } else {\n        LOG.debug(\"Cache entry not usable; calling backend\");\n        return callBackend(target, request, scope, chain);\n    }\n}",
        "accept_response": "private ClassicHttpResponse handleCacheHit(\n        final HttpHost target,\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain,\n        final HttpCacheEntry entry) throws IOException, HttpException {\n    final HttpClientContext context  = scope.clientContext;\n    context.setAttribute(HttpCoreContext.HTTP_REQUEST, request);\n    recordCacheHit(target, request);\n    final Date now = getCurrentDate();\n    if (suitabilityChecker.canCachedResponseBeUsed(target, request, entry, now)) {\n        LOG.debug(\"Cache hit\");\n        try {\n            return convert(generateCachedResponse(request, context, entry, now), scope);\n        } catch (final ResourceIOException ex) {\n            recordCacheFailure(target, request);\n            if (!mayCallBackend(request)) {\n                return convert(generateGatewayTimeout(context), scope);\n            }\n            setResponseStatus(scope.clientContext, CacheResponseStatus.FAILURE);\n            return chain.proceed(request, scope);\n        }\n    } else if (!mayCallBackend(request)) {\n        LOG.debug(\"Cache entry not suitable but only-if-cached requested\");\n        return convert(generateGatewayTimeout(context), scope);\n    } else if (!(entry.getStatus() == HttpStatus.SC_NOT_MODIFIED && !suitabilityChecker.isConditional(request))) {\n        LOG.debug(\"Revalidating cache entry\");\n        try {\n            if (cacheRevalidator != null\n                    && !staleResponseNotAllowed(request, entry, now)\n                    && validityPolicy.mayReturnStaleWhileRevalidating(entry, now)) {\n                LOG.debug(\"Serving stale with asynchronous revalidation\");\n                final String exchangeId = ExecSupport.getNextExchangeId();\n                final ExecChain.Scope fork = new ExecChain.Scope(\n                        exchangeId,\n                        scope.route,\n                        scope.originalRequest,\n                        scope.execRuntime.fork(null),\n                        HttpClientContext.create());\n                final SimpleHttpResponse response = generateCachedResponse(request, context, entry, now);\n                cacheRevalidator.revalidateCacheEntry(\n                        responseCache.generateKey(target, request, entry),\n                        new DefaultCacheRevalidator.RevalidationCall() {\n\n                    @Override\n                    public ClassicHttpResponse execute() throws HttpException, IOException {\n                        return revalidateCacheEntry(target, request, fork, chain, entry);\n                    }\n\n                });\n                return convert(response, scope);\n            }\n            return revalidateCacheEntry(target, request, scope, chain, entry);\n        } catch (final IOException ioex) {\n            return convert(handleRevalidationFailure(request, context, entry, now), scope);\n        }\n    } else {\n        LOG.debug(\"Cache entry not usable; calling backend\");\n        return callBackend(target, request, scope, chain);\n    }\n}",
        "reject_response": "private ClassicHttpResponse handleCacheHit(\n        final HttpHost target,\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain,\n        final HttpCacheEntry entry) throws IOException, HttpException {\n    final HttpClientContext context  = scope.clientContext;\n    context.setAttribute(HttpCoreContext.HTTP_REQUEST, request);\n    recordCacheHit(target, request);\n    final Date now = getCurrentDate();\n    if (suitabilityChecker.canCachedResponseBeUsed(target, request, entry, now)) {\n        LOG.debug(\"Cache hit\");\n        try {\n            return convert(generateCachedResponse(request, context, entry, now), scope);\n        } catch (final ResourceIOException ex) {\n            recordCacheFailure(target, request);\n            if (!mayCallBackend(request)) {\n                return convert(generateGatewayTimeout(context), scope);\n            }\n            setResponseStatus(scope.clientContext, CacheResponseStatus.FAILURE);\n            return chain.proceed(request, scope);\n        }\n    } else if (!mayCallBackend(request)) {\n        LOG.debug(\"Cache entry not suitable but only-if-cached requested\");\n        return convert(generateGatewayTimeout(context), scope);\n    } else if (!(entry.getStatus() == HttpStatus.SC_NOT_MODIFIED && !suitabilityChecker.isConditional(request))) {\n        log.debug(\"Revalidating cache entry\");\n        try {\n            if (cacheRevalidator != null\n                    && !staleResponseNotAllowed(request, entry, now)\n                    && validityPolicy.mayReturnStaleWhileRevalidating(entry, now)) {\n                LOG.debug(\"Serving stale with asynchronous revalidation\");\n                final String exchangeId = ExecSupport.getNextExchangeId();\n                final ExecChain.Scope fork = new ExecChain.Scope(\n                        exchangeId,\n                        scope.route,\n                        scope.originalRequest,\n                        scope.execRuntime.fork(null),\n                        HttpClientContext.create());\n                final SimpleHttpResponse response = generateCachedResponse(request, context, entry, now);\n                cacheRevalidator.revalidateCacheEntry(\n                        responseCache.generateKey(target, request, entry),\n                        new DefaultCacheRevalidator.RevalidationCall() {\n\n                    @Override\n                    public ClassicHttpResponse execute() throws HttpException, IOException {\n                        return revalidateCacheEntry(target, request, fork, chain, entry);\n                    }\n\n                });\n                return convert(response, scope);\n            }\n            return revalidateCacheEntry(target, request, scope, chain, entry);\n        } catch (final IOException ioex) {\n            return convert(handleRevalidationFailure(request, context, entry, now), scope);\n        }\n    } else {\n        LOG.debug(\"Cache entry not usable; calling backend\");\n        return callBackend(target, request, scope, chain);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2919,
        "instruction": "private void addStreamTasks(Collection<TopicPartition> assignment) {\n    if (partitionAssignor == null)\n        throw new IllegalStateException(String.format(\"stream-thread [%s] Partition assignor has not been initialized while adding stream tasks: this should not happen.\", this.getName()));\n\n    HashMap<TaskId, Set<TopicPartition>> partitionsForTask = new HashMap<>();\n\n    for (TopicPartition partition : assignment) {\n        Set<TaskId> taskIds = partitionAssignor.tasksForPartition(partition);\n        for (TaskId taskId : taskIds) {\n            Set<TopicPartition> partitions = partitionsForTask.get(taskId);\n            if (partitions == null) {\n                partitions = new HashSet<>();\n                partitionsForTask.put(taskId, partitions);\n            }\n            partitions.add(partition);\n        }\n    }\n\n    // create the active tasks\n    for (Map.Entry<TaskId, Set<TopicPartition>> entry : partitionsForTask.entrySet()) {\n        TaskId taskId = entry.getKey();\n        Set<TopicPartition> partitions = entry.getValue();\n\n        try {\n            StreamTask task = createStreamTask(taskId, partitions);\n            activeTasks.put(taskId, task);\n\n            for (TopicPartition partition : partitions)\n                activeTasksByPartition.put(partition, task);\n        } catch (StreamsException e) {\n            throw e;\n        }\n    }\n}",
        "accept_response": "private void addStreamTasks(Collection<TopicPartition> assignment) {\n    if (partitionAssignor == null)\n        throw new IllegalStateException(String.format(\"stream-thread [%s] Partition assignor has not been initialized while adding stream tasks: this should not happen.\", this.getName()));\n\n    HashMap<TaskId, Set<TopicPartition>> partitionsForTask = new HashMap<>();\n\n    for (TopicPartition partition : assignment) {\n        Set<TaskId> taskIds = partitionAssignor.tasksForPartition(partition);\n        for (TaskId taskId : taskIds) {\n            Set<TopicPartition> partitions = partitionsForTask.get(taskId);\n            if (partitions == null) {\n                partitions = new HashSet<>();\n                partitionsForTask.put(taskId, partitions);\n            }\n            partitions.add(partition);\n        }\n    }\n\n    // create the active tasks\n    for (Map.Entry<TaskId, Set<TopicPartition>> entry : partitionsForTask.entrySet()) {\n        TaskId taskId = entry.getKey();\n        Set<TopicPartition> partitions = entry.getValue();\n\n        try {\n            StreamTask task = createStreamTask(taskId, partitions);\n            activeTasks.put(taskId, task);\n\n            for (TopicPartition partition : partitions)\n                activeTasksByPartition.put(partition, task);\n        } catch (StreamsException e) {\n            log.error(\"stream-thread [{}] Failed to create an active task #{}\", this.getName(), taskId, e);\n            throw e;\n        }\n    }\n}",
        "reject_response": "private void addStreamTasks(Collection<TopicPartition> assignment) {\n    if (partitionAssignor == null)\n        throw new IllegalStateException(String.format(\"stream-thread [%s] Partition assignor has not been initialized while adding stream tasks: this should not happen.\", this.getName()));\n\n    HashMap<TaskId, Set<TopicPartition>> partitionsForTask = new HashMap<>();\n\n    for (TopicPartition partition : assignment) {\n        Set<TaskId> taskIds = partitionAssignor.tasksForPartition(partition);\n        for (TaskId taskId : taskIds) {\n            Set<TopicPartition> partitions = partitionsForTask.get(taskId);\n            if (partitions == null) {\n                partitions = new HashSet<>();\n                partitionsForTask.put(taskId, partitions);\n            }\n            partitions.add(partition);\n        }\n    }\n\n    // create the active tasks\n    for (Map.Entry<TaskId, Set<TopicPartition>> entry : partitionsForTask.entrySet()) {\n        TaskId taskId = entry.getKey();\n        Set<TopicPartition> partitions = entry.getValue();\n\n        try {\n            StreamTask task = createStreamTask(taskId, partitions);\n            activeTasks.put(taskId, task);\n\n            for (TopicPartition partition : partitions)\n                activeTasksByPartition.put(partition, task);\n        } catch (StreamsException e) {\n            log.error(\"Failed to create an active task #\" + taskId + \" in thread [\" + this.getName() + \"]: \", e);\n            throw e;\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2479,
        "instruction": "public ApiSurface includingClass(Class<?> clazz) {\n  Set<Class<?>> newRootClasses = Sets.newHashSet();\n  newRootClasses.add(clazz);\n  newRootClasses.addAll(rootClasses);\n  return new ApiSurface(newRootClasses, patternsToPrune);\n}",
        "accept_response": "public ApiSurface includingClass(Class<?> clazz) {\n  Set<Class<?>> newRootClasses = Sets.newHashSet();\n  LOG.debug(\"Including class {}\", clazz);\n  newRootClasses.add(clazz);\n  newRootClasses.addAll(rootClasses);\n  return new ApiSurface(newRootClasses, patternsToPrune);\n}",
        "reject_response": "public ApiSurface includingClass(Class<?> clazz) {\n  Set<Class<?>> newRootClasses = Sets.newHashSet();\n  logger.debug(\"Including class {}\", clazz);\n  newRootClasses.add(clazz);\n  newRootClasses.addAll(rootClasses);\n  return new ApiSurface(newRootClasses, patternsToPrune);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3212,
        "instruction": "private boolean doAccept() {\n    boolean accepted = false;\n    SocketChannel sc = null;\n    try {\n        sc = acceptSocket.accept();\n        accepted = true;\n        InetAddress ia = sc.socket().getInetAddress();\n        int cnxncount = getClientCnxnCount(ia);\n\n        if (maxClientCnxns > 0 && cnxncount >= maxClientCnxns){\n            throw new IOException(\"Too many connections from \" + ia\n                                  + \" - max is \" + maxClientCnxns );\n        }\n\n        sc.configureBlocking(false);\n\n        // Round-robin assign this connection to a selector thread\n        if (!selectorIterator.hasNext()) {\n            selectorIterator = selectorThreads.iterator();\n        }\n        SelectorThread selectorThread = selectorIterator.next();\n        if (!selectorThread.addAcceptedConnection(sc)) {\n            throw new IOException(\n                \"Unable to add connection to selector queue\"\n                + (stopped ? \" (shutdown in progress)\" : \"\"));\n        }\n        acceptErrorLogger.flush();\n    } catch (IOException e) {\n        // accept, maxClientCnxns, configureBlocking\n        acceptErrorLogger.rateLimitLog(\n            \"Error accepting new connection: \" + e.getMessage());\n        fastCloseSock(sc);\n    }\n    return accepted;\n}",
        "accept_response": "private boolean doAccept() {\n    boolean accepted = false;\n    SocketChannel sc = null;\n    try {\n        sc = acceptSocket.accept();\n        accepted = true;\n        InetAddress ia = sc.socket().getInetAddress();\n        int cnxncount = getClientCnxnCount(ia);\n\n        if (maxClientCnxns > 0 && cnxncount >= maxClientCnxns){\n            throw new IOException(\"Too many connections from \" + ia\n                                  + \" - max is \" + maxClientCnxns );\n        }\n\n        LOG.debug(\"Accepted socket connection from \"\n                 + sc.socket().getRemoteSocketAddress());\n        sc.configureBlocking(false);\n\n        // Round-robin assign this connection to a selector thread\n        if (!selectorIterator.hasNext()) {\n            selectorIterator = selectorThreads.iterator();\n        }\n        SelectorThread selectorThread = selectorIterator.next();\n        if (!selectorThread.addAcceptedConnection(sc)) {\n            throw new IOException(\n                \"Unable to add connection to selector queue\"\n                + (stopped ? \" (shutdown in progress)\" : \"\"));\n        }\n        acceptErrorLogger.flush();\n    } catch (IOException e) {\n        // accept, maxClientCnxns, configureBlocking\n        acceptErrorLogger.rateLimitLog(\n            \"Error accepting new connection: \" + e.getMessage());\n        fastCloseSock(sc);\n    }\n    return accepted;\n}",
        "reject_response": "private boolean doAccept() {\n    boolean accepted = false;\n    SocketChannel sc = null;\n    try {\n        sc = acceptSocket.accept();\n        accepted = true;\n        InetAddress ia = sc.socket().getInetAddress();\n        int cnxncount = getClientCnxnCount(ia);\n\n        if (maxClientCnxns > 0 && cnxncount >= maxClientCnxns){\n            throw new IOException(\"Too many connections from \" + ia\n                                  + \" - max is \" + maxClientCnxns );\n        }\n\n        LOG.info(\"Accepted socket connection from \"\n        sc.configureBlocking(false);\n\n        // Round-robin assign this connection to a selector thread\n        if (!selectorIterator.hasNext()) {\n            selectorIterator = selectorThreads.iterator();\n        }\n        SelectorThread selectorThread = selectorIterator.next();\n        if (!selectorThread.addAcceptedConnection(sc)) {\n            throw new IOException(\n                \"Unable to add connection to selector queue\"\n                + (stopped ? \" (shutdown in progress)\" : \"\"));\n        }\n        acceptErrorLogger.flush();\n    } catch (IOException e) {\n        // accept, maxClientCnxns, configureBlocking\n        acceptErrorLogger.rateLimitLog(\n            \"Error accepting new connection: \" + e.getMessage());\n        fastCloseSock(sc);\n    }\n    return accepted;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3132,
        "instruction": "public void run() {\n  LOG.info(\"DAGAppMasterShutdownHook invoked\");\n  if(appMaster.getServiceState() == STATE.STOPPED) {\n    synchronized (appMaster.shutdownHandlerRunning) {\n      try {\n        if (appMaster.shutdownHandlerRunning.get()) {\n          LOG.info(\"The shutdown handler is still running, waiting for it to complete\");\n          appMaster.shutdownHandlerRunning.wait();\n          LOG.info(\"The shutdown handler has completed\");\n        }\n      } catch (InterruptedException e) {\n        // Ignore\n      }\n    }\n    return;\n  }\n\n  if(appMaster.getServiceState() == STATE.STARTED) {\n    // Notify TaskScheduler that a SIGTERM has been received so that it\n    // unregisters quickly with proper status\n    LOG.info(\"DAGAppMaster received a signal. Signaling TaskScheduler\");\n    appMaster.taskSchedulerManager.setSignalled(true);\n  }\n\n  if (EnumSet.of(DAGAppMasterState.NEW, DAGAppMasterState.INITED,\n      DAGAppMasterState.IDLE).contains(appMaster.state)) {\n        // DAG not in a final state. Must have receive a KILL signal\n    appMaster.state = DAGAppMasterState.KILLED;\n  } else if (appMaster.state == DAGAppMasterState.RUNNING) {\n    appMaster.state = DAGAppMasterState.ERROR;\n  }\n\n  appMaster.stop();\n\n}",
        "accept_response": "public void run() {\n  LOG.info(\"DAGAppMasterShutdownHook invoked\");\n  if(appMaster.getServiceState() == STATE.STOPPED) {\n    LOG.debug(\"DAGAppMaster already stopped. Ignoring signal\");\n    synchronized (appMaster.shutdownHandlerRunning) {\n      try {\n        if (appMaster.shutdownHandlerRunning.get()) {\n          LOG.info(\"The shutdown handler is still running, waiting for it to complete\");\n          appMaster.shutdownHandlerRunning.wait();\n          LOG.info(\"The shutdown handler has completed\");\n        }\n      } catch (InterruptedException e) {\n        // Ignore\n      }\n    }\n    return;\n  }\n\n  if(appMaster.getServiceState() == STATE.STARTED) {\n    // Notify TaskScheduler that a SIGTERM has been received so that it\n    // unregisters quickly with proper status\n    LOG.info(\"DAGAppMaster received a signal. Signaling TaskScheduler\");\n    appMaster.taskSchedulerManager.setSignalled(true);\n  }\n\n  if (EnumSet.of(DAGAppMasterState.NEW, DAGAppMasterState.INITED,\n      DAGAppMasterState.IDLE).contains(appMaster.state)) {\n        // DAG not in a final state. Must have receive a KILL signal\n    appMaster.state = DAGAppMasterState.KILLED;\n  } else if (appMaster.state == DAGAppMasterState.RUNNING) {\n    appMaster.state = DAGAppMasterState.ERROR;\n  }\n\n  appMaster.stop();\n\n}",
        "reject_response": "public void run() {\n  LOG.info(\"DAGAppMasterShutdownHook invoked\");\n  if(appMaster.getServiceState() == STATE.STOPPED) {\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"DAGAppMaster already stopped. Ignoring signal\");\n    }\n    synchronized (appMaster.shutdownHandlerRunning) {\n      try {\n        if (appMaster.shutdownHandlerRunning.get()) {\n          LOG.info(\"The shutdown handler is still running, waiting for it to complete\");\n          appMaster.shutdownHandlerRunning.wait();\n          LOG.info(\"The shutdown handler has completed\");\n        }\n      } catch (InterruptedException e) {\n        // Ignore\n      }\n    }\n    return;\n  }\n\n  if(appMaster.getServiceState() == STATE.STARTED) {\n    // Notify TaskScheduler that a SIGTERM has been received so that it\n    // unregisters quickly with proper status\n    LOG.info(\"DAGAppMaster received a signal. Signaling TaskScheduler\");\n    appMaster.taskSchedulerManager.setSignalled(true);\n  }\n\n  if (EnumSet.of(DAGAppMasterState.NEW, DAGAppMasterState.INITED,\n      DAGAppMasterState.IDLE).contains(appMaster.state)) {\n        // DAG not in a final state. Must have receive a KILL signal\n    appMaster.state = DAGAppMasterState.KILLED;\n  } else if (appMaster.state == DAGAppMasterState.RUNNING) {\n    appMaster.state = DAGAppMasterState.ERROR;\n  }\n\n  appMaster.stop();\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3245,
        "instruction": "public void disconnect() {\n\n    sendThread.close();\n    try {\n        sendThread.join();\n    } catch (InterruptedException ex) {\n        LOG.warn(\"Got interrupted while waiting for the sender thread to close\", ex);\n    }\n    eventThread.queueEventOfDeath();\n    if (zooKeeperSaslClient != null) {\n        zooKeeperSaslClient.shutdown();\n    }\n}",
        "accept_response": "public void disconnect() {\n    LOG.debug(\"Disconnecting client for session: 0x{}\", Long.toHexString(getSessionId()));\n\n    sendThread.close();\n    try {\n        sendThread.join();\n    } catch (InterruptedException ex) {\n        LOG.warn(\"Got interrupted while waiting for the sender thread to close\", ex);\n    }\n    eventThread.queueEventOfDeath();\n    if (zooKeeperSaslClient != null) {\n        zooKeeperSaslClient.shutdown();\n    }\n}",
        "reject_response": "public void disconnect() {\n    if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Disconnecting client for session: 0x\" + Long.toHexString(getSessionId()));\n    }\n\n    sendThread.close();\n    try {\n        sendThread.join();\n    } catch (InterruptedException ex) {\n        LOG.warn(\"Got interrupted while waiting for the sender thread to close\", ex);\n    }\n    eventThread.queueEventOfDeath();\n    if (zooKeeperSaslClient != null) {\n        zooKeeperSaslClient.shutdown();\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2414,
        "instruction": "@Override\npublic synchronized void deleteCluster(ClusterRequest request)\n    throws AmbariException {\n\n  if (request.getClusterName() == null\n      || request.getClusterName().isEmpty()) {\n    // FIXME throw correct error\n    throw new AmbariException(\"Invalid arguments\");\n  }\n  if (request.getHostNames() != null) {\n    // FIXME treat this as removing a host from a cluster?\n  } else {\n    // deleting whole cluster\n    clusters.deleteCluster(request.getClusterName());\n  }\n}",
        "accept_response": "@Override\npublic synchronized void deleteCluster(ClusterRequest request)\n    throws AmbariException {\n\n  if (request.getClusterName() == null\n      || request.getClusterName().isEmpty()) {\n    // FIXME throw correct error\n    throw new AmbariException(\"Invalid arguments\");\n  }\n  LOG.info(\"Received a delete cluster request, clusterName = \" + request.getClusterName());\n  if (request.getHostNames() != null) {\n    // FIXME treat this as removing a host from a cluster?\n  } else {\n    // deleting whole cluster\n    clusters.deleteCluster(request.getClusterName());\n  }\n}",
        "reject_response": "@Override\npublic synchronized void deleteCluster(ClusterRequest request)\n    throws AmbariException {\n\n  if (request.getClusterName() == null\n      || request.getClusterName().isEmpty()) {\n    // FIXME throw correct error\n    throw new AmbariException(\"Invalid arguments\");\n  }\n  LOG.info(\"Received a delete cluster request\"\n      + \", clusterName=\" + request.getClusterName());\n  if (request.getHostNames() != null) {\n    // FIXME treat this as removing a host from a cluster?\n  } else {\n    // deleting whole cluster\n    clusters.deleteCluster(request.getClusterName());\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3190,
        "instruction": "@VisibleForTesting\npublic long renewTokenForAppCollector(\n    AppLevelTimelineCollector appCollector) throws IOException {\n  if (appCollector.getDelegationTokenForApp() != null) {\n    return tokenMgrService.renewToken(appCollector.getDelegationTokenForApp(),\n        appCollector.getAppDelegationTokenRenewer());\n  } else {\n    return -1;\n  }\n}",
        "accept_response": "@VisibleForTesting\npublic long renewTokenForAppCollector(\n    AppLevelTimelineCollector appCollector) throws IOException {\n  if (appCollector.getDelegationTokenForApp() != null) {\n    return tokenMgrService.renewToken(appCollector.getDelegationTokenForApp(),\n        appCollector.getAppDelegationTokenRenewer());\n  } else {\n    LOG.info(\"Delegation token not available for renewal for app {}\",\n        appCollector.getTimelineEntityContext().getAppId());\n    return -1;\n  }\n}",
        "reject_response": "@VisibleForTesting\npublic long renewTokenForAppCollector(\n    AppLevelTimelineCollector appCollector) throws IOException {\n  if (appCollector.getDelegationTokenForApp() != null) {\n    return tokenMgrService.renewToken(appCollector.getDelegationTokenForApp(),\n        appCollector.getAppDelegationTokenRenewer());\n  } else {\n    LOG.info(\"Delegation token not available for renewal for app \" +\n    return -1;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2754,
        "instruction": "@Override // FsDatasetSpi\npublic Replica recoverClose(ExtendedBlock b, long newGS,\n    long expectedBlockLen) throws IOException {\n  while (true) {\n    try {\n      try (AutoCloseableLock lock = lockManager.writeLock(LockLevel.VOLUME,\n          b.getBlockPoolId(), getStorageUuidForLock(b))) {\n        // check replica's state\n        ReplicaInfo replicaInfo = recoverCheck(b, newGS, expectedBlockLen);\n        // bump the replica's GS\n        replicaInfo.bumpReplicaGS(newGS);\n        // finalize the replica if RBW\n        if (replicaInfo.getState() == ReplicaState.RBW) {\n          finalizeReplica(b.getBlockPoolId(), replicaInfo);\n        }\n        return replicaInfo;\n      }\n    } catch (MustStopExistingWriter e) {\n      e.getReplicaInPipeline()\n          .stopWriter(datanode.getDnConf().getXceiverStopTimeout());\n    }\n  }\n}",
        "accept_response": "@Override // FsDatasetSpi\npublic Replica recoverClose(ExtendedBlock b, long newGS,\n    long expectedBlockLen) throws IOException {\n  LOG.info(\"Recover failed close {}, new GS:{}, expectedBlockLen:{}\",\n      b, newGS, expectedBlockLen);\n  while (true) {\n    try {\n      try (AutoCloseableLock lock = lockManager.writeLock(LockLevel.VOLUME,\n          b.getBlockPoolId(), getStorageUuidForLock(b))) {\n        // check replica's state\n        ReplicaInfo replicaInfo = recoverCheck(b, newGS, expectedBlockLen);\n        // bump the replica's GS\n        replicaInfo.bumpReplicaGS(newGS);\n        // finalize the replica if RBW\n        if (replicaInfo.getState() == ReplicaState.RBW) {\n          finalizeReplica(b.getBlockPoolId(), replicaInfo);\n        }\n        return replicaInfo;\n      }\n    } catch (MustStopExistingWriter e) {\n      e.getReplicaInPipeline()\n          .stopWriter(datanode.getDnConf().getXceiverStopTimeout());\n    }\n  }\n}",
        "reject_response": "@Override // FsDatasetSpi\npublic Replica recoverClose(ExtendedBlock b, long newGS,\n    long expectedBlockLen) throws IOException {\n  LOG.info(\"Recover failed close \" + b);\n  while (true) {\n    try {\n      try (AutoCloseableLock lock = lockManager.writeLock(LockLevel.VOLUME,\n          b.getBlockPoolId(), getStorageUuidForLock(b))) {\n        // check replica's state\n        ReplicaInfo replicaInfo = recoverCheck(b, newGS, expectedBlockLen);\n        // bump the replica's GS\n        replicaInfo.bumpReplicaGS(newGS);\n        // finalize the replica if RBW\n        if (replicaInfo.getState() == ReplicaState.RBW) {\n          finalizeReplica(b.getBlockPoolId(), replicaInfo);\n        }\n        return replicaInfo;\n      }\n    } catch (MustStopExistingWriter e) {\n      e.getReplicaInPipeline()\n          .stopWriter(datanode.getDnConf().getXceiverStopTimeout());\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2708,
        "instruction": "@GET\n@Path(\"getTableMetadata\")\n@Produces(\"application/json\")\npublic Response read(@Context final ServletContext servletContext,\n                     @Context final HttpHeaders headers,\n                     @QueryParam(\"table\") final String table)\n        throws Exception {\n    LOG.debug(\"getTableMetadata started\");\n    String jsonOutput;\n    try {\n        // 1. start MetadataFetcher\n        MetadataFetcher metadataFetcher = MetadataFetcherFactory.create(\"org.apache.hawq.pxf.plugins.hive.HiveMetadataFetcher\"); // TODO:\n                                                                                                                                 // nhorn\n                                                                                                                                 // -\n                                                                                                                                 // 09-03-15\n                                                                                                                                 // -\n                                                                                                                                 // pass\n                                                                                                                                 // as\n                                                                                                                                 // param\n\n        // 2. get Metadata\n        Metadata metadata = metadataFetcher.getTableMetadata(table);\n\n        // 3. serialize to JSON\n        jsonOutput = MetadataResponseFormatter.formatResponseString(metadata);\n\n        LOG.debug(\"getTableMetadata output: \" + jsonOutput);\n\n    } catch (ClientAbortException e) {\n        LOG.error(\"Remote connection closed by HAWQ\", e);\n        throw e;\n    } catch (java.io.IOException e) {\n        throw e;\n    }\n\n    return Response.ok(jsonOutput, MediaType.APPLICATION_JSON_TYPE).build();\n}",
        "accept_response": "@GET\n@Path(\"getTableMetadata\")\n@Produces(\"application/json\")\npublic Response read(@Context final ServletContext servletContext,\n                     @Context final HttpHeaders headers,\n                     @QueryParam(\"table\") final String table)\n        throws Exception {\n    LOG.debug(\"getTableMetadata started\");\n    String jsonOutput;\n    try {\n        // 1. start MetadataFetcher\n        MetadataFetcher metadataFetcher = MetadataFetcherFactory.create(\"org.apache.hawq.pxf.plugins.hive.HiveMetadataFetcher\"); // TODO:\n                                                                                                                                 // nhorn\n                                                                                                                                 // -\n                                                                                                                                 // 09-03-15\n                                                                                                                                 // -\n                                                                                                                                 // pass\n                                                                                                                                 // as\n                                                                                                                                 // param\n\n        // 2. get Metadata\n        Metadata metadata = metadataFetcher.getTableMetadata(table);\n\n        // 3. serialize to JSON\n        jsonOutput = MetadataResponseFormatter.formatResponseString(metadata);\n\n        LOG.debug(\"getTableMetadata output: \" + jsonOutput);\n\n    } catch (ClientAbortException e) {\n        LOG.error(\"Remote connection closed by HAWQ\", e);\n        throw e;\n    } catch (java.io.IOException e) {\n        LOG.error(\"Unhandled exception thrown\", e);\n        throw e;\n    }\n\n    return Response.ok(jsonOutput, MediaType.APPLICATION_JSON_TYPE).build();\n}",
        "reject_response": "@GET\n@Path(\"getTableMetadata\")\n@Produces(\"application/json\")\npublic Response read(@Context final ServletContext servletContext,\n                     @Context final HttpHeaders headers,\n                     @QueryParam(\"table\") final String table)\n        throws Exception {\n    LOG.debug(\"getTableMetadata started\");\n    String jsonOutput;\n    try {\n        // 1. start MetadataFetcher\n        MetadataFetcher metadataFetcher = MetadataFetcherFactory.create(\"org.apache.hawq.pxf.plugins.hive.HiveMetadataFetcher\"); // TODO:\n                                                                                                                                 // nhorn\n                                                                                                                                 // -\n                                                                                                                                 // 09-03-15\n                                                                                                                                 // -\n                                                                                                                                 // pass\n                                                                                                                                 // as\n                                                                                                                                 // param\n\n        // 2. get Metadata\n        Metadata metadata = metadataFetcher.getTableMetadata(table);\n\n        // 3. serialize to JSON\n        jsonOutput = MetadataResponseFormatter.formatResponseString(metadata);\n\n        LOG.debug(\"getTableMetadata output: \" + jsonOutput);\n\n    } catch (ClientAbortException e) {\n        LOG.error(\"Remote connection closed by HAWQ\", e);\n        throw e;\n    } catch (java.io.IOException e) {\n        Log.error(\"Unhandled exception thrown\", e);\n        throw e;\n    }\n\n    return Response.ok(jsonOutput, MediaType.APPLICATION_JSON_TYPE).build();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3182,
        "instruction": "public void getContainerStatusAsync(ContainerId containerId, NodeId nodeId) {\n  try {\n    events.put(new ContainerEvent(containerId, nodeId, null,\n        ContainerEventType.QUERY_CONTAINER));\n  } catch (InterruptedException e) {\n    callbackHandler.onGetContainerStatusError(containerId, e);\n  }\n}",
        "accept_response": "public void getContainerStatusAsync(ContainerId containerId, NodeId nodeId) {\n  try {\n    events.put(new ContainerEvent(containerId, nodeId, null,\n        ContainerEventType.QUERY_CONTAINER));\n  } catch (InterruptedException e) {\n    LOG.warn(\"Exception when scheduling the event of querying \" +\n            \"the status of Container {}\", containerId);\n    callbackHandler.onGetContainerStatusError(containerId, e);\n  }\n}",
        "reject_response": "public void getContainerStatusAsync(ContainerId containerId, NodeId nodeId) {\n  try {\n    events.put(new ContainerEvent(containerId, nodeId, null,\n        ContainerEventType.QUERY_CONTAINER));\n  } catch (InterruptedException e) {\n    LOG.warn(\"Exception when scheduling the event of querying the status\" +\n        \" of Container \" + containerId);\n    callbackHandler.onGetContainerStatusError(containerId, e);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3016,
        "instruction": "@Override\npublic void write(final T message) {\n  final ChannelFuture future = channel.writeAndFlush(Unpooled.wrappedBuffer(encoder.encode(message)));\n  if (listener !=  null) {\n    future.addListener(new NettyChannelFutureListener<>(message, listener));\n  }\n}",
        "accept_response": "@Override\npublic void write(final T message) {\n  LOG.log(Level.FINEST, \"write {0} :: {1}\", new Object[] {channel, message});\n  final ChannelFuture future = channel.writeAndFlush(Unpooled.wrappedBuffer(encoder.encode(message)));\n  if (listener !=  null) {\n    future.addListener(new NettyChannelFutureListener<>(message, listener));\n  }\n}",
        "reject_response": "@Override\npublic void write(final T message) {\n  LOG.log(Level.FINEST, \"write {0} {1}\", new Object[]{channel, message});\n  final byte[] allData = encoder.encode(message);\n  // byte[] -> ByteBuf\n  final ChannelFuture future = channel.writeAndFlush(Unpooled.wrappedBuffer(encoder.encode(message)));\n  if (listener !=  null) {\n    future.addListener(new NettyChannelFutureListener<>(message, listener));\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2979,
        "instruction": "@Override\npublic AuthorizationResponse authorize(AuthorizationContext context) {\n\tboolean isDenied = false;\n\n\ttry {\n\t\tif (logger.isDebugEnabled()) {\n\t\t\tlogger.debug(\"==> RangerSolrAuthorizer.authorize()\");\n\t\t\tlogAuthorizationConext(context);\n\t\t}\n\n\t\tRangerMultiResourceAuditHandler auditHandler = new RangerMultiResourceAuditHandler();\n\n\t\tString userName = getUserName(context);\n\t\tSet<String> userGroups = getGroupsForUser(userName);\n\t\tString ip = null;\n\t\tDate eventTime = new Date();\n\n\t\t// // Set the IP\n\t\tif (useProxyIP) {\n\t\t\tip = context.getHttpHeader(proxyIPHeader);\n\t\t}\n\t\tif (ip == null) {\n\t\t\tip = context.getHttpHeader(\"REMOTE_ADDR\");\n\t\t}\n\t\tif (ip == null) {\n\t\t\tip = context.getRemoteAddr();\n\t\t}\n\n\t\t// Create the list of requests for access check. Each field is\n\t\t// broken\n\t\t// into a request\n\t\tList<RangerAccessRequestImpl> rangerRequests = new ArrayList<RangerAccessRequestImpl>();\n\t\tfor (CollectionRequest collectionRequest : context\n\t\t\t\t.getCollectionRequests()) {\n\n\t\t\tRangerAccessRequestImpl requestForCollection = createRequest(\n\t\t\t\t\tuserName, userGroups, ip, eventTime, context,\n\t\t\t\t\tcollectionRequest);\n\t\t\tif (requestForCollection != null) {\n\t\t\t\trangerRequests.add(requestForCollection);\n\t\t\t}\n\t\t}\n\t\tif (logger.isDebugEnabled()) {\n\t\t\tlogger.debug(\"rangerRequests.size()=\" + rangerRequests.size());\n\t\t}\n\t\ttry {\n\t\t\t// Let's check the access for each request/resource\n\t\t\tfor (RangerAccessRequestImpl rangerRequest : rangerRequests) {\n\t\t\t\tRangerAccessResult result = solrPlugin.isAccessAllowed(\n\t\t\t\t\t\trangerRequest, auditHandler);\n\t\t\t\tif (logger.isDebugEnabled()) {\n\t\t\t\t\tlogger.debug(\"rangerRequest=\" + result);\n\t\t\t\t}\n\t\t\t\tif (result == null || !result.getIsAllowed()) {\n\t\t\t\t\tisDenied = true;\n\t\t\t\t\t// rejecting on first failure\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t} finally {\n\t\t\tauditHandler.flushAudit();\n\t\t}\n\t} catch (Throwable t) {\n\t\tisDenied = true;\n\t\tMiscUtil.logErrorMessageByInterval(logger, t.getMessage(), t);\n\t}\n\tAuthorizationResponse response = null;\n\tif (isDenied) {\n\t\tresponse = new AuthorizationResponse(403);\n\t} else {\n\t\tresponse = new AuthorizationResponse(200);\n\t}\n\treturn response;\n}",
        "accept_response": "@Override\npublic AuthorizationResponse authorize(AuthorizationContext context) {\n\tboolean isDenied = false;\n\n\ttry {\n\t\tif (logger.isDebugEnabled()) {\n\t\t\tlogger.debug(\"==> RangerSolrAuthorizer.authorize()\");\n\t\t\tlogAuthorizationConext(context);\n\t\t}\n\n\t\tRangerMultiResourceAuditHandler auditHandler = new RangerMultiResourceAuditHandler();\n\n\t\tString userName = getUserName(context);\n\t\tSet<String> userGroups = getGroupsForUser(userName);\n\t\tString ip = null;\n\t\tDate eventTime = new Date();\n\n\t\t// // Set the IP\n\t\tif (useProxyIP) {\n\t\t\tip = context.getHttpHeader(proxyIPHeader);\n\t\t}\n\t\tif (ip == null) {\n\t\t\tip = context.getHttpHeader(\"REMOTE_ADDR\");\n\t\t}\n\t\tif (ip == null) {\n\t\t\tip = context.getRemoteAddr();\n\t\t}\n\n\t\t// Create the list of requests for access check. Each field is\n\t\t// broken\n\t\t// into a request\n\t\tList<RangerAccessRequestImpl> rangerRequests = new ArrayList<RangerAccessRequestImpl>();\n\t\tfor (CollectionRequest collectionRequest : context\n\t\t\t\t.getCollectionRequests()) {\n\n\t\t\tRangerAccessRequestImpl requestForCollection = createRequest(\n\t\t\t\t\tuserName, userGroups, ip, eventTime, context,\n\t\t\t\t\tcollectionRequest);\n\t\t\tif (requestForCollection != null) {\n\t\t\t\trangerRequests.add(requestForCollection);\n\t\t\t}\n\t\t}\n\t\tif (logger.isDebugEnabled()) {\n\t\t\tlogger.debug(\"rangerRequests.size()=\" + rangerRequests.size());\n\t\t}\n\t\ttry {\n\t\t\t// Let's check the access for each request/resource\n\t\t\tfor (RangerAccessRequestImpl rangerRequest : rangerRequests) {\n\t\t\t\tRangerAccessResult result = solrPlugin.isAccessAllowed(\n\t\t\t\t\t\trangerRequest, auditHandler);\n\t\t\t\tif (logger.isDebugEnabled()) {\n\t\t\t\t\tlogger.debug(\"rangerRequest=\" + result);\n\t\t\t\t}\n\t\t\t\tif (result == null || !result.getIsAllowed()) {\n\t\t\t\t\tisDenied = true;\n\t\t\t\t\t// rejecting on first failure\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t} finally {\n\t\t\tauditHandler.flushAudit();\n\t\t}\n\t} catch (Throwable t) {\n\t\tisDenied = true;\n\t\tMiscUtil.logErrorMessageByInterval(logger, t.getMessage(), t);\n\t}\n\tAuthorizationResponse response = null;\n\tif (isDenied) {\n\t\tresponse = new AuthorizationResponse(403);\n\t} else {\n\t\tresponse = new AuthorizationResponse(200);\n\t}\n\tif (logger.isDebugEnabled()) {\n\t\tlogger.debug( \"<== RangerSolrAuthorizer.authorize() result: \" + isDenied + \"Response : \" + response.getMessage());\n\t}\n\treturn response;\n}",
        "reject_response": "@Override\npublic AuthorizationResponse authorize(AuthorizationContext context) {\n\tboolean isDenied = false;\n\n\ttry {\n\t\tif (logger.isDebugEnabled()) {\n\t\t\tlogger.debug(\"==> RangerSolrAuthorizer.authorize()\");\n\t\t\tlogAuthorizationConext(context);\n\t\t}\n\n\t\tRangerMultiResourceAuditHandler auditHandler = new RangerMultiResourceAuditHandler();\n\n\t\tString userName = getUserName(context);\n\t\tSet<String> userGroups = getGroupsForUser(userName);\n\t\tString ip = null;\n\t\tDate eventTime = new Date();\n\n\t\t// // Set the IP\n\t\tif (useProxyIP) {\n\t\t\tip = context.getHttpHeader(proxyIPHeader);\n\t\t}\n\t\tif (ip == null) {\n\t\t\tip = context.getHttpHeader(\"REMOTE_ADDR\");\n\t\t}\n\t\tif (ip == null) {\n\t\t\tip = context.getRemoteAddr();\n\t\t}\n\n\t\t// Create the list of requests for access check. Each field is\n\t\t// broken\n\t\t// into a request\n\t\tList<RangerAccessRequestImpl> rangerRequests = new ArrayList<RangerAccessRequestImpl>();\n\t\tfor (CollectionRequest collectionRequest : context\n\t\t\t\t.getCollectionRequests()) {\n\n\t\t\tRangerAccessRequestImpl requestForCollection = createRequest(\n\t\t\t\t\tuserName, userGroups, ip, eventTime, context,\n\t\t\t\t\tcollectionRequest);\n\t\t\tif (requestForCollection != null) {\n\t\t\t\trangerRequests.add(requestForCollection);\n\t\t\t}\n\t\t}\n\t\tif (logger.isDebugEnabled()) {\n\t\t\tlogger.debug(\"rangerRequests.size()=\" + rangerRequests.size());\n\t\t}\n\t\ttry {\n\t\t\t// Let's check the access for each request/resource\n\t\t\tfor (RangerAccessRequestImpl rangerRequest : rangerRequests) {\n\t\t\t\tRangerAccessResult result = solrPlugin.isAccessAllowed(\n\t\t\t\t\t\trangerRequest, auditHandler);\n\t\t\t\tif (logger.isDebugEnabled()) {\n\t\t\t\t\tlogger.debug(\"rangerRequest=\" + result);\n\t\t\t\t}\n\t\t\t\tif (result == null || !result.getIsAllowed()) {\n\t\t\t\t\tisDenied = true;\n\t\t\t\t\t// rejecting on first failure\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t} finally {\n\t\t\tauditHandler.flushAudit();\n\t\t}\n\t} catch (Throwable t) {\n\t\tisDenied = true;\n\t\tMiscUtil.logErrorMessageByInterval(logger, t.getMessage(), t);\n\t}\n\tAuthorizationResponse response = null;\n\tif (isDenied) {\n\t\tresponse = new AuthorizationResponse(403);\n\t} else {\n\t\tresponse = new AuthorizationResponse(200);\n\t}\n\tif (logger.isDebugEnabled()) {\n\t\tlogger.debug(\"context=\" + context + \": returning: \" + isDenied);\n\t}\n\treturn response;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2430,
        "instruction": "protected Broker createRegionBroker(DestinationInterceptor destinationInterceptor) throws IOException {\n    RegionBroker regionBroker;\n    if (isUseJmx()) {\n        try {\n            regionBroker = new ManagedRegionBroker(this, getManagementContext(), getBrokerObjectName(),\n                getTaskRunnerFactory(), getConsumerSystemUsage(), destinationFactory, destinationInterceptor,getScheduler(),getExecutor());\n        } catch(MalformedObjectNameException me){\n            throw new IOException(me);\n        }\n    } else {\n        regionBroker = new RegionBroker(this, getTaskRunnerFactory(), getConsumerSystemUsage(), destinationFactory,\n                destinationInterceptor,getScheduler(),getExecutor());\n    }\n    destinationFactory.setRegionBroker(regionBroker);\n    regionBroker.setKeepDurableSubsActive(keepDurableSubsActive);\n    regionBroker.setBrokerName(getBrokerName());\n    regionBroker.getDestinationStatistics().setEnabled(enableStatistics);\n    regionBroker.setAllowTempAutoCreationOnSend(isAllowTempAutoCreationOnSend());\n    if (brokerId != null) {\n        regionBroker.setBrokerId(brokerId);\n    }\n    return regionBroker;\n}",
        "accept_response": "protected Broker createRegionBroker(DestinationInterceptor destinationInterceptor) throws IOException {\n    RegionBroker regionBroker;\n    if (isUseJmx()) {\n        try {\n            regionBroker = new ManagedRegionBroker(this, getManagementContext(), getBrokerObjectName(),\n                getTaskRunnerFactory(), getConsumerSystemUsage(), destinationFactory, destinationInterceptor,getScheduler(),getExecutor());\n        } catch(MalformedObjectNameException me){\n            LOG.warn(\"Cannot create ManagedRegionBroker due {}\", me.getMessage(), me);\n            throw new IOException(me);\n        }\n    } else {\n        regionBroker = new RegionBroker(this, getTaskRunnerFactory(), getConsumerSystemUsage(), destinationFactory,\n                destinationInterceptor,getScheduler(),getExecutor());\n    }\n    destinationFactory.setRegionBroker(regionBroker);\n    regionBroker.setKeepDurableSubsActive(keepDurableSubsActive);\n    regionBroker.setBrokerName(getBrokerName());\n    regionBroker.getDestinationStatistics().setEnabled(enableStatistics);\n    regionBroker.setAllowTempAutoCreationOnSend(isAllowTempAutoCreationOnSend());\n    if (brokerId != null) {\n        regionBroker.setBrokerId(brokerId);\n    }\n    return regionBroker;\n}",
        "reject_response": "protected Broker createRegionBroker(DestinationInterceptor destinationInterceptor) throws IOException {\n    RegionBroker regionBroker;\n    if (isUseJmx()) {\n        try {\n            regionBroker = new ManagedRegionBroker(this, getManagementContext(), getBrokerObjectName(),\n                getTaskRunnerFactory(), getConsumerSystemUsage(), destinationFactory, destinationInterceptor,getScheduler(),getExecutor());\n        } catch(MalformedObjectNameException me){\n            LOG.warn(\"Cannot create ManagedRegionBroker due \" + me.getMessage(), me);\n            throw new IOException(me);\n        }\n    } else {\n        regionBroker = new RegionBroker(this, getTaskRunnerFactory(), getConsumerSystemUsage(), destinationFactory,\n                destinationInterceptor,getScheduler(),getExecutor());\n    }\n    destinationFactory.setRegionBroker(regionBroker);\n    regionBroker.setKeepDurableSubsActive(keepDurableSubsActive);\n    regionBroker.setBrokerName(getBrokerName());\n    regionBroker.getDestinationStatistics().setEnabled(enableStatistics);\n    regionBroker.setAllowTempAutoCreationOnSend(isAllowTempAutoCreationOnSend());\n    if (brokerId != null) {\n        regionBroker.setBrokerId(brokerId);\n    }\n    return regionBroker;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2748,
        "instruction": "public void handleLifeline(DatanodeRegistration nodeReg,\n    StorageReport[] reports, long cacheCapacity,\n    long cacheUsed, int xceiverCount, int failedVolumes,\n    VolumeFailureSummary volumeFailureSummary) throws IOException {\n  DatanodeDescriptor nodeinfo = getDatanode(nodeReg);\n  if (nodeinfo == null || !nodeinfo.isRegistered()) {\n    // This can happen if the lifeline message comes when DataNode is either\n    // not registered at all or its marked dead at NameNode and expectes\n    // re-registration. Ignore lifeline messages without registration.\n    // Lifeline message handling can't send commands back to the DataNode to\n    // tell it to register, so simply exit.\n    return;\n  }\n  if (nodeinfo.isDisallowed()) {\n    // This is highly unlikely, because heartbeat handling is much more\n    // frequent and likely would have already sent the disallowed error.\n    // Lifeline messages are not intended to send any kind of control response\n    // back to the DataNode, so simply exit.\n    return;\n  }\n  heartbeatManager.updateLifeline(nodeinfo, reports, cacheCapacity, cacheUsed,\n      xceiverCount, failedVolumes, volumeFailureSummary);\n}",
        "accept_response": "public void handleLifeline(DatanodeRegistration nodeReg,\n    StorageReport[] reports, long cacheCapacity,\n    long cacheUsed, int xceiverCount, int failedVolumes,\n    VolumeFailureSummary volumeFailureSummary) throws IOException {\n  LOG.debug(\"Received handleLifeline from nodeReg = {}.\", nodeReg);\n  DatanodeDescriptor nodeinfo = getDatanode(nodeReg);\n  if (nodeinfo == null || !nodeinfo.isRegistered()) {\n    // This can happen if the lifeline message comes when DataNode is either\n    // not registered at all or its marked dead at NameNode and expectes\n    // re-registration. Ignore lifeline messages without registration.\n    // Lifeline message handling can't send commands back to the DataNode to\n    // tell it to register, so simply exit.\n    return;\n  }\n  if (nodeinfo.isDisallowed()) {\n    // This is highly unlikely, because heartbeat handling is much more\n    // frequent and likely would have already sent the disallowed error.\n    // Lifeline messages are not intended to send any kind of control response\n    // back to the DataNode, so simply exit.\n    return;\n  }\n  heartbeatManager.updateLifeline(nodeinfo, reports, cacheCapacity, cacheUsed,\n      xceiverCount, failedVolumes, volumeFailureSummary);\n}",
        "reject_response": "public void handleLifeline(DatanodeRegistration nodeReg,\n    StorageReport[] reports, long cacheCapacity,\n    long cacheUsed, int xceiverCount, int failedVolumes,\n    VolumeFailureSummary volumeFailureSummary) throws IOException {\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"Received handleLifeline from nodeReg = \" + nodeReg);\n  }\n  DatanodeDescriptor nodeinfo = getDatanode(nodeReg);\n  if (nodeinfo == null || !nodeinfo.isRegistered()) {\n    // This can happen if the lifeline message comes when DataNode is either\n    // not registered at all or its marked dead at NameNode and expectes\n    // re-registration. Ignore lifeline messages without registration.\n    // Lifeline message handling can't send commands back to the DataNode to\n    // tell it to register, so simply exit.\n    return;\n  }\n  if (nodeinfo.isDisallowed()) {\n    // This is highly unlikely, because heartbeat handling is much more\n    // frequent and likely would have already sent the disallowed error.\n    // Lifeline messages are not intended to send any kind of control response\n    // back to the DataNode, so simply exit.\n    return;\n  }\n  heartbeatManager.updateLifeline(nodeinfo, reports, cacheCapacity, cacheUsed,\n      xceiverCount, failedVolumes, volumeFailureSummary);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3121,
        "instruction": "@VisibleForTesting\nprotected DAGStatus getDAGStatusViaRM() throws TezException, IOException {\n  ApplicationReport appReport;\n  try {\n    appReport = frameworkClient.getApplicationReport(appId);\n  } catch (ApplicationNotFoundException e) {\n    LOG.info(\"DAG is no longer running - application not found by YARN\", e);\n    throw new DAGNotRunningException(e);\n  } catch (YarnException e) {\n    throw new TezException(e);\n  }\n\n  if(appReport == null) {\n    throw new TezException(\"Unknown/Invalid appId: \" + appId);\n  }\n\n  DAGProtos.DAGStatusProto.Builder builder = DAGProtos.DAGStatusProto.newBuilder();\n  DAGStatus dagStatus = new DAGStatus(builder, DagStatusSource.RM);\n  DAGProtos.DAGStatusStateProto dagState;\n  switch (appReport.getYarnApplicationState()) {\n    case NEW:\n    case NEW_SAVING:\n    case SUBMITTED:\n    case ACCEPTED:\n      dagState = DAGProtos.DAGStatusStateProto.DAG_SUBMITTED;\n      break;\n    case RUNNING:\n      dagState = DAGProtos.DAGStatusStateProto.DAG_RUNNING;\n      break;\n    case FAILED:\n      dagState = DAGProtos.DAGStatusStateProto.DAG_FAILED;\n      break;\n    case KILLED:\n      dagState = DAGProtos.DAGStatusStateProto.DAG_KILLED;\n      break;\n    case FINISHED:\n      switch(appReport.getFinalApplicationStatus()) {\n        case UNDEFINED:\n        case FAILED:\n          dagState = DAGProtos.DAGStatusStateProto.DAG_FAILED;\n          break;\n        case KILLED:\n          dagState = DAGProtos.DAGStatusStateProto.DAG_KILLED;\n          break;\n        case SUCCEEDED:\n          dagState = DAGProtos.DAGStatusStateProto.DAG_SUCCEEDED;\n          break;\n        default:\n          throw new TezUncheckedException(\"Encountered unknown final application\"\n              + \" status from YARN\"\n              + \", appState=\" + appReport.getYarnApplicationState()\n              + \", finalStatus=\" + appReport.getFinalApplicationStatus());\n      }\n      break;\n    default:\n      throw new TezUncheckedException(\"Encountered unknown application state\"\n          + \" from YARN, appState=\" + appReport.getYarnApplicationState());\n  }\n\n  builder.setState(dagState);\n  // workaround before YARN-2560 is fixed\n  if (appReport.getFinalApplicationStatus() == FinalApplicationStatus.FAILED\n      || appReport.getFinalApplicationStatus() == FinalApplicationStatus.KILLED) {\n    long startTime = System.currentTimeMillis();\n    while((appReport.getDiagnostics() == null\n        || appReport.getDiagnostics().isEmpty())\n        && (System.currentTimeMillis() - startTime) < diagnoticsWaitTimeout) {\n      try {\n        Thread.sleep(100);\n        appReport = frameworkClient.getApplicationReport(appId);\n      } catch (YarnException e) {\n        throw new TezException(e);\n      } catch (InterruptedException e) {\n        throw new TezException(e);\n      }\n    }\n  }\n  if (appReport.getDiagnostics() != null) {\n    builder.addAllDiagnostics(Collections.singleton(appReport.getDiagnostics()));\n  }\n  return dagStatus;\n}",
        "accept_response": "@VisibleForTesting\nprotected DAGStatus getDAGStatusViaRM() throws TezException, IOException {\n  LOG.debug(\"GetDAGStatus via AM for app: {} dag:{}\", appId, dagId);\n  ApplicationReport appReport;\n  try {\n    appReport = frameworkClient.getApplicationReport(appId);\n  } catch (ApplicationNotFoundException e) {\n    LOG.info(\"DAG is no longer running - application not found by YARN\", e);\n    throw new DAGNotRunningException(e);\n  } catch (YarnException e) {\n    throw new TezException(e);\n  }\n\n  if(appReport == null) {\n    throw new TezException(\"Unknown/Invalid appId: \" + appId);\n  }\n\n  DAGProtos.DAGStatusProto.Builder builder = DAGProtos.DAGStatusProto.newBuilder();\n  DAGStatus dagStatus = new DAGStatus(builder, DagStatusSource.RM);\n  DAGProtos.DAGStatusStateProto dagState;\n  switch (appReport.getYarnApplicationState()) {\n    case NEW:\n    case NEW_SAVING:\n    case SUBMITTED:\n    case ACCEPTED:\n      dagState = DAGProtos.DAGStatusStateProto.DAG_SUBMITTED;\n      break;\n    case RUNNING:\n      dagState = DAGProtos.DAGStatusStateProto.DAG_RUNNING;\n      break;\n    case FAILED:\n      dagState = DAGProtos.DAGStatusStateProto.DAG_FAILED;\n      break;\n    case KILLED:\n      dagState = DAGProtos.DAGStatusStateProto.DAG_KILLED;\n      break;\n    case FINISHED:\n      switch(appReport.getFinalApplicationStatus()) {\n        case UNDEFINED:\n        case FAILED:\n          dagState = DAGProtos.DAGStatusStateProto.DAG_FAILED;\n          break;\n        case KILLED:\n          dagState = DAGProtos.DAGStatusStateProto.DAG_KILLED;\n          break;\n        case SUCCEEDED:\n          dagState = DAGProtos.DAGStatusStateProto.DAG_SUCCEEDED;\n          break;\n        default:\n          throw new TezUncheckedException(\"Encountered unknown final application\"\n              + \" status from YARN\"\n              + \", appState=\" + appReport.getYarnApplicationState()\n              + \", finalStatus=\" + appReport.getFinalApplicationStatus());\n      }\n      break;\n    default:\n      throw new TezUncheckedException(\"Encountered unknown application state\"\n          + \" from YARN, appState=\" + appReport.getYarnApplicationState());\n  }\n\n  builder.setState(dagState);\n  // workaround before YARN-2560 is fixed\n  if (appReport.getFinalApplicationStatus() == FinalApplicationStatus.FAILED\n      || appReport.getFinalApplicationStatus() == FinalApplicationStatus.KILLED) {\n    long startTime = System.currentTimeMillis();\n    while((appReport.getDiagnostics() == null\n        || appReport.getDiagnostics().isEmpty())\n        && (System.currentTimeMillis() - startTime) < diagnoticsWaitTimeout) {\n      try {\n        Thread.sleep(100);\n        appReport = frameworkClient.getApplicationReport(appId);\n      } catch (YarnException e) {\n        throw new TezException(e);\n      } catch (InterruptedException e) {\n        throw new TezException(e);\n      }\n    }\n  }\n  if (appReport.getDiagnostics() != null) {\n    builder.addAllDiagnostics(Collections.singleton(appReport.getDiagnostics()));\n  }\n  return dagStatus;\n}",
        "reject_response": "@VisibleForTesting\nprotected DAGStatus getDAGStatusViaRM() throws TezException, IOException {\n  if(LOG.isDebugEnabled()) {\n    LOG.debug(\"GetDAGStatus via AM for app: \" + appId + \" dag:\" + dagId);\n  }\n  ApplicationReport appReport;\n  try {\n    appReport = frameworkClient.getApplicationReport(appId);\n  } catch (ApplicationNotFoundException e) {\n    LOG.info(\"DAG is no longer running - application not found by YARN\", e);\n    throw new DAGNotRunningException(e);\n  } catch (YarnException e) {\n    throw new TezException(e);\n  }\n\n  if(appReport == null) {\n    throw new TezException(\"Unknown/Invalid appId: \" + appId);\n  }\n\n  DAGProtos.DAGStatusProto.Builder builder = DAGProtos.DAGStatusProto.newBuilder();\n  DAGStatus dagStatus = new DAGStatus(builder, DagStatusSource.RM);\n  DAGProtos.DAGStatusStateProto dagState;\n  switch (appReport.getYarnApplicationState()) {\n    case NEW:\n    case NEW_SAVING:\n    case SUBMITTED:\n    case ACCEPTED:\n      dagState = DAGProtos.DAGStatusStateProto.DAG_SUBMITTED;\n      break;\n    case RUNNING:\n      dagState = DAGProtos.DAGStatusStateProto.DAG_RUNNING;\n      break;\n    case FAILED:\n      dagState = DAGProtos.DAGStatusStateProto.DAG_FAILED;\n      break;\n    case KILLED:\n      dagState = DAGProtos.DAGStatusStateProto.DAG_KILLED;\n      break;\n    case FINISHED:\n      switch(appReport.getFinalApplicationStatus()) {\n        case UNDEFINED:\n        case FAILED:\n          dagState = DAGProtos.DAGStatusStateProto.DAG_FAILED;\n          break;\n        case KILLED:\n          dagState = DAGProtos.DAGStatusStateProto.DAG_KILLED;\n          break;\n        case SUCCEEDED:\n          dagState = DAGProtos.DAGStatusStateProto.DAG_SUCCEEDED;\n          break;\n        default:\n          throw new TezUncheckedException(\"Encountered unknown final application\"\n              + \" status from YARN\"\n              + \", appState=\" + appReport.getYarnApplicationState()\n              + \", finalStatus=\" + appReport.getFinalApplicationStatus());\n      }\n      break;\n    default:\n      throw new TezUncheckedException(\"Encountered unknown application state\"\n          + \" from YARN, appState=\" + appReport.getYarnApplicationState());\n  }\n\n  builder.setState(dagState);\n  // workaround before YARN-2560 is fixed\n  if (appReport.getFinalApplicationStatus() == FinalApplicationStatus.FAILED\n      || appReport.getFinalApplicationStatus() == FinalApplicationStatus.KILLED) {\n    long startTime = System.currentTimeMillis();\n    while((appReport.getDiagnostics() == null\n        || appReport.getDiagnostics().isEmpty())\n        && (System.currentTimeMillis() - startTime) < diagnoticsWaitTimeout) {\n      try {\n        Thread.sleep(100);\n        appReport = frameworkClient.getApplicationReport(appId);\n      } catch (YarnException e) {\n        throw new TezException(e);\n      } catch (InterruptedException e) {\n        throw new TezException(e);\n      }\n    }\n  }\n  if (appReport.getDiagnostics() != null) {\n    builder.addAllDiagnostics(Collections.singleton(appReport.getDiagnostics()));\n  }\n  return dagStatus;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2375,
        "instruction": "private void getMasterLock(final String zMasterLoc) throws KeeperException, InterruptedException {\n\n  final String masterClientAddress = hostname + \":\" + getConfiguration().getPort(Property.MASTER_CLIENTPORT)[0];\n\n  while (true) {\n\n    MasterLockWatcher masterLockWatcher = new MasterLockWatcher();\n    masterLock = new ZooLock(zMasterLoc);\n    masterLock.lockAsync(masterLockWatcher, masterClientAddress.getBytes());\n\n    masterLockWatcher.waitForChange();\n\n    if (masterLockWatcher.acquiredLock) {\n      break;\n    }\n\n    if (!masterLockWatcher.failedToAcquireLock) {\n      throw new IllegalStateException(\"master lock in unknown state\");\n    }\n\n    masterLock.tryToCancelAsyncLockOrUnlock();\n\n    sleepUninterruptibly(TIME_TO_WAIT_BETWEEN_LOCK_CHECKS, TimeUnit.MILLISECONDS);\n  }\n  log.info(\"Acquired Master Lock\");\n  setMasterState(MasterState.HAVE_LOCK);\n}",
        "accept_response": "private void getMasterLock(final String zMasterLoc) throws KeeperException, InterruptedException {\n  log.info(\"Attempting to acquire Master Lock\");\n\n  final String masterClientAddress = hostname + \":\" + getConfiguration().getPort(Property.MASTER_CLIENTPORT)[0];\n\n  while (true) {\n\n    MasterLockWatcher masterLockWatcher = new MasterLockWatcher();\n    masterLock = new ZooLock(zMasterLoc);\n    masterLock.lockAsync(masterLockWatcher, masterClientAddress.getBytes());\n\n    masterLockWatcher.waitForChange();\n\n    if (masterLockWatcher.acquiredLock) {\n      break;\n    }\n\n    if (!masterLockWatcher.failedToAcquireLock) {\n      throw new IllegalStateException(\"master lock in unknown state\");\n    }\n\n    masterLock.tryToCancelAsyncLockOrUnlock();\n\n    sleepUninterruptibly(TIME_TO_WAIT_BETWEEN_LOCK_CHECKS, TimeUnit.MILLISECONDS);\n  }\n  log.info(\"Acquired Master Lock\");\n  setMasterState(MasterState.HAVE_LOCK);\n}",
        "reject_response": "private void getMasterLock(final String zMasterLoc) throws KeeperException, InterruptedException {\n  log.info(\"trying to get master lock\");\n\n  final String masterClientAddress = hostname + \":\" + getConfiguration().getPort(Property.MASTER_CLIENTPORT)[0];\n\n  while (true) {\n\n    MasterLockWatcher masterLockWatcher = new MasterLockWatcher();\n    masterLock = new ZooLock(zMasterLoc);\n    masterLock.lockAsync(masterLockWatcher, masterClientAddress.getBytes());\n\n    masterLockWatcher.waitForChange();\n\n    if (masterLockWatcher.acquiredLock) {\n      break;\n    }\n\n    if (!masterLockWatcher.failedToAcquireLock) {\n      throw new IllegalStateException(\"master lock in unknown state\");\n    }\n\n    masterLock.tryToCancelAsyncLockOrUnlock();\n\n    sleepUninterruptibly(TIME_TO_WAIT_BETWEEN_LOCK_CHECKS, TimeUnit.MILLISECONDS);\n  }\n  log.info(\"Acquired Master Lock\");\n  setMasterState(MasterState.HAVE_LOCK);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2690,
        "instruction": "private int readPktLen(DataInput in) throws IOException {\n    pktlen = EOF;\n\n    try {\n        pktlen = in.readInt();\n    } catch (EOFException e) {\n        return EOF;\n    }\n    if (pktlen == EOF) {\n        LOG.debug(\"Reached end of stream (returned -1)\");\n    }\n\n    return pktlen;\n}",
        "accept_response": "private int readPktLen(DataInput in) throws IOException {\n    pktlen = EOF;\n\n    try {\n        pktlen = in.readInt();\n    } catch (EOFException e) {\n        LOG.debug(\"Reached end of stream (EOFException)\");\n        return EOF;\n    }\n    if (pktlen == EOF) {\n        LOG.debug(\"Reached end of stream (returned -1)\");\n    }\n\n    return pktlen;\n}",
        "reject_response": "private int readPktLen(DataInput in) throws IOException {\n    pktlen = EOF;\n\n    try {\n        pktlen = in.readInt();\n    } catch (EOFException e) {\n        Log.debug(\"Reached end of stream (EOFException)\");\n        return EOF;\n    }\n    if (pktlen == EOF) {\n        LOG.debug(\"Reached end of stream (returned -1)\");\n    }\n\n    return pktlen;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2539,
        "instruction": "@Override\npublic void send(LogEvent event) {\n    String cat = \"org.apache.cxf.services.\" + event.getPortTypeName().getLocalPart() + \".\" + event.getType();\n    Logger log = LoggerFactory.getLogger(cat);\n    Set<String> keys = new HashSet<>();\n    try {\n        put(keys, \"Type\", event.getType().toString());\n        put(keys, \"Address\", event.getAddress());\n        put(keys, \"HttpMethod\", event.getHttpMethod());\n        put(keys, \"Content-Type\", event.getContentType());\n        put(keys, \"ResponseCode\", event.getResponseCode());\n        put(keys, \"ExchangeId\", event.getExchangeId());\n        put(keys, \"MessageId\", event.getMessageId());\n        if (event.getServiceName() != null) {\n            put(keys, \"ServiceName\", localPart(event.getServiceName()));\n            put(keys, \"PortName\", localPart(event.getPortName()));\n            put(keys, \"PortTypeName\", localPart(event.getPortTypeName()));\n        }\n        if (event.getFullContentFile() != null) {\n            put(keys, \"FullContentFile\", event.getFullContentFile().getAbsolutePath());\n        }\n        put(keys, \"Headers\", event.getHeaders().toString());\n    } finally {\n        for (String key : keys) {\n            MDC.remove(key);\n        }\n    }\n\n}",
        "accept_response": "@Override\npublic void send(LogEvent event) {\n    String cat = \"org.apache.cxf.services.\" + event.getPortTypeName().getLocalPart() + \".\" + event.getType();\n    Logger log = LoggerFactory.getLogger(cat);\n    Set<String> keys = new HashSet<>();\n    try {\n        put(keys, \"Type\", event.getType().toString());\n        put(keys, \"Address\", event.getAddress());\n        put(keys, \"HttpMethod\", event.getHttpMethod());\n        put(keys, \"Content-Type\", event.getContentType());\n        put(keys, \"ResponseCode\", event.getResponseCode());\n        put(keys, \"ExchangeId\", event.getExchangeId());\n        put(keys, \"MessageId\", event.getMessageId());\n        if (event.getServiceName() != null) {\n            put(keys, \"ServiceName\", localPart(event.getServiceName()));\n            put(keys, \"PortName\", localPart(event.getPortName()));\n            put(keys, \"PortTypeName\", localPart(event.getPortTypeName()));\n        }\n        if (event.getFullContentFile() != null) {\n            put(keys, \"FullContentFile\", event.getFullContentFile().getAbsolutePath());\n        }\n        put(keys, \"Headers\", event.getHeaders().toString());\n        log.info(MarkerFactory.getMarker(event.getServiceName() != null ? \"SOAP\" : \"REST\"), getLogMessage(event));\n    } finally {\n        for (String key : keys) {\n            MDC.remove(key);\n        }\n    }\n\n}",
        "reject_response": "@Override\npublic void send(LogEvent event) {\n    String cat = \"org.apache.cxf.services.\" + event.getPortTypeName().getLocalPart() + \".\" + event.getType();\n    Logger log = LoggerFactory.getLogger(cat);\n    Set<String> keys = new HashSet<>();\n    try {\n        put(keys, \"Type\", event.getType().toString());\n        put(keys, \"Address\", event.getAddress());\n        put(keys, \"HttpMethod\", event.getHttpMethod());\n        put(keys, \"Content-Type\", event.getContentType());\n        put(keys, \"ResponseCode\", event.getResponseCode());\n        put(keys, \"ExchangeId\", event.getExchangeId());\n        put(keys, \"MessageId\", event.getMessageId());\n        if (event.getServiceName() != null) {\n            put(keys, \"ServiceName\", localPart(event.getServiceName()));\n            put(keys, \"PortName\", localPart(event.getPortName()));\n            put(keys, \"PortTypeName\", localPart(event.getPortTypeName()));\n        }\n        if (event.getFullContentFile() != null) {\n            put(keys, \"FullContentFile\", event.getFullContentFile().getAbsolutePath());\n        }\n        put(keys, \"Headers\", event.getHeaders().toString());\n        log.info(getLogMessage(event));\n    } finally {\n        for (String key : keys) {\n            MDC.remove(key);\n        }\n    }\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2610,
        "instruction": "@Override\nprotected List<Tuple2<RestHandlerSpecification, ChannelInboundHandler>> initializeHandlers(CompletableFuture<String> restAddressFuture) {\n\tList<Tuple2<RestHandlerSpecification, ChannelInboundHandler>> handlers = super.initializeHandlers(restAddressFuture);\n\n\t// Add the Dispatcher specific handlers\n\n\tfinal Time timeout = restConfiguration.getTimeout();\n\tfinal Map<String, String> responseHeaders = restConfiguration.getResponseHeaders();\n\n\tBlobServerPortHandler blobServerPortHandler = new BlobServerPortHandler(\n\t\trestAddressFuture,\n\t\tleaderRetriever,\n\t\ttimeout,\n\t\tresponseHeaders);\n\n\tJobSubmitHandler jobSubmitHandler = new JobSubmitHandler(\n\t\trestAddressFuture,\n\t\tleaderRetriever,\n\t\ttimeout,\n\t\tresponseHeaders);\n\n\tif (clusterConfiguration.getBoolean(WebOptions.SUBMIT_ENABLE)) {\n\t\ttry {\n\t\t\twebSubmissionExtension = WebMonitorUtils.loadWebSubmissionExtension(\n\t\t\t\tleaderRetriever,\n\t\t\t\trestAddressFuture,\n\t\t\t\ttimeout,\n\t\t\t\tresponseHeaders,\n\t\t\t\tuploadDir,\n\t\t\t\texecutor,\n\t\t\t\tclusterConfiguration);\n\n\t\t\t// register extension handlers\n\t\t\thandlers.addAll(webSubmissionExtension.getHandlers());\n\t\t} catch (FlinkException e) { else {\n\t\t\t\tlog.info(\"Failed to load web based job submission extension. \" +\n\t\t\t\t\t\"Probable reason: flink-runtime-web is not in the classpath.\");\n\t\t\t}\n\t\t}\n\t} else {\n\t\tlog.info(\"Web-based job submission is not enabled.\");\n\t}\n\n\thandlers.add(Tuple2.of(blobServerPortHandler.getMessageHeaders(), blobServerPortHandler));\n\thandlers.add(Tuple2.of(jobSubmitHandler.getMessageHeaders(), jobSubmitHandler));\n\n\treturn handlers;\n}",
        "accept_response": "@Override\nprotected List<Tuple2<RestHandlerSpecification, ChannelInboundHandler>> initializeHandlers(CompletableFuture<String> restAddressFuture) {\n\tList<Tuple2<RestHandlerSpecification, ChannelInboundHandler>> handlers = super.initializeHandlers(restAddressFuture);\n\n\t// Add the Dispatcher specific handlers\n\n\tfinal Time timeout = restConfiguration.getTimeout();\n\tfinal Map<String, String> responseHeaders = restConfiguration.getResponseHeaders();\n\n\tBlobServerPortHandler blobServerPortHandler = new BlobServerPortHandler(\n\t\trestAddressFuture,\n\t\tleaderRetriever,\n\t\ttimeout,\n\t\tresponseHeaders);\n\n\tJobSubmitHandler jobSubmitHandler = new JobSubmitHandler(\n\t\trestAddressFuture,\n\t\tleaderRetriever,\n\t\ttimeout,\n\t\tresponseHeaders);\n\n\tif (clusterConfiguration.getBoolean(WebOptions.SUBMIT_ENABLE)) {\n\t\ttry {\n\t\t\twebSubmissionExtension = WebMonitorUtils.loadWebSubmissionExtension(\n\t\t\t\tleaderRetriever,\n\t\t\t\trestAddressFuture,\n\t\t\t\ttimeout,\n\t\t\t\tresponseHeaders,\n\t\t\t\tuploadDir,\n\t\t\t\texecutor,\n\t\t\t\tclusterConfiguration);\n\n\t\t\t// register extension handlers\n\t\t\thandlers.addAll(webSubmissionExtension.getHandlers());\n\t\t} catch (FlinkException e) {\n\t\t\tif (log.isDebugEnabled()) {\n\t\t\t\tlog.debug(\"Failed to load web based job submission extension.\", e);\n\t\t\t} else {\n\t\t\t\tlog.info(\"Failed to load web based job submission extension. \" +\n\t\t\t\t\t\"Probable reason: flink-runtime-web is not in the classpath.\");\n\t\t\t}\n\t\t}\n\t} else {\n\t\tlog.info(\"Web-based job submission is not enabled.\");\n\t}\n\n\thandlers.add(Tuple2.of(blobServerPortHandler.getMessageHeaders(), blobServerPortHandler));\n\thandlers.add(Tuple2.of(jobSubmitHandler.getMessageHeaders(), jobSubmitHandler));\n\n\treturn handlers;\n}",
        "reject_response": "@Override\nprotected List<Tuple2<RestHandlerSpecification, ChannelInboundHandler>> initializeHandlers(CompletableFuture<String> restAddressFuture) {\n\tList<Tuple2<RestHandlerSpecification, ChannelInboundHandler>> handlers = super.initializeHandlers(restAddressFuture);\n\n\t// Add the Dispatcher specific handlers\n\n\tfinal Time timeout = restConfiguration.getTimeout();\n\tfinal Map<String, String> responseHeaders = restConfiguration.getResponseHeaders();\n\n\tBlobServerPortHandler blobServerPortHandler = new BlobServerPortHandler(\n\t\trestAddressFuture,\n\t\tleaderRetriever,\n\t\ttimeout,\n\t\tresponseHeaders);\n\n\tJobSubmitHandler jobSubmitHandler = new JobSubmitHandler(\n\t\trestAddressFuture,\n\t\tleaderRetriever,\n\t\ttimeout,\n\t\tresponseHeaders);\n\n\tif (clusterConfiguration.getBoolean(WebOptions.SUBMIT_ENABLE)) {\n\t\ttry {\n\t\t\twebSubmissionExtension = WebMonitorUtils.loadWebSubmissionExtension(\n\t\t\t\tleaderRetriever,\n\t\t\t\trestAddressFuture,\n\t\t\t\ttimeout,\n\t\t\t\tresponseHeaders,\n\t\t\t\tuploadDir,\n\t\t\t\texecutor,\n\t\t\t\tclusterConfiguration);\n\n\t\t\t// register extension handlers\n\t\t\thandlers.addAll(webSubmissionExtension.getHandlers());\n\t\t} catch (FlinkException e) {\n\t\t\tif (log.isDebugEnabled()) {\n\t\t\tlog.info(\"Failed to load web based job submission extension.\", e);\n\t\t\t} else {\n\t\t\t\tlog.info(\"Failed to load web based job submission extension. \" +\n\t\t\t\t\t\"Probable reason: flink-runtime-web is not in the classpath.\");\n\t\t\t}\n\t\t}\n\t} else {\n\t\tlog.info(\"Web-based job submission is not enabled.\");\n\t}\n\n\thandlers.add(Tuple2.of(blobServerPortHandler.getMessageHeaders(), blobServerPortHandler));\n\thandlers.add(Tuple2.of(jobSubmitHandler.getMessageHeaders(), jobSubmitHandler));\n\n\treturn handlers;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2910,
        "instruction": "public List<String> getLocalDomainNames() {\n    List<String> res = dnsService.getLocalDomainNames();\n    StringBuffer logBuff = new StringBuffer();\n    logBuff.append(\"getLocalDomainNames() = \");\n    if (res != null) {\n        for (int i = 0; i < res.size(); i++) {\n            logBuff.append(res.get(i));\n            if (i == res.size() - 1) {\n                logBuff.append(\"\");\n            } else {\n                logBuff.append(\",\");\n            }\n        }\n    } else {\n        logBuff.append(\"getLocalDomainNames-ret: null\");\n    }\n    return res;\n\n}",
        "accept_response": "public List<String> getLocalDomainNames() {\n    List<String> res = dnsService.getLocalDomainNames();\n    StringBuffer logBuff = new StringBuffer();\n    logBuff.append(\"getLocalDomainNames() = \");\n    if (res != null) {\n        for (int i = 0; i < res.size(); i++) {\n            logBuff.append(res.get(i));\n            if (i == res.size() - 1) {\n                logBuff.append(\"\");\n            } else {\n                logBuff.append(\",\");\n            }\n        }\n    } else {\n        logBuff.append(\"getLocalDomainNames-ret: null\");\n    }\n    LOGGER.debug(logBuff.toString());\n    return res;\n\n}",
        "reject_response": "public List<String> getLocalDomainNames() {\n    List<String> res = dnsService.getLocalDomainNames();\n    StringBuffer logBuff = new StringBuffer();\n    logBuff.append(\"getLocalDomainNames() = \");\n    if (res != null) {\n        for (int i = 0; i < res.size(); i++) {\n            logBuff.append(res.get(i));\n            if (i == res.size() - 1) {\n                logBuff.append(\"\");\n            } else {\n                logBuff.append(\",\");\n            }\n        }\n    } else {\n        logBuff.append(\"getLocalDomainNames-ret: null\");\n    }\n    logger.debug(logBuff.toString());\n    return res;\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2465,
        "instruction": "@AfterClass\npublic static void checkLibaio() throws Throwable {\n   if (failingTotalMaxIoMessage != null) {\n      // Test class was already failed if this is set, nothing to do here.\n      return;\n   }\n\n   if (!Wait.waitFor(() -> LibaioContext.getTotalMaxIO() == 0)) {\n      long totalMaxIO = LibaioContext.getTotalMaxIO();\n\n      // Set messsage to fail subsequent runs with\n      failingTotalMaxIoMessage = String.format(\"Aborting, LibaioContext TotalMaxIO > 0 issue previously detected by test class %s(), see its output.\", testClassName);\n\n      // Now fail this run\n      String message = String.format(\"LibaioContext TotalMaxIO > 0 leak detected after class %s(), TotalMaxIO=%s(). Check output to determine if occurred before/during.\", testClassName, totalMaxIO);\n      Assert.fail(message);\n   }\n}",
        "accept_response": "@AfterClass\npublic static void checkLibaio() throws Throwable {\n   if (failingTotalMaxIoMessage != null) {\n      // Test class was already failed if this is set, nothing to do here.\n      return;\n   }\n\n   if (!Wait.waitFor(() -> LibaioContext.getTotalMaxIO() == 0)) {\n      long totalMaxIO = LibaioContext.getTotalMaxIO();\n\n      // Set messsage to fail subsequent runs with\n      failingTotalMaxIoMessage = String.format(\"Aborting, LibaioContext TotalMaxIO > 0 issue previously detected by test class %s(), see its output.\", testClassName);\n\n      // Now fail this run\n      String message = String.format(\"LibaioContext TotalMaxIO > 0 leak detected after class %s(), TotalMaxIO=%s(). Check output to determine if occurred before/during.\", testClassName, totalMaxIO);\n      logger.error(message);\n      Assert.fail(message);\n   }\n}",
        "reject_response": "@AfterClass\npublic static void checkLibaio() throws Throwable {\n   if (failingTotalMaxIoMessage != null) {\n      // Test class was already failed if this is set, nothing to do here.\n      return;\n   }\n\n   if (!Wait.waitFor(() -> LibaioContext.getTotalMaxIO() == 0)) {\n      long totalMaxIO = LibaioContext.getTotalMaxIO();\n\n      // Set messsage to fail subsequent runs with\n      failingTotalMaxIoMessage = String.format(\"Aborting, LibaioContext TotalMaxIO > 0 issue previously detected by test class %s(), see its output.\", testClassName);\n\n      // Now fail this run\n      String message = String.format(\"LibaioContext TotalMaxIO > 0 leak detected after class %s(), TotalMaxIO=%s(). Check output to determine if occurred before/during.\", testClassName, totalMaxIO);\n      log.error(message);\n      Assert.fail(message);\n   }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3144,
        "instruction": "void sendErrorResponse(int sc, String msg, Exception e) {\n\n  try {\n    response().sendError(sc, msg);\n  } catch (IOException e1) {\n    throw new WebAppException(e);\n  }\n}",
        "accept_response": "void sendErrorResponse(int sc, String msg, Exception e) {\n  LOG.debug(msg, e);\n\n  try {\n    response().sendError(sc, msg);\n  } catch (IOException e1) {\n    throw new WebAppException(e);\n  }\n}",
        "reject_response": "void sendErrorResponse(int sc, String msg, Exception e) {\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(msg, e);\n  }\n\n  try {\n    response().sendError(sc, msg);\n  } catch (IOException e1) {\n    throw new WebAppException(e);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2986,
        "instruction": "@GET\n@Path(KMSRESTConstants.KEYS_METADATA_RESOURCE)\n@Produces(MediaType.APPLICATION_JSON)\npublic Response getKeysMetadata(@QueryParam(KMSRESTConstants.KEY)\n    List<String> keyNamesList, @Context HttpServletRequest request) throws Exception {\n  try {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Entering getKeysMetadata method.\");\n    }\n    KMSWebApp.getAdminCallsMeter().mark();\n    UserGroupInformation user = HttpUserGroupInformation.get();\n    final String[] keyNames = keyNamesList.toArray( new String[keyNamesList.size()]);\n    assertAccess(Type.GET_METADATA, user, KMSOp.GET_KEYS_METADATA, request.getRemoteAddr());\n    KeyProvider.Metadata[] keysMeta = user.doAs(new PrivilegedExceptionAction<KeyProvider.Metadata[]>() {\n      @Override\n      public KeyProvider.Metadata[] run() throws Exception {\n        return provider.getKeysMetadata(keyNames);\n      }\n    });\n    Object json = KMSServerJSONUtils.toJSON(keyNames, keysMeta);\n    kmsAudit.ok(user, KMSOp.GET_KEYS_METADATA, \"\");\n    return Response.ok().type(MediaType.APPLICATION_JSON).entity(json).build();\n  } catch (Exception e) {\n    LOG.error(\"Exception in getKeysmetadata.\", e);\n    throw e;\n  }\n}",
        "accept_response": "@GET\n@Path(KMSRESTConstants.KEYS_METADATA_RESOURCE)\n@Produces(MediaType.APPLICATION_JSON)\npublic Response getKeysMetadata(@QueryParam(KMSRESTConstants.KEY)\n    List<String> keyNamesList, @Context HttpServletRequest request) throws Exception {\n  try {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Entering getKeysMetadata method.\");\n    }\n    KMSWebApp.getAdminCallsMeter().mark();\n    UserGroupInformation user = HttpUserGroupInformation.get();\n    final String[] keyNames = keyNamesList.toArray( new String[keyNamesList.size()]);\n    assertAccess(Type.GET_METADATA, user, KMSOp.GET_KEYS_METADATA, request.getRemoteAddr());\n    KeyProvider.Metadata[] keysMeta = user.doAs(new PrivilegedExceptionAction<KeyProvider.Metadata[]>() {\n      @Override\n      public KeyProvider.Metadata[] run() throws Exception {\n        return provider.getKeysMetadata(keyNames);\n      }\n    });\n    Object json = KMSServerJSONUtils.toJSON(keyNames, keysMeta);\n    kmsAudit.ok(user, KMSOp.GET_KEYS_METADATA, \"\");\n    if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Exiting getKeysMetadata method.\");\n    }\n    return Response.ok().type(MediaType.APPLICATION_JSON).entity(json).build();\n  } catch (Exception e) {\n    LOG.error(\"Exception in getKeysmetadata.\", e);\n    throw e;\n  }\n}",
        "reject_response": "@GET\n@Path(KMSRESTConstants.KEYS_METADATA_RESOURCE)\n@Produces(MediaType.APPLICATION_JSON)\npublic Response getKeysMetadata(@QueryParam(KMSRESTConstants.KEY)\n    List<String> keyNamesList, @Context HttpServletRequest request) throws Exception {\n  try {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Entering getKeysMetadata method.\");\n    }\n    KMSWebApp.getAdminCallsMeter().mark();\n    UserGroupInformation user = HttpUserGroupInformation.get();\n    final String[] keyNames = keyNamesList.toArray( new String[keyNamesList.size()]);\n    assertAccess(Type.GET_METADATA, user, KMSOp.GET_KEYS_METADATA, request.getRemoteAddr());\n    KeyProvider.Metadata[] keysMeta = user.doAs(new PrivilegedExceptionAction<KeyProvider.Metadata[]>() {\n      @Override\n      public KeyProvider.Metadata[] run() throws Exception {\n        return provider.getKeysMetadata(keyNames);\n      }\n    });\n    Object json = KMSServerJSONUtils.toJSON(keyNames, keysMeta);\n    kmsAudit.ok(user, KMSOp.GET_KEYS_METADATA, \"\");\n    if (LOG.isDebugEnabled()) {\n    LOG.info(\"Exiting getKeysMetadata method.\");\n    }\n    return Response.ok().type(MediaType.APPLICATION_JSON).entity(json).build();\n  } catch (Exception e) {\n    LOG.error(\"Exception in getKeysmetadata.\", e);\n    throw e;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3252,
        "instruction": "@Override\nvoid connect(InetSocketAddress addr) throws IOException {\n    SocketChannel sock = createSock();\n    try {\n        registerAndConnect(sock, addr);\n    } catch (IOException e) {\n        sock.close();\n        throw e;\n    }\n    initialized = false;\n\n    /*\n     * Reset incomingBuffer\n     */\n    lenBuffer.clear();\n    incomingBuffer = lenBuffer;\n}",
        "accept_response": "@Override\nvoid connect(InetSocketAddress addr) throws IOException {\n    SocketChannel sock = createSock();\n    try {\n        registerAndConnect(sock, addr);\n    } catch (IOException e) {\n        LOG.error(\"Unable to open socket to {}\", addr);\n        sock.close();\n        throw e;\n    }\n    initialized = false;\n\n    /*\n     * Reset incomingBuffer\n     */\n    lenBuffer.clear();\n    incomingBuffer = lenBuffer;\n}",
        "reject_response": "@Override\nvoid connect(InetSocketAddress addr) throws IOException {\n    SocketChannel sock = createSock();\n    try {\n        registerAndConnect(sock, addr);\n    } catch (IOException e) {\n        LOG.error(\"Unable to open socket to \" + addr);\n        sock.close();\n        throw e;\n    }\n    initialized = false;\n\n    /*\n     * Reset incomingBuffer\n     */\n    lenBuffer.clear();\n    incomingBuffer = lenBuffer;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3166,
        "instruction": "private static void trySetXercesSecurityManager(DocumentBuilderFactory dbf, XmlOptionsBean options) {\n    // Try built-in JVM one first, standalone if not\n    for (String securityManagerClassName : new String[]{\n            //\"com.sun.org.apache.xerces.internal.util.SecurityManager\",\n            \"org.apache.xerces.util.SecurityManager\"\n    }) {\n        try {\n            Object mgr = Class.forName(securityManagerClassName).getDeclaredConstructor().newInstance();\n            Method setLimit = mgr.getClass().getMethod(\"setEntityExpansionLimit\", Integer.TYPE);\n            setLimit.invoke(mgr, options.getEntityExpansionLimit());\n            dbf.setAttribute(XMLBeansConstants.SECURITY_MANAGER, mgr);\n            // Stop once one can be setup without error\n            return;\n        } catch (ClassNotFoundException e) {\n            // continue without log, this is expected in some setups\n        } catch (Throwable e) {     // NOSONAR - also catch things like NoClassDefError here\n            if(System.currentTimeMillis() > lastLog + TimeUnit.MINUTES.toMillis(5)) {\n                lastLog = System.currentTimeMillis();\n            }\n        }\n    }\n\n    // separate old version of Xerces not found => use the builtin way of setting the property\n    try {\n        dbf.setAttribute(XMLBeansConstants.ENTITY_EXPANSION_LIMIT, options.getEntityExpansionLimit());\n    } catch (Throwable e) {\n        if(System.currentTimeMillis() > lastLog + TimeUnit.MINUTES.toMillis(5)) {\n            logger.warn(\"DocumentBuilderFactory Entity Expansion Limit could not be setup [log suppressed for 5 minutes]\", e);\n            lastLog = System.currentTimeMillis();\n        }\n    }\n}",
        "accept_response": "private static void trySetXercesSecurityManager(DocumentBuilderFactory dbf, XmlOptionsBean options) {\n    // Try built-in JVM one first, standalone if not\n    for (String securityManagerClassName : new String[]{\n            //\"com.sun.org.apache.xerces.internal.util.SecurityManager\",\n            \"org.apache.xerces.util.SecurityManager\"\n    }) {\n        try {\n            Object mgr = Class.forName(securityManagerClassName).getDeclaredConstructor().newInstance();\n            Method setLimit = mgr.getClass().getMethod(\"setEntityExpansionLimit\", Integer.TYPE);\n            setLimit.invoke(mgr, options.getEntityExpansionLimit());\n            dbf.setAttribute(XMLBeansConstants.SECURITY_MANAGER, mgr);\n            // Stop once one can be setup without error\n            return;\n        } catch (ClassNotFoundException e) {\n            // continue without log, this is expected in some setups\n        } catch (Throwable e) {     // NOSONAR - also catch things like NoClassDefError here\n            if(System.currentTimeMillis() > lastLog + TimeUnit.MINUTES.toMillis(5)) {\n                logger.warn(\"DocumentBuilderFactory Security Manager could not be setup [log suppressed for 5 minutes]\", e);\n                lastLog = System.currentTimeMillis();\n            }\n        }\n    }\n\n    // separate old version of Xerces not found => use the builtin way of setting the property\n    try {\n        dbf.setAttribute(XMLBeansConstants.ENTITY_EXPANSION_LIMIT, options.getEntityExpansionLimit());\n    } catch (Throwable e) {\n        if(System.currentTimeMillis() > lastLog + TimeUnit.MINUTES.toMillis(5)) {\n            logger.warn(\"DocumentBuilderFactory Entity Expansion Limit could not be setup [log suppressed for 5 minutes]\", e);\n            lastLog = System.currentTimeMillis();\n        }\n    }\n}",
        "reject_response": "private static void trySetXercesSecurityManager(DocumentBuilderFactory dbf, XmlOptionsBean options) {\n    // Try built-in JVM one first, standalone if not\n    for (String securityManagerClassName : new String[]{\n            //\"com.sun.org.apache.xerces.internal.util.SecurityManager\",\n            \"org.apache.xerces.util.SecurityManager\"\n    }) {\n        try {\n            Object mgr = Class.forName(securityManagerClassName).getDeclaredConstructor().newInstance();\n            Method setLimit = mgr.getClass().getMethod(\"setEntityExpansionLimit\", Integer.TYPE);\n            setLimit.invoke(mgr, options.getEntityExpansionLimit());\n            dbf.setAttribute(XMLBeansConstants.SECURITY_MANAGER, mgr);\n            // Stop once one can be setup without error\n            return;\n        } catch (ClassNotFoundException e) {\n            // continue without log, this is expected in some setups\n        } catch (Throwable e) {     // NOSONAR - also catch things like NoClassDefError here\n            if(System.currentTimeMillis() > lastLog + TimeUnit.MINUTES.toMillis(5)) {\n                logger.log(XBLogger.WARN, \"DocumentBuilderFactory Security Manager could not be setup [log suppressed for 5 minutes]\", e);\n                lastLog = System.currentTimeMillis();\n            }\n        }\n    }\n\n    // separate old version of Xerces not found => use the builtin way of setting the property\n    try {\n        dbf.setAttribute(XMLBeansConstants.ENTITY_EXPANSION_LIMIT, options.getEntityExpansionLimit());\n    } catch (Throwable e) {\n        if(System.currentTimeMillis() > lastLog + TimeUnit.MINUTES.toMillis(5)) {\n            logger.warn(\"DocumentBuilderFactory Entity Expansion Limit could not be setup [log suppressed for 5 minutes]\", e);\n            lastLog = System.currentTimeMillis();\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2859,
        "instruction": "private static DelaysAndMaxRetry addExtraAttemptToLastDelay(int intendedMaxRetries, int extra, List<Delay> delayTimesList) throws MessagingException {\n    if (delayTimesList.size() != 0) {\n        Delay lastDelay = delayTimesList.get(delayTimesList.size() - 1);\n        Duration lastDelayTime = lastDelay.getDelayTime();\n        return new DelaysAndMaxRetry(intendedMaxRetries,\n            ImmutableList.copyOf(\n                Iterables.concat(\n                    Iterables.limit(delayTimesList, delayTimesList.size() - 1),\n                    ImmutableList.of(new Delay(lastDelay.getAttempts() + extra, lastDelayTime)))));\n    } else {\n        throw new MessagingException(\"No delaytimes, cannot continue\");\n    }\n}",
        "accept_response": "private static DelaysAndMaxRetry addExtraAttemptToLastDelay(int intendedMaxRetries, int extra, List<Delay> delayTimesList) throws MessagingException {\n    if (delayTimesList.size() != 0) {\n        Delay lastDelay = delayTimesList.get(delayTimesList.size() - 1);\n        Duration lastDelayTime = lastDelay.getDelayTime();\n        LOGGER.warn(\"Delay of {} is now attempted: {} times\",\n            DurationFormatUtils.formatDurationWords(lastDelayTime.toMillis(), true, true),\n            lastDelay.getAttempts());\n        return new DelaysAndMaxRetry(intendedMaxRetries,\n            ImmutableList.copyOf(\n                Iterables.concat(\n                    Iterables.limit(delayTimesList, delayTimesList.size() - 1),\n                    ImmutableList.of(new Delay(lastDelay.getAttempts() + extra, lastDelayTime)))));\n    } else {\n        throw new MessagingException(\"No delaytimes, cannot continue\");\n    }\n}",
        "reject_response": "private static DelaysAndMaxRetry addExtraAttemptToLastDelay(int intendedMaxRetries, int extra, List<Delay> delayTimesList) throws MessagingException {\n    if (delayTimesList.size() != 0) {\n        Delay lastDelay = delayTimesList.get(delayTimesList.size() - 1);\n        Duration lastDelayTime = lastDelay.getDelayTime();\n        LOGGER.warn(\"Delay of {} msecs is now attempted: {} times\", lastDelay.getDelayTime(), lastDelay.getAttempts());\n        return new DelaysAndMaxRetry(intendedMaxRetries,\n            ImmutableList.copyOf(\n                Iterables.concat(\n                    Iterables.limit(delayTimesList, delayTimesList.size() - 1),\n                    ImmutableList.of(new Delay(lastDelay.getAttempts() + extra, lastDelayTime)))));\n    } else {\n        throw new MessagingException(\"No delaytimes, cannot continue\");\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3113,
        "instruction": "public EvalNode getPreCompiledEval(Schema schema, EvalNode eval) {\n  if (codeGenEnabled) {\n\n    Pair<Schema, EvalNode> key = new Pair<Schema, EvalNode>(schema, eval);\n    if (compilationContext.getPrecompiedEvals().containsKey(key)) {\n      return compilationContext.getPrecompiedEvals().get(key);\n    } else {\n      try {\n        LOG.warn(eval.toString() + \" does not exists. Immediately compile it: \" + eval);\n        return compileEval(schema, eval);\n      } catch (Throwable t) {\n        return eval;\n      }\n    }\n  } else {\n    throw new IllegalStateException(\"CodeGen is disabled\");\n  }\n}",
        "accept_response": "public EvalNode getPreCompiledEval(Schema schema, EvalNode eval) {\n  if (codeGenEnabled) {\n\n    Pair<Schema, EvalNode> key = new Pair<Schema, EvalNode>(schema, eval);\n    if (compilationContext.getPrecompiedEvals().containsKey(key)) {\n      return compilationContext.getPrecompiedEvals().get(key);\n    } else {\n      try {\n        LOG.warn(eval.toString() + \" does not exists. Immediately compile it: \" + eval);\n        return compileEval(schema, eval);\n      } catch (Throwable t) {\n        LOG.warn(t, t);\n        return eval;\n      }\n    }\n  } else {\n    throw new IllegalStateException(\"CodeGen is disabled\");\n  }\n}",
        "reject_response": "public EvalNode getPreCompiledEval(Schema schema, EvalNode eval) {\n  if (codeGenEnabled) {\n\n    Pair<Schema, EvalNode> key = new Pair<Schema, EvalNode>(schema, eval);\n    if (compilationContext.getPrecompiedEvals().containsKey(key)) {\n      return compilationContext.getPrecompiedEvals().get(key);\n    } else {\n      try {\n        LOG.warn(eval.toString() + \" does not exists. Immediately compile it: \" + eval);\n        return compileEval(schema, eval);\n      } catch (Throwable t) {\n        LOG.warn(t);\n        return eval;\n      }\n    }\n  } else {\n    throw new IllegalStateException(\"CodeGen is disabled\");\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2452,
        "instruction": "@Override\npublic void unlock() {\n   lock.lock();\n\n   failingOver = false;\n\n   failoverCondition.signalAll();\n\n   lock.unlock();\n}",
        "accept_response": "@Override\npublic void unlock() {\n   if (logger.isTraceEnabled()) {\n      logger.trace(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" unlock channel \" + this);\n   }\n   lock.lock();\n\n   failingOver = false;\n\n   failoverCondition.signalAll();\n\n   lock.unlock();\n}",
        "reject_response": "@Override\npublic void unlock() {\n   if (logger.isTraceEnabled()) {\n      logger.trace(\"unlock channel \" + this);\n   }\n   lock.lock();\n\n   failingOver = false;\n\n   failoverCondition.signalAll();\n\n   lock.unlock();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3059,
        "instruction": "@SuppressWarnings(\"unchecked\")\nprivate void getFileList(SolrParams solrParams, SolrQueryResponse rsp) {\n  String v = solrParams.get(GENERATION);\n  if (v == null) {\n    rsp.add(\"status\", \"no index generation specified\");\n    return;\n  }\n  long gen = Long.parseLong(v);\n  IndexCommit commit = core.getDeletionPolicy().getCommitPoint(gen);\n\n  //System.out.println(\"ask for files for gen:\" + commit.getGeneration() + core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName());\n  if (commit == null) {\n    rsp.add(\"status\", \"invalid index generation\");\n    return;\n  }\n\n  // reserve the indexcommit for sometime\n  core.getDeletionPolicy().setReserveDuration(gen, reserveCommitDuration);\n  List<Map<String, Object>> result = new ArrayList<>();\n  Directory dir = null;\n  try {\n    dir = core.getDirectoryFactory().get(core.getNewIndexDir(), DirContext.DEFAULT, core.getSolrConfig().indexConfig.lockType);\n    SegmentInfos infos = SegmentInfos.readCommit(dir, commit.getSegmentsFileName());\n    for (SegmentCommitInfo commitInfo : infos) {\n      for (String file : commitInfo.files()) {\n        Map<String, Object> fileMeta = new HashMap<>();\n        fileMeta.put(NAME, file);\n        fileMeta.put(SIZE, dir.fileLength(file));\n\n        try (final IndexInput in = dir.openInput(file, IOContext.READONCE)) {\n          try {\n            long checksum = CodecUtil.retrieveChecksum(in);\n            fileMeta.put(CHECKSUM, checksum);\n          } catch (Exception e) {\n            LOG.warn(\"Could not read checksum from index file: \" + file, e);\n          }\n        }\n\n        result.add(fileMeta);\n      }\n    }\n\n    // add the segments_N file\n\n    Map<String, Object> fileMeta = new HashMap<>();\n    fileMeta.put(NAME, infos.getSegmentsFileName());\n    fileMeta.put(SIZE, dir.fileLength(infos.getSegmentsFileName()));\n    if (infos.getId() != null) {\n      try (final IndexInput in = dir.openInput(infos.getSegmentsFileName(), IOContext.READONCE)) {\n        try {\n          fileMeta.put(CHECKSUM, CodecUtil.retrieveChecksum(in));\n        } catch (Exception e) {\n          LOG.warn(\"Could not read checksum from index file: \" + infos.getSegmentsFileName(), e);\n        }\n      }\n    }\n    result.add(fileMeta);\n  } catch (IOException e) {\n    rsp.add(\"status\", \"unable to get file names for given index generation\");\n    rsp.add(EXCEPTION, e);\n    LOG.error(\"Unable to get file names for indexCommit generation: \" + gen, e);\n  } finally {\n    if (dir != null) {\n      try {\n        core.getDirectoryFactory().release(dir);\n      } catch (IOException e) {\n        SolrException.log(LOG, \"Could not release directory after fetching file list\", e);\n      }\n    }\n  }\n  rsp.add(CMD_GET_FILE_LIST, result);\n\n  // fetch list of tlog files only if cdcr is activated\n  if (solrParams.getBool(TLOG_FILES, true) && core.getUpdateHandler().getUpdateLog() != null\n      && core.getUpdateHandler().getUpdateLog() instanceof CdcrUpdateLog) {\n    try {\n      List<Map<String, Object>> tlogfiles = getTlogFileList(commit);\n      LOG.info(\"Adding tlog files to list: \" + tlogfiles);\n      rsp.add(TLOG_FILES, tlogfiles);\n    }\n    catch (IOException e) {\n      rsp.add(\"status\", \"unable to get tlog file names for given index generation\");\n      rsp.add(EXCEPTION, e);\n      LOG.error(\"Unable to get tlog file names for indexCommit generation: \" + gen, e);\n    }\n  }\n\n  if (confFileNameAlias.size() < 1 || core.getCoreContainer().isZooKeeperAware())\n    return;\n  //if configuration files need to be included get their details\n  rsp.add(CONF_FILES, getConfFileInfoFromCache(confFileNameAlias, confFileInfoCache));\n}",
        "accept_response": "@SuppressWarnings(\"unchecked\")\nprivate void getFileList(SolrParams solrParams, SolrQueryResponse rsp) {\n  String v = solrParams.get(GENERATION);\n  if (v == null) {\n    rsp.add(\"status\", \"no index generation specified\");\n    return;\n  }\n  long gen = Long.parseLong(v);\n  IndexCommit commit = core.getDeletionPolicy().getCommitPoint(gen);\n\n  //System.out.println(\"ask for files for gen:\" + commit.getGeneration() + core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName());\n  if (commit == null) {\n    rsp.add(\"status\", \"invalid index generation\");\n    return;\n  }\n\n  // reserve the indexcommit for sometime\n  core.getDeletionPolicy().setReserveDuration(gen, reserveCommitDuration);\n  List<Map<String, Object>> result = new ArrayList<>();\n  Directory dir = null;\n  try {\n    dir = core.getDirectoryFactory().get(core.getNewIndexDir(), DirContext.DEFAULT, core.getSolrConfig().indexConfig.lockType);\n    SegmentInfos infos = SegmentInfos.readCommit(dir, commit.getSegmentsFileName());\n    for (SegmentCommitInfo commitInfo : infos) {\n      for (String file : commitInfo.files()) {\n        Map<String, Object> fileMeta = new HashMap<>();\n        fileMeta.put(NAME, file);\n        fileMeta.put(SIZE, dir.fileLength(file));\n\n        try (final IndexInput in = dir.openInput(file, IOContext.READONCE)) {\n          try {\n            long checksum = CodecUtil.retrieveChecksum(in);\n            fileMeta.put(CHECKSUM, checksum);\n          } catch (Exception e) {\n            LOG.warn(\"Could not read checksum from index file: \" + file, e);\n          }\n        }\n\n        result.add(fileMeta);\n      }\n    }\n\n    // add the segments_N file\n\n    Map<String, Object> fileMeta = new HashMap<>();\n    fileMeta.put(NAME, infos.getSegmentsFileName());\n    fileMeta.put(SIZE, dir.fileLength(infos.getSegmentsFileName()));\n    if (infos.getId() != null) {\n      try (final IndexInput in = dir.openInput(infos.getSegmentsFileName(), IOContext.READONCE)) {\n        try {\n          fileMeta.put(CHECKSUM, CodecUtil.retrieveChecksum(in));\n        } catch (Exception e) {\n          LOG.warn(\"Could not read checksum from index file: \" + infos.getSegmentsFileName(), e);\n        }\n      }\n    }\n    result.add(fileMeta);\n  } catch (IOException e) {\n    rsp.add(\"status\", \"unable to get file names for given index generation\");\n    rsp.add(EXCEPTION, e);\n    LOG.error(\"Unable to get file names for indexCommit generation: \" + gen, e);\n  } finally {\n    if (dir != null) {\n      try {\n        core.getDirectoryFactory().release(dir);\n      } catch (IOException e) {\n        SolrException.log(LOG, \"Could not release directory after fetching file list\", e);\n      }\n    }\n  }\n  rsp.add(CMD_GET_FILE_LIST, result);\n\n  // fetch list of tlog files only if cdcr is activated\n  if (solrParams.getBool(TLOG_FILES, true) && core.getUpdateHandler().getUpdateLog() != null\n      && core.getUpdateHandler().getUpdateLog() instanceof CdcrUpdateLog) {\n    try {\n      List<Map<String, Object>> tlogfiles = getTlogFileList(commit);\n      LOG.info(\"Adding tlog files to list: \" + tlogfiles);\n      rsp.add(TLOG_FILES, tlogfiles);\n    }\n    catch (IOException e) {\n      rsp.add(\"status\", \"unable to get tlog file names for given index generation\");\n      rsp.add(EXCEPTION, e);\n      LOG.error(\"Unable to get tlog file names for indexCommit generation: \" + gen, e);\n    }\n  }\n\n  if (confFileNameAlias.size() < 1 || core.getCoreContainer().isZooKeeperAware())\n    return;\n  LOG.debug(\"Adding config files to list: {}\", includeConfFiles);\n  //if configuration files need to be included get their details\n  rsp.add(CONF_FILES, getConfFileInfoFromCache(confFileNameAlias, confFileInfoCache));\n}",
        "reject_response": "@SuppressWarnings(\"unchecked\")\nprivate void getFileList(SolrParams solrParams, SolrQueryResponse rsp) {\n  String v = solrParams.get(GENERATION);\n  if (v == null) {\n    rsp.add(\"status\", \"no index generation specified\");\n    return;\n  }\n  long gen = Long.parseLong(v);\n  IndexCommit commit = core.getDeletionPolicy().getCommitPoint(gen);\n\n  //System.out.println(\"ask for files for gen:\" + commit.getGeneration() + core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName());\n  if (commit == null) {\n    rsp.add(\"status\", \"invalid index generation\");\n    return;\n  }\n\n  // reserve the indexcommit for sometime\n  core.getDeletionPolicy().setReserveDuration(gen, reserveCommitDuration);\n  List<Map<String, Object>> result = new ArrayList<>();\n  Directory dir = null;\n  try {\n    dir = core.getDirectoryFactory().get(core.getNewIndexDir(), DirContext.DEFAULT, core.getSolrConfig().indexConfig.lockType);\n    SegmentInfos infos = SegmentInfos.readCommit(dir, commit.getSegmentsFileName());\n    for (SegmentCommitInfo commitInfo : infos) {\n      for (String file : commitInfo.files()) {\n        Map<String, Object> fileMeta = new HashMap<>();\n        fileMeta.put(NAME, file);\n        fileMeta.put(SIZE, dir.fileLength(file));\n\n        try (final IndexInput in = dir.openInput(file, IOContext.READONCE)) {\n          try {\n            long checksum = CodecUtil.retrieveChecksum(in);\n            fileMeta.put(CHECKSUM, checksum);\n          } catch (Exception e) {\n            LOG.warn(\"Could not read checksum from index file: \" + file, e);\n          }\n        }\n\n        result.add(fileMeta);\n      }\n    }\n\n    // add the segments_N file\n\n    Map<String, Object> fileMeta = new HashMap<>();\n    fileMeta.put(NAME, infos.getSegmentsFileName());\n    fileMeta.put(SIZE, dir.fileLength(infos.getSegmentsFileName()));\n    if (infos.getId() != null) {\n      try (final IndexInput in = dir.openInput(infos.getSegmentsFileName(), IOContext.READONCE)) {\n        try {\n          fileMeta.put(CHECKSUM, CodecUtil.retrieveChecksum(in));\n        } catch (Exception e) {\n          LOG.warn(\"Could not read checksum from index file: \" + infos.getSegmentsFileName(), e);\n        }\n      }\n    }\n    result.add(fileMeta);\n  } catch (IOException e) {\n    rsp.add(\"status\", \"unable to get file names for given index generation\");\n    rsp.add(EXCEPTION, e);\n    LOG.error(\"Unable to get file names for indexCommit generation: \" + gen, e);\n  } finally {\n    if (dir != null) {\n      try {\n        core.getDirectoryFactory().release(dir);\n      } catch (IOException e) {\n        SolrException.log(LOG, \"Could not release directory after fetching file list\", e);\n      }\n    }\n  }\n  rsp.add(CMD_GET_FILE_LIST, result);\n\n  // fetch list of tlog files only if cdcr is activated\n  if (solrParams.getBool(TLOG_FILES, true) && core.getUpdateHandler().getUpdateLog() != null\n      && core.getUpdateHandler().getUpdateLog() instanceof CdcrUpdateLog) {\n    try {\n      List<Map<String, Object>> tlogfiles = getTlogFileList(commit);\n      LOG.info(\"Adding tlog files to list: \" + tlogfiles);\n      rsp.add(TLOG_FILES, tlogfiles);\n    }\n    catch (IOException e) {\n      rsp.add(\"status\", \"unable to get tlog file names for given index generation\");\n      rsp.add(EXCEPTION, e);\n      LOG.error(\"Unable to get tlog file names for indexCommit generation: \" + gen, e);\n    }\n  }\n\n  if (confFileNameAlias.size() < 1 || core.getCoreContainer().isZooKeeperAware())\n    return;\n  LOG.debug(\"Adding config files to list: \" + includeConfFiles);\n  //if configuration files need to be included get their details\n  rsp.add(CONF_FILES, getConfFileInfoFromCache(confFileNameAlias, confFileInfoCache));\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2620,
        "instruction": "@Override\npublic void contextInitialized(ServletContextEvent event) {\n\n  messagesToBeLogged = messagesToBeLogged\n      .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_CONTEXT_INITIALIZED\")));\n\n  // Load Pulse version details\n  loadPulseVersionDetails();\n\n  // Load Pulse Properties\n  pulseProperties = loadProperties(PulseConstants.PULSE_PROPERTIES_FILE);\n\n  if (pulseProperties.isEmpty()) {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_NOT_FOUND\")));\n  } else {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_FOUND\")));\n\n    // set Pulse product support into the Pulse controller for access from\n    // client side\n    // to display the appropriate ui depending on which product is supported\n    // in present deployment\n    String pulseProduct =\n        pulseProperties.getProperty(PulseConstants.APPLICATION_PROPERTY_PULSE_PRODUCTSUPPORT);\n  }\n\n  pulseSecurityProperties = loadProperties(PulseConstants.PULSE_SECURITY_PROPERTIES_FILE);\n\n  // Initialize logger\n  initializeLogger();\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n  logger.info(resourceBundle.getString(\"LOG_MSG_CHECK_APP_RUNNING_MODE\"));\n\n  boolean sysIsEmbedded = Boolean.getBoolean(PulseConstants.SYSTEM_PROPERTY_PULSE_EMBEDDED);\n\n  if (sysIsEmbedded) {\n    // Application Pulse is running in Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_EMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(true);\n\n    sysPulseUseLocator = Boolean.FALSE;\n    try {\n      // Get host name of machine running pulse in embedded mode\n      sysPulseHost = InetAddress.getLocalHost().getCanonicalHostName();\n    } catch (Exception e) {\n      logger.debug(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_UNKNOWN_HOST\"), e);\n      // Set default host name\n      sysPulseHost = PulseConstants.GEMFIRE_DEFAULT_HOST;\n    }\n    sysPulsePort = PulseConstants.GEMFIRE_DEFAULT_PORT;\n\n  } else {\n    // Application Pulse is running in Non-Embedded Mode\n    repository.setIsEmbeddedMode(false);\n\n    // Load JMX User Details\n    loadJMXUserDetails();\n    // Load locator and/or manager details\n    loadLocatorManagerDetails();\n\n    useGemFireCredentials = areWeUsingGemFireSecurityProfile(event);\n  }\n\n  // Set user details in repository\n  repository.setJmxUserName(jmxUserName);\n  repository.setJmxUserPassword(jmxUserPassword);\n\n  // Set locator/Manager details in repository\n  repository.setJmxUseLocator(sysPulseUseLocator);\n  repository.setJmxHost(sysPulseHost);\n  repository.setJmxPort(sysPulsePort);\n\n  // set SSL info\n  initializeSSL();\n  repository.setUseSSLLocator(sysPulseUseSSLLocator);\n  repository.setUseSSLManager(sysPulseUseSSLManager);\n\n  repository.setUseGemFireCredentials(useGemFireCredentials);\n\n}",
        "accept_response": "@Override\npublic void contextInitialized(ServletContextEvent event) {\n\n  messagesToBeLogged = messagesToBeLogged\n      .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_CONTEXT_INITIALIZED\")));\n\n  // Load Pulse version details\n  loadPulseVersionDetails();\n\n  // Load Pulse Properties\n  pulseProperties = loadProperties(PulseConstants.PULSE_PROPERTIES_FILE);\n\n  if (pulseProperties.isEmpty()) {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_NOT_FOUND\")));\n  } else {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_FOUND\")));\n\n    // set Pulse product support into the Pulse controller for access from\n    // client side\n    // to display the appropriate ui depending on which product is supported\n    // in present deployment\n    String pulseProduct =\n        pulseProperties.getProperty(PulseConstants.APPLICATION_PROPERTY_PULSE_PRODUCTSUPPORT);\n  }\n\n  pulseSecurityProperties = loadProperties(PulseConstants.PULSE_SECURITY_PROPERTIES_FILE);\n\n  // Initialize logger\n  initializeLogger();\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n  logger.info(resourceBundle.getString(\"LOG_MSG_CHECK_APP_RUNNING_MODE\"));\n\n  boolean sysIsEmbedded = Boolean.getBoolean(PulseConstants.SYSTEM_PROPERTY_PULSE_EMBEDDED);\n\n  if (sysIsEmbedded) {\n    // Application Pulse is running in Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_EMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(true);\n\n    sysPulseUseLocator = Boolean.FALSE;\n    try {\n      // Get host name of machine running pulse in embedded mode\n      sysPulseHost = InetAddress.getLocalHost().getCanonicalHostName();\n    } catch (Exception e) {\n      logger.debug(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_UNKNOWN_HOST\"), e);\n      // Set default host name\n      sysPulseHost = PulseConstants.GEMFIRE_DEFAULT_HOST;\n    }\n    sysPulsePort = PulseConstants.GEMFIRE_DEFAULT_PORT;\n\n  } else {\n    // Application Pulse is running in Non-Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_NONEMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(false);\n\n    // Load JMX User Details\n    loadJMXUserDetails();\n    // Load locator and/or manager details\n    loadLocatorManagerDetails();\n\n    useGemFireCredentials = areWeUsingGemFireSecurityProfile(event);\n  }\n\n  // Set user details in repository\n  repository.setJmxUserName(jmxUserName);\n  repository.setJmxUserPassword(jmxUserPassword);\n\n  // Set locator/Manager details in repository\n  repository.setJmxUseLocator(sysPulseUseLocator);\n  repository.setJmxHost(sysPulseHost);\n  repository.setJmxPort(sysPulsePort);\n\n  // set SSL info\n  initializeSSL();\n  repository.setUseSSLLocator(sysPulseUseSSLLocator);\n  repository.setUseSSLManager(sysPulseUseSSLManager);\n\n  repository.setUseGemFireCredentials(useGemFireCredentials);\n\n}",
        "reject_response": "@Override\npublic void contextInitialized(ServletContextEvent event) {\n\n  messagesToBeLogged = messagesToBeLogged\n      .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_CONTEXT_INITIALIZED\")));\n\n  // Load Pulse version details\n  loadPulseVersionDetails();\n\n  // Load Pulse Properties\n  pulseProperties = loadProperties(PulseConstants.PULSE_PROPERTIES_FILE);\n\n  if (pulseProperties.isEmpty()) {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_NOT_FOUND\")));\n  } else {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_FOUND\")));\n\n    // set Pulse product support into the Pulse controller for access from\n    // client side\n    // to display the appropriate ui depending on which product is supported\n    // in present deployment\n    String pulseProduct =\n        pulseProperties.getProperty(PulseConstants.APPLICATION_PROPERTY_PULSE_PRODUCTSUPPORT);\n  }\n\n  pulseSecurityProperties = loadProperties(PulseConstants.PULSE_SECURITY_PROPERTIES_FILE);\n\n  // Initialize logger\n  initializeLogger();\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n  logger.info(resourceBundle.getString(\"LOG_MSG_CHECK_APP_RUNNING_MODE\"));\n\n  boolean sysIsEmbedded = Boolean.getBoolean(PulseConstants.SYSTEM_PROPERTY_PULSE_EMBEDDED);\n\n  if (sysIsEmbedded) {\n    // Application Pulse is running in Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_EMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(true);\n\n    sysPulseUseLocator = Boolean.FALSE;\n    try {\n      // Get host name of machine running pulse in embedded mode\n      sysPulseHost = InetAddress.getLocalHost().getCanonicalHostName();\n    } catch (Exception e) {\n      logger.debug(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_UNKNOWN_HOST\"), e);\n      // Set default host name\n      sysPulseHost = PulseConstants.GEMFIRE_DEFAULT_HOST;\n    }\n    sysPulsePort = PulseConstants.GEMFIRE_DEFAULT_PORT;\n\n  } else {\n    // Application Pulse is running in Non-Embedded Mode\n    if (LOGGER.infoEnabled()) {\n      LOGGER.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_NONEMBEDDED_MODE\"));\n    }\n    repository.setIsEmbeddedMode(false);\n\n    // Load JMX User Details\n    loadJMXUserDetails();\n    // Load locator and/or manager details\n    loadLocatorManagerDetails();\n\n    useGemFireCredentials = areWeUsingGemFireSecurityProfile(event);\n  }\n\n  // Set user details in repository\n  repository.setJmxUserName(jmxUserName);\n  repository.setJmxUserPassword(jmxUserPassword);\n\n  // Set locator/Manager details in repository\n  repository.setJmxUseLocator(sysPulseUseLocator);\n  repository.setJmxHost(sysPulseHost);\n  repository.setJmxPort(sysPulsePort);\n\n  // set SSL info\n  initializeSSL();\n  repository.setUseSSLLocator(sysPulseUseSSLLocator);\n  repository.setUseSSLManager(sysPulseUseSSLManager);\n\n  repository.setUseGemFireCredentials(useGemFireCredentials);\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2629,
        "instruction": "@Override\npublic void run() {\n  try {\n    while (!this.stopUpdates) {\n      try {\n        if (!this.updateData()) {\n          this.stale++;\n        } else {\n          this.stale = 0;\n        }\n      } catch (Exception e) {\n        logger.info(\"Exception Occurred while updating cluster data : \", e);\n      }\n\n      clusterHasBeenInitialized.countDown();\n      try {\n        Thread.sleep(POLL_INTERVAL);\n      } catch (InterruptedException e) {\n        logger.info(\"InterruptedException Occurred : \", e);\n      }\n    }\n\n  } finally {\n    clusterHasBeenInitialized.countDown();\n  }\n}",
        "accept_response": "@Override\npublic void run() {\n  try {\n    while (!this.stopUpdates) {\n      try {\n        if (!this.updateData()) {\n          this.stale++;\n        } else {\n          this.stale = 0;\n        }\n      } catch (Exception e) {\n        logger.info(\"Exception Occurred while updating cluster data : \", e);\n      }\n\n      clusterHasBeenInitialized.countDown();\n      try {\n        Thread.sleep(POLL_INTERVAL);\n      } catch (InterruptedException e) {\n        logger.info(\"InterruptedException Occurred : \", e);\n      }\n    }\n\n    logger.info(\"{} :: {}:{}\", resourceBundle.getString(\"LOG_MSG_STOP_THREAD_UPDATES\"),\n        this.serverName, this.port);\n  } finally {\n    clusterHasBeenInitialized.countDown();\n  }\n}",
        "reject_response": "@Override\npublic void run() {\n  try {\n    while (!this.stopUpdates) {\n      try {\n        if (!this.updateData()) {\n          this.stale++;\n        } else {\n          this.stale = 0;\n        }\n      } catch (Exception e) {\n        logger.info(\"Exception Occurred while updating cluster data : \", e);\n      }\n\n      clusterHasBeenInitialized.countDown();\n      try {\n        Thread.sleep(POLL_INTERVAL);\n      } catch (InterruptedException e) {\n        logger.info(\"InterruptedException Occurred : \", e);\n      }\n    }\n\n    if (LOGGER.infoEnabled()) {\n      LOGGER.info(resourceBundle.getString(\"LOG_MSG_STOP_THREAD_UPDATES\") + \" :: \"\n          + this.serverName + \":\" + this.port);\n    }\n  } finally {\n    clusterHasBeenInitialized.countDown();\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2799,
        "instruction": "@Override\npublic Appendable append(final char ch) throws IOException {\n    if (ch == '\\n') {\n        buffer.setLength(0);\n    } else if (ch != '\\r') {\n        buffer.append(ch);\n    }\n    return this;\n}",
        "accept_response": "@Override\npublic Appendable append(final char ch) throws IOException {\n    if (ch == '\\n') {\n        log.debug(\"{} {}\", prefix, buffer);\n        buffer.setLength(0);\n    } else if (ch != '\\r') {\n        buffer.append(ch);\n    }\n    return this;\n}",
        "reject_response": "@Override\npublic Appendable append(final char ch) throws IOException {\n    if (ch == '\\n') {\n        log.debug(prefix + \" \" + buffer.toString());\n        buffer.setLength(0);\n    } else if (ch != '\\r') {\n        buffer.append(ch);\n    }\n    return this;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2811,
        "instruction": "@SuppressWarnings(\"unchecked\")\n@Override public void delete(Object key) {\n    try {\n        if (log.isDebugEnabled())\n\n        updateCache((K)key, null, StoreOperation.RMV);\n    }\n    catch (IgniteInterruptedCheckedException e) {\n        throw new CacheWriterException(U.convertExceptionNoWrap(e));\n    }\n}",
        "accept_response": "@SuppressWarnings(\"unchecked\")\n@Override public void delete(Object key) {\n    try {\n        if (log.isDebugEnabled())\n            log.debug(S.toString(\"Store remove\",\n                \"key\", key, true));\n\n        updateCache((K)key, null, StoreOperation.RMV);\n    }\n    catch (IgniteInterruptedCheckedException e) {\n        throw new CacheWriterException(U.convertExceptionNoWrap(e));\n    }\n}",
        "reject_response": "@SuppressWarnings(\"unchecked\")\n@Override public void delete(Object key) {\n    try {\n        if (log.isDebugEnabled())\n            log.debug(\"Store remove [key=\" + key + ']');\n\n        updateCache((K)key, null, StoreOperation.RMV);\n    }\n    catch (IgniteInterruptedCheckedException e) {\n        throw new CacheWriterException(U.convertExceptionNoWrap(e));\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2934,
        "instruction": "public void afterExecute(CommandSession session, CharSequence command, Exception exception) {\n    if (LOGGER.isDebugEnabled()) {\n        command = filter(command); else {\n            LOGGER.debug(\"Command: '\" + command + \"' failed: \" + exception);\n        }\n    }\n}",
        "accept_response": "public void afterExecute(CommandSession session, CharSequence command, Exception exception) {\n    if (LOGGER.isDebugEnabled()) {\n        command = filter(command);\n        if (LOGGER.isTraceEnabled()) {\n            LOGGER.debug(\"Command: '\" + command + \"' failed\", exception);\n        } else {\n            LOGGER.debug(\"Command: '\" + command + \"' failed: \" + exception);\n        }\n    }\n}",
        "reject_response": "public void afterExecute(CommandSession session, CharSequence command, Exception exception) {\n    if (LOGGER.isDebugEnabled()) {\n        command = filter(command);\n        if (LOGGER.isTraceEnabled()) {\n    if (LOGGER.isTraceEnabled()) {\n        LOGGER.debug(\"Command: '\" + command + \"' failed\", exception);\n    } else {\n        LOGGER.debug(\"Command: '\" + command + \"' failed: \" + exception);\n        } else {\n            LOGGER.debug(\"Command: '\" + command + \"' failed: \" + exception);\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3199,
        "instruction": "private static ApplicationId parseApplicationId(String appId) {\n  try {\n    if (appId != null) {\n      return ApplicationId.fromString(appId.trim());\n    } else {\n      return null;\n    }\n  } catch (IllegalFormatException e) {\n    return null;\n  }\n}",
        "accept_response": "private static ApplicationId parseApplicationId(String appId) {\n  try {\n    if (appId != null) {\n      return ApplicationId.fromString(appId.trim());\n    } else {\n      return null;\n    }\n  } catch (IllegalFormatException e) {\n    LOG.error(\"Invalid application ID: {}\", appId);\n    return null;\n  }\n}",
        "reject_response": "private static ApplicationId parseApplicationId(String appId) {\n  try {\n    if (appId != null) {\n      return ApplicationId.fromString(appId.trim());\n    } else {\n      return null;\n    }\n  } catch (IllegalFormatException e) {\n    LOG.error(\"Invalid application ID: \" + appId);\n    return null;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2568,
        "instruction": "private void handleGLClosureIntegrityIssues(final JsonCommand command, final DataIntegrityViolationException dve) {\n    final Throwable realCause = dve.getMostSpecificCause();\n    if (realCause.getMessage().contains(\"office_id_closing_date\")) { throw new GLClosureDuplicateException(\n            command.longValueOfParameterNamed(GLClosureJsonInputParams.OFFICE_ID.getValue()), new LocalDate(\n                    command.DateValueOfParameterNamed(GLClosureJsonInputParams.CLOSING_DATE.getValue()))); }\n\n    throw new PlatformDataIntegrityException(\"error.msg.glClosure.unknown.data.integrity.issue\",\n            \"Unknown data integrity issue with resource GL Closure: \" + realCause.getMessage());\n}",
        "accept_response": "private void handleGLClosureIntegrityIssues(final JsonCommand command, final DataIntegrityViolationException dve) {\n    final Throwable realCause = dve.getMostSpecificCause();\n    if (realCause.getMessage().contains(\"office_id_closing_date\")) { throw new GLClosureDuplicateException(\n            command.longValueOfParameterNamed(GLClosureJsonInputParams.OFFICE_ID.getValue()), new LocalDate(\n                    command.DateValueOfParameterNamed(GLClosureJsonInputParams.CLOSING_DATE.getValue()))); }\n\n    logger.error(\"Error occured.\", dve);\n    throw new PlatformDataIntegrityException(\"error.msg.glClosure.unknown.data.integrity.issue\",\n            \"Unknown data integrity issue with resource GL Closure: \" + realCause.getMessage());\n}",
        "reject_response": "private void handleGLClosureIntegrityIssues(final JsonCommand command, final DataIntegrityViolationException dve) {\n    final Throwable realCause = dve.getMostSpecificCause();\n    if (realCause.getMessage().contains(\"office_id_closing_date\")) { throw new GLClosureDuplicateException(\n            command.longValueOfParameterNamed(GLClosureJsonInputParams.OFFICE_ID.getValue()), new LocalDate(\n                    command.DateValueOfParameterNamed(GLClosureJsonInputParams.CLOSING_DATE.getValue()))); }\n\n    logger.error(dve.getMessage(), dve);\n    throw new PlatformDataIntegrityException(\"error.msg.glClosure.unknown.data.integrity.issue\",\n            \"Unknown data integrity issue with resource GL Closure: \" + realCause.getMessage());\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2912,
        "instruction": "@Override\npublic void poll(long timeout) throws IOException {\n    clear();\n    if (hasStagedReceives())\n        timeout = 0;\n    /* check ready keys */\n    long startSelect = time.nanoseconds();\n    int readyKeys = select(timeout);\n    long endSelect = time.nanoseconds();\n    currentTimeNanos = endSelect;\n    this.sensors.selectTime.record(endSelect - startSelect, time.milliseconds());\n\n    if (readyKeys > 0) {\n        Set<SelectionKey> keys = this.nioSelector.selectedKeys();\n        Iterator<SelectionKey> iter = keys.iterator();\n        while (iter.hasNext()) {\n            SelectionKey key = iter.next();\n            iter.remove();\n            KafkaChannel channel = channel(key);\n\n            // register all per-connection metrics at once\n            sensors.maybeRegisterConnectionMetrics(channel.id());\n            lruConnections.put(channel.id(), currentTimeNanos);\n\n            try {\n                /* complete any connections that have finished their handshake */\n                if (key.isConnectable()) {\n                    channel.finishConnect();\n                    this.connected.add(channel.id());\n                    this.sensors.connectionCreated.record();\n                }\n\n                /* if channel is not ready finish prepare */\n                if (channel.isConnected() && !channel.ready())\n                    channel.prepare();\n\n                /* if channel is ready read from any connections that have readable data */\n                if (channel.ready() && key.isReadable() && !hasStagedReceive(channel)) {\n                    NetworkReceive networkReceive;\n                    try {\n                        while ((networkReceive = channel.read()) != null) {\n                            addToStagedReceives(channel, networkReceive);\n                        }\n                    } catch (InvalidReceiveException e) {\n                        log.error(\"Invalid data received from \" + channel.id() + \" closing connection\", e);\n                        close(channel);\n                        this.disconnected.add(channel.id());\n                        throw e;\n                    }\n                }\n\n                /* if channel is ready write to any sockets that have space in their buffer and for which we have data */\n                if (channel.ready() && key.isWritable()) {\n                    Send send = channel.write();\n                    if (send != null) {\n                        this.completedSends.add(send);\n                        this.sensors.recordBytesSent(channel.id(), send.size());\n                    }\n                }\n\n                /* cancel any defunct sockets */\n                if (!key.isValid()) {\n                    close(channel);\n                    this.disconnected.add(channel.id());\n                }\n            } catch (IOException e) {\n                String desc = channel.socketDescription();\n                close(channel);\n                this.disconnected.add(channel.id());\n            }\n        }\n    }\n\n    addToCompletedReceives();\n\n    long endIo = time.nanoseconds();\n    this.sensors.ioTime.record(endIo - endSelect, time.milliseconds());\n    maybeCloseOldestConnection();\n}",
        "accept_response": "@Override\npublic void poll(long timeout) throws IOException {\n    clear();\n    if (hasStagedReceives())\n        timeout = 0;\n    /* check ready keys */\n    long startSelect = time.nanoseconds();\n    int readyKeys = select(timeout);\n    long endSelect = time.nanoseconds();\n    currentTimeNanos = endSelect;\n    this.sensors.selectTime.record(endSelect - startSelect, time.milliseconds());\n\n    if (readyKeys > 0) {\n        Set<SelectionKey> keys = this.nioSelector.selectedKeys();\n        Iterator<SelectionKey> iter = keys.iterator();\n        while (iter.hasNext()) {\n            SelectionKey key = iter.next();\n            iter.remove();\n            KafkaChannel channel = channel(key);\n\n            // register all per-connection metrics at once\n            sensors.maybeRegisterConnectionMetrics(channel.id());\n            lruConnections.put(channel.id(), currentTimeNanos);\n\n            try {\n                /* complete any connections that have finished their handshake */\n                if (key.isConnectable()) {\n                    channel.finishConnect();\n                    this.connected.add(channel.id());\n                    this.sensors.connectionCreated.record();\n                }\n\n                /* if channel is not ready finish prepare */\n                if (channel.isConnected() && !channel.ready())\n                    channel.prepare();\n\n                /* if channel is ready read from any connections that have readable data */\n                if (channel.ready() && key.isReadable() && !hasStagedReceive(channel)) {\n                    NetworkReceive networkReceive;\n                    try {\n                        while ((networkReceive = channel.read()) != null) {\n                            addToStagedReceives(channel, networkReceive);\n                        }\n                    } catch (InvalidReceiveException e) {\n                        log.error(\"Invalid data received from \" + channel.id() + \" closing connection\", e);\n                        close(channel);\n                        this.disconnected.add(channel.id());\n                        throw e;\n                    }\n                }\n\n                /* if channel is ready write to any sockets that have space in their buffer and for which we have data */\n                if (channel.ready() && key.isWritable()) {\n                    Send send = channel.write();\n                    if (send != null) {\n                        this.completedSends.add(send);\n                        this.sensors.recordBytesSent(channel.id(), send.size());\n                    }\n                }\n\n                /* cancel any defunct sockets */\n                if (!key.isValid()) {\n                    close(channel);\n                    this.disconnected.add(channel.id());\n                }\n            } catch (IOException e) {\n                String desc = channel.socketDescription();\n                log.debug(\"Connection with {} disconnected\", desc, e);\n                close(channel);\n                this.disconnected.add(channel.id());\n            }\n        }\n    }\n\n    addToCompletedReceives();\n\n    long endIo = time.nanoseconds();\n    this.sensors.ioTime.record(endIo - endSelect, time.milliseconds());\n    maybeCloseOldestConnection();\n}",
        "reject_response": "@Override\npublic void poll(long timeout) throws IOException {\n    clear();\n    if (hasStagedReceives())\n        timeout = 0;\n    /* check ready keys */\n    long startSelect = time.nanoseconds();\n    int readyKeys = select(timeout);\n    long endSelect = time.nanoseconds();\n    currentTimeNanos = endSelect;\n    this.sensors.selectTime.record(endSelect - startSelect, time.milliseconds());\n\n    if (readyKeys > 0) {\n        Set<SelectionKey> keys = this.nioSelector.selectedKeys();\n        Iterator<SelectionKey> iter = keys.iterator();\n        while (iter.hasNext()) {\n            SelectionKey key = iter.next();\n            iter.remove();\n            KafkaChannel channel = channel(key);\n\n            // register all per-connection metrics at once\n            sensors.maybeRegisterConnectionMetrics(channel.id());\n            lruConnections.put(channel.id(), currentTimeNanos);\n\n            try {\n                /* complete any connections that have finished their handshake */\n                if (key.isConnectable()) {\n                    channel.finishConnect();\n                    this.connected.add(channel.id());\n                    this.sensors.connectionCreated.record();\n                }\n\n                /* if channel is not ready finish prepare */\n                if (channel.isConnected() && !channel.ready())\n                    channel.prepare();\n\n                /* if channel is ready read from any connections that have readable data */\n                if (channel.ready() && key.isReadable() && !hasStagedReceive(channel)) {\n                    NetworkReceive networkReceive;\n                    try {\n                        while ((networkReceive = channel.read()) != null) {\n                            addToStagedReceives(channel, networkReceive);\n                        }\n                    } catch (InvalidReceiveException e) {\n                        log.error(\"Invalid data received from \" + channel.id() + \" closing connection\", e);\n                        close(channel);\n                        this.disconnected.add(channel.id());\n                        throw e;\n                    }\n                }\n\n                /* if channel is ready write to any sockets that have space in their buffer and for which we have data */\n                if (channel.ready() && key.isWritable()) {\n                    Send send = channel.write();\n                    if (send != null) {\n                        this.completedSends.add(send);\n                        this.sensors.recordBytesSent(channel.id(), send.size());\n                    }\n                }\n\n                /* cancel any defunct sockets */\n                if (!key.isValid()) {\n                    close(channel);\n                    this.disconnected.add(channel.id());\n                }\n            } catch (IOException e) {\n                String desc = channel.socketDescription();\n                if (e instanceof EOFException || e instanceof ConnectException)\n                    log.debug(\"Connection {} disconnected\", desc);\n                else\n                    log.warn(\"Error in I/O with connection to {}\", desc, e);\n                close(channel);\n                this.disconnected.add(channel.id());\n            }\n        }\n    }\n\n    addToCompletedReceives();\n\n    long endIo = time.nanoseconds();\n    this.sensors.ioTime.record(endIo - endSelect, time.milliseconds());\n    maybeCloseOldestConnection();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3054,
        "instruction": "public String getVal(String path, boolean errIfMissing) {\n  Node nd = getNode(path,errIfMissing);\n  if (nd==null) return null;\n\n  String txt = DOMUtil.getText(nd);\n  return txt;\n\n  /******\n  short typ = nd.getNodeType();\n  if (typ==Node.ATTRIBUTE_NODE || typ==Node.TEXT_NODE) {\n    return nd.getNodeValue();\n  }\n  return nd.getTextContent();\n  ******/\n}",
        "accept_response": "public String getVal(String path, boolean errIfMissing) {\n  Node nd = getNode(path,errIfMissing);\n  if (nd==null) return null;\n\n  String txt = DOMUtil.getText(nd);\n  log.debug(\"{} {}={}\", name, path, txt);\n  return txt;\n\n  /******\n  short typ = nd.getNodeType();\n  if (typ==Node.ATTRIBUTE_NODE || typ==Node.TEXT_NODE) {\n    return nd.getNodeValue();\n  }\n  return nd.getTextContent();\n  ******/\n}",
        "reject_response": "public String getVal(String path, boolean errIfMissing) {\n  Node nd = getNode(path,errIfMissing);\n  if (nd==null) return null;\n\n  String txt = DOMUtil.getText(nd);\n\n  log.debug(name + ' '+path+'='+txt);\n  return txt;\n\n  /******\n  short typ = nd.getNodeType();\n  if (typ==Node.ATTRIBUTE_NODE || typ==Node.TEXT_NODE) {\n    return nd.getNodeValue();\n  }\n  return nd.getTextContent();\n  ******/\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2388,
        "instruction": "@Override\npublic void invalidateCache(Collection<KeyExtent> keySet) {\n  wLock.lock();\n  try {\n    badExtents.addAll(keySet);\n  } finally {\n    wLock.unlock();\n  }\n  if (log.isTraceEnabled())\n}",
        "accept_response": "@Override\npublic void invalidateCache(Collection<KeyExtent> keySet) {\n  wLock.lock();\n  try {\n    badExtents.addAll(keySet);\n  } finally {\n    wLock.unlock();\n  }\n  if (log.isTraceEnabled())\n    log.trace(\"Invalidated {} cache entries for table {}\", keySet.size(), tableId);\n}",
        "reject_response": "@Override\npublic void invalidateCache(Collection<KeyExtent> keySet) {\n  wLock.lock();\n  try {\n    badExtents.addAll(keySet);\n  } finally {\n    wLock.unlock();\n  }\n  if (log.isTraceEnabled())\n    log.trace(\"Invalidated \" + keySet.size() + \" cache entries for table \" + tableId);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2868,
        "instruction": "public static Model loadAsModel(Model model, String filenameOrURI, ResultsFormat format) {\n    if (model == null)\n        model = GraphFactory.makeDefaultModel();\n\n    if (format == null)\n        format = ResultsFormat.guessSyntax(filenameOrURI);\n\n    if (format == null) {\n        Log.warn(ResultSet.class, \"Null format - defaulting to XML\");\n        format = ResultsFormat.FMT_RS_XML;\n    }\n\n    if (format.equals(ResultsFormat.FMT_TEXT)) {\n        Log.error(ResultSet.class, \"Can't read a text result set\");\n        throw new ResultSetException(\"Can't read a text result set\");\n    }\n\n    if (format.equals(ResultsFormat.FMT_RS_XML) || format.equals(ResultsFormat.FMT_RS_JSON)) {\n        InputStream in = null;\n        try {\n            in = FileManager.get().open(filenameOrURI);\n            if (in == null)\n                throw new NotFoundException(filenameOrURI);\n        } catch (NotFoundException ex) {\n            throw new NotFoundException(\"File not found: \" + filenameOrURI);\n        }\n\n        SPARQLResult x = null;\n\n        if (format.equals(ResultsFormat.FMT_RS_JSON))\n            x = JSONInput.make(in, GraphFactory.makeDefaultModel());\n        else\n            x = XMLInput.make(in, GraphFactory.makeDefaultModel());\n\n        if (x.isResultSet())\n            RDFOutput.encodeAsRDF(model, x.getResultSet());\n        else\n            RDFOutput.encodeAsRDF(model, x.getBooleanResult());\n\n        return model;\n    }\n\n    if (ResultsFormat.isRDFGraphSyntax(format))\n        return FileManager.get().readModel(model, filenameOrURI);\n\n    return null;\n}",
        "accept_response": "public static Model loadAsModel(Model model, String filenameOrURI, ResultsFormat format) {\n    if (model == null)\n        model = GraphFactory.makeDefaultModel();\n\n    if (format == null)\n        format = ResultsFormat.guessSyntax(filenameOrURI);\n\n    if (format == null) {\n        Log.warn(ResultSet.class, \"Null format - defaulting to XML\");\n        format = ResultsFormat.FMT_RS_XML;\n    }\n\n    if (format.equals(ResultsFormat.FMT_TEXT)) {\n        Log.error(ResultSet.class, \"Can't read a text result set\");\n        throw new ResultSetException(\"Can't read a text result set\");\n    }\n\n    if (format.equals(ResultsFormat.FMT_RS_XML) || format.equals(ResultsFormat.FMT_RS_JSON)) {\n        InputStream in = null;\n        try {\n            in = FileManager.get().open(filenameOrURI);\n            if (in == null)\n                throw new NotFoundException(filenameOrURI);\n        } catch (NotFoundException ex) {\n            throw new NotFoundException(\"File not found: \" + filenameOrURI);\n        }\n\n        SPARQLResult x = null;\n\n        if (format.equals(ResultsFormat.FMT_RS_JSON))\n            x = JSONInput.make(in, GraphFactory.makeDefaultModel());\n        else\n            x = XMLInput.make(in, GraphFactory.makeDefaultModel());\n\n        if (x.isResultSet())\n            RDFOutput.encodeAsRDF(model, x.getResultSet());\n        else\n            RDFOutput.encodeAsRDF(model, x.getBooleanResult());\n\n        return model;\n    }\n\n    if (ResultsFormat.isRDFGraphSyntax(format))\n        return FileManager.get().readModel(model, filenameOrURI);\n\n    Log.error(ResultSet.class, \"Unknown result set syntax: \" + format);\n    return null;\n}",
        "reject_response": "public static Model loadAsModel(Model model, String filenameOrURI, ResultsFormat format) {\n    if (model == null)\n        model = GraphFactory.makeDefaultModel();\n\n    if (format == null)\n        format = ResultsFormat.guessSyntax(filenameOrURI);\n\n    if (format == null) {\n        Log.warn(ResultSet.class, \"Null format - defaulting to XML\");\n        format = ResultsFormat.FMT_RS_XML;\n    }\n\n    if (format.equals(ResultsFormat.FMT_TEXT)) {\n        Log.error(ResultSet.class, \"Can't read a text result set\");\n        throw new ResultSetException(\"Can't read a text result set\");\n    }\n\n    if (format.equals(ResultsFormat.FMT_RS_XML) || format.equals(ResultsFormat.FMT_RS_JSON)) {\n        InputStream in = null;\n        try {\n            in = FileManager.get().open(filenameOrURI);\n            if (in == null)\n                throw new NotFoundException(filenameOrURI);\n        } catch (NotFoundException ex) {\n            throw new NotFoundException(\"File not found: \" + filenameOrURI);\n        }\n\n        SPARQLResult x = null;\n\n        if (format.equals(ResultsFormat.FMT_RS_JSON))\n            x = JSONInput.make(in, GraphFactory.makeDefaultModel());\n        else\n            x = XMLInput.make(in, GraphFactory.makeDefaultModel());\n\n        if (x.isResultSet())\n            RDFOutput.encodeAsRDF(model, x.getResultSet());\n        else\n            RDFOutput.encodeAsRDF(model, x.getBooleanResult());\n\n        return model;\n    }\n\n    if (ResultsFormat.isRDFGraphSyntax(format))\n        return FileManager.get().readModel(model, filenameOrURI);\n\n    Log.fatal(ResultSet.class, \"Unknown result set syntax: \" + format);\n    return null;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2654,
        "instruction": "@Override\npublic void contextInitialized(ServletContextEvent event) {\n\n  messagesToBeLogged = messagesToBeLogged\n      .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_CONTEXT_INITIALIZED\")));\n\n  // Load Pulse version details\n  loadPulseVersionDetails();\n\n  // Load Pulse Properties\n  pulseProperties = loadProperties(PulseConstants.PULSE_PROPERTIES_FILE);\n\n  if (pulseProperties.isEmpty()) {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_NOT_FOUND\")));\n  } else {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_FOUND\")));\n\n    // set Pulse product support into the Pulse controller for access from\n    // client side\n    // to display the appropriate ui depending on which product is supported\n    // in present deployment\n    String pulseProduct =\n        pulseProperties.getProperty(PulseConstants.APPLICATION_PROPERTY_PULSE_PRODUCTSUPPORT);\n  }\n\n  pulseSecurityProperties = loadProperties(PulseConstants.PULSE_SECURITY_PROPERTIES_FILE);\n\n  // Initialize logger\n  initializeLogger();\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n  logger.info(resourceBundle.getString(\"LOG_MSG_CHECK_APP_RUNNING_MODE\"));\n\n  boolean sysIsEmbedded = Boolean.getBoolean(PulseConstants.SYSTEM_PROPERTY_PULSE_EMBEDDED);\n\n  if (sysIsEmbedded) {\n    // Application Pulse is running in Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_EMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(true);\n\n    sysPulseUseLocator = Boolean.FALSE;\n    try {\n      // Get host name of machine running pulse in embedded mode\n      sysPulseHost = InetAddress.getLocalHost().getCanonicalHostName();\n    } catch (Exception e) {\n      logger.debug(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_UNKNOWN_HOST\"), e);\n      // Set default host name\n      sysPulseHost = PulseConstants.GEMFIRE_DEFAULT_HOST;\n    }\n    sysPulsePort = System.getProperty(PulseConstants.SYSTEM_PROPERTY_PULSE_PORT);\n    if (StringUtils.isBlank(sysPulsePort)) {\n      sysPulsePort = PulseConstants.GEMFIRE_DEFAULT_PORT;\n    }\n\n  } else {\n    // Application Pulse is running in Non-Embedded Mode\n    repository.setIsEmbeddedMode(false);\n\n    // Load JMX User Details\n    loadJMXUserDetails();\n    // Load locator and/or manager details\n    loadLocatorManagerDetails();\n\n    useGemFireCredentials = areWeUsingGemFireSecurityProfile(event);\n  }\n\n  // Set user details in repository\n  repository.setJmxUserName(jmxUserName);\n  repository.setJmxUserPassword(jmxUserPassword);\n\n  // Set locator/Manager details in repository\n  repository.setJmxUseLocator(sysPulseUseLocator);\n  repository.setJmxHost(sysPulseHost);\n  repository.setJmxPort(sysPulsePort);\n\n  // set SSL info\n  initializeSSL();\n  repository.setUseSSLLocator(sysPulseUseSSLLocator);\n  repository.setUseSSLManager(sysPulseUseSSLManager);\n\n  repository.setUseGemFireCredentials(useGemFireCredentials);\n\n}",
        "accept_response": "@Override\npublic void contextInitialized(ServletContextEvent event) {\n\n  messagesToBeLogged = messagesToBeLogged\n      .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_CONTEXT_INITIALIZED\")));\n\n  // Load Pulse version details\n  loadPulseVersionDetails();\n\n  // Load Pulse Properties\n  pulseProperties = loadProperties(PulseConstants.PULSE_PROPERTIES_FILE);\n\n  if (pulseProperties.isEmpty()) {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_NOT_FOUND\")));\n  } else {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_FOUND\")));\n\n    // set Pulse product support into the Pulse controller for access from\n    // client side\n    // to display the appropriate ui depending on which product is supported\n    // in present deployment\n    String pulseProduct =\n        pulseProperties.getProperty(PulseConstants.APPLICATION_PROPERTY_PULSE_PRODUCTSUPPORT);\n  }\n\n  pulseSecurityProperties = loadProperties(PulseConstants.PULSE_SECURITY_PROPERTIES_FILE);\n\n  // Initialize logger\n  initializeLogger();\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n  logger.info(resourceBundle.getString(\"LOG_MSG_CHECK_APP_RUNNING_MODE\"));\n\n  boolean sysIsEmbedded = Boolean.getBoolean(PulseConstants.SYSTEM_PROPERTY_PULSE_EMBEDDED);\n\n  if (sysIsEmbedded) {\n    // Application Pulse is running in Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_EMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(true);\n\n    sysPulseUseLocator = Boolean.FALSE;\n    try {\n      // Get host name of machine running pulse in embedded mode\n      sysPulseHost = InetAddress.getLocalHost().getCanonicalHostName();\n    } catch (Exception e) {\n      logger.debug(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_UNKNOWN_HOST\"), e);\n      // Set default host name\n      sysPulseHost = PulseConstants.GEMFIRE_DEFAULT_HOST;\n    }\n    sysPulsePort = System.getProperty(PulseConstants.SYSTEM_PROPERTY_PULSE_PORT);\n    if (StringUtils.isBlank(sysPulsePort)) {\n      sysPulsePort = PulseConstants.GEMFIRE_DEFAULT_PORT;\n    }\n\n  } else {\n    // Application Pulse is running in Non-Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_NONEMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(false);\n\n    // Load JMX User Details\n    loadJMXUserDetails();\n    // Load locator and/or manager details\n    loadLocatorManagerDetails();\n\n    useGemFireCredentials = areWeUsingGemFireSecurityProfile(event);\n  }\n\n  // Set user details in repository\n  repository.setJmxUserName(jmxUserName);\n  repository.setJmxUserPassword(jmxUserPassword);\n\n  // Set locator/Manager details in repository\n  repository.setJmxUseLocator(sysPulseUseLocator);\n  repository.setJmxHost(sysPulseHost);\n  repository.setJmxPort(sysPulsePort);\n\n  // set SSL info\n  initializeSSL();\n  repository.setUseSSLLocator(sysPulseUseSSLLocator);\n  repository.setUseSSLManager(sysPulseUseSSLManager);\n\n  repository.setUseGemFireCredentials(useGemFireCredentials);\n\n}",
        "reject_response": "@Override\npublic void contextInitialized(ServletContextEvent event) {\n\n  messagesToBeLogged = messagesToBeLogged\n      .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_CONTEXT_INITIALIZED\")));\n\n  // Load Pulse version details\n  loadPulseVersionDetails();\n\n  // Load Pulse Properties\n  pulseProperties = loadProperties(PulseConstants.PULSE_PROPERTIES_FILE);\n\n  if (pulseProperties.isEmpty()) {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_NOT_FOUND\")));\n  } else {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_FOUND\")));\n\n    // set Pulse product support into the Pulse controller for access from\n    // client side\n    // to display the appropriate ui depending on which product is supported\n    // in present deployment\n    String pulseProduct =\n        pulseProperties.getProperty(PulseConstants.APPLICATION_PROPERTY_PULSE_PRODUCTSUPPORT);\n  }\n\n  pulseSecurityProperties = loadProperties(PulseConstants.PULSE_SECURITY_PROPERTIES_FILE);\n\n  // Initialize logger\n  initializeLogger();\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n  logger.info(resourceBundle.getString(\"LOG_MSG_CHECK_APP_RUNNING_MODE\"));\n\n  boolean sysIsEmbedded = Boolean.getBoolean(PulseConstants.SYSTEM_PROPERTY_PULSE_EMBEDDED);\n\n  if (sysIsEmbedded) {\n    // Application Pulse is running in Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_EMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(true);\n\n    sysPulseUseLocator = Boolean.FALSE;\n    try {\n      // Get host name of machine running pulse in embedded mode\n      sysPulseHost = InetAddress.getLocalHost().getCanonicalHostName();\n    } catch (Exception e) {\n      logger.debug(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_UNKNOWN_HOST\"), e);\n      // Set default host name\n      sysPulseHost = PulseConstants.GEMFIRE_DEFAULT_HOST;\n    }\n    sysPulsePort = System.getProperty(PulseConstants.SYSTEM_PROPERTY_PULSE_PORT);\n    if (StringUtils.isBlank(sysPulsePort)) {\n      sysPulsePort = PulseConstants.GEMFIRE_DEFAULT_PORT;\n    }\n\n  } else {\n    // Application Pulse is running in Non-Embedded Mode\n    if (LOGGER.infoEnabled()) {\n      LOGGER.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_NONEMBEDDED_MODE\"));\n    }\n    repository.setIsEmbeddedMode(false);\n\n    // Load JMX User Details\n    loadJMXUserDetails();\n    // Load locator and/or manager details\n    loadLocatorManagerDetails();\n\n    useGemFireCredentials = areWeUsingGemFireSecurityProfile(event);\n  }\n\n  // Set user details in repository\n  repository.setJmxUserName(jmxUserName);\n  repository.setJmxUserPassword(jmxUserPassword);\n\n  // Set locator/Manager details in repository\n  repository.setJmxUseLocator(sysPulseUseLocator);\n  repository.setJmxHost(sysPulseHost);\n  repository.setJmxPort(sysPulsePort);\n\n  // set SSL info\n  initializeSSL();\n  repository.setUseSSLLocator(sysPulseUseSSLLocator);\n  repository.setUseSSLManager(sysPulseUseSSLManager);\n\n  repository.setUseGemFireCredentials(useGemFireCredentials);\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2601,
        "instruction": "private void handlePartiallyFailedRequest(\n        PutRecordBatchResponse response,\n        List<Record> requestEntries,\n        Consumer<List<Record>> requestResult) {\n    numRecordsOutErrorsCounter.inc(response.failedPutCount());\n    numRecordsSendErrorsCounter.inc(response.failedPutCount());\n\n    if (failOnError) {\n        getFatalExceptionCons()\n                .accept(new KinesisFirehoseException.KinesisFirehoseFailFastException());\n        return;\n    }\n    List<Record> failedRequestEntries = new ArrayList<>(response.failedPutCount());\n    List<PutRecordBatchResponseEntry> records = response.requestResponses();\n\n    for (int i = 0; i < records.size(); i++) {\n        if (records.get(i).errorCode() != null) {\n            failedRequestEntries.add(requestEntries.get(i));\n        }\n    }\n\n    requestResult.accept(failedRequestEntries);\n}",
        "accept_response": "private void handlePartiallyFailedRequest(\n        PutRecordBatchResponse response,\n        List<Record> requestEntries,\n        Consumer<List<Record>> requestResult) {\n    LOG.debug(\n            \"KDF Sink failed to write and will retry {} entries to KDF first request was {}\",\n            requestEntries.size(),\n            requestEntries.get(0).toString());\n    numRecordsOutErrorsCounter.inc(response.failedPutCount());\n    numRecordsSendErrorsCounter.inc(response.failedPutCount());\n\n    if (failOnError) {\n        getFatalExceptionCons()\n                .accept(new KinesisFirehoseException.KinesisFirehoseFailFastException());\n        return;\n    }\n    List<Record> failedRequestEntries = new ArrayList<>(response.failedPutCount());\n    List<PutRecordBatchResponseEntry> records = response.requestResponses();\n\n    for (int i = 0; i < records.size(); i++) {\n        if (records.get(i).errorCode() != null) {\n            failedRequestEntries.add(requestEntries.get(i));\n        }\n    }\n\n    requestResult.accept(failedRequestEntries);\n}",
        "reject_response": "private void handlePartiallyFailedRequest(\n        PutRecordBatchResponse response,\n        List<Record> requestEntries,\n        Consumer<List<Record>> requestResult) {\n    LOG.warn(\n            \"KDF Sink failed to persist {} entries to KDF first request was {}\",\n    numRecordsOutErrorsCounter.inc(response.failedPutCount());\n    numRecordsSendErrorsCounter.inc(response.failedPutCount());\n\n    if (failOnError) {\n        getFatalExceptionCons()\n                .accept(new KinesisFirehoseException.KinesisFirehoseFailFastException());\n        return;\n    }\n    List<Record> failedRequestEntries = new ArrayList<>(response.failedPutCount());\n    List<PutRecordBatchResponseEntry> records = response.requestResponses();\n\n    for (int i = 0; i < records.size(); i++) {\n        if (records.get(i).errorCode() != null) {\n            failedRequestEntries.add(requestEntries.get(i));\n        }\n    }\n\n    requestResult.accept(failedRequestEntries);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2669,
        "instruction": "private void runCompiler(String[] commandLine) {\n    // hand crank it so we can add our own compiler configuration\n    try {\n        Options options = FileSystemCompiler.createCompilationOptions();\n\n        CommandLineParser cliParser = new GroovyInternalPosixParser();\n\n        CommandLine cli;\n        cli = cliParser.parse(options, commandLine);\n\n        configuration = FileSystemCompiler.generateCompilerConfigurationFromOptions(cli);\n        configuration.setScriptExtensions(getScriptExtensions());\n        String tmpExtension = getScriptExtension();\n        if (tmpExtension.startsWith(\"*.\"))\n            tmpExtension = tmpExtension.substring(1);\n        configuration.setDefaultScriptExtension(tmpExtension);\n\n        // Load the file name list\n        String[] filenames = FileSystemCompiler.generateFileNamesFromOptions(cli);\n        boolean fileNameErrors = filenames == null;\n\n        fileNameErrors = fileNameErrors && !FileSystemCompiler.validateFiles(filenames);\n\n        if (targetBytecode != null) {\n            configuration.setTargetBytecode(targetBytecode);\n        }\n\n        if (!fileNameErrors) {\n            FileSystemCompiler.doCompilation(configuration, makeCompileUnit(), filenames, forceLookupUnnamedFiles);\n        }\n\n    } catch (Exception re) {\n        Throwable t = re;\n        if ((re.getClass() == RuntimeException.class) && (re.getCause() != null)) {\n            // unwrap to the real exception\n            t = re.getCause();\n        }\n        StringWriter writer = new StringWriter();\n        new ErrorReporter(t, false).write(new PrintWriter(writer));\n        String message = writer.toString();\n\n        taskSuccess = false;\n        if (errorProperty != null) {\n            getProject().setNewProperty(errorProperty, \"true\");\n        }\n\n        if (failOnError) {\n            throw new BuildException(\"Compilation Failed\", t, getLocation());\n        } else {\n            log.error(message);\n        }\n    }\n}",
        "accept_response": "private void runCompiler(String[] commandLine) {\n    // hand crank it so we can add our own compiler configuration\n    try {\n        Options options = FileSystemCompiler.createCompilationOptions();\n\n        CommandLineParser cliParser = new GroovyInternalPosixParser();\n\n        CommandLine cli;\n        cli = cliParser.parse(options, commandLine);\n\n        configuration = FileSystemCompiler.generateCompilerConfigurationFromOptions(cli);\n        configuration.setScriptExtensions(getScriptExtensions());\n        String tmpExtension = getScriptExtension();\n        if (tmpExtension.startsWith(\"*.\"))\n            tmpExtension = tmpExtension.substring(1);\n        configuration.setDefaultScriptExtension(tmpExtension);\n\n        // Load the file name list\n        String[] filenames = FileSystemCompiler.generateFileNamesFromOptions(cli);\n        boolean fileNameErrors = filenames == null;\n\n        fileNameErrors = fileNameErrors && !FileSystemCompiler.validateFiles(filenames);\n\n        if (targetBytecode != null) {\n            configuration.setTargetBytecode(targetBytecode);\n        }\n\n        if (!fileNameErrors) {\n            FileSystemCompiler.doCompilation(configuration, makeCompileUnit(), filenames, forceLookupUnnamedFiles);\n        }\n\n    } catch (Exception re) {\n        Throwable t = re;\n        if ((re.getClass() == RuntimeException.class) && (re.getCause() != null)) {\n            // unwrap to the real exception\n            t = re.getCause();\n        }\n        StringWriter writer = new StringWriter();\n        new ErrorReporter(t, false).write(new PrintWriter(writer));\n        String message = writer.toString();\n\n        taskSuccess = false;\n        if (errorProperty != null) {\n            getProject().setNewProperty(errorProperty, \"true\");\n        }\n\n        if (failOnError) {\n            log.error(message);\n            throw new BuildException(\"Compilation Failed\", t, getLocation());\n        } else {\n            log.error(message);\n        }\n    }\n}",
        "reject_response": "private void runCompiler(String[] commandLine) {\n    // hand crank it so we can add our own compiler configuration\n    try {\n        Options options = FileSystemCompiler.createCompilationOptions();\n\n        CommandLineParser cliParser = new GroovyInternalPosixParser();\n\n        CommandLine cli;\n        cli = cliParser.parse(options, commandLine);\n\n        configuration = FileSystemCompiler.generateCompilerConfigurationFromOptions(cli);\n        configuration.setScriptExtensions(getScriptExtensions());\n        String tmpExtension = getScriptExtension();\n        if (tmpExtension.startsWith(\"*.\"))\n            tmpExtension = tmpExtension.substring(1);\n        configuration.setDefaultScriptExtension(tmpExtension);\n\n        // Load the file name list\n        String[] filenames = FileSystemCompiler.generateFileNamesFromOptions(cli);\n        boolean fileNameErrors = filenames == null;\n\n        fileNameErrors = fileNameErrors && !FileSystemCompiler.validateFiles(filenames);\n\n        if (targetBytecode != null) {\n            configuration.setTargetBytecode(targetBytecode);\n        }\n\n        if (!fileNameErrors) {\n            FileSystemCompiler.doCompilation(configuration, makeCompileUnit(), filenames, forceLookupUnnamedFiles);\n        }\n\n    } catch (Exception re) {\n        Throwable t = re;\n        if ((re.getClass() == RuntimeException.class) && (re.getCause() != null)) {\n            // unwrap to the real exception\n            t = re.getCause();\n        }\n        StringWriter writer = new StringWriter();\n        new ErrorReporter(t, false).write(new PrintWriter(writer));\n        String message = writer.toString();\n\n        taskSuccess = false;\n        if (errorProperty != null) {\n            getProject().setNewProperty(errorProperty, \"true\");\n        }\n\n        if (failOnError) {\n            log.info(message);\n            throw new BuildException(\"Compilation Failed\", t, getLocation());\n        } else {\n            log.error(message);\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2994,
        "instruction": "public List<String> getTableList(String needle, List<String> catalogs, List<String> schemas, List<String> tables) throws HadoopException {\n  final String ndl = needle;\n  final List<String> cats = catalogs;\n  final List<String> shms = schemas;\n  final List<String> tbls = tables;\n\n  List<String> tableList = Subject.doAs(getLoginSubject(), new PrivilegedAction<List<String>>() {\n    @Override\n    public List<String> run() {\n      List<String> ret = null;\n      try {\n        ret = getTables(ndl, cats, shms, tbls);\n      } catch (HadoopException he) {\n        throw he;\n      }\n      return ret;\n    }\n  });\n\n  return tableList;\n}",
        "accept_response": "public List<String> getTableList(String needle, List<String> catalogs, List<String> schemas, List<String> tables) throws HadoopException {\n  final String ndl = needle;\n  final List<String> cats = catalogs;\n  final List<String> shms = schemas;\n  final List<String> tbls = tables;\n\n  List<String> tableList = Subject.doAs(getLoginSubject(), new PrivilegedAction<List<String>>() {\n    @Override\n    public List<String> run() {\n      List<String> ret = null;\n      try {\n        ret = getTables(ndl, cats, shms, tbls);\n      } catch (HadoopException he) {\n        LOG.error(\"<== PrestoClient.getTableList() :Unable to get the Column List\", he);\n        throw he;\n      }\n      return ret;\n    }\n  });\n\n  return tableList;\n}",
        "reject_response": "public List<String> getTableList(String needle, List<String> catalogs, List<String> schemas, List<String> tables) throws HadoopException {\n  final String ndl = needle;\n  final List<String> cats = catalogs;\n  final List<String> shms = schemas;\n  final List<String> tbls = tables;\n\n  List<String> tableList = Subject.doAs(getLoginSubject(), new PrivilegedAction<List<String>>() {\n    @Override\n    public List<String> run() {\n      List<String> ret = null;\n      try {\n        ret = getTables(ndl, cats, shms, tbls);\n      } catch (HadoopException he) {\n        LOG.error(\"<== PrestoClient getTableList() :Unable to get the Column List\", he);\n        throw he;\n      }\n      return ret;\n    }\n  });\n\n  return tableList;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2618,
        "instruction": "@Override\npublic void contextInitialized(ServletContextEvent event) {\n\n  messagesToBeLogged = messagesToBeLogged\n      .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_CONTEXT_INITIALIZED\")));\n\n  // Load Pulse version details\n  loadPulseVersionDetails();\n\n  // Load Pulse Properties\n  pulseProperties = loadProperties(PulseConstants.PULSE_PROPERTIES_FILE);\n\n  if (pulseProperties.isEmpty()) {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_NOT_FOUND\")));\n  } else {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_FOUND\")));\n\n    // set Pulse product support into the Pulse controller for access from\n    // client side\n    // to display the appropriate ui depending on which product is supported\n    // in present deployment\n    String pulseProduct =\n        pulseProperties.getProperty(PulseConstants.APPLICATION_PROPERTY_PULSE_PRODUCTSUPPORT);\n  }\n\n  pulseSecurityProperties = loadProperties(PulseConstants.PULSE_SECURITY_PROPERTIES_FILE);\n\n  // Initialize logger\n  initializeLogger();\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n\n  boolean sysIsEmbedded = Boolean.getBoolean(PulseConstants.SYSTEM_PROPERTY_PULSE_EMBEDDED);\n\n  if (sysIsEmbedded) {\n    // Application Pulse is running in Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_EMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(true);\n\n    sysPulseUseLocator = Boolean.FALSE;\n    try {\n      // Get host name of machine running pulse in embedded mode\n      sysPulseHost = InetAddress.getLocalHost().getCanonicalHostName();\n    } catch (Exception e) {\n      logger.debug(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_UNKNOWN_HOST\"), e);\n      // Set default host name\n      sysPulseHost = PulseConstants.GEMFIRE_DEFAULT_HOST;\n    }\n    sysPulsePort = PulseConstants.GEMFIRE_DEFAULT_PORT;\n\n  } else {\n    // Application Pulse is running in Non-Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_NONEMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(false);\n\n    // Load JMX User Details\n    loadJMXUserDetails();\n    // Load locator and/or manager details\n    loadLocatorManagerDetails();\n\n    useGemFireCredentials = areWeUsingGemFireSecurityProfile(event);\n  }\n\n  // Set user details in repository\n  repository.setJmxUserName(jmxUserName);\n  repository.setJmxUserPassword(jmxUserPassword);\n\n  // Set locator/Manager details in repository\n  repository.setJmxUseLocator(sysPulseUseLocator);\n  repository.setJmxHost(sysPulseHost);\n  repository.setJmxPort(sysPulsePort);\n\n  // set SSL info\n  initializeSSL();\n  repository.setUseSSLLocator(sysPulseUseSSLLocator);\n  repository.setUseSSLManager(sysPulseUseSSLManager);\n\n  repository.setUseGemFireCredentials(useGemFireCredentials);\n\n}",
        "accept_response": "@Override\npublic void contextInitialized(ServletContextEvent event) {\n\n  messagesToBeLogged = messagesToBeLogged\n      .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_CONTEXT_INITIALIZED\")));\n\n  // Load Pulse version details\n  loadPulseVersionDetails();\n\n  // Load Pulse Properties\n  pulseProperties = loadProperties(PulseConstants.PULSE_PROPERTIES_FILE);\n\n  if (pulseProperties.isEmpty()) {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_NOT_FOUND\")));\n  } else {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_FOUND\")));\n\n    // set Pulse product support into the Pulse controller for access from\n    // client side\n    // to display the appropriate ui depending on which product is supported\n    // in present deployment\n    String pulseProduct =\n        pulseProperties.getProperty(PulseConstants.APPLICATION_PROPERTY_PULSE_PRODUCTSUPPORT);\n  }\n\n  pulseSecurityProperties = loadProperties(PulseConstants.PULSE_SECURITY_PROPERTIES_FILE);\n\n  // Initialize logger\n  initializeLogger();\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n  logger.info(resourceBundle.getString(\"LOG_MSG_CHECK_APP_RUNNING_MODE\"));\n\n  boolean sysIsEmbedded = Boolean.getBoolean(PulseConstants.SYSTEM_PROPERTY_PULSE_EMBEDDED);\n\n  if (sysIsEmbedded) {\n    // Application Pulse is running in Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_EMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(true);\n\n    sysPulseUseLocator = Boolean.FALSE;\n    try {\n      // Get host name of machine running pulse in embedded mode\n      sysPulseHost = InetAddress.getLocalHost().getCanonicalHostName();\n    } catch (Exception e) {\n      logger.debug(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_UNKNOWN_HOST\"), e);\n      // Set default host name\n      sysPulseHost = PulseConstants.GEMFIRE_DEFAULT_HOST;\n    }\n    sysPulsePort = PulseConstants.GEMFIRE_DEFAULT_PORT;\n\n  } else {\n    // Application Pulse is running in Non-Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_NONEMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(false);\n\n    // Load JMX User Details\n    loadJMXUserDetails();\n    // Load locator and/or manager details\n    loadLocatorManagerDetails();\n\n    useGemFireCredentials = areWeUsingGemFireSecurityProfile(event);\n  }\n\n  // Set user details in repository\n  repository.setJmxUserName(jmxUserName);\n  repository.setJmxUserPassword(jmxUserPassword);\n\n  // Set locator/Manager details in repository\n  repository.setJmxUseLocator(sysPulseUseLocator);\n  repository.setJmxHost(sysPulseHost);\n  repository.setJmxPort(sysPulsePort);\n\n  // set SSL info\n  initializeSSL();\n  repository.setUseSSLLocator(sysPulseUseSSLLocator);\n  repository.setUseSSLManager(sysPulseUseSSLManager);\n\n  repository.setUseGemFireCredentials(useGemFireCredentials);\n\n}",
        "reject_response": "@Override\npublic void contextInitialized(ServletContextEvent event) {\n\n  messagesToBeLogged = messagesToBeLogged\n      .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_CONTEXT_INITIALIZED\")));\n\n  // Load Pulse version details\n  loadPulseVersionDetails();\n\n  // Load Pulse Properties\n  pulseProperties = loadProperties(PulseConstants.PULSE_PROPERTIES_FILE);\n\n  if (pulseProperties.isEmpty()) {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_NOT_FOUND\")));\n  } else {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_FOUND\")));\n\n    // set Pulse product support into the Pulse controller for access from\n    // client side\n    // to display the appropriate ui depending on which product is supported\n    // in present deployment\n    String pulseProduct =\n        pulseProperties.getProperty(PulseConstants.APPLICATION_PROPERTY_PULSE_PRODUCTSUPPORT);\n  }\n\n  pulseSecurityProperties = loadProperties(PulseConstants.PULSE_SECURITY_PROPERTIES_FILE);\n\n  // Initialize logger\n  initializeLogger();\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n  if (LOGGER.infoEnabled()) {\n    LOGGER.info(resourceBundle.getString(\"LOG_MSG_CHECK_APP_RUNNING_MODE\"));\n  }\n\n  boolean sysIsEmbedded = Boolean.getBoolean(PulseConstants.SYSTEM_PROPERTY_PULSE_EMBEDDED);\n\n  if (sysIsEmbedded) {\n    // Application Pulse is running in Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_EMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(true);\n\n    sysPulseUseLocator = Boolean.FALSE;\n    try {\n      // Get host name of machine running pulse in embedded mode\n      sysPulseHost = InetAddress.getLocalHost().getCanonicalHostName();\n    } catch (Exception e) {\n      logger.debug(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_UNKNOWN_HOST\"), e);\n      // Set default host name\n      sysPulseHost = PulseConstants.GEMFIRE_DEFAULT_HOST;\n    }\n    sysPulsePort = PulseConstants.GEMFIRE_DEFAULT_PORT;\n\n  } else {\n    // Application Pulse is running in Non-Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_NONEMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(false);\n\n    // Load JMX User Details\n    loadJMXUserDetails();\n    // Load locator and/or manager details\n    loadLocatorManagerDetails();\n\n    useGemFireCredentials = areWeUsingGemFireSecurityProfile(event);\n  }\n\n  // Set user details in repository\n  repository.setJmxUserName(jmxUserName);\n  repository.setJmxUserPassword(jmxUserPassword);\n\n  // Set locator/Manager details in repository\n  repository.setJmxUseLocator(sysPulseUseLocator);\n  repository.setJmxHost(sysPulseHost);\n  repository.setJmxPort(sysPulsePort);\n\n  // set SSL info\n  initializeSSL();\n  repository.setUseSSLLocator(sysPulseUseSSLLocator);\n  repository.setUseSSLManager(sysPulseUseSSLManager);\n\n  repository.setUseGemFireCredentials(useGemFireCredentials);\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3151,
        "instruction": "@InterfaceStability.Unstable\npublic static InputSplitInfoMem generateInputSplitsToMem(Configuration conf,\n    boolean groupSplits, boolean sortSplits, int targetTasks)\n    throws IOException, ClassNotFoundException, InterruptedException {\n\n  InputSplitInfoMem splitInfoMem = null;\n  JobConf jobConf = new JobConf(conf);\n  if (jobConf.getUseNewMapper()) {\n    Job job = Job.getInstance(conf);\n    org.apache.hadoop.mapreduce.InputSplit[] splits =\n        generateNewSplits(job, groupSplits, sortSplits, targetTasks);\n    splitInfoMem = new InputSplitInfoMem(splits, createTaskLocationHintsFromSplits(splits),\n        splits.length, job.getCredentials(), job.getConfiguration());\n  } else {\n    LOG.debug(\"Generating mapred api input splits\");\n    org.apache.hadoop.mapred.InputSplit[] splits =\n        generateOldSplits(jobConf, groupSplits, sortSplits, targetTasks);\n    splitInfoMem = new InputSplitInfoMem(splits, createTaskLocationHintsFromSplits(splits),\n        splits.length, jobConf.getCredentials(), jobConf);\n  }\n  LOG.info(\"NumSplits: \" + splitInfoMem.getNumTasks() + \", SerializedSize: \"\n      + splitInfoMem.getSplitsProto().getSerializedSize());\n  return splitInfoMem;\n}",
        "accept_response": "@InterfaceStability.Unstable\npublic static InputSplitInfoMem generateInputSplitsToMem(Configuration conf,\n    boolean groupSplits, boolean sortSplits, int targetTasks)\n    throws IOException, ClassNotFoundException, InterruptedException {\n\n  InputSplitInfoMem splitInfoMem = null;\n  JobConf jobConf = new JobConf(conf);\n  if (jobConf.getUseNewMapper()) {\n    LOG.debug(\"Generating mapreduce api input splits\");\n    Job job = Job.getInstance(conf);\n    org.apache.hadoop.mapreduce.InputSplit[] splits =\n        generateNewSplits(job, groupSplits, sortSplits, targetTasks);\n    splitInfoMem = new InputSplitInfoMem(splits, createTaskLocationHintsFromSplits(splits),\n        splits.length, job.getCredentials(), job.getConfiguration());\n  } else {\n    LOG.debug(\"Generating mapred api input splits\");\n    org.apache.hadoop.mapred.InputSplit[] splits =\n        generateOldSplits(jobConf, groupSplits, sortSplits, targetTasks);\n    splitInfoMem = new InputSplitInfoMem(splits, createTaskLocationHintsFromSplits(splits),\n        splits.length, jobConf.getCredentials(), jobConf);\n  }\n  LOG.info(\"NumSplits: \" + splitInfoMem.getNumTasks() + \", SerializedSize: \"\n      + splitInfoMem.getSplitsProto().getSerializedSize());\n  return splitInfoMem;\n}",
        "reject_response": "@InterfaceStability.Unstable\npublic static InputSplitInfoMem generateInputSplitsToMem(Configuration conf,\n    boolean groupSplits, boolean sortSplits, int targetTasks)\n    throws IOException, ClassNotFoundException, InterruptedException {\n\n  InputSplitInfoMem splitInfoMem = null;\n  JobConf jobConf = new JobConf(conf);\n  if (jobConf.getUseNewMapper()) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Generating mapreduce api input splits\");\n    }\n    Job job = Job.getInstance(conf);\n    org.apache.hadoop.mapreduce.InputSplit[] splits =\n        generateNewSplits(job, groupSplits, sortSplits, targetTasks);\n    splitInfoMem = new InputSplitInfoMem(splits, createTaskLocationHintsFromSplits(splits),\n        splits.length, job.getCredentials(), job.getConfiguration());\n  } else {\n    LOG.debug(\"Generating mapred api input splits\");\n    org.apache.hadoop.mapred.InputSplit[] splits =\n        generateOldSplits(jobConf, groupSplits, sortSplits, targetTasks);\n    splitInfoMem = new InputSplitInfoMem(splits, createTaskLocationHintsFromSplits(splits),\n        splits.length, jobConf.getCredentials(), jobConf);\n  }\n  LOG.info(\"NumSplits: \" + splitInfoMem.getNumTasks() + \", SerializedSize: \"\n      + splitInfoMem.getSplitsProto().getSerializedSize());\n  return splitInfoMem;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3111,
        "instruction": "public synchronized Collection<QueryInfo> getFinishedQueries() {\n  try {\n    Set<QueryInfo> result = Sets.newTreeSet();\n    result.addAll(this.masterContext.getHistoryReader().getQueries(null));\n    synchronized (historyCache) {\n      result.addAll(historyCache.values());\n    }\n    return result;\n  } catch (Throwable e) {\n    return Lists.newArrayList();\n  }\n}",
        "accept_response": "public synchronized Collection<QueryInfo> getFinishedQueries() {\n  try {\n    Set<QueryInfo> result = Sets.newTreeSet();\n    result.addAll(this.masterContext.getHistoryReader().getQueries(null));\n    synchronized (historyCache) {\n      result.addAll(historyCache.values());\n    }\n    return result;\n  } catch (Throwable e) {\n    LOG.error(e, e);\n    return Lists.newArrayList();\n  }\n}",
        "reject_response": "public synchronized Collection<QueryInfo> getFinishedQueries() {\n  try {\n    Set<QueryInfo> result = Sets.newTreeSet();\n    result.addAll(this.masterContext.getHistoryReader().getQueries(null));\n    synchronized (historyCache) {\n      result.addAll(historyCache.values());\n    }\n    return result;\n  } catch (Throwable e) {\n    LOG.error(e);\n    return Lists.newArrayList();\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2978,
        "instruction": "@Override\npublic void flush() {\n\tlogger.info(\"Flush called. name=\" + getName());\n\tif (ostream != null) {\n\t\ttry {\n\t\t\tsynchronized (this) {\n\t\t\t\tif (ostream != null)\n\t\t\t\t\t// 1) PrinterWriter does not have bufferring of its own so\n\t\t\t\t\t// we need to flush its underlying stream\n\t\t\t\t\t// 2) HDFS flush() does not really flush all the way to disk.\n\t\t\t\t\tostream.hflush();\n\t\t\t}\n\t\t} catch (IOException e) {\n\t\t\tlogger.error(\"Error on flushing log writer: \" + e.getMessage() +\n\t\t\t \"\\nException will be ignored. name=\" + getName() + \", fileName=\" + currentFileName);\n\t\t}\n\t}\n}",
        "accept_response": "@Override\npublic void flush() {\n\tlogger.info(\"Flush called. name=\" + getName());\n\tif (ostream != null) {\n\t\ttry {\n\t\t\tsynchronized (this) {\n\t\t\t\tif (ostream != null)\n\t\t\t\t\t// 1) PrinterWriter does not have bufferring of its own so\n\t\t\t\t\t// we need to flush its underlying stream\n\t\t\t\t\t// 2) HDFS flush() does not really flush all the way to disk.\n\t\t\t\t\tostream.hflush();\n\t\t\t\t\tlogger.info(\"Flush HDFS audit logs completed.....\");\n\t\t\t}\n\t\t} catch (IOException e) {\n\t\t\tlogger.error(\"Error on flushing log writer: \" + e.getMessage() +\n\t\t\t \"\\nException will be ignored. name=\" + getName() + \", fileName=\" + currentFileName);\n\t\t}\n\t}\n}",
        "reject_response": "@Override\npublic void flush() {\n\tlogger.info(\"Flush called. name=\" + getName());\n\tif (ostream != null) {\n\t\ttry {\n\t\t\tsynchronized (this) {\n\t\t\t\tif (ostream != null)\n\t\t\t\t\t// 1) PrinterWriter does not have bufferring of its own so\n\t\t\t\t\t// we need to flush its underlying stream\n\t\t\t\t\t// 2) HDFS flush() does not really flush all the way to disk.\n\t\t\t\t\tostream.hflush();\n\tif ( logWriter != null) {\n\t\tlogWriter.flush();\n\t\tlogger.info(\"Flush HDFS audit logs completed.....\");\n\t }\n\t\t\t}\n\t\t} catch (IOException e) {\n\t\t\tlogger.error(\"Error on flushing log writer: \" + e.getMessage() +\n\t\t\t \"\\nException will be ignored. name=\" + getName() + \", fileName=\" + currentFileName);\n\t\t}\n\t}\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2736,
        "instruction": "private void authorizeConnection() throws RpcServerException {\n  try {\n    // If auth method is TOKEN, the token was obtained by the\n    // real user for the effective user, therefore not required to\n    // authorize real user. doAs is allowed only for simple or kerberos\n    // authentication\n    if (user != null && user.getRealUser() != null\n        && (authMethod != AuthMethod.TOKEN)) {\n      ProxyUsers.authorize(user, this.getHostAddress());\n    }\n    authorize(user, protocolName, getHostInetAddress());\n    rpcMetrics.incrAuthorizationSuccesses();\n  } catch (AuthorizationException ae) {\n    LOG.info(\"Connection from \" + this\n        + \" for protocol \" + connectionContext.getProtocol()\n        + \" is unauthorized for user \" + user);\n    rpcMetrics.incrAuthorizationFailures();\n    throw new FatalRpcServerException(\n        RpcErrorCodeProto.FATAL_UNAUTHORIZED, ae);\n  }\n}",
        "accept_response": "private void authorizeConnection() throws RpcServerException {\n  try {\n    // If auth method is TOKEN, the token was obtained by the\n    // real user for the effective user, therefore not required to\n    // authorize real user. doAs is allowed only for simple or kerberos\n    // authentication\n    if (user != null && user.getRealUser() != null\n        && (authMethod != AuthMethod.TOKEN)) {\n      ProxyUsers.authorize(user, this.getHostAddress());\n    }\n    authorize(user, protocolName, getHostInetAddress());\n    LOG.debug(\"Successfully authorized {}.\", connectionContext);\n    rpcMetrics.incrAuthorizationSuccesses();\n  } catch (AuthorizationException ae) {\n    LOG.info(\"Connection from \" + this\n        + \" for protocol \" + connectionContext.getProtocol()\n        + \" is unauthorized for user \" + user);\n    rpcMetrics.incrAuthorizationFailures();\n    throw new FatalRpcServerException(\n        RpcErrorCodeProto.FATAL_UNAUTHORIZED, ae);\n  }\n}",
        "reject_response": "private void authorizeConnection() throws RpcServerException {\n  try {\n    // If auth method is TOKEN, the token was obtained by the\n    // real user for the effective user, therefore not required to\n    // authorize real user. doAs is allowed only for simple or kerberos\n    // authentication\n    if (user != null && user.getRealUser() != null\n        && (authMethod != AuthMethod.TOKEN)) {\n      ProxyUsers.authorize(user, this.getHostAddress());\n    }\n    authorize(user, protocolName, getHostInetAddress());\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Successfully authorized \" + connectionContext);\n    }\n    rpcMetrics.incrAuthorizationSuccesses();\n  } catch (AuthorizationException ae) {\n    LOG.info(\"Connection from \" + this\n        + \" for protocol \" + connectionContext.getProtocol()\n        + \" is unauthorized for user \" + user);\n    rpcMetrics.incrAuthorizationFailures();\n    throw new FatalRpcServerException(\n        RpcErrorCodeProto.FATAL_UNAUTHORIZED, ae);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2513,
        "instruction": "public void response(MessageIn<Boolean> msg)\n{\n\n    if (msg.payload)\n        accepts.incrementAndGet();\n\n    latch.countDown();\n\n    if (isSuccessful() || (failFast && (latch.getCount() + accepts.get() < requiredAccepts)))\n    {\n        while (latch.getCount() > 0)\n            latch.countDown();\n    }\n}",
        "accept_response": "public void response(MessageIn<Boolean> msg)\n{\n    logger.trace(\"Propose response {} from {}\", msg.payload, msg.from);\n\n    if (msg.payload)\n        accepts.incrementAndGet();\n\n    latch.countDown();\n\n    if (isSuccessful() || (failFast && (latch.getCount() + accepts.get() < requiredAccepts)))\n    {\n        while (latch.getCount() > 0)\n            latch.countDown();\n    }\n}",
        "reject_response": "public void response(MessageIn<Boolean> msg)\n{\n    logger.debug(\"Propose response {} from {}\", msg.payload, msg.from);\n\n    if (msg.payload)\n        accepts.incrementAndGet();\n\n    latch.countDown();\n\n    if (isSuccessful() || (failFast && (latch.getCount() + accepts.get() < requiredAccepts)))\n    {\n        while (latch.getCount() > 0)\n            latch.countDown();\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2451,
        "instruction": "@Override\npublic void lock() {\n   lock.lock();\n\n   reconnectID.incrementAndGet();\n\n   failingOver = true;\n\n   lock.unlock();\n}",
        "accept_response": "@Override\npublic void lock() {\n   if (logger.isTraceEnabled()) {\n      logger.trace(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" lock channel \" + this);\n   }\n   lock.lock();\n\n   reconnectID.incrementAndGet();\n\n   failingOver = true;\n\n   lock.unlock();\n}",
        "reject_response": "@Override\npublic void lock() {\n   if (logger.isTraceEnabled()) {\n      logger.trace(\"lock channel \" + this);\n   }\n   lock.lock();\n\n   reconnectID.incrementAndGet();\n\n   failingOver = true;\n\n   lock.unlock();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3012,
        "instruction": "public synchronized void removeClosedEvaluator(final EvaluatorManager evaluatorManager) {\n\n  final String evaluatorId = evaluatorManager.getId();\n  LOG.log(Level.FINE, \"Removing closed evaluator: {0}\", evaluatorId);\n\n  if (!evaluatorManager.isClosed()) {\n    throw new IllegalArgumentException(\"Removing evaluator that has not been closed yet: \" + evaluatorId);\n  }\n\n  if (!this.evaluators.containsKey(evaluatorId) && !this.closedEvaluatorIds.contains(evaluatorId)) {\n    throw new IllegalArgumentException(\"Removing unknown evaluator: \" + evaluatorId);\n  }\n\n  if (!this.evaluators.containsKey(evaluatorId) && this.closedEvaluatorIds.contains(evaluatorId)) {\n    return;\n  }\n\n  evaluatorManager.shutdown();\n  this.evaluators.remove(evaluatorId);\n  this.closedEvaluatorIds.add(evaluatorId);\n\n  LOG.log(Level.FINEST, \"Closed evaluator removed: {0}\", evaluatorId);\n}",
        "accept_response": "public synchronized void removeClosedEvaluator(final EvaluatorManager evaluatorManager) {\n\n  final String evaluatorId = evaluatorManager.getId();\n  LOG.log(Level.FINE, \"Removing closed evaluator: {0}\", evaluatorId);\n\n  if (!evaluatorManager.isClosed()) {\n    throw new IllegalArgumentException(\"Removing evaluator that has not been closed yet: \" + evaluatorId);\n  }\n\n  if (!this.evaluators.containsKey(evaluatorId) && !this.closedEvaluatorIds.contains(evaluatorId)) {\n    throw new IllegalArgumentException(\"Removing unknown evaluator: \" + evaluatorId);\n  }\n\n  if (!this.evaluators.containsKey(evaluatorId) && this.closedEvaluatorIds.contains(evaluatorId)) {\n    LOG.log(Level.FINE, \"Removing closed evaluator which has already been removed: {0}\", evaluatorId);\n    return;\n  }\n\n  evaluatorManager.shutdown();\n  this.evaluators.remove(evaluatorId);\n  this.closedEvaluatorIds.add(evaluatorId);\n\n  LOG.log(Level.FINEST, \"Closed evaluator removed: {0}\", evaluatorId);\n}",
        "reject_response": "public synchronized void removeClosedEvaluator(final EvaluatorManager evaluatorManager) {\n\n  final String evaluatorId = evaluatorManager.getId();\n  LOG.log(Level.FINE, \"Removing closed evaluator: {0}\", evaluatorId);\n\n  if (!evaluatorManager.isClosed()) {\n    throw new IllegalArgumentException(\"Removing evaluator that has not been closed yet: \" + evaluatorId);\n  }\n\n  if (!this.evaluators.containsKey(evaluatorId) && !this.closedEvaluatorIds.contains(evaluatorId)) {\n    throw new IllegalArgumentException(\"Removing unknown evaluator: \" + evaluatorId);\n  }\n\n  if (!this.evaluators.containsKey(evaluatorId) && this.closedEvaluatorIds.contains(evaluatorId)) {\n    LOG.log(Level.FINE, \"Trying to remove closed evaluator {0} which has already been removed.\", evaluatorId);\n    return;\n  }\n\n  evaluatorManager.shutdown();\n  this.evaluators.remove(evaluatorId);\n  this.closedEvaluatorIds.add(evaluatorId);\n\n  LOG.log(Level.FINEST, \"Closed evaluator removed: {0}\", evaluatorId);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2389,
        "instruction": "@Override\npublic void invalidateCache(Instance instance, String server) {\n  int invalidatedCount = 0;\n\n  wLock.lock();\n  try {\n    for (TabletLocation cacheEntry : metaCache.values())\n      if (cacheEntry.tablet_location.equals(server)) {\n        badExtents.add(cacheEntry.tablet_extent);\n        invalidatedCount++;\n      }\n  } finally {\n    wLock.unlock();\n  }\n\n  lockChecker.invalidateCache(server);\n\n  if (log.isTraceEnabled())\n\n}",
        "accept_response": "@Override\npublic void invalidateCache(Instance instance, String server) {\n  int invalidatedCount = 0;\n\n  wLock.lock();\n  try {\n    for (TabletLocation cacheEntry : metaCache.values())\n      if (cacheEntry.tablet_location.equals(server)) {\n        badExtents.add(cacheEntry.tablet_extent);\n        invalidatedCount++;\n      }\n  } finally {\n    wLock.unlock();\n  }\n\n  lockChecker.invalidateCache(server);\n\n  if (log.isTraceEnabled())\n    log.trace(\"invalidated {} cache entries  table={} server={}\", invalidatedCount, tableId, server);\n\n}",
        "reject_response": "@Override\npublic void invalidateCache(Instance instance, String server) {\n  int invalidatedCount = 0;\n\n  wLock.lock();\n  try {\n    for (TabletLocation cacheEntry : metaCache.values())\n      if (cacheEntry.tablet_location.equals(server)) {\n        badExtents.add(cacheEntry.tablet_extent);\n        invalidatedCount++;\n      }\n  } finally {\n    wLock.unlock();\n  }\n\n  lockChecker.invalidateCache(server);\n\n  if (log.isTraceEnabled())\n    log.trace(\"invalidated \" + invalidatedCount + \" cache entries  table=\" + tableId + \" server=\" + server);\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3053,
        "instruction": "public static NamedList readFromResourceLoader(SolrResourceLoader loader, String name) {\n  InputStreamReader reader;\n  try {\n    reader = new InputStreamReader(loader.openResource(name), StandardCharsets.UTF_8);\n  } catch (SolrResourceNotFoundException ex) {\n    return null;\n  } catch (Exception ex) {\n    throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to load reader for ConfigSet properties: \" + name, ex);\n  }\n\n  try {\n    return readFromInputStream(reader);\n  } finally {\n    IOUtils.closeQuietly(reader);\n  }\n}",
        "accept_response": "public static NamedList readFromResourceLoader(SolrResourceLoader loader, String name) {\n  InputStreamReader reader;\n  try {\n    reader = new InputStreamReader(loader.openResource(name), StandardCharsets.UTF_8);\n  } catch (SolrResourceNotFoundException ex) {\n    log.debug(\"Did not find ConfigSet properties, assuming default properties: {}\", ex.getMessage());\n    return null;\n  } catch (Exception ex) {\n    throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to load reader for ConfigSet properties: \" + name, ex);\n  }\n\n  try {\n    return readFromInputStream(reader);\n  } finally {\n    IOUtils.closeQuietly(reader);\n  }\n}",
        "reject_response": "public static NamedList readFromResourceLoader(SolrResourceLoader loader, String name) {\n  InputStreamReader reader;\n  try {\n    reader = new InputStreamReader(loader.openResource(name), StandardCharsets.UTF_8);\n  } catch (SolrResourceNotFoundException ex) {\n    log.debug(\"Did not find ConfigSet properties, assuming default properties: \" + ex.getMessage());\n    return null;\n  } catch (Exception ex) {\n    throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to load reader for ConfigSet properties: \" + name, ex);\n  }\n\n  try {\n    return readFromInputStream(reader);\n  } finally {\n    IOUtils.closeQuietly(reader);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3248,
        "instruction": "void readConnectResult() throws IOException {\n    if (LOG.isTraceEnabled()) {\n        StringBuilder buf = new StringBuilder(\"0x[\");\n        for (byte b : incomingBuffer.array()) {\n            buf.append(Integer.toHexString(b)).append(\",\");\n        }\n        buf.append(\"]\");\n    }\n\n    ByteBufferInputStream bbis = new ByteBufferInputStream(incomingBuffer);\n    BinaryInputArchive bbia = BinaryInputArchive.getArchive(bbis);\n    ConnectResponse conRsp = new ConnectResponse();\n    conRsp.deserialize(bbia, \"connect\");\n\n    // read \"is read-only\" flag\n    boolean isRO = false;\n    try {\n        isRO = bbia.readBool(\"readOnly\");\n    } catch (IOException e) {\n        // this is ok -- just a packet from an old server which\n        // doesn't contain readOnly field\n        LOG.warn(\"Connected to an old server; r-o mode will be unavailable\");\n    }\n\n    this.sessionId = conRsp.getSessionId();\n    sendThread.onConnected(conRsp.getTimeOut(), this.sessionId, conRsp.getPasswd(), isRO);\n}",
        "accept_response": "void readConnectResult() throws IOException {\n    if (LOG.isTraceEnabled()) {\n        StringBuilder buf = new StringBuilder(\"0x[\");\n        for (byte b : incomingBuffer.array()) {\n            buf.append(Integer.toHexString(b)).append(\",\");\n        }\n        buf.append(\"]\");\n        if (LOG.isTraceEnabled()) {\n            LOG.trace(\"readConnectResult {} {}\", incomingBuffer.remaining(), buf.toString());\n        }\n    }\n\n    ByteBufferInputStream bbis = new ByteBufferInputStream(incomingBuffer);\n    BinaryInputArchive bbia = BinaryInputArchive.getArchive(bbis);\n    ConnectResponse conRsp = new ConnectResponse();\n    conRsp.deserialize(bbia, \"connect\");\n\n    // read \"is read-only\" flag\n    boolean isRO = false;\n    try {\n        isRO = bbia.readBool(\"readOnly\");\n    } catch (IOException e) {\n        // this is ok -- just a packet from an old server which\n        // doesn't contain readOnly field\n        LOG.warn(\"Connected to an old server; r-o mode will be unavailable\");\n    }\n\n    this.sessionId = conRsp.getSessionId();\n    sendThread.onConnected(conRsp.getTimeOut(), this.sessionId, conRsp.getPasswd(), isRO);\n}",
        "reject_response": "void readConnectResult() throws IOException {\n    if (LOG.isTraceEnabled()) {\n        StringBuilder buf = new StringBuilder(\"0x[\");\n        for (byte b : incomingBuffer.array()) {\n            buf.append(Integer.toHexString(b)).append(\",\");\n        }\n        buf.append(\"]\");\n        if (LOG.isTraceEnabled()) {\n        LOG.trace(\"readConnectResult \" + incomingBuffer.remaining() + \" \" + buf.toString());\n        }\n    }\n\n    ByteBufferInputStream bbis = new ByteBufferInputStream(incomingBuffer);\n    BinaryInputArchive bbia = BinaryInputArchive.getArchive(bbis);\n    ConnectResponse conRsp = new ConnectResponse();\n    conRsp.deserialize(bbia, \"connect\");\n\n    // read \"is read-only\" flag\n    boolean isRO = false;\n    try {\n        isRO = bbia.readBool(\"readOnly\");\n    } catch (IOException e) {\n        // this is ok -- just a packet from an old server which\n        // doesn't contain readOnly field\n        LOG.warn(\"Connected to an old server; r-o mode will be unavailable\");\n    }\n\n    this.sessionId = conRsp.getSessionId();\n    sendThread.onConnected(conRsp.getTimeOut(), this.sessionId, conRsp.getPasswd(), isRO);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2588,
        "instruction": "public void retriggerPartitionRequest(IntermediateResultPartitionID partitionId) throws IOException, InterruptedException {\n\tsynchronized (requestLock) {\n\t\tif (!isReleased) {\n\t\t\tfinal InputChannel ch = inputChannels.get(partitionId);\n\n\t\t\tcheckNotNull(ch, \"Unknown input channel with ID \" + partitionId);\n\n\n\t\t\tif (ch.getClass() == RemoteInputChannel.class) {\n\t\t\t\tfinal RemoteInputChannel rch = (RemoteInputChannel) ch;\n\t\t\t\trch.retriggerSubpartitionRequest(consumedSubpartitionIndex);\n\t\t\t}\n\t\t\telse if (ch.getClass() == LocalInputChannel.class) {\n\t\t\t\tfinal LocalInputChannel ich = (LocalInputChannel) ch;\n\n\t\t\t\tif (retriggerLocalRequestTimer == null) {\n\t\t\t\t\tretriggerLocalRequestTimer = new Timer(true);\n\t\t\t\t}\n\n\t\t\t\tich.retriggerSubpartitionRequest(retriggerLocalRequestTimer, consumedSubpartitionIndex);\n\t\t\t}\n\t\t\telse {\n\t\t\t\tthrow new IllegalStateException(\n\t\t\t\t\t\t\"Unexpected type of channel to retrigger partition: \" + ch.getClass());\n\t\t\t}\n\t\t}\n\t}\n}",
        "accept_response": "public void retriggerPartitionRequest(IntermediateResultPartitionID partitionId) throws IOException, InterruptedException {\n\tsynchronized (requestLock) {\n\t\tif (!isReleased) {\n\t\t\tfinal InputChannel ch = inputChannels.get(partitionId);\n\n\t\t\tcheckNotNull(ch, \"Unknown input channel with ID \" + partitionId);\n\n\t\t\tLOG.debug(\"{}: Retriggering partition request {}:{}.\", owningTaskName, ch.partitionId, consumedSubpartitionIndex);\n\n\t\t\tif (ch.getClass() == RemoteInputChannel.class) {\n\t\t\t\tfinal RemoteInputChannel rch = (RemoteInputChannel) ch;\n\t\t\t\trch.retriggerSubpartitionRequest(consumedSubpartitionIndex);\n\t\t\t}\n\t\t\telse if (ch.getClass() == LocalInputChannel.class) {\n\t\t\t\tfinal LocalInputChannel ich = (LocalInputChannel) ch;\n\n\t\t\t\tif (retriggerLocalRequestTimer == null) {\n\t\t\t\t\tretriggerLocalRequestTimer = new Timer(true);\n\t\t\t\t}\n\n\t\t\t\tich.retriggerSubpartitionRequest(retriggerLocalRequestTimer, consumedSubpartitionIndex);\n\t\t\t}\n\t\t\telse {\n\t\t\t\tthrow new IllegalStateException(\n\t\t\t\t\t\t\"Unexpected type of channel to retrigger partition: \" + ch.getClass());\n\t\t\t}\n\t\t}\n\t}\n}",
        "reject_response": "public void retriggerPartitionRequest(IntermediateResultPartitionID partitionId) throws IOException, InterruptedException {\n\tsynchronized (requestLock) {\n\t\tif (!isReleased) {\n\t\t\tfinal InputChannel ch = inputChannels.get(partitionId);\n\n\t\t\tcheckNotNull(ch, \"Unknown input channel with ID \" + partitionId);\n\n\t\t\tLOG.debug(\"Retriggering partition request {}:{}.\", ch.partitionId, consumedSubpartitionIndex);\n\n\t\t\tif (ch.getClass() == RemoteInputChannel.class) {\n\t\t\t\tfinal RemoteInputChannel rch = (RemoteInputChannel) ch;\n\t\t\t\trch.retriggerSubpartitionRequest(consumedSubpartitionIndex);\n\t\t\t}\n\t\t\telse if (ch.getClass() == LocalInputChannel.class) {\n\t\t\t\tfinal LocalInputChannel ich = (LocalInputChannel) ch;\n\n\t\t\t\tif (retriggerLocalRequestTimer == null) {\n\t\t\t\t\tretriggerLocalRequestTimer = new Timer(true);\n\t\t\t\t}\n\n\t\t\t\tich.retriggerSubpartitionRequest(retriggerLocalRequestTimer, consumedSubpartitionIndex);\n\t\t\t}\n\t\t\telse {\n\t\t\t\tthrow new IllegalStateException(\n\t\t\t\t\t\t\"Unexpected type of channel to retrigger partition: \" + ch.getClass());\n\t\t\t}\n\t\t}\n\t}\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2428,
        "instruction": "@Override\npublic void run() {\n    long now = System.currentTimeMillis();\n\n    if ((now - startTime) >= connectionTimeout && connectCheckerTask != null && !ASYNC_TASKS.isShutdown()) {\n        try {\n            ASYNC_TASKS.execute(new Runnable() {\n                @Override\n                public void run() {\n                    onException(new InactivityIOException(\n                        \"Channel was inactive for too (>\" + (connectionTimeout) + \") long: \" + next.getRemoteAddress()));\n                }\n            });\n        } catch (RejectedExecutionException ex) {\n            if (!ASYNC_TASKS.isShutdown()) {\n                LOG.error(\"Async connection timeout task was rejected from the executor: \", ex);\n                throw ex;\n            }\n        }\n    }\n}",
        "accept_response": "@Override\npublic void run() {\n    long now = System.currentTimeMillis();\n\n    if ((now - startTime) >= connectionTimeout && connectCheckerTask != null && !ASYNC_TASKS.isShutdown()) {\n        LOG.debug(\"No connection attempt made in time for {}! Throwing InactivityIOException.\", AmqpInactivityMonitor.this);\n        try {\n            ASYNC_TASKS.execute(new Runnable() {\n                @Override\n                public void run() {\n                    onException(new InactivityIOException(\n                        \"Channel was inactive for too (>\" + (connectionTimeout) + \") long: \" + next.getRemoteAddress()));\n                }\n            });\n        } catch (RejectedExecutionException ex) {\n            if (!ASYNC_TASKS.isShutdown()) {\n                LOG.error(\"Async connection timeout task was rejected from the executor: \", ex);\n                throw ex;\n            }\n        }\n    }\n}",
        "reject_response": "@Override\npublic void run() {\n    long now = System.currentTimeMillis();\n\n    if ((now - startTime) >= connectionTimeout && connectCheckerTask != null && !ASYNC_TASKS.isShutdown()) {\n        LOG.debug(\"No connection attempt made in time for {}! Throwing InactivityIOException.\", AmqpInactivityMonitor.this.toString());\n        try {\n            ASYNC_TASKS.execute(new Runnable() {\n                @Override\n                public void run() {\n                    onException(new InactivityIOException(\n                        \"Channel was inactive for too (>\" + (connectionTimeout) + \") long: \" + next.getRemoteAddress()));\n                }\n            });\n        } catch (RejectedExecutionException ex) {\n            if (!ASYNC_TASKS.isShutdown()) {\n                LOG.error(\"Async connection timeout task was rejected from the executor: \", ex);\n                throw ex;\n            }\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2719,
        "instruction": "protected Path closeRecoveredEditsWriter(RecoveredEditsWriter editsWriter,\n    List<IOException> thrown) throws IOException {\n  try {\n    editsWriter.writer.close();\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not close recovered edits at \" + editsWriter.path;\n    LOG.error(errorMsg, ioe);\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  final String msg = \"Closed recovered edits writer path=\" + editsWriter.path + \" (wrote \"\n    + editsWriter.editsWritten + \" edits, skipped \" + editsWriter.editsSkipped + \" edits in \" + (\n    editsWriter.nanosSpent / 1000 / 1000) + \" ms)\";\n  LOG.info(msg);\n  updateStatusWithMsg(msg);\n  if (editsWriter.editsWritten == 0) {\n    // just remove the empty recovered.edits file\n    if (walSplitter.walFS.exists(editsWriter.path)\n        && !walSplitter.walFS.delete(editsWriter.path, false)) {\n      final String errorMsg = \"Failed deleting empty \" + editsWriter.path;\n      updateStatusWithMsg(errorMsg);\n      throw new IOException(\"Failed deleting empty  \" + editsWriter.path);\n    }\n    return null;\n  }\n\n  Path dst = getCompletedRecoveredEditsFilePath(editsWriter.path,\n    regionMaximumEditLogSeqNum.get(Bytes.toString(editsWriter.encodedRegionName)));\n  try {\n    if (!dst.equals(editsWriter.path) && walSplitter.walFS.exists(dst)) {\n      deleteOneWithFewerEntries(editsWriter, dst);\n    }\n    // Skip the unit tests which create a splitter that reads and\n    // writes the data without touching disk.\n    // TestHLogSplit#testThreading is an example.\n    if (walSplitter.walFS.exists(editsWriter.path)) {\n      if (!walSplitter.walFS.rename(editsWriter.path, dst)) {\n        final String errorMsg =\n          \"Failed renaming recovered edits \" + editsWriter.path + \" to \" + dst;\n        updateStatusWithMsg(errorMsg);\n        throw new IOException(errorMsg);\n      }\n      final String renameEditMsg = \"Rename recovered edits \" + editsWriter.path + \" to \" + dst;\n      LOG.info(renameEditMsg);\n      updateStatusWithMsg(renameEditMsg);\n    }\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not rename recovered edits \" + editsWriter.path\n      + \" to \" + dst;\n    LOG.error(errorMsg, ioe);\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  return dst;\n}",
        "accept_response": "protected Path closeRecoveredEditsWriter(RecoveredEditsWriter editsWriter,\n    List<IOException> thrown) throws IOException {\n  try {\n    editsWriter.writer.close();\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not close recovered edits at \" + editsWriter.path;\n    LOG.error(errorMsg, ioe);\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  final String msg = \"Closed recovered edits writer path=\" + editsWriter.path + \" (wrote \"\n    + editsWriter.editsWritten + \" edits, skipped \" + editsWriter.editsSkipped + \" edits in \" + (\n    editsWriter.nanosSpent / 1000 / 1000) + \" ms)\";\n  LOG.info(msg);\n  updateStatusWithMsg(msg);\n  if (editsWriter.editsWritten == 0) {\n    // just remove the empty recovered.edits file\n    if (walSplitter.walFS.exists(editsWriter.path)\n        && !walSplitter.walFS.delete(editsWriter.path, false)) {\n      final String errorMsg = \"Failed deleting empty \" + editsWriter.path;\n      LOG.warn(errorMsg);\n      updateStatusWithMsg(errorMsg);\n      throw new IOException(\"Failed deleting empty  \" + editsWriter.path);\n    }\n    return null;\n  }\n\n  Path dst = getCompletedRecoveredEditsFilePath(editsWriter.path,\n    regionMaximumEditLogSeqNum.get(Bytes.toString(editsWriter.encodedRegionName)));\n  try {\n    if (!dst.equals(editsWriter.path) && walSplitter.walFS.exists(dst)) {\n      deleteOneWithFewerEntries(editsWriter, dst);\n    }\n    // Skip the unit tests which create a splitter that reads and\n    // writes the data without touching disk.\n    // TestHLogSplit#testThreading is an example.\n    if (walSplitter.walFS.exists(editsWriter.path)) {\n      if (!walSplitter.walFS.rename(editsWriter.path, dst)) {\n        final String errorMsg =\n          \"Failed renaming recovered edits \" + editsWriter.path + \" to \" + dst;\n        updateStatusWithMsg(errorMsg);\n        throw new IOException(errorMsg);\n      }\n      final String renameEditMsg = \"Rename recovered edits \" + editsWriter.path + \" to \" + dst;\n      LOG.info(renameEditMsg);\n      updateStatusWithMsg(renameEditMsg);\n    }\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not rename recovered edits \" + editsWriter.path\n      + \" to \" + dst;\n    LOG.error(errorMsg, ioe);\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  return dst;\n}",
        "reject_response": "protected Path closeRecoveredEditsWriter(RecoveredEditsWriter editsWriter,\n    List<IOException> thrown) throws IOException {\n  try {\n    editsWriter.writer.close();\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not close recovered edits at \" + editsWriter.path;\n    LOG.error(errorMsg, ioe);\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  final String msg = \"Closed recovered edits writer path=\" + editsWriter.path + \" (wrote \"\n    + editsWriter.editsWritten + \" edits, skipped \" + editsWriter.editsSkipped + \" edits in \" + (\n    editsWriter.nanosSpent / 1000 / 1000) + \" ms)\";\n  LOG.info(msg);\n  updateStatusWithMsg(msg);\n  if (editsWriter.editsWritten == 0) {\n    // just remove the empty recovered.edits file\n    if (walSplitter.walFS.exists(editsWriter.path)\n        && !walSplitter.walFS.delete(editsWriter.path, false)) {\n      final String errorMsg = \"Failed deleting empty \" + editsWriter.path;\n    if (walSplitter.walFS.exists(editsWriter.path) &&\n      !walSplitter.walFS.delete(editsWriter.path, false)) {\n      LOG.warn(\"Failed deleting empty {}\", editsWriter.path);\n      updateStatusWithMsg(errorMsg);\n      throw new IOException(\"Failed deleting empty  \" + editsWriter.path);\n    }\n    return null;\n  }\n\n  Path dst = getCompletedRecoveredEditsFilePath(editsWriter.path,\n    regionMaximumEditLogSeqNum.get(Bytes.toString(editsWriter.encodedRegionName)));\n  try {\n    if (!dst.equals(editsWriter.path) && walSplitter.walFS.exists(dst)) {\n      deleteOneWithFewerEntries(editsWriter, dst);\n    }\n    // Skip the unit tests which create a splitter that reads and\n    // writes the data without touching disk.\n    // TestHLogSplit#testThreading is an example.\n    if (walSplitter.walFS.exists(editsWriter.path)) {\n      if (!walSplitter.walFS.rename(editsWriter.path, dst)) {\n        final String errorMsg =\n          \"Failed renaming recovered edits \" + editsWriter.path + \" to \" + dst;\n        updateStatusWithMsg(errorMsg);\n        throw new IOException(errorMsg);\n      }\n      final String renameEditMsg = \"Rename recovered edits \" + editsWriter.path + \" to \" + dst;\n      LOG.info(renameEditMsg);\n      updateStatusWithMsg(renameEditMsg);\n    }\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not rename recovered edits \" + editsWriter.path\n      + \" to \" + dst;\n    LOG.error(errorMsg, ioe);\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  return dst;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2687,
        "instruction": "@Override\npublic boolean writeNextObject(OneRow onerow) throws IOException {\n    Writable value = (Writable) onerow.getData();\n    Writable key = (Writable) onerow.getKey();\n\n    // init writer on first approach here, based on onerow.getData type\n    // TODO: verify data is serializable.\n    if (writer == null) {\n        Class<? extends Writable> valueClass = value.getClass();\n        Class<? extends Writable> keyClass = (key == null) ? LongWritable.class\n                : key.getClass();\n        // create writer - do not allow overwriting existing file\n        writer = SequenceFile.createWriter(fc, conf, file, keyClass,\n                valueClass, compressionType, codec,\n                new SequenceFile.Metadata(), EnumSet.of(CreateFlag.CREATE));\n    }\n\n    try {\n        writer.append((key == null) ? defaultKey : key, value);\n    } catch (IOException e) {\n        return false;\n    }\n\n    return true;\n}",
        "accept_response": "@Override\npublic boolean writeNextObject(OneRow onerow) throws IOException {\n    Writable value = (Writable) onerow.getData();\n    Writable key = (Writable) onerow.getKey();\n\n    // init writer on first approach here, based on onerow.getData type\n    // TODO: verify data is serializable.\n    if (writer == null) {\n        Class<? extends Writable> valueClass = value.getClass();\n        Class<? extends Writable> keyClass = (key == null) ? LongWritable.class\n                : key.getClass();\n        // create writer - do not allow overwriting existing file\n        writer = SequenceFile.createWriter(fc, conf, file, keyClass,\n                valueClass, compressionType, codec,\n                new SequenceFile.Metadata(), EnumSet.of(CreateFlag.CREATE));\n    }\n\n    try {\n        writer.append((key == null) ? defaultKey : key, value);\n    } catch (IOException e) {\n        LOG.error(\"Failed to write data to file: \" + e.getMessage());\n        return false;\n    }\n\n    return true;\n}",
        "reject_response": "@Override\npublic boolean writeNextObject(OneRow onerow) throws IOException {\n    Writable value = (Writable) onerow.getData();\n    Writable key = (Writable) onerow.getKey();\n\n    // init writer on first approach here, based on onerow.getData type\n    // TODO: verify data is serializable.\n    if (writer == null) {\n        Class<? extends Writable> valueClass = value.getClass();\n        Class<? extends Writable> keyClass = (key == null) ? LongWritable.class\n                : key.getClass();\n        // create writer - do not allow overwriting existing file\n        writer = SequenceFile.createWriter(fc, conf, file, keyClass,\n                valueClass, compressionType, codec,\n                new SequenceFile.Metadata(), EnumSet.of(CreateFlag.CREATE));\n    }\n\n    try {\n        writer.append((key == null) ? defaultKey : key, value);\n    } catch (IOException e) {\n        Log.error(\"Failed to write data to file: \" + e.getMessage());\n        return false;\n    }\n\n    return true;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3183,
        "instruction": "public void stopContainerAsync(ContainerId containerId, NodeId nodeId) {\n  if (containers.get(containerId) == null) {\n    callbackHandler.onStopContainerError(containerId,\n        RPCUtil.getRemoteException(\"Container \" + containerId +\n            \" is neither started nor scheduled to start\"));\n  }\n  try {\n    events.put(new ContainerEvent(containerId, nodeId, null,\n        ContainerEventType.STOP_CONTAINER));\n  } catch (InterruptedException e) {\n    callbackHandler.onStopContainerError(containerId, e);\n  }\n}",
        "accept_response": "public void stopContainerAsync(ContainerId containerId, NodeId nodeId) {\n  if (containers.get(containerId) == null) {\n    callbackHandler.onStopContainerError(containerId,\n        RPCUtil.getRemoteException(\"Container \" + containerId +\n            \" is neither started nor scheduled to start\"));\n  }\n  try {\n    events.put(new ContainerEvent(containerId, nodeId, null,\n        ContainerEventType.STOP_CONTAINER));\n  } catch (InterruptedException e) {\n    LOG.warn(\"Exception when scheduling the event of stopping Container {}\", containerId);\n    callbackHandler.onStopContainerError(containerId, e);\n  }\n}",
        "reject_response": "public void stopContainerAsync(ContainerId containerId, NodeId nodeId) {\n  if (containers.get(containerId) == null) {\n    callbackHandler.onStopContainerError(containerId,\n        RPCUtil.getRemoteException(\"Container \" + containerId +\n            \" is neither started nor scheduled to start\"));\n  }\n  try {\n    events.put(new ContainerEvent(containerId, nodeId, null,\n        ContainerEventType.STOP_CONTAINER));\n  } catch (InterruptedException e) {\n    LOG.warn(\"Exception when scheduling the event of stopping Container \" +\n        containerId);\n    callbackHandler.onStopContainerError(containerId, e);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2384,
        "instruction": "private void waitForTableStateTransition(String tableId, TableState expectedState) throws AccumuloException, TableNotFoundException,\n    AccumuloSecurityException {\n\n  Text startRow = null;\n  Text lastRow = null;\n\n  while (true) {\n\n    if (Tables.getTableState(context.getInstance(), tableId) != expectedState) {\n      Tables.clearCache(context.getInstance());\n      if (Tables.getTableState(context.getInstance(), tableId) != expectedState) {\n        if (!Tables.exists(context.getInstance(), tableId))\n          throw new TableDeletedException(tableId);\n        throw new AccumuloException(\"Unexpected table state \" + tableId + \" \" + Tables.getTableState(context.getInstance(), tableId) + \" != \" + expectedState);\n      }\n    }\n\n    Range range = new KeyExtent(new Text(tableId), null, null).toMetadataRange();\n    if (startRow == null || lastRow == null)\n      range = new KeyExtent(new Text(tableId), null, null).toMetadataRange();\n    else\n      range = new Range(startRow, lastRow);\n\n    String metaTable = MetadataTable.NAME;\n    if (tableId.equals(MetadataTable.ID))\n      metaTable = RootTable.NAME;\n\n    Scanner scanner = createMetadataScanner(metaTable, range);\n\n    RowIterator rowIter = new RowIterator(scanner);\n\n    KeyExtent lastExtent = null;\n\n    int total = 0;\n    int waitFor = 0;\n    int holes = 0;\n    Text continueRow = null;\n    MapCounter<String> serverCounts = new MapCounter<String>();\n\n    while (rowIter.hasNext()) {\n      Iterator<Entry<Key,Value>> row = rowIter.next();\n\n      total++;\n\n      KeyExtent extent = null;\n      String future = null;\n      String current = null;\n\n      while (row.hasNext()) {\n        Entry<Key,Value> entry = row.next();\n        Key key = entry.getKey();\n\n        if (key.getColumnFamily().equals(TabletsSection.FutureLocationColumnFamily.NAME))\n          future = entry.getValue().toString();\n\n        if (key.getColumnFamily().equals(TabletsSection.CurrentLocationColumnFamily.NAME))\n          current = entry.getValue().toString();\n\n        if (TabletsSection.TabletColumnFamily.PREV_ROW_COLUMN.hasColumns(key))\n          extent = new KeyExtent(key.getRow(), entry.getValue());\n      }\n\n      if ((expectedState == TableState.ONLINE && current == null) || (expectedState == TableState.OFFLINE && (future != null || current != null))) {\n        if (continueRow == null)\n          continueRow = extent.getMetadataEntry();\n        waitFor++;\n        lastRow = extent.getMetadataEntry();\n\n        if (current != null)\n          serverCounts.increment(current, 1);\n        if (future != null)\n          serverCounts.increment(future, 1);\n      }\n\n      if (!extent.getTableId().toString().equals(tableId)) {\n        throw new AccumuloException(\"Saw unexpected table Id \" + tableId + \" \" + extent);\n      }\n\n      if (lastExtent != null && !extent.isPreviousExtent(lastExtent)) {\n        holes++;\n      }\n\n      lastExtent = extent;\n    }\n\n    if (continueRow != null) {\n      startRow = continueRow;\n    }\n\n    if (holes > 0 || total == 0) {\n      startRow = null;\n      lastRow = null;\n    }\n\n    if (waitFor > 0 || holes > 0 || total == 0) {\n      long waitTime;\n      long maxPerServer = 0;\n      if (serverCounts.size() > 0) {\n        maxPerServer = Collections.max(serverCounts.values());\n        waitTime = maxPerServer * 10;\n      } else\n        waitTime = waitFor * 10;\n      waitTime = Math.max(100, waitTime);\n      waitTime = Math.min(5000, waitTime);\n      UtilWaitThread.sleep(waitTime);\n    } else {\n      break;\n    }\n\n  }\n}",
        "accept_response": "private void waitForTableStateTransition(String tableId, TableState expectedState) throws AccumuloException, TableNotFoundException,\n    AccumuloSecurityException {\n\n  Text startRow = null;\n  Text lastRow = null;\n\n  while (true) {\n\n    if (Tables.getTableState(context.getInstance(), tableId) != expectedState) {\n      Tables.clearCache(context.getInstance());\n      if (Tables.getTableState(context.getInstance(), tableId) != expectedState) {\n        if (!Tables.exists(context.getInstance(), tableId))\n          throw new TableDeletedException(tableId);\n        throw new AccumuloException(\"Unexpected table state \" + tableId + \" \" + Tables.getTableState(context.getInstance(), tableId) + \" != \" + expectedState);\n      }\n    }\n\n    Range range = new KeyExtent(new Text(tableId), null, null).toMetadataRange();\n    if (startRow == null || lastRow == null)\n      range = new KeyExtent(new Text(tableId), null, null).toMetadataRange();\n    else\n      range = new Range(startRow, lastRow);\n\n    String metaTable = MetadataTable.NAME;\n    if (tableId.equals(MetadataTable.ID))\n      metaTable = RootTable.NAME;\n\n    Scanner scanner = createMetadataScanner(metaTable, range);\n\n    RowIterator rowIter = new RowIterator(scanner);\n\n    KeyExtent lastExtent = null;\n\n    int total = 0;\n    int waitFor = 0;\n    int holes = 0;\n    Text continueRow = null;\n    MapCounter<String> serverCounts = new MapCounter<String>();\n\n    while (rowIter.hasNext()) {\n      Iterator<Entry<Key,Value>> row = rowIter.next();\n\n      total++;\n\n      KeyExtent extent = null;\n      String future = null;\n      String current = null;\n\n      while (row.hasNext()) {\n        Entry<Key,Value> entry = row.next();\n        Key key = entry.getKey();\n\n        if (key.getColumnFamily().equals(TabletsSection.FutureLocationColumnFamily.NAME))\n          future = entry.getValue().toString();\n\n        if (key.getColumnFamily().equals(TabletsSection.CurrentLocationColumnFamily.NAME))\n          current = entry.getValue().toString();\n\n        if (TabletsSection.TabletColumnFamily.PREV_ROW_COLUMN.hasColumns(key))\n          extent = new KeyExtent(key.getRow(), entry.getValue());\n      }\n\n      if ((expectedState == TableState.ONLINE && current == null) || (expectedState == TableState.OFFLINE && (future != null || current != null))) {\n        if (continueRow == null)\n          continueRow = extent.getMetadataEntry();\n        waitFor++;\n        lastRow = extent.getMetadataEntry();\n\n        if (current != null)\n          serverCounts.increment(current, 1);\n        if (future != null)\n          serverCounts.increment(future, 1);\n      }\n\n      if (!extent.getTableId().toString().equals(tableId)) {\n        throw new AccumuloException(\"Saw unexpected table Id \" + tableId + \" \" + extent);\n      }\n\n      if (lastExtent != null && !extent.isPreviousExtent(lastExtent)) {\n        holes++;\n      }\n\n      lastExtent = extent;\n    }\n\n    if (continueRow != null) {\n      startRow = continueRow;\n    }\n\n    if (holes > 0 || total == 0) {\n      startRow = null;\n      lastRow = null;\n    }\n\n    if (waitFor > 0 || holes > 0 || total == 0) {\n      long waitTime;\n      long maxPerServer = 0;\n      if (serverCounts.size() > 0) {\n        maxPerServer = Collections.max(serverCounts.values());\n        waitTime = maxPerServer * 10;\n      } else\n        waitTime = waitFor * 10;\n      waitTime = Math.max(100, waitTime);\n      waitTime = Math.min(5000, waitTime);\n      log.trace(\"Waiting for {}({}) tablets, startRow = {} lastRow = {}, holes={} sleeping:{}ms\",\n          waitFor, maxPerServer, startRow, lastRow, holes, waitTime);\n      UtilWaitThread.sleep(waitTime);\n    } else {\n      break;\n    }\n\n  }\n}",
        "reject_response": "private void waitForTableStateTransition(String tableId, TableState expectedState) throws AccumuloException, TableNotFoundException,\n    AccumuloSecurityException {\n\n  Text startRow = null;\n  Text lastRow = null;\n\n  while (true) {\n\n    if (Tables.getTableState(context.getInstance(), tableId) != expectedState) {\n      Tables.clearCache(context.getInstance());\n      if (Tables.getTableState(context.getInstance(), tableId) != expectedState) {\n        if (!Tables.exists(context.getInstance(), tableId))\n          throw new TableDeletedException(tableId);\n        throw new AccumuloException(\"Unexpected table state \" + tableId + \" \" + Tables.getTableState(context.getInstance(), tableId) + \" != \" + expectedState);\n      }\n    }\n\n    Range range = new KeyExtent(new Text(tableId), null, null).toMetadataRange();\n    if (startRow == null || lastRow == null)\n      range = new KeyExtent(new Text(tableId), null, null).toMetadataRange();\n    else\n      range = new Range(startRow, lastRow);\n\n    String metaTable = MetadataTable.NAME;\n    if (tableId.equals(MetadataTable.ID))\n      metaTable = RootTable.NAME;\n\n    Scanner scanner = createMetadataScanner(metaTable, range);\n\n    RowIterator rowIter = new RowIterator(scanner);\n\n    KeyExtent lastExtent = null;\n\n    int total = 0;\n    int waitFor = 0;\n    int holes = 0;\n    Text continueRow = null;\n    MapCounter<String> serverCounts = new MapCounter<String>();\n\n    while (rowIter.hasNext()) {\n      Iterator<Entry<Key,Value>> row = rowIter.next();\n\n      total++;\n\n      KeyExtent extent = null;\n      String future = null;\n      String current = null;\n\n      while (row.hasNext()) {\n        Entry<Key,Value> entry = row.next();\n        Key key = entry.getKey();\n\n        if (key.getColumnFamily().equals(TabletsSection.FutureLocationColumnFamily.NAME))\n          future = entry.getValue().toString();\n\n        if (key.getColumnFamily().equals(TabletsSection.CurrentLocationColumnFamily.NAME))\n          current = entry.getValue().toString();\n\n        if (TabletsSection.TabletColumnFamily.PREV_ROW_COLUMN.hasColumns(key))\n          extent = new KeyExtent(key.getRow(), entry.getValue());\n      }\n\n      if ((expectedState == TableState.ONLINE && current == null) || (expectedState == TableState.OFFLINE && (future != null || current != null))) {\n        if (continueRow == null)\n          continueRow = extent.getMetadataEntry();\n        waitFor++;\n        lastRow = extent.getMetadataEntry();\n\n        if (current != null)\n          serverCounts.increment(current, 1);\n        if (future != null)\n          serverCounts.increment(future, 1);\n      }\n\n      if (!extent.getTableId().toString().equals(tableId)) {\n        throw new AccumuloException(\"Saw unexpected table Id \" + tableId + \" \" + extent);\n      }\n\n      if (lastExtent != null && !extent.isPreviousExtent(lastExtent)) {\n        holes++;\n      }\n\n      lastExtent = extent;\n    }\n\n    if (continueRow != null) {\n      startRow = continueRow;\n    }\n\n    if (holes > 0 || total == 0) {\n      startRow = null;\n      lastRow = null;\n    }\n\n    if (waitFor > 0 || holes > 0 || total == 0) {\n      long waitTime;\n      long maxPerServer = 0;\n      if (serverCounts.size() > 0) {\n        maxPerServer = Collections.max(serverCounts.values());\n        waitTime = maxPerServer * 10;\n      } else\n        waitTime = waitFor * 10;\n      waitTime = Math.max(100, waitTime);\n      waitTime = Math.min(5000, waitTime);\n      log.trace(\"Waiting for \" + waitFor + \"(\" + maxPerServer + \") tablets, startRow = \" + startRow + \" lastRow = \" + lastRow + \", holes=\" + holes\n          + \" sleeping:\" + waitTime + \"ms\");\n      UtilWaitThread.sleep(waitTime);\n    } else {\n      break;\n    }\n\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2490,
        "instruction": "public void commit(IndexWriter writer) throws CorruptIndexException, IOException {\n  synchronized (_running) {\n    long s = System.nanoTime();\n    writer.commit();\n    long m = System.nanoTime();\n    rollLog();\n    long e = System.nanoTime();\n    LOG.info(\"Log roller took [{0} ms] for [{1}/{2}]\", (e - m) / 1000000.0, _table, _shard);\n  }\n}",
        "accept_response": "public void commit(IndexWriter writer) throws CorruptIndexException, IOException {\n  synchronized (_running) {\n    long s = System.nanoTime();\n    writer.commit();\n    long m = System.nanoTime();\n    LOG.info(\"Commit took [{0} ms] for [{1}/{2}]\", (m - s) / 1000000.0, _table, _shard);\n    rollLog();\n    long e = System.nanoTime();\n    LOG.info(\"Log roller took [{0} ms] for [{1}/{2}]\", (e - m) / 1000000.0, _table, _shard);\n  }\n}",
        "reject_response": "public void commit(IndexWriter writer) throws CorruptIndexException, IOException {\n  synchronized (_running) {\n    long s = System.nanoTime();\n    writer.commit();\n    long m = System.nanoTime();\n    LOG.info(\"Commit took [{0}] for [{1}]\", (m - s) / 1000000.0, writer);\n    rollLog();\n    long e = System.nanoTime();\n    LOG.info(\"Log roller took [{0} ms] for [{1}/{2}]\", (e - m) / 1000000.0, _table, _shard);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2726,
        "instruction": "void load(File file) throws IOException {\n  long start = Time.monotonicNow();\n  imgDigest = MD5FileUtils.computeMd5ForFile(file);\n  RandomAccessFile raFile = new RandomAccessFile(file, \"r\");\n  FileInputStream fin = new FileInputStream(file);\n  try {\n    loadInternal(raFile, fin);\n    long end = Time.monotonicNow();\n  } finally {\n    fin.close();\n    raFile.close();\n  }\n}",
        "accept_response": "void load(File file) throws IOException {\n  long start = Time.monotonicNow();\n  imgDigest = MD5FileUtils.computeMd5ForFile(file);\n  RandomAccessFile raFile = new RandomAccessFile(file, \"r\");\n  FileInputStream fin = new FileInputStream(file);\n  try {\n    loadInternal(raFile, fin);\n    long end = Time.monotonicNow();\n    LOG.info(\"Loaded FSImage in {} seconds.\", (end - start) / 1000);\n  } finally {\n    fin.close();\n    raFile.close();\n  }\n}",
        "reject_response": "void load(File file) throws IOException {\n  long start = Time.monotonicNow();\n  imgDigest = MD5FileUtils.computeMd5ForFile(file);\n  RandomAccessFile raFile = new RandomAccessFile(file, \"r\");\n  FileInputStream fin = new FileInputStream(file);\n  try {\n    loadInternal(raFile, fin);\n    long end = Time.monotonicNow();\n    LOG.info(\"Loaded FSImage in \" + (end - start) / 1000 + \" seconds.\");\n  } finally {\n    fin.close();\n    raFile.close();\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3158,
        "instruction": "public final void internalRenderComponent()\n{\n\tfinal IMarkupFragment markup = getMarkup();\n\tif (markup == null)\n\t{\n\t\tthrow new MarkupException(\"Markup not found. Component: \" + toString());\n\t}\n\n\tfinal MarkupStream markupStream = new MarkupStream(markup);\n\n\t// Get mutable copy of next tag\n\tfinal ComponentTag openTag = markupStream.getTag();\n\tfinal ComponentTag tag = openTag.mutable();\n\n\t// Call any tag handler\n\tonComponentTag(tag);\n\n\t// If we're an openclose tag\n\tif (!tag.isOpenClose() && !tag.isOpen())\n\t{\n\t\t// We were something other than <tag> or <tag/>\n\t\tmarkupStream.throwMarkupException(\"Method renderComponent called on bad markup element: \" +\n\t\t\ttag);\n\t}\n\n\tif (tag.isOpenClose() && openTag.isOpen())\n\t{\n\t\tmarkupStream.throwMarkupException(\"You can not modify a open tag to open-close: \" + tag);\n\t}\n\n\ttry\n\t{\n\t\t// Render open tag\n\t\tboolean renderBodyOnly = getRenderBodyOnly();\n\t\tif (renderBodyOnly)\n\t\t{\n\t\t\tExceptionSettings.NotRenderableErrorStrategy notRenderableErrorStrategy = ExceptionSettings.NotRenderableErrorStrategy.LOG_WARNING;\n\t\t\tif (Application.exists())\n\t\t\t{\n\t\t\t\tnotRenderableErrorStrategy = getApplication().getExceptionSettings().getNotRenderableErrorStrategy();\n\t\t\t}\n\n\t\t\tif (getFlag(FLAG_OUTPUT_MARKUP_ID))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Markup id set on a component that renders its body only. \" +\n\t\t\t\t                               \"Markup id: %s, component id: %s.\", getMarkupId(), getId());\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t\tif (getFlag(FLAG_PLACEHOLDER))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Placeholder tag set on a component that renders its body only. \" +\n\t\t\t\t                               \"Component id: %s.\", getId());\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\telse\n\t\t{\n\t\t\trenderComponentTag(tag);\n\t\t}\n\t\tmarkupStream.next();\n\n\t\t// Render the body only if open-body-close. Do not render if open-close.\n\t\tif (tag.isOpen())\n\t\t{\n\t\t\t// Render the body. The default strategy will simply call the component's\n\t\t\t// onComponentTagBody() implementation.\n\t\t\tgetMarkupSourcingStrategy().onComponentTagBody(this, markupStream, tag);\n\n\t\t\t// Render close tag\n\t\t\tif (openTag.isOpen())\n\t\t\t{\n\t\t\t\trenderClosingComponentTag(markupStream, tag, renderBodyOnly);\n\t\t\t}\n\t\t\telse if (renderBodyOnly == false)\n\t\t\t{\n\t\t\t\tif (needToRenderTag(openTag))\n\t\t\t\t{\n\t\t\t\t\t// Close the manually opened tag. And since the user might have changed the\n\t\t\t\t\t// tag name ...\n\t\t\t\t\tgetResponse().write(tag.syntheticCloseTagString());\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tcatch (WicketRuntimeException wre)\n\t{\n\t\tthrow wre;\n\t}\n\tcatch (RuntimeException re)\n\t{\n\t\tthrow new WicketRuntimeException(\"Exception in rendering component: \" + this, re);\n\t}\n}",
        "accept_response": "public final void internalRenderComponent()\n{\n\tfinal IMarkupFragment markup = getMarkup();\n\tif (markup == null)\n\t{\n\t\tthrow new MarkupException(\"Markup not found. Component: \" + toString());\n\t}\n\n\tfinal MarkupStream markupStream = new MarkupStream(markup);\n\n\t// Get mutable copy of next tag\n\tfinal ComponentTag openTag = markupStream.getTag();\n\tfinal ComponentTag tag = openTag.mutable();\n\n\t// Call any tag handler\n\tonComponentTag(tag);\n\n\t// If we're an openclose tag\n\tif (!tag.isOpenClose() && !tag.isOpen())\n\t{\n\t\t// We were something other than <tag> or <tag/>\n\t\tmarkupStream.throwMarkupException(\"Method renderComponent called on bad markup element: \" +\n\t\t\ttag);\n\t}\n\n\tif (tag.isOpenClose() && openTag.isOpen())\n\t{\n\t\tmarkupStream.throwMarkupException(\"You can not modify a open tag to open-close: \" + tag);\n\t}\n\n\ttry\n\t{\n\t\t// Render open tag\n\t\tboolean renderBodyOnly = getRenderBodyOnly();\n\t\tif (renderBodyOnly)\n\t\t{\n\t\t\tExceptionSettings.NotRenderableErrorStrategy notRenderableErrorStrategy = ExceptionSettings.NotRenderableErrorStrategy.LOG_WARNING;\n\t\t\tif (Application.exists())\n\t\t\t{\n\t\t\t\tnotRenderableErrorStrategy = getApplication().getExceptionSettings().getNotRenderableErrorStrategy();\n\t\t\t}\n\n\t\t\tif (getFlag(FLAG_OUTPUT_MARKUP_ID))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Markup id set on a component that renders its body only. \" +\n\t\t\t\t                               \"Markup id: %s, component id: %s.\", getMarkupId(), getId());\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t\tif (getFlag(FLAG_PLACEHOLDER))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Placeholder tag set on a component that renders its body only. \" +\n\t\t\t\t                               \"Component id: %s.\", getId());\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t}\n\t\telse\n\t\t{\n\t\t\trenderComponentTag(tag);\n\t\t}\n\t\tmarkupStream.next();\n\n\t\t// Render the body only if open-body-close. Do not render if open-close.\n\t\tif (tag.isOpen())\n\t\t{\n\t\t\t// Render the body. The default strategy will simply call the component's\n\t\t\t// onComponentTagBody() implementation.\n\t\t\tgetMarkupSourcingStrategy().onComponentTagBody(this, markupStream, tag);\n\n\t\t\t// Render close tag\n\t\t\tif (openTag.isOpen())\n\t\t\t{\n\t\t\t\trenderClosingComponentTag(markupStream, tag, renderBodyOnly);\n\t\t\t}\n\t\t\telse if (renderBodyOnly == false)\n\t\t\t{\n\t\t\t\tif (needToRenderTag(openTag))\n\t\t\t\t{\n\t\t\t\t\t// Close the manually opened tag. And since the user might have changed the\n\t\t\t\t\t// tag name ...\n\t\t\t\t\tgetResponse().write(tag.syntheticCloseTagString());\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tcatch (WicketRuntimeException wre)\n\t{\n\t\tthrow wre;\n\t}\n\tcatch (RuntimeException re)\n\t{\n\t\tthrow new WicketRuntimeException(\"Exception in rendering component: \" + this, re);\n\t}\n}",
        "reject_response": "public final void internalRenderComponent()\n{\n\tfinal IMarkupFragment markup = getMarkup();\n\tif (markup == null)\n\t{\n\t\tthrow new MarkupException(\"Markup not found. Component: \" + toString());\n\t}\n\n\tfinal MarkupStream markupStream = new MarkupStream(markup);\n\n\t// Get mutable copy of next tag\n\tfinal ComponentTag openTag = markupStream.getTag();\n\tfinal ComponentTag tag = openTag.mutable();\n\n\t// Call any tag handler\n\tonComponentTag(tag);\n\n\t// If we're an openclose tag\n\tif (!tag.isOpenClose() && !tag.isOpen())\n\t{\n\t\t// We were something other than <tag> or <tag/>\n\t\tmarkupStream.throwMarkupException(\"Method renderComponent called on bad markup element: \" +\n\t\t\ttag);\n\t}\n\n\tif (tag.isOpenClose() && openTag.isOpen())\n\t{\n\t\tmarkupStream.throwMarkupException(\"You can not modify a open tag to open-close: \" + tag);\n\t}\n\n\ttry\n\t{\n\t\t// Render open tag\n\t\tboolean renderBodyOnly = getRenderBodyOnly();\n\t\tif (renderBodyOnly)\n\t\t{\n\t\t\tExceptionSettings.NotRenderableErrorStrategy notRenderableErrorStrategy = ExceptionSettings.NotRenderableErrorStrategy.LOG_WARNING;\n\t\t\tif (Application.exists())\n\t\t\t{\n\t\t\t\tnotRenderableErrorStrategy = getApplication().getExceptionSettings().getNotRenderableErrorStrategy();\n\t\t\t}\n\n\t\t\tif (getFlag(FLAG_OUTPUT_MARKUP_ID))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Markup id set on a component that renders its body only. \" +\n\t\t\t\t                               \"Markup id: %s, component id: %s.\", getMarkupId(), getId());\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t\tif (getFlag(FLAG_PLACEHOLDER))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Placeholder tag set on a component that renders its body only. \" +\n\t\t\t\t                               \"Component id: %s.\", getId());\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(String.format(\n\t\t\t\t\t\"Placeholder tag set on a component that renders its body only. \"\n\t\t\t\t\t\t+ \"Component id: %s.\", getId()));\n\t\t\t}\n\t\t}\n\t\telse\n\t\t{\n\t\t\trenderComponentTag(tag);\n\t\t}\n\t\tmarkupStream.next();\n\n\t\t// Render the body only if open-body-close. Do not render if open-close.\n\t\tif (tag.isOpen())\n\t\t{\n\t\t\t// Render the body. The default strategy will simply call the component's\n\t\t\t// onComponentTagBody() implementation.\n\t\t\tgetMarkupSourcingStrategy().onComponentTagBody(this, markupStream, tag);\n\n\t\t\t// Render close tag\n\t\t\tif (openTag.isOpen())\n\t\t\t{\n\t\t\t\trenderClosingComponentTag(markupStream, tag, renderBodyOnly);\n\t\t\t}\n\t\t\telse if (renderBodyOnly == false)\n\t\t\t{\n\t\t\t\tif (needToRenderTag(openTag))\n\t\t\t\t{\n\t\t\t\t\t// Close the manually opened tag. And since the user might have changed the\n\t\t\t\t\t// tag name ...\n\t\t\t\t\tgetResponse().write(tag.syntheticCloseTagString());\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tcatch (WicketRuntimeException wre)\n\t{\n\t\tthrow wre;\n\t}\n\tcatch (RuntimeException re)\n\t{\n\t\tthrow new WicketRuntimeException(\"Exception in rendering component: \" + this, re);\n\t}\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2968,
        "instruction": "private Segment copySegmentFromPrimary(UUID uuid) throws Exception {\n    Segment result = cache.get(uuid);\n\n    if (result != null) {\n        return result;\n    }\n\n    byte[] data = client.getSegment(uuid.toString());\n\n    if (data == null) {\n        throw new IllegalStateException(\"Unable to read segment \" + uuid);\n    }\n\n    long msb = uuid.getMostSignificantBits();\n    long lsb = uuid.getLeastSignificantBits();\n    SegmentId segmentId = store.newSegmentId(msb, lsb);\n    store.writeSegment(segmentId, data, 0, data.length);\n    result = segmentId.getSegment();\n    cache.put(uuid, result);\n    return result;\n}",
        "accept_response": "private Segment copySegmentFromPrimary(UUID uuid) throws Exception {\n    Segment result = cache.get(uuid);\n\n    if (result != null) {\n        log.debug(\"Segment {} was found in the local cache\", uuid);\n        return result;\n    }\n\n    byte[] data = client.getSegment(uuid.toString());\n\n    if (data == null) {\n        throw new IllegalStateException(\"Unable to read segment \" + uuid);\n    }\n\n    long msb = uuid.getMostSignificantBits();\n    long lsb = uuid.getLeastSignificantBits();\n    SegmentId segmentId = store.newSegmentId(msb, lsb);\n    store.writeSegment(segmentId, data, 0, data.length);\n    result = segmentId.getSegment();\n    cache.put(uuid, result);\n    return result;\n}",
        "reject_response": "private Segment copySegmentFromPrimary(UUID uuid) throws Exception {\n    Segment result = cache.get(uuid);\n\n    if (result != null) {\n        log.info(\"Segment {} was found in the local cache\", uuid);\n        return result;\n    }\n\n    byte[] data = client.getSegment(uuid.toString());\n\n    if (data == null) {\n        throw new IllegalStateException(\"Unable to read segment \" + uuid);\n    }\n\n    long msb = uuid.getMostSignificantBits();\n    long lsb = uuid.getLeastSignificantBits();\n    SegmentId segmentId = store.newSegmentId(msb, lsb);\n    store.writeSegment(segmentId, data, 0, data.length);\n    result = segmentId.getSegment();\n    cache.put(uuid, result);\n    return result;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2949,
        "instruction": "@Override\npublic ListFlowFileStatus listFlowFiles(final String requestIdentifier, final int maxResults) {\n    // purge any old requests from the map just to keep it clean. But if there are very few requests, which is usually the case, then don't bother\n    if (listRequestMap.size() > 10) {\n        final List<String> toDrop = new ArrayList<>();\n        for (final Map.Entry<String, ListFlowFileRequest> entry : listRequestMap.entrySet()) {\n            final ListFlowFileRequest request = entry.getValue();\n            final boolean completed = request.getState() == ListFlowFileState.COMPLETE || request.getState() == ListFlowFileState.FAILURE;\n\n            if (completed && System.currentTimeMillis() - request.getLastUpdated() > TimeUnit.MINUTES.toMillis(5L)) {\n                toDrop.add(entry.getKey());\n            }\n        }\n\n        for (final String requestId : toDrop) {\n            listRequestMap.remove(requestId);\n        }\n    }\n\n    // numSteps = 1 for each swap location + 1 for active queue + 1 for swap queue.\n    final ListFlowFileRequest listRequest = new ListFlowFileRequest(requestIdentifier, maxResults, size());\n\n    final Thread t = new Thread(new Runnable() {\n        @Override\n        public void run() {\n            int position = 0;\n            final List<FlowFileSummary> summaries = new ArrayList<>();\n\n            // Create an ArrayList that contains all of the contents of the active queue.\n            // We do this so that we don't have to hold the lock any longer than absolutely necessary.\n            // We cannot simply pull the first 'maxResults' records from the queue, however, because the\n            // Iterator provided by PriorityQueue does not return records in order. So we would have to either\n            // use a writeLock and 'pop' the first 'maxResults' records off the queue or use a read lock and\n            // do a shallow copy of the queue. The shallow copy is generally quicker because it doesn't have to do\n            // the sorting to put the records back. So even though this has an expensive of Java Heap to create the\n            // extra collection, we are making this trade-off to avoid locking the queue any longer than required.\n            final List<FlowFileRecord> allFlowFiles = getListableFlowFiles();\n            final QueuePrioritizer prioritizer = new QueuePrioritizer(getPriorities());\n\n            listRequest.setState(ListFlowFileState.CALCULATING_LIST);\n\n            // sort the FlowFileRecords so that we have the list in the same order as on the queue.\n            allFlowFiles.sort(prioritizer);\n\n            for (final FlowFileRecord flowFile : allFlowFiles) {\n                summaries.add(summarize(flowFile, ++position));\n                if (summaries.size() >= maxResults) {\n                    break;\n                }\n            }\n\n            listRequest.setFlowFileSummaries(summaries);\n            listRequest.setState(ListFlowFileState.COMPLETE);\n        }\n    }, \"List FlowFiles for Connection \" + getIdentifier());\n    t.setDaemon(true);\n    t.start();\n\n    listRequestMap.put(requestIdentifier, listRequest);\n    return listRequest;\n}",
        "accept_response": "@Override\npublic ListFlowFileStatus listFlowFiles(final String requestIdentifier, final int maxResults) {\n    // purge any old requests from the map just to keep it clean. But if there are very few requests, which is usually the case, then don't bother\n    if (listRequestMap.size() > 10) {\n        final List<String> toDrop = new ArrayList<>();\n        for (final Map.Entry<String, ListFlowFileRequest> entry : listRequestMap.entrySet()) {\n            final ListFlowFileRequest request = entry.getValue();\n            final boolean completed = request.getState() == ListFlowFileState.COMPLETE || request.getState() == ListFlowFileState.FAILURE;\n\n            if (completed && System.currentTimeMillis() - request.getLastUpdated() > TimeUnit.MINUTES.toMillis(5L)) {\n                toDrop.add(entry.getKey());\n            }\n        }\n\n        for (final String requestId : toDrop) {\n            listRequestMap.remove(requestId);\n        }\n    }\n\n    // numSteps = 1 for each swap location + 1 for active queue + 1 for swap queue.\n    final ListFlowFileRequest listRequest = new ListFlowFileRequest(requestIdentifier, maxResults, size());\n\n    final Thread t = new Thread(new Runnable() {\n        @Override\n        public void run() {\n            int position = 0;\n            final List<FlowFileSummary> summaries = new ArrayList<>();\n\n            // Create an ArrayList that contains all of the contents of the active queue.\n            // We do this so that we don't have to hold the lock any longer than absolutely necessary.\n            // We cannot simply pull the first 'maxResults' records from the queue, however, because the\n            // Iterator provided by PriorityQueue does not return records in order. So we would have to either\n            // use a writeLock and 'pop' the first 'maxResults' records off the queue or use a read lock and\n            // do a shallow copy of the queue. The shallow copy is generally quicker because it doesn't have to do\n            // the sorting to put the records back. So even though this has an expensive of Java Heap to create the\n            // extra collection, we are making this trade-off to avoid locking the queue any longer than required.\n            final List<FlowFileRecord> allFlowFiles = getListableFlowFiles();\n            final QueuePrioritizer prioritizer = new QueuePrioritizer(getPriorities());\n\n            listRequest.setState(ListFlowFileState.CALCULATING_LIST);\n\n            // sort the FlowFileRecords so that we have the list in the same order as on the queue.\n            allFlowFiles.sort(prioritizer);\n\n            for (final FlowFileRecord flowFile : allFlowFiles) {\n                summaries.add(summarize(flowFile, ++position));\n                if (summaries.size() >= maxResults) {\n                    break;\n                }\n            }\n\n            logger.debug(\"{} Finished listing FlowFiles for active queue with a total of {} results out of {} FlowFiles\", this, summaries.size(), allFlowFiles.size());\n            listRequest.setFlowFileSummaries(summaries);\n            listRequest.setState(ListFlowFileState.COMPLETE);\n        }\n    }, \"List FlowFiles for Connection \" + getIdentifier());\n    t.setDaemon(true);\n    t.start();\n\n    listRequestMap.put(requestIdentifier, listRequest);\n    return listRequest;\n}",
        "reject_response": "@Override\npublic ListFlowFileStatus listFlowFiles(final String requestIdentifier, final int maxResults) {\n    // purge any old requests from the map just to keep it clean. But if there are very few requests, which is usually the case, then don't bother\n    if (listRequestMap.size() > 10) {\n        final List<String> toDrop = new ArrayList<>();\n        for (final Map.Entry<String, ListFlowFileRequest> entry : listRequestMap.entrySet()) {\n            final ListFlowFileRequest request = entry.getValue();\n            final boolean completed = request.getState() == ListFlowFileState.COMPLETE || request.getState() == ListFlowFileState.FAILURE;\n\n            if (completed && System.currentTimeMillis() - request.getLastUpdated() > TimeUnit.MINUTES.toMillis(5L)) {\n                toDrop.add(entry.getKey());\n            }\n        }\n\n        for (final String requestId : toDrop) {\n            listRequestMap.remove(requestId);\n        }\n    }\n\n    // numSteps = 1 for each swap location + 1 for active queue + 1 for swap queue.\n    final ListFlowFileRequest listRequest = new ListFlowFileRequest(requestIdentifier, maxResults, size());\n\n    final Thread t = new Thread(new Runnable() {\n        @Override\n        public void run() {\n            int position = 0;\n            final List<FlowFileSummary> summaries = new ArrayList<>();\n\n            // Create an ArrayList that contains all of the contents of the active queue.\n            // We do this so that we don't have to hold the lock any longer than absolutely necessary.\n            // We cannot simply pull the first 'maxResults' records from the queue, however, because the\n            // Iterator provided by PriorityQueue does not return records in order. So we would have to either\n            // use a writeLock and 'pop' the first 'maxResults' records off the queue or use a read lock and\n            // do a shallow copy of the queue. The shallow copy is generally quicker because it doesn't have to do\n            // the sorting to put the records back. So even though this has an expensive of Java Heap to create the\n            // extra collection, we are making this trade-off to avoid locking the queue any longer than required.\n            final List<FlowFileRecord> allFlowFiles = getListableFlowFiles();\n            final QueuePrioritizer prioritizer = new QueuePrioritizer(getPriorities());\n\n            listRequest.setState(ListFlowFileState.CALCULATING_LIST);\n\n            // sort the FlowFileRecords so that we have the list in the same order as on the queue.\n            allFlowFiles.sort(prioritizer);\n\n            for (final FlowFileRecord flowFile : allFlowFiles) {\n                summaries.add(summarize(flowFile, ++position));\n                if (summaries.size() >= maxResults) {\n                    break;\n                }\n            }\n\n            logger.debug(\"{} Finished listing FlowFiles for active queue with a total of {} results\", this, resultCount);\n            listRequest.setFlowFileSummaries(summaries);\n            listRequest.setState(ListFlowFileState.COMPLETE);\n        }\n    }, \"List FlowFiles for Connection \" + getIdentifier());\n    t.setDaemon(true);\n    t.start();\n\n    listRequestMap.put(requestIdentifier, listRequest);\n    return listRequest;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2436,
        "instruction": "protected Set<GroupPrincipal> getACLs(String destinationBase, SearchControls constraints, String roleBase, String roleAttribute) {\n    try {\n        Set<GroupPrincipal> roles = new HashSet<GroupPrincipal>();\n        Set<String> acls = new HashSet<String>();\n        NamingEnumeration<?> results = context.search(destinationBase, roleBase, constraints);\n        while (results.hasMore()) {\n            SearchResult result = (SearchResult)results.next();\n            Attributes attrs = result.getAttributes();\n            if (attrs == null) {\n                continue;\n            }\n            acls = addAttributeValues(roleAttribute, attrs, acls);\n        }\n        for (Iterator<String> iter = acls.iterator(); iter.hasNext();) {\n            String roleName = iter.next();\n            LdapName ldapname = new LdapName(roleName);\n            Rdn rdn = ldapname.getRdn(ldapname.size() - 1);\n            roles.add(new GroupPrincipal(rdn.getValue().toString()));\n        }\n        return roles;\n    } catch (NamingException e) {\n        LOG.error(e.toString());\n        return new HashSet<GroupPrincipal>();\n    }\n}",
        "accept_response": "protected Set<GroupPrincipal> getACLs(String destinationBase, SearchControls constraints, String roleBase, String roleAttribute) {\n    try {\n        Set<GroupPrincipal> roles = new HashSet<GroupPrincipal>();\n        Set<String> acls = new HashSet<String>();\n        NamingEnumeration<?> results = context.search(destinationBase, roleBase, constraints);\n        while (results.hasMore()) {\n            SearchResult result = (SearchResult)results.next();\n            Attributes attrs = result.getAttributes();\n            if (attrs == null) {\n                continue;\n            }\n            acls = addAttributeValues(roleAttribute, attrs, acls);\n        }\n        for (Iterator<String> iter = acls.iterator(); iter.hasNext();) {\n            String roleName = iter.next();\n            LdapName ldapname = new LdapName(roleName);\n            Rdn rdn = ldapname.getRdn(ldapname.size() - 1);\n            LOG.debug(\"Found role: [{}]\", rdn.getValue());\n            roles.add(new GroupPrincipal(rdn.getValue().toString()));\n        }\n        return roles;\n    } catch (NamingException e) {\n        LOG.error(e.toString());\n        return new HashSet<GroupPrincipal>();\n    }\n}",
        "reject_response": "protected Set<GroupPrincipal> getACLs(String destinationBase, SearchControls constraints, String roleBase, String roleAttribute) {\n    try {\n        Set<GroupPrincipal> roles = new HashSet<GroupPrincipal>();\n        Set<String> acls = new HashSet<String>();\n        NamingEnumeration<?> results = context.search(destinationBase, roleBase, constraints);\n        while (results.hasMore()) {\n            SearchResult result = (SearchResult)results.next();\n            Attributes attrs = result.getAttributes();\n            if (attrs == null) {\n                continue;\n            }\n            acls = addAttributeValues(roleAttribute, attrs, acls);\n        }\n        for (Iterator<String> iter = acls.iterator(); iter.hasNext();) {\n            String roleName = iter.next();\n            LdapName ldapname = new LdapName(roleName);\n            Rdn rdn = ldapname.getRdn(ldapname.size() - 1);\n            LOG.debug(\"Found role: [\" + rdn.getValue().toString() + \"]\");\n            roles.add(new GroupPrincipal(rdn.getValue().toString()));\n        }\n        return roles;\n    } catch (NamingException e) {\n        LOG.error(e.toString());\n        return new HashSet<GroupPrincipal>();\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2976,
        "instruction": "void fireConnectionEstablished() {\n\n    long now = System.currentTimeMillis();\n    long deadline = protonTransport.tick(now);\n    if (deadline > 0) {\n        long delay = deadline - now;\n        nextIdleTimeoutCheck = serializer.schedule(new IdleTimeoutCheck(), delay, TimeUnit.MILLISECONDS);\n    }\n\n    ProviderListener listener = this.listener;\n    if (listener != null) {\n        listener.onConnectionEstablished(remoteURI);\n    }\n}",
        "accept_response": "void fireConnectionEstablished() {\n\n    long now = System.currentTimeMillis();\n    long deadline = protonTransport.tick(now);\n    if (deadline > 0) {\n        long delay = deadline - now;\n        LOG.trace(\"IdleTimeoutCheck being initiated, initial delay: {}\", delay);\n        nextIdleTimeoutCheck = serializer.schedule(new IdleTimeoutCheck(), delay, TimeUnit.MILLISECONDS);\n    }\n\n    ProviderListener listener = this.listener;\n    if (listener != null) {\n        listener.onConnectionEstablished(remoteURI);\n    }\n}",
        "reject_response": "void fireConnectionEstablished() {\n\n    long now = System.currentTimeMillis();\n    long deadline = protonTransport.tick(now);\n    if (deadline > 0) {\n        long delay = deadline - now;\n        LOG.trace(\"IdleTimeoutCheck being initiated, initial delay: {}\", deadline);\n        nextIdleTimeoutCheck = serializer.schedule(new IdleTimeoutCheck(), delay, TimeUnit.MILLISECONDS);\n    }\n\n    ProviderListener listener = this.listener;\n    if (listener != null) {\n        listener.onConnectionEstablished(remoteURI);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2649,
        "instruction": "@Override\npublic void contextDestroyed(ServletContextEvent event) {\n\n  // Stop all running threads those are created in Pulse\n  // Stop cluster threads\n  Repository.get().removeAllClusters();\n\n}",
        "accept_response": "@Override\npublic void contextDestroyed(ServletContextEvent event) {\n\n  // Stop all running threads those are created in Pulse\n  // Stop cluster threads\n  Repository.get().removeAllClusters();\n\n  logger.info(\"{}{}\", resourceBundle.getString(\"LOG_MSG_CONTEXT_DESTROYED\"),\n      event.getServletContext().getContextPath());\n}",
        "reject_response": "@Override\npublic void contextDestroyed(ServletContextEvent event) {\n\n  // Stop all running threads those are created in Pulse\n  // Stop cluster threads\n  Repository.get().removeAllClusters();\n\n  if (LOGGER.infoEnabled()) {\n    LOGGER.info(resourceBundle.getString(\"LOG_MSG_CONTEXT_DESTROYED\")\n        + event.getServletContext().getContextPath());\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2751,
        "instruction": "@Override\n// ClientProtocol\npublic void renameSnapshot(String snapshotRoot, String snapshotOldName,\n    String snapshotNewName) throws IOException {\n  checkNNStartup();\n  if (snapshotNewName == null || snapshotNewName.isEmpty()) {\n    throw new IOException(\"The new snapshot name is null or empty.\");\n  }\n  namesystem.checkOperation(OperationCategory.WRITE);\n  metrics.incrRenameSnapshotOps();\n  CacheEntry cacheEntry = getCacheEntry();\n  if (cacheEntry != null && cacheEntry.isSuccess()) {\n    return; // Return previous response\n  }\n  boolean success = false;\n  try {\n    namesystem.renameSnapshot(snapshotRoot, snapshotOldName,\n        snapshotNewName, cacheEntry != null);\n    success = true;\n  } finally {\n    RetryCache.setState(cacheEntry, success);\n  }\n}",
        "accept_response": "@Override\n// ClientProtocol\npublic void renameSnapshot(String snapshotRoot, String snapshotOldName,\n    String snapshotNewName) throws IOException {\n  checkNNStartup();\n  LOG.debug(\n      \"*DIR* NameNode.renameSnapshot: Snapshot Path {},snapshotOldName {}, snapshotNewName {}\",\n      snapshotRoot, snapshotOldName, snapshotNewName);\n  if (snapshotNewName == null || snapshotNewName.isEmpty()) {\n    throw new IOException(\"The new snapshot name is null or empty.\");\n  }\n  namesystem.checkOperation(OperationCategory.WRITE);\n  metrics.incrRenameSnapshotOps();\n  CacheEntry cacheEntry = getCacheEntry();\n  if (cacheEntry != null && cacheEntry.isSuccess()) {\n    return; // Return previous response\n  }\n  boolean success = false;\n  try {\n    namesystem.renameSnapshot(snapshotRoot, snapshotOldName,\n        snapshotNewName, cacheEntry != null);\n    success = true;\n  } finally {\n    RetryCache.setState(cacheEntry, success);\n  }\n}",
        "reject_response": "@Override\n// ClientProtocol\npublic void renameSnapshot(String snapshotRoot, String snapshotOldName,\n    String snapshotNewName) throws IOException {\n  checkNNStartup();\n  LOG.debug(\"*DIR* NameNode.renameSnapshot: Snapshot Path {}, \" +\n      \"snapshotOldName {}, snapshotNewName {}\", snapshotRoot,\n      snapshotOldName, snapshotNewName);\n  if (snapshotNewName == null || snapshotNewName.isEmpty()) {\n    throw new IOException(\"The new snapshot name is null or empty.\");\n  }\n  namesystem.checkOperation(OperationCategory.WRITE);\n  metrics.incrRenameSnapshotOps();\n  CacheEntry cacheEntry = getCacheEntry();\n  if (cacheEntry != null && cacheEntry.isSuccess()) {\n    return; // Return previous response\n  }\n  boolean success = false;\n  try {\n    namesystem.renameSnapshot(snapshotRoot, snapshotOldName,\n        snapshotNewName, cacheEntry != null);\n    success = true;\n  } finally {\n    RetryCache.setState(cacheEntry, success);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3017,
        "instruction": "@Override\npublic List<T> receive() throws NetworkException, InterruptedException {\n  LOG.entering(\"GatherReceiver\", \"receive\");\n  final Map<String, T> mapOfTaskIdToData = receiveMapOfTaskIdToData();\n\n  final TreeMap<String, T> sortedMapOfTaskIdToData = new TreeMap<>(mapOfTaskIdToData);\n  final List<T> retList = new LinkedList<>(sortedMapOfTaskIdToData.values());\n\n  LOG.exiting(\"GatherReceiver\", \"receive\");\n  return retList;\n}",
        "accept_response": "@Override\npublic List<T> receive() throws NetworkException, InterruptedException {\n  LOG.entering(\"GatherReceiver\", \"receive\");\n  final Map<String, T> mapOfTaskIdToData = receiveMapOfTaskIdToData();\n\n  LOG.log(Level.FINE, \"{0} Sorting data according to lexicographical order of task identifiers.\", this);\n  final TreeMap<String, T> sortedMapOfTaskIdToData = new TreeMap<>(mapOfTaskIdToData);\n  final List<T> retList = new LinkedList<>(sortedMapOfTaskIdToData.values());\n\n  LOG.exiting(\"GatherReceiver\", \"receive\");\n  return retList;\n}",
        "reject_response": "@Override\npublic List<T> receive() throws NetworkException, InterruptedException {\n  LOG.entering(\"GatherReceiver\", \"receive\");\n  final Map<String, T> mapOfTaskIdToData = receiveMapOfTaskIdToData();\n\n  LOG.fine(this + \" Sorting data according to lexicographical order of task identifiers.\");\n  final TreeMap<String, T> sortedMapOfTaskIdToData = new TreeMap<>(mapOfTaskIdToData);\n  final List<T> retList = new LinkedList<>(sortedMapOfTaskIdToData.values());\n\n  LOG.exiting(\"GatherReceiver\", \"receive\");\n  return retList;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2731,
        "instruction": "public static void registerProtocolEngine(RPC.RpcKind rpcKind,\n        Class<? extends Writable> rpcRequestWrapperClass,\n        RpcInvoker rpcInvoker) {\n  RpcKindMapValue  old =\n      rpcKindMap.put(rpcKind, new RpcKindMapValue(rpcRequestWrapperClass, rpcInvoker));\n  if (old != null) {\n    rpcKindMap.put(rpcKind, old);\n    throw new IllegalArgumentException(\"ReRegistration of rpcKind: \" +\n        rpcKind);\n  }\n}",
        "accept_response": "public static void registerProtocolEngine(RPC.RpcKind rpcKind,\n        Class<? extends Writable> rpcRequestWrapperClass,\n        RpcInvoker rpcInvoker) {\n  RpcKindMapValue  old =\n      rpcKindMap.put(rpcKind, new RpcKindMapValue(rpcRequestWrapperClass, rpcInvoker));\n  if (old != null) {\n    rpcKindMap.put(rpcKind, old);\n    throw new IllegalArgumentException(\"ReRegistration of rpcKind: \" +\n        rpcKind);\n  }\n  LOG.debug(\"rpcKind={}, rpcRequestWrapperClass={}, rpcInvoker={}.\",\n      rpcKind, rpcRequestWrapperClass, rpcInvoker);\n}",
        "reject_response": "public static void registerProtocolEngine(RPC.RpcKind rpcKind,\n        Class<? extends Writable> rpcRequestWrapperClass,\n        RpcInvoker rpcInvoker) {\n  RpcKindMapValue  old =\n      rpcKindMap.put(rpcKind, new RpcKindMapValue(rpcRequestWrapperClass, rpcInvoker));\n  if (old != null) {\n    rpcKindMap.put(rpcKind, old);\n    throw new IllegalArgumentException(\"ReRegistration of rpcKind: \" +\n        rpcKind);\n  }\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"rpcKind=\" + rpcKind +\n        \", rpcRequestWrapperClass=\" + rpcRequestWrapperClass +\n        \", rpcInvoker=\" + rpcInvoker);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3145,
        "instruction": "private void mainLoop() {\n  while(running) {\n    // Try assigning all containers if there's a request to do so.\n    if (tryAssigningAll) {\n      doAssignAll();\n      tryAssigningAll = false;\n    }\n\n    // Try allocating containers which have timed out.\n    // Required since these containers may get assigned without\n    // locality at this point.\n    synchronized(this) {\n      if (delayedContainers.peek() == null) {\n        try {\n          // test only signaling to make TestTaskScheduler work\n          if (drainedDelayedContainersForTest != null) {\n            synchronized (drainedDelayedContainersForTest) {\n              drainedDelayedContainersForTest.set(true);\n              drainedDelayedContainersForTest.notifyAll();\n            }\n          }\n          this.wait();\n          // Re-loop to see if tryAssignAll is set.\n          continue;\n        } catch (InterruptedException e) {\n          LOG.info(\"AllocatedContainerManager Thread interrupted\");\n        }\n      }\n    }\n    // test only sleep to prevent tight loop cycling that makes tests stall\n    if (drainedDelayedContainersForTest != null) {\n      try {\n        Thread.sleep(100);\n      } catch (InterruptedException e) {\n        e.printStackTrace();\n      }\n    }\n    HeldContainer delayedContainer = delayedContainers.peek();\n    if (delayedContainer == null) {\n      continue;\n    }\n    long currentTs = System.currentTimeMillis();\n    long nextScheduleTs = delayedContainer.getNextScheduleTime();\n    if (currentTs >= nextScheduleTs) {\n      Map<CookieContainerRequest, Container> assignedContainers = null;\n      synchronized(YarnTaskSchedulerService.this) {\n        // Remove the container and try scheduling it.\n        // TEZ-587 what if container is released by RM after this\n        // in onContainerCompleted()\n        delayedContainer = delayedContainers.poll();\n        if (delayedContainer == null) {\n          continue;\n        }\n        if (null !=\n            heldContainers.get(delayedContainer.getContainer().getId())) {\n          assignedContainers = assignDelayedContainer(delayedContainer);\n        } else {\n          // non standard scenario\n          LOG.info(\"Skipping delayed container as container is no longer\"\n              + \" running, containerId=\"\n              + delayedContainer.getContainer().getId());\n        }\n      }\n      // Inform App should be done outside of the lock\n      informAppAboutAssignments(assignedContainers);\n    } else {\n      synchronized(this) {\n        try {\n          // Wait for the next container to be assignable\n          delayedContainer = delayedContainers.peek();\n          long diff = localitySchedulingDelay;\n          if (delayedContainer != null) {\n            diff = delayedContainer.getNextScheduleTime() - currentTs;\n          }\n          if (diff > 0) {\n            this.wait(diff);\n          }\n        } catch (InterruptedException e) {\n          LOG.info(\"AllocatedContainerManager Thread interrupted\");\n        }\n      }\n    }\n  }\n  releasePendingContainers();\n}",
        "accept_response": "private void mainLoop() {\n  while(running) {\n    // Try assigning all containers if there's a request to do so.\n    if (tryAssigningAll) {\n      doAssignAll();\n      tryAssigningAll = false;\n    }\n\n    // Try allocating containers which have timed out.\n    // Required since these containers may get assigned without\n    // locality at this point.\n    synchronized(this) {\n      if (delayedContainers.peek() == null) {\n        try {\n          // test only signaling to make TestTaskScheduler work\n          if (drainedDelayedContainersForTest != null) {\n            synchronized (drainedDelayedContainersForTest) {\n              drainedDelayedContainersForTest.set(true);\n              drainedDelayedContainersForTest.notifyAll();\n            }\n          }\n          this.wait();\n          // Re-loop to see if tryAssignAll is set.\n          continue;\n        } catch (InterruptedException e) {\n          LOG.info(\"AllocatedContainerManager Thread interrupted\");\n        }\n      }\n    }\n    // test only sleep to prevent tight loop cycling that makes tests stall\n    if (drainedDelayedContainersForTest != null) {\n      try {\n        Thread.sleep(100);\n      } catch (InterruptedException e) {\n        e.printStackTrace();\n      }\n    }\n    HeldContainer delayedContainer = delayedContainers.peek();\n    if (delayedContainer == null) {\n      continue;\n    }\n    LOG.debug(\"Considering HeldContainer: {} for assignment\", delayedContainer);\n    long currentTs = System.currentTimeMillis();\n    long nextScheduleTs = delayedContainer.getNextScheduleTime();\n    if (currentTs >= nextScheduleTs) {\n      Map<CookieContainerRequest, Container> assignedContainers = null;\n      synchronized(YarnTaskSchedulerService.this) {\n        // Remove the container and try scheduling it.\n        // TEZ-587 what if container is released by RM after this\n        // in onContainerCompleted()\n        delayedContainer = delayedContainers.poll();\n        if (delayedContainer == null) {\n          continue;\n        }\n        if (null !=\n            heldContainers.get(delayedContainer.getContainer().getId())) {\n          assignedContainers = assignDelayedContainer(delayedContainer);\n        } else {\n          // non standard scenario\n          LOG.info(\"Skipping delayed container as container is no longer\"\n              + \" running, containerId=\"\n              + delayedContainer.getContainer().getId());\n        }\n      }\n      // Inform App should be done outside of the lock\n      informAppAboutAssignments(assignedContainers);\n    } else {\n      synchronized(this) {\n        try {\n          // Wait for the next container to be assignable\n          delayedContainer = delayedContainers.peek();\n          long diff = localitySchedulingDelay;\n          if (delayedContainer != null) {\n            diff = delayedContainer.getNextScheduleTime() - currentTs;\n          }\n          if (diff > 0) {\n            this.wait(diff);\n          }\n        } catch (InterruptedException e) {\n          LOG.info(\"AllocatedContainerManager Thread interrupted\");\n        }\n      }\n    }\n  }\n  releasePendingContainers();\n}",
        "reject_response": "private void mainLoop() {\n  while(running) {\n    // Try assigning all containers if there's a request to do so.\n    if (tryAssigningAll) {\n      doAssignAll();\n      tryAssigningAll = false;\n    }\n\n    // Try allocating containers which have timed out.\n    // Required since these containers may get assigned without\n    // locality at this point.\n    synchronized(this) {\n      if (delayedContainers.peek() == null) {\n        try {\n          // test only signaling to make TestTaskScheduler work\n          if (drainedDelayedContainersForTest != null) {\n            synchronized (drainedDelayedContainersForTest) {\n              drainedDelayedContainersForTest.set(true);\n              drainedDelayedContainersForTest.notifyAll();\n            }\n          }\n          this.wait();\n          // Re-loop to see if tryAssignAll is set.\n          continue;\n        } catch (InterruptedException e) {\n          LOG.info(\"AllocatedContainerManager Thread interrupted\");\n        }\n      }\n    }\n    // test only sleep to prevent tight loop cycling that makes tests stall\n    if (drainedDelayedContainersForTest != null) {\n      try {\n        Thread.sleep(100);\n      } catch (InterruptedException e) {\n        e.printStackTrace();\n      }\n    }\n    HeldContainer delayedContainer = delayedContainers.peek();\n    if (delayedContainer == null) {\n      continue;\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Considering HeldContainer: \"\n          + delayedContainer + \" for assignment\");\n    }\n    long currentTs = System.currentTimeMillis();\n    long nextScheduleTs = delayedContainer.getNextScheduleTime();\n    if (currentTs >= nextScheduleTs) {\n      Map<CookieContainerRequest, Container> assignedContainers = null;\n      synchronized(YarnTaskSchedulerService.this) {\n        // Remove the container and try scheduling it.\n        // TEZ-587 what if container is released by RM after this\n        // in onContainerCompleted()\n        delayedContainer = delayedContainers.poll();\n        if (delayedContainer == null) {\n          continue;\n        }\n        if (null !=\n            heldContainers.get(delayedContainer.getContainer().getId())) {\n          assignedContainers = assignDelayedContainer(delayedContainer);\n        } else {\n          // non standard scenario\n          LOG.info(\"Skipping delayed container as container is no longer\"\n              + \" running, containerId=\"\n              + delayedContainer.getContainer().getId());\n        }\n      }\n      // Inform App should be done outside of the lock\n      informAppAboutAssignments(assignedContainers);\n    } else {\n      synchronized(this) {\n        try {\n          // Wait for the next container to be assignable\n          delayedContainer = delayedContainers.peek();\n          long diff = localitySchedulingDelay;\n          if (delayedContainer != null) {\n            diff = delayedContainer.getNextScheduleTime() - currentTs;\n          }\n          if (diff > 0) {\n            this.wait(diff);\n          }\n        } catch (InterruptedException e) {\n          LOG.info(\"AllocatedContainerManager Thread interrupted\");\n        }\n      }\n    }\n  }\n  releasePendingContainers();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2843,
        "instruction": "private void processSinglePartitionUpdate(final ClusterNode node, final GridDhtPartitionsSingleMessage msg) {\n    if (!enterBusy())\n        return;\n\n    try {\n        if (msg.exchangeId() == null) {\n            if (log.isTraceEnabled())\n                log.trace(\"Received local partition update [nodeId=\" + node.id() + \", parts=\" +\n                    msg + ']');\n\n            boolean updated = false;\n\n            for (Map.Entry<Integer, GridDhtPartitionMap> entry : msg.partitions().entrySet()) {\n                Integer grpId = entry.getKey();\n\n                CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                if (grp != null &&\n                    grp.localStartVersion().compareTo(entry.getValue().topologyVersion()) > 0)\n                    continue;\n\n                GridDhtPartitionTopology top = null;\n\n                if (grp == null)\n                    top = clientTops.get(grpId);\n                else if (!grp.isLocal())\n                    top = grp.topology();\n\n                if (top != null) {\n                    updated |= top.update(null, entry.getValue(), false);\n\n                    cctx.affinity().checkRebalanceState(top, grpId);\n                }\n            }\n\n            if (updated) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Partitions have been scheduled to resend [reason=Single update from \" + node.id() + \"]\");\n\n                scheduleResendPartitions();\n            }\n        }\n        else {\n            GridDhtPartitionsExchangeFuture exchFut = exchangeFuture(msg.exchangeId());\n\n            if (log.isTraceEnabled())\n\n            if (msg.client()) {\n                AffinityTopologyVersion initVer = exchFut.initialVersion();\n                AffinityTopologyVersion readyVer = readyAffinityVersion();\n\n                if (initVer.compareTo(readyVer) < 0 && !exchFut.isDone()) {\n                    U.warn(log, \"Client node tries to connect but its exchange \" +\n                        \"info is cleaned up from exchange history. \" +\n                        \"Consider increasing 'IGNITE_EXCHANGE_HISTORY_SIZE' property \" +\n                        \"or start clients in  smaller batches. \" +\n                        \"Current settings and versions: \" +\n                        \"[IGNITE_EXCHANGE_HISTORY_SIZE=\" + EXCHANGE_HISTORY_SIZE + \", \" +\n                        \"initVer=\" + initVer + \", \" +\n                        \"readyVer=\" + readyVer + \"].\"\n                    );\n\n                    exchFut.forceClientReconnect(node, msg);\n\n                    return;\n                }\n            }\n\n            exchFut.onReceiveSingleMessage(node, msg);\n        }\n    }\n    finally {\n        leaveBusy();\n    }\n}",
        "accept_response": "private void processSinglePartitionUpdate(final ClusterNode node, final GridDhtPartitionsSingleMessage msg) {\n    if (!enterBusy())\n        return;\n\n    try {\n        if (msg.exchangeId() == null) {\n            if (log.isTraceEnabled())\n                log.trace(\"Received local partition update [nodeId=\" + node.id() + \", parts=\" +\n                    msg + ']');\n\n            boolean updated = false;\n\n            for (Map.Entry<Integer, GridDhtPartitionMap> entry : msg.partitions().entrySet()) {\n                Integer grpId = entry.getKey();\n\n                CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                if (grp != null &&\n                    grp.localStartVersion().compareTo(entry.getValue().topologyVersion()) > 0)\n                    continue;\n\n                GridDhtPartitionTopology top = null;\n\n                if (grp == null)\n                    top = clientTops.get(grpId);\n                else if (!grp.isLocal())\n                    top = grp.topology();\n\n                if (top != null) {\n                    updated |= top.update(null, entry.getValue(), false);\n\n                    cctx.affinity().checkRebalanceState(top, grpId);\n                }\n            }\n\n            if (updated) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Partitions have been scheduled to resend [reason=Single update from \" + node.id() + \"]\");\n\n                scheduleResendPartitions();\n            }\n        }\n        else {\n            GridDhtPartitionsExchangeFuture exchFut = exchangeFuture(msg.exchangeId());\n\n            if (log.isTraceEnabled())\n                log.trace(\"Notifying exchange future about single message: \" + exchFut);\n\n            if (msg.client()) {\n                AffinityTopologyVersion initVer = exchFut.initialVersion();\n                AffinityTopologyVersion readyVer = readyAffinityVersion();\n\n                if (initVer.compareTo(readyVer) < 0 && !exchFut.isDone()) {\n                    U.warn(log, \"Client node tries to connect but its exchange \" +\n                        \"info is cleaned up from exchange history. \" +\n                        \"Consider increasing 'IGNITE_EXCHANGE_HISTORY_SIZE' property \" +\n                        \"or start clients in  smaller batches. \" +\n                        \"Current settings and versions: \" +\n                        \"[IGNITE_EXCHANGE_HISTORY_SIZE=\" + EXCHANGE_HISTORY_SIZE + \", \" +\n                        \"initVer=\" + initVer + \", \" +\n                        \"readyVer=\" + readyVer + \"].\"\n                    );\n\n                    exchFut.forceClientReconnect(node, msg);\n\n                    return;\n                }\n            }\n\n            exchFut.onReceiveSingleMessage(node, msg);\n        }\n    }\n    finally {\n        leaveBusy();\n    }\n}",
        "reject_response": "private void processSinglePartitionUpdate(final ClusterNode node, final GridDhtPartitionsSingleMessage msg) {\n    if (!enterBusy())\n        return;\n\n    try {\n        if (msg.exchangeId() == null) {\n            if (log.isTraceEnabled())\n                log.trace(\"Received local partition update [nodeId=\" + node.id() + \", parts=\" +\n                    msg + ']');\n\n            boolean updated = false;\n\n            for (Map.Entry<Integer, GridDhtPartitionMap> entry : msg.partitions().entrySet()) {\n                Integer grpId = entry.getKey();\n\n                CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                if (grp != null &&\n                    grp.localStartVersion().compareTo(entry.getValue().topologyVersion()) > 0)\n                    continue;\n\n                GridDhtPartitionTopology top = null;\n\n                if (grp == null)\n                    top = clientTops.get(grpId);\n                else if (!grp.isLocal())\n                    top = grp.topology();\n\n                if (top != null) {\n                    updated |= top.update(null, entry.getValue(), false);\n\n                    cctx.affinity().checkRebalanceState(top, grpId);\n                }\n            }\n\n            if (updated) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Partitions have been scheduled to resend [reason=Single update from \" + node.id() + \"]\");\n\n                scheduleResendPartitions();\n            }\n        }\n        else {\n            GridDhtPartitionsExchangeFuture exchFut = exchangeFuture(msg.exchangeId());\n\n            if (log.isTraceEnabled())\n            if (log.isDebugEnabled())\n                log.debug(\"Notifying exchange future about single message: \" + exchFut);\n\n            if (msg.client()) {\n                AffinityTopologyVersion initVer = exchFut.initialVersion();\n                AffinityTopologyVersion readyVer = readyAffinityVersion();\n\n                if (initVer.compareTo(readyVer) < 0 && !exchFut.isDone()) {\n                    U.warn(log, \"Client node tries to connect but its exchange \" +\n                        \"info is cleaned up from exchange history. \" +\n                        \"Consider increasing 'IGNITE_EXCHANGE_HISTORY_SIZE' property \" +\n                        \"or start clients in  smaller batches. \" +\n                        \"Current settings and versions: \" +\n                        \"[IGNITE_EXCHANGE_HISTORY_SIZE=\" + EXCHANGE_HISTORY_SIZE + \", \" +\n                        \"initVer=\" + initVer + \", \" +\n                        \"readyVer=\" + readyVer + \"].\"\n                    );\n\n                    exchFut.forceClientReconnect(node, msg);\n\n                    return;\n                }\n            }\n\n            exchFut.onReceiveSingleMessage(node, msg);\n        }\n    }\n    finally {\n        leaveBusy();\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2822,
        "instruction": "public void start() {\n    if (getBoolean(IGNITE_JVM_PAUSE_DETECTOR_DISABLED, false)) {\n        if (log.isDebugEnabled())\n            log.debug(\"JVM Pause Detector is disabled.\");\n\n        return;\n    }\n\n    final Thread worker = new Thread(\"jvm-pause-detector-worker\") {\n        private long prev = System.currentTimeMillis();\n\n        @Override public void run() {\n            if (log.isDebugEnabled())\n\n            while (true) {\n                try {\n                    Thread.sleep(PRECISION);\n\n                    final long now = System.currentTimeMillis();\n                    final long pause = now - PRECISION - prev;\n\n                    prev = now;\n\n                    if (pause >= THRESHOLD) {\n                        log.warning(\"Possible too long JVM pause: \" + pause + \" milliseconds.\");\n\n                        synchronized (LongJVMPauseDetector.this) {\n                            final int next = (int)(longPausesCnt % EVT_CNT);\n\n                            longPausesCnt++;\n\n                            longPausesTotalDuration += pause;\n\n                            longPausesTimestamps[next] = now;\n\n                            longPausesDurations[next] = pause;\n                        }\n                    }\n                }\n                catch (InterruptedException e) {\n                    log.error(getName() + \" has been interrupted\", e);\n\n                    break;\n                }\n            }\n        }\n    };\n\n    if (!workerRef.compareAndSet(null, worker)) {\n        log.warning(LongJVMPauseDetector.class.getSimpleName() + \" already started!\");\n\n        return;\n    }\n\n    worker.setDaemon(true);\n    worker.start();\n\n    if (log.isDebugEnabled())\n        log.debug(\"LongJVMPauseDetector was successfully started\");\n}",
        "accept_response": "public void start() {\n    if (getBoolean(IGNITE_JVM_PAUSE_DETECTOR_DISABLED, false)) {\n        if (log.isDebugEnabled())\n            log.debug(\"JVM Pause Detector is disabled.\");\n\n        return;\n    }\n\n    final Thread worker = new Thread(\"jvm-pause-detector-worker\") {\n        private long prev = System.currentTimeMillis();\n\n        @Override public void run() {\n            if (log.isDebugEnabled())\n                log.debug(getName() + \" has been started.\");\n\n            while (true) {\n                try {\n                    Thread.sleep(PRECISION);\n\n                    final long now = System.currentTimeMillis();\n                    final long pause = now - PRECISION - prev;\n\n                    prev = now;\n\n                    if (pause >= THRESHOLD) {\n                        log.warning(\"Possible too long JVM pause: \" + pause + \" milliseconds.\");\n\n                        synchronized (LongJVMPauseDetector.this) {\n                            final int next = (int)(longPausesCnt % EVT_CNT);\n\n                            longPausesCnt++;\n\n                            longPausesTotalDuration += pause;\n\n                            longPausesTimestamps[next] = now;\n\n                            longPausesDurations[next] = pause;\n                        }\n                    }\n                }\n                catch (InterruptedException e) {\n                    log.error(getName() + \" has been interrupted\", e);\n\n                    break;\n                }\n            }\n        }\n    };\n\n    if (!workerRef.compareAndSet(null, worker)) {\n        log.warning(LongJVMPauseDetector.class.getSimpleName() + \" already started!\");\n\n        return;\n    }\n\n    worker.setDaemon(true);\n    worker.start();\n\n    if (log.isDebugEnabled())\n        log.debug(\"LongJVMPauseDetector was successfully started\");\n}",
        "reject_response": "public void start() {\n    if (getBoolean(IGNITE_JVM_PAUSE_DETECTOR_DISABLED, false)) {\n        if (log.isDebugEnabled())\n            log.debug(\"JVM Pause Detector is disabled.\");\n\n        return;\n    }\n\n    final Thread worker = new Thread(\"jvm-pause-detector-worker\") {\n        private long prev = System.currentTimeMillis();\n\n        @Override public void run() {\n            if (log.isDebugEnabled())\n            if (LOG.isDebugEnabled())\n                LOG.debug(getName() + \" has been started.\");\n\n            while (true) {\n                try {\n                    Thread.sleep(PRECISION);\n\n                    final long now = System.currentTimeMillis();\n                    final long pause = now - PRECISION - prev;\n\n                    prev = now;\n\n                    if (pause >= THRESHOLD) {\n                        log.warning(\"Possible too long JVM pause: \" + pause + \" milliseconds.\");\n\n                        synchronized (LongJVMPauseDetector.this) {\n                            final int next = (int)(longPausesCnt % EVT_CNT);\n\n                            longPausesCnt++;\n\n                            longPausesTotalDuration += pause;\n\n                            longPausesTimestamps[next] = now;\n\n                            longPausesDurations[next] = pause;\n                        }\n                    }\n                }\n                catch (InterruptedException e) {\n                    log.error(getName() + \" has been interrupted\", e);\n\n                    break;\n                }\n            }\n        }\n    };\n\n    if (!workerRef.compareAndSet(null, worker)) {\n        log.warning(LongJVMPauseDetector.class.getSimpleName() + \" already started!\");\n\n        return;\n    }\n\n    worker.setDaemon(true);\n    worker.start();\n\n    if (log.isDebugEnabled())\n        log.debug(\"LongJVMPauseDetector was successfully started\");\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2483,
        "instruction": "private void addExposedTypes(Field field, Class<?> cause) {\n  addExposedTypes(field.getGenericType(), cause);\n  for (Annotation annotation : field.getDeclaredAnnotations()) {\n    addExposedTypes(annotation.annotationType(), cause);\n  }\n}",
        "accept_response": "private void addExposedTypes(Field field, Class<?> cause) {\n  addExposedTypes(field.getGenericType(), cause);\n  for (Annotation annotation : field.getDeclaredAnnotations()) {\n    LOG.debug(\n        \"Adding exposed types from {}, which is an annotation on field {}\", annotation, field);\n    addExposedTypes(annotation.annotationType(), cause);\n  }\n}",
        "reject_response": "private void addExposedTypes(Field field, Class<?> cause) {\n  addExposedTypes(field.getGenericType(), cause);\n  for (Annotation annotation : field.getDeclaredAnnotations()) {\n    logger.debug(\n    addExposedTypes(annotation.annotationType(), cause);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3039,
        "instruction": "@Activate\npublic void activate(TopicsConfiguration config) {\n    this.packageTopic = config.packageTopic();\n    this.discoveryTopic = config.discoveryTopic();\n    this.statusTopic = config.statusTopic();\n    this.commandTopic = config.commandTopic();\n    this.eventTopic = config.eventTopic();\n}",
        "accept_response": "@Activate\npublic void activate(TopicsConfiguration config) {\n    this.packageTopic = config.packageTopic();\n    this.discoveryTopic = config.discoveryTopic();\n    this.statusTopic = config.statusTopic();\n    this.commandTopic = config.commandTopic();\n    this.eventTopic = config.eventTopic();\n    LOG.info(String.format(\"Topics service started with packageTopic '%s' discoveryTopic '%s' statusTopic '%s' eventTopic '%s' commandTopic '%s'\",\n            packageTopic, discoveryTopic, statusTopic, eventTopic, commandTopic));\n}",
        "reject_response": "@Activate\npublic void activate(TopicsConfiguration config) {\n    this.packageTopic = config.packageTopic();\n    this.discoveryTopic = config.discoveryTopic();\n    this.statusTopic = config.statusTopic();\n    this.commandTopic = config.commandTopic();\n    this.eventTopic = config.eventTopic();\n    LOG.info(String.format(\"Topics service started with packageTopic '%s' discoveryTopic '%s' statusTopic '%s' eventTopic '%s'\",\n            packageTopic, discoveryTopic, statusTopic, eventTopic));\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2631,
        "instruction": "private void updateClusterStatement(int iNum) {\n\n  Cluster.Statement statement = new Cluster.Statement();\n  Random randomGenerator = new Random();\n  String statementDefinition = \"select * from member where member_name = member-\" + iNum\n      + \" and lastUpdatedTime = '\" + new Date().toString() + \"'\";\n  Integer intVal = randomGenerator.nextInt(5);\n  statement.setQueryDefinition(statementDefinition);\n  statement.setNumTimesCompiled(intVal.longValue());\n  statement.setNumExecution(intVal.longValue());\n  statement.setNumExecutionsInProgress(intVal.longValue());\n  statement.setNumTimesGlobalIndexLookup(intVal.longValue());\n  statement.setNumRowsModified(intVal.longValue());\n  statement.setParseTime(randomGenerator.nextLong());\n  statement.setBindTime(randomGenerator.nextLong());\n  statement.setOptimizeTime(randomGenerator.nextLong());\n  statement.setRoutingInfoTime(randomGenerator.nextLong());\n  statement.setGenerateTime(randomGenerator.nextLong());\n  statement.setTotalCompilationTime(randomGenerator.nextLong());\n  statement.setExecutionTime(randomGenerator.nextLong());\n  statement.setProjectionTime(randomGenerator.nextLong());\n  statement.setTotalExecutionTime(randomGenerator.nextLong());\n  statement.setRowsModificationTime(randomGenerator.nextLong());\n  statement.setqNNumRowsSeen(intVal.longValue());\n  statement.setqNMsgSendTime(randomGenerator.nextLong());\n  statement.setqNMsgSerTime(randomGenerator.nextLong());\n  statement.setqNRespDeSerTime(randomGenerator.nextLong());\n  addClusterStatement(statementDefinition, statement);\n\n}",
        "accept_response": "private void updateClusterStatement(int iNum) {\n\n  Cluster.Statement statement = new Cluster.Statement();\n  Random randomGenerator = new Random();\n  String statementDefinition = \"select * from member where member_name = member-\" + iNum\n      + \" and lastUpdatedTime = '\" + new Date().toString() + \"'\";\n  Integer intVal = randomGenerator.nextInt(5);\n  statement.setQueryDefinition(statementDefinition);\n  statement.setNumTimesCompiled(intVal.longValue());\n  statement.setNumExecution(intVal.longValue());\n  statement.setNumExecutionsInProgress(intVal.longValue());\n  statement.setNumTimesGlobalIndexLookup(intVal.longValue());\n  statement.setNumRowsModified(intVal.longValue());\n  statement.setParseTime(randomGenerator.nextLong());\n  statement.setBindTime(randomGenerator.nextLong());\n  statement.setOptimizeTime(randomGenerator.nextLong());\n  statement.setRoutingInfoTime(randomGenerator.nextLong());\n  statement.setGenerateTime(randomGenerator.nextLong());\n  statement.setTotalCompilationTime(randomGenerator.nextLong());\n  statement.setExecutionTime(randomGenerator.nextLong());\n  statement.setProjectionTime(randomGenerator.nextLong());\n  statement.setTotalExecutionTime(randomGenerator.nextLong());\n  statement.setRowsModificationTime(randomGenerator.nextLong());\n  statement.setqNNumRowsSeen(intVal.longValue());\n  statement.setqNMsgSendTime(randomGenerator.nextLong());\n  statement.setqNMsgSerTime(randomGenerator.nextLong());\n  statement.setqNRespDeSerTime(randomGenerator.nextLong());\n  addClusterStatement(statementDefinition, statement);\n\n  logger.info(\"statementDefinition [{}]{}\", iNum, statementDefinition);\n}",
        "reject_response": "private void updateClusterStatement(int iNum) {\n\n  Cluster.Statement statement = new Cluster.Statement();\n  Random randomGenerator = new Random();\n  String statementDefinition = \"select * from member where member_name = member-\" + iNum\n      + \" and lastUpdatedTime = '\" + new Date().toString() + \"'\";\n  Integer intVal = randomGenerator.nextInt(5);\n  statement.setQueryDefinition(statementDefinition);\n  statement.setNumTimesCompiled(intVal.longValue());\n  statement.setNumExecution(intVal.longValue());\n  statement.setNumExecutionsInProgress(intVal.longValue());\n  statement.setNumTimesGlobalIndexLookup(intVal.longValue());\n  statement.setNumRowsModified(intVal.longValue());\n  statement.setParseTime(randomGenerator.nextLong());\n  statement.setBindTime(randomGenerator.nextLong());\n  statement.setOptimizeTime(randomGenerator.nextLong());\n  statement.setRoutingInfoTime(randomGenerator.nextLong());\n  statement.setGenerateTime(randomGenerator.nextLong());\n  statement.setTotalCompilationTime(randomGenerator.nextLong());\n  statement.setExecutionTime(randomGenerator.nextLong());\n  statement.setProjectionTime(randomGenerator.nextLong());\n  statement.setTotalExecutionTime(randomGenerator.nextLong());\n  statement.setRowsModificationTime(randomGenerator.nextLong());\n  statement.setqNNumRowsSeen(intVal.longValue());\n  statement.setqNMsgSendTime(randomGenerator.nextLong());\n  statement.setqNMsgSerTime(randomGenerator.nextLong());\n  statement.setqNRespDeSerTime(randomGenerator.nextLong());\n  addClusterStatement(statementDefinition, statement);\n\n  if (LOGGER.infoEnabled()) {\n    LOGGER.info(\"statementDefinition [\" + iNum + \"]\" + statementDefinition);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2459,
        "instruction": "protected <T> T serialClone(Object object) throws Exception {\n   ByteArrayOutputStream bout = new ByteArrayOutputStream();\n   ObjectOutputStream obOut = new ObjectOutputStream(bout);\n   obOut.writeObject(object);\n\n   ByteArrayInputStream binput = new ByteArrayInputStream(bout.toByteArray());\n   ObjectInputStream obinp = new ObjectInputStream(binput);\n   return (T) obinp.readObject();\n\n}",
        "accept_response": "protected <T> T serialClone(Object object) throws Exception {\n   logger.debug(\"object::{}\", object);\n   ByteArrayOutputStream bout = new ByteArrayOutputStream();\n   ObjectOutputStream obOut = new ObjectOutputStream(bout);\n   obOut.writeObject(object);\n\n   ByteArrayInputStream binput = new ByteArrayInputStream(bout.toByteArray());\n   ObjectInputStream obinp = new ObjectInputStream(binput);\n   return (T) obinp.readObject();\n\n}",
        "reject_response": "protected <T> T serialClone(Object object) throws Exception {\n   log.debug(\"object::\" + object);\n   ByteArrayOutputStream bout = new ByteArrayOutputStream();\n   ObjectOutputStream obOut = new ObjectOutputStream(bout);\n   obOut.writeObject(object);\n\n   ByteArrayInputStream binput = new ByteArrayInputStream(bout.toByteArray());\n   ObjectInputStream obinp = new ObjectInputStream(binput);\n   return (T) obinp.readObject();\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2881,
        "instruction": "private /*public*/ static Node decode(String s, PrefixMapping pmap)\n{\n    if ( s.startsWith(\"_:\") )\n    {\n        s = s.substring(2) ;\n        return NodeFactory.createBlankNode(s) ;\n    }\n\n    if ( s.startsWith(\"<\") )\n    {\n        s = s.substring(1,s.length()-1) ;\n        s = StrUtils.decodeHex(s, MarkerChar) ;\n        return NodeFactory.createURI(s) ;\n    }\n\n    try {\n        // SSE invocation is expensive (??).\n        // Try TokenizerText?\n        // Use for literals only.\n        Node n = SSE.parseNode(s, pmap) ;\n        return n ;\n    } catch (SSEParseException ex)\n    {\n        throw ex ;\n    }\n}",
        "accept_response": "private /*public*/ static Node decode(String s, PrefixMapping pmap)\n{\n    if ( s.startsWith(\"_:\") )\n    {\n        s = s.substring(2) ;\n        return NodeFactory.createBlankNode(s) ;\n    }\n\n    if ( s.startsWith(\"<\") )\n    {\n        s = s.substring(1,s.length()-1) ;\n        s = StrUtils.decodeHex(s, MarkerChar) ;\n        return NodeFactory.createURI(s) ;\n    }\n\n    try {\n        // SSE invocation is expensive (??).\n        // Try TokenizerText?\n        // Use for literals only.\n        Node n = SSE.parseNode(s, pmap) ;\n        return n ;\n    } catch (SSEParseException ex)\n    {\n        Log.error(NodeLib.class, \"decode: Failed to parse: \"+s) ;\n        throw ex ;\n    }\n}",
        "reject_response": "private /*public*/ static Node decode(String s, PrefixMapping pmap)\n{\n    if ( s.startsWith(\"_:\") )\n    {\n        s = s.substring(2) ;\n        return NodeFactory.createBlankNode(s) ;\n    }\n\n    if ( s.startsWith(\"<\") )\n    {\n        s = s.substring(1,s.length()-1) ;\n        s = StrUtils.decodeHex(s, MarkerChar) ;\n        return NodeFactory.createURI(s) ;\n    }\n\n    try {\n        // SSE invocation is expensive (??).\n        // Try TokenizerText?\n        // Use for literals only.\n        Node n = SSE.parseNode(s, pmap) ;\n        return n ;\n    } catch (SSEParseException ex)\n    {\n        Log.fatal(NodeLib.class, \"decode: Failed to parse: \"+s) ;\n        throw ex ;\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2914,
        "instruction": "public void flush() {\n    if (!this.stores.isEmpty()) {\n        for (StateStore store : this.stores.values())\n            store.flush();\n    }\n}",
        "accept_response": "public void flush() {\n    if (!this.stores.isEmpty()) {\n        log.debug(\"task [{}] Flushing stores.\", taskId);\n        for (StateStore store : this.stores.values())\n            store.flush();\n    }\n}",
        "reject_response": "public void flush() {\n    if (!this.stores.isEmpty()) {\n        log.debug(\"Flushing stores.\");\n        for (StateStore store : this.stores.values())\n            store.flush();\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2443,
        "instruction": "private ArrowDictionaryBatch readDictionaryBatch(SeekableReadChannel in,\n                                                 ArrowBlock block,\n                                                 BufferAllocator allocator) throws IOException {\n  in.setPosition(block.getOffset());\n  ArrowDictionaryBatch batch = MessageSerializer.deserializeDictionaryBatch(in, block, allocator);\n  if (batch == null) {\n    throw new IOException(\"Invalid file. No batch at offset: \" + block.getOffset());\n  }\n  return batch;\n}",
        "accept_response": "private ArrowDictionaryBatch readDictionaryBatch(SeekableReadChannel in,\n                                                 ArrowBlock block,\n                                                 BufferAllocator allocator) throws IOException {\n  LOGGER.debug(\"DictionaryRecordBatch at {}, metadata: {}, body: {}\",\n      block.getOffset(), block.getMetadataLength(), block.getBodyLength());\n  in.setPosition(block.getOffset());\n  ArrowDictionaryBatch batch = MessageSerializer.deserializeDictionaryBatch(in, block, allocator);\n  if (batch == null) {\n    throw new IOException(\"Invalid file. No batch at offset: \" + block.getOffset());\n  }\n  return batch;\n}",
        "reject_response": "private ArrowDictionaryBatch readDictionaryBatch(SeekableReadChannel in,\n                                                 ArrowBlock block,\n                                                 BufferAllocator allocator) throws IOException {\n  LOGGER.debug(String.format(\"DictionaryRecordBatch at %d, metadata: %d, body: %d\",\n      block.getOffset(), block.getMetadataLength(), block.getBodyLength()));\n  in.setPosition(block.getOffset());\n  ArrowDictionaryBatch batch = MessageSerializer.deserializeDictionaryBatch(in, block, allocator);\n  if (batch == null) {\n    throw new IOException(\"Invalid file. No batch at offset: \" + block.getOffset());\n  }\n  return batch;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2892,
        "instruction": "public Node parse(InputStream inputStream) throws ParseException {\n    try {\n        final SimpleNode node = new SieveParser(inputStream, \"UTF-8\")\n                .start();\n        SieveValidationVisitor visitor = new SieveValidationVisitor(\n                commandManager, testManager, comparatorManager);\n        node.jjtAccept(visitor, null);\n        return node;\n    } catch (ParseException ex) {\n        LOGGER.error(\"Parse failed.\", ex);\n        throw ex;\n    } catch (SieveException ex) {\n        throw new ParseException(ex.getMessage());\n    }\n}",
        "accept_response": "public Node parse(InputStream inputStream) throws ParseException {\n    try {\n        final SimpleNode node = new SieveParser(inputStream, \"UTF-8\")\n                .start();\n        SieveValidationVisitor visitor = new SieveValidationVisitor(\n                commandManager, testManager, comparatorManager);\n        node.jjtAccept(visitor, null);\n        return node;\n    } catch (ParseException ex) {\n        LOGGER.error(\"Parse failed.\", ex);\n        throw ex;\n    } catch (SieveException ex) {\n        LOGGER.error(\"Parse failed.\", ex);\n        throw new ParseException(ex.getMessage());\n    }\n}",
        "reject_response": "public Node parse(InputStream inputStream) throws ParseException {\n    try {\n        final SimpleNode node = new SieveParser(inputStream, \"UTF-8\")\n                .start();\n        SieveValidationVisitor visitor = new SieveValidationVisitor(\n                commandManager, testManager, comparatorManager);\n        node.jjtAccept(visitor, null);\n        return node;\n    } catch (ParseException ex) {\n        LOGGER.error(\"Parse failed.\", ex);\n        throw ex;\n    } catch (SieveException ex) {\n        if (log.isErrorEnabled())\n            log.error(\"Parse failed. Reason: \" + ex.getMessage());\n        if (log.isDebugEnabled())\n            log.debug(\"Parse failed.\", ex);\n        throw new ParseException(ex.getMessage());\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3207,
        "instruction": "private void createGatewayServerAndStartScript() throws UnknownHostException {\n  createPythonScript();\n  if (System.getenv(\"ZEPPELIN_HOME\") != null) {\n    py4jLibPath = System.getenv(\"ZEPPELIN_HOME\") + File.separator + ZEPPELIN_PY4JPATH;\n    pythonLibPath = System.getenv(\"ZEPPELIN_HOME\") + File.separator + ZEPPELIN_PYTHON_LIBS;\n  } else {\n    Path workingPath = Paths.get(\"..\").toAbsolutePath();\n    py4jLibPath = workingPath + File.separator + ZEPPELIN_PY4JPATH;\n    pythonLibPath = workingPath + File.separator + ZEPPELIN_PYTHON_LIBS;\n  }\n\n  port = findRandomOpenPortOnAllLocalInterfaces();\n  gatewayServer = new GatewayServer(this,\n      port,\n      GatewayServer.DEFAULT_PYTHON_PORT,\n      InetAddress.getByName(\"0.0.0.0\"),\n      InetAddress.getByName(\"0.0.0.0\"),\n      GatewayServer.DEFAULT_CONNECT_TIMEOUT,\n      GatewayServer.DEFAULT_READ_TIMEOUT,\n      (List) null);\n\n  gatewayServer.start();\n\n  // Run python shell\n  String pythonCmd = getPythonCommand();\n  CommandLine cmd = CommandLine.parse(pythonCmd);\n\n  if (!pythonCmd.endsWith(\".py\")) {\n    // PythonDockerInterpreter set pythoncmd with script\n    cmd.addArgument(getScriptPath(), false);\n  }\n  cmd.addArgument(Integer.toString(port), false);\n  cmd.addArgument(getLocalIp(), false);\n\n  executor = new DefaultExecutor();\n  outputStream = new InterpreterOutputStream(logger);\n  PipedOutputStream ps = new PipedOutputStream();\n  in = null;\n  try {\n    in = new PipedInputStream(ps);\n  } catch (IOException e1) {\n    throw new InterpreterException(e1);\n  }\n  ins = new BufferedWriter(new OutputStreamWriter(ps));\n  input = new ByteArrayOutputStream();\n\n  PumpStreamHandler streamHandler = new PumpStreamHandler(outputStream, outputStream, in);\n  executor.setStreamHandler(streamHandler);\n  executor.setWatchdog(new ExecuteWatchdog(ExecuteWatchdog.INFINITE_TIMEOUT));\n\n  try {\n    Map env = EnvironmentUtils.getProcEnvironment();\n    if (!env.containsKey(\"PYTHONPATH\")) {\n      env.put(\"PYTHONPATH\", py4jLibPath + File.pathSeparator + pythonLibPath);\n    } else {\n      env.put(\"PYTHONPATH\", env.get(\"PYTHONPATH\") + File.pathSeparator +\n              py4jLibPath + File.pathSeparator + pythonLibPath);\n    }\n\n    executor.execute(cmd, env, this);\n    pythonscriptRunning = true;\n  } catch (IOException e) {\n    throw new InterpreterException(e);\n  }\n\n  try {\n    input.write(\"import sys, getopt\\n\".getBytes());\n    ins.flush();\n  } catch (IOException e) {\n    throw new InterpreterException(e);\n  }\n}",
        "accept_response": "private void createGatewayServerAndStartScript() throws UnknownHostException {\n  createPythonScript();\n  if (System.getenv(\"ZEPPELIN_HOME\") != null) {\n    py4jLibPath = System.getenv(\"ZEPPELIN_HOME\") + File.separator + ZEPPELIN_PY4JPATH;\n    pythonLibPath = System.getenv(\"ZEPPELIN_HOME\") + File.separator + ZEPPELIN_PYTHON_LIBS;\n  } else {\n    Path workingPath = Paths.get(\"..\").toAbsolutePath();\n    py4jLibPath = workingPath + File.separator + ZEPPELIN_PY4JPATH;\n    pythonLibPath = workingPath + File.separator + ZEPPELIN_PYTHON_LIBS;\n  }\n\n  port = findRandomOpenPortOnAllLocalInterfaces();\n  gatewayServer = new GatewayServer(this,\n      port,\n      GatewayServer.DEFAULT_PYTHON_PORT,\n      InetAddress.getByName(\"0.0.0.0\"),\n      InetAddress.getByName(\"0.0.0.0\"),\n      GatewayServer.DEFAULT_CONNECT_TIMEOUT,\n      GatewayServer.DEFAULT_READ_TIMEOUT,\n      (List) null);\n\n  gatewayServer.start();\n\n  // Run python shell\n  String pythonCmd = getPythonCommand();\n  CommandLine cmd = CommandLine.parse(pythonCmd);\n\n  if (!pythonCmd.endsWith(\".py\")) {\n    // PythonDockerInterpreter set pythoncmd with script\n    cmd.addArgument(getScriptPath(), false);\n  }\n  cmd.addArgument(Integer.toString(port), false);\n  cmd.addArgument(getLocalIp(), false);\n\n  executor = new DefaultExecutor();\n  outputStream = new InterpreterOutputStream(logger);\n  PipedOutputStream ps = new PipedOutputStream();\n  in = null;\n  try {\n    in = new PipedInputStream(ps);\n  } catch (IOException e1) {\n    throw new InterpreterException(e1);\n  }\n  ins = new BufferedWriter(new OutputStreamWriter(ps));\n  input = new ByteArrayOutputStream();\n\n  PumpStreamHandler streamHandler = new PumpStreamHandler(outputStream, outputStream, in);\n  executor.setStreamHandler(streamHandler);\n  executor.setWatchdog(new ExecuteWatchdog(ExecuteWatchdog.INFINITE_TIMEOUT));\n\n  try {\n    Map env = EnvironmentUtils.getProcEnvironment();\n    if (!env.containsKey(\"PYTHONPATH\")) {\n      env.put(\"PYTHONPATH\", py4jLibPath + File.pathSeparator + pythonLibPath);\n    } else {\n      env.put(\"PYTHONPATH\", env.get(\"PYTHONPATH\") + File.pathSeparator +\n              py4jLibPath + File.pathSeparator + pythonLibPath);\n    }\n\n    logger.info(\"cmd = {}\", cmd.toString());\n    executor.execute(cmd, env, this);\n    pythonscriptRunning = true;\n  } catch (IOException e) {\n    throw new InterpreterException(e);\n  }\n\n  try {\n    input.write(\"import sys, getopt\\n\".getBytes());\n    ins.flush();\n  } catch (IOException e) {\n    throw new InterpreterException(e);\n  }\n}",
        "reject_response": "private void createGatewayServerAndStartScript() throws UnknownHostException {\n  createPythonScript();\n  if (System.getenv(\"ZEPPELIN_HOME\") != null) {\n    py4jLibPath = System.getenv(\"ZEPPELIN_HOME\") + File.separator + ZEPPELIN_PY4JPATH;\n    pythonLibPath = System.getenv(\"ZEPPELIN_HOME\") + File.separator + ZEPPELIN_PYTHON_LIBS;\n  } else {\n    Path workingPath = Paths.get(\"..\").toAbsolutePath();\n    py4jLibPath = workingPath + File.separator + ZEPPELIN_PY4JPATH;\n    pythonLibPath = workingPath + File.separator + ZEPPELIN_PYTHON_LIBS;\n  }\n\n  port = findRandomOpenPortOnAllLocalInterfaces();\n  gatewayServer = new GatewayServer(this,\n      port,\n      GatewayServer.DEFAULT_PYTHON_PORT,\n      InetAddress.getByName(\"0.0.0.0\"),\n      InetAddress.getByName(\"0.0.0.0\"),\n      GatewayServer.DEFAULT_CONNECT_TIMEOUT,\n      GatewayServer.DEFAULT_READ_TIMEOUT,\n      (List) null);\n\n  gatewayServer.start();\n\n  // Run python shell\n  String pythonCmd = getPythonCommand();\n  CommandLine cmd = CommandLine.parse(pythonCmd);\n\n  if (!pythonCmd.endsWith(\".py\")) {\n    // PythonDockerInterpreter set pythoncmd with script\n    cmd.addArgument(getScriptPath(), false);\n  }\n  cmd.addArgument(Integer.toString(port), false);\n  cmd.addArgument(getLocalIp(), false);\n\n  executor = new DefaultExecutor();\n  outputStream = new InterpreterOutputStream(logger);\n  PipedOutputStream ps = new PipedOutputStream();\n  in = null;\n  try {\n    in = new PipedInputStream(ps);\n  } catch (IOException e1) {\n    throw new InterpreterException(e1);\n  }\n  ins = new BufferedWriter(new OutputStreamWriter(ps));\n  input = new ByteArrayOutputStream();\n\n  PumpStreamHandler streamHandler = new PumpStreamHandler(outputStream, outputStream, in);\n  executor.setStreamHandler(streamHandler);\n  executor.setWatchdog(new ExecuteWatchdog(ExecuteWatchdog.INFINITE_TIMEOUT));\n\n  try {\n    Map env = EnvironmentUtils.getProcEnvironment();\n    if (!env.containsKey(\"PYTHONPATH\")) {\n      env.put(\"PYTHONPATH\", py4jLibPath + File.pathSeparator + pythonLibPath);\n    } else {\n      env.put(\"PYTHONPATH\", env.get(\"PYTHONPATH\") + File.pathSeparator +\n              py4jLibPath + File.pathSeparator + pythonLibPath);\n    }\n\n    LOG.info(\"Bootstrap interpreter with \" + BOOTSTRAP_PY);\n    bootStrapInterpreter(BOOTSTRAP_PY);\n    executor.execute(cmd, env, this);\n    pythonscriptRunning = true;\n  } catch (IOException e) {\n    throw new InterpreterException(e);\n  }\n\n  try {\n    input.write(\"import sys, getopt\\n\".getBytes());\n    ins.flush();\n  } catch (IOException e) {\n    throw new InterpreterException(e);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3008,
        "instruction": "@Override\n   protected void processProperties(\n    ConfigurableListableBeanFactory beanFactory, Properties props)\n    throws BeansException {\n\n   // First let's add the system properties\nSet<Object> keySet = System.getProperties().keySet();\nfor (Object key : keySet) {\n    String keyStr = key.toString();\n    propertiesMap.put(keyStr, System.getProperties()\n\t    .getProperty(keyStr).trim());\n}\n\n// Let's add our properties now\nkeySet = props.keySet();\nfor (Object key : keySet) {\n    String keyStr = key.toString();\n    propertiesMap.put(keyStr, props.getProperty(keyStr).trim());\n}\n\nString storeType = propertiesMap.get(\"ranger.keystore.file.type\");\n// update system trust store path with custom trust store.\nif (propertiesMap!=null && propertiesMap.containsKey(\"ranger.truststore.file\")) {\n\tif(!StringUtils.isEmpty(propertiesMap.get(\"ranger.truststore.file\"))){\n\t\tSystem.setProperty(\"javax.net.ssl.trustStore\", propertiesMap.get(\"ranger.truststore.file\"));\n\t\tSystem.setProperty(\"javax.net.ssl.trustStoreType\", KeyStore.getDefaultType());\n\t\tPath trustStoreFile = Paths.get(propertiesMap.get(\"ranger.truststore.file\"));\n\t\tif (!Files.exists(trustStoreFile) || !Files.isReadable(trustStoreFile)) {\n\t\t\tlogger.debug(\"Could not find or read truststore file '\"+propertiesMap.get(\"ranger.truststore.file\")+\"'\");\n\t\t}else{\n\t\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\")){\n\t\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\t\tString trustStoreAlias=getProperty(\"ranger.truststore.alias\",\"trustStoreAlias\");\n\t\t\t\tif(path!=null && trustStoreAlias!=null){\n\t\t\t\t\tString trustStorePassword=CredentialReader.getDecryptedString(path.trim(), trustStoreAlias.trim(), storeType);\n\t\t\t\t\tif(trustStorePassword!=null&& !trustStorePassword.trim().isEmpty() && !trustStorePassword.trim().equalsIgnoreCase(\"none\")){\n\t\t\t\t\t\tpropertiesMap.put(\"ranger.truststore.password\", trustStorePassword);\n\t\t\t\t\t\tprops.put(\"ranger.truststore.password\", trustStorePassword);\n\t\t\t\t\t}else{\n\t\t\t\t\t\tlogger.info(\"trustStorePassword password not applied; clear text password shall be applicable\");\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tSystem.setProperty(\"javax.net.ssl.trustStorePassword\", propertiesMap.get(\"ranger.truststore.password\"));\n}\n\n// update system key store path with custom key store.\nif (propertiesMap!=null && propertiesMap.containsKey(\"ranger.keystore.file\")) {\n\tif(!StringUtils.isEmpty(propertiesMap.get(\"ranger.keystore.file\"))){\n\t\tSystem.setProperty(\"javax.net.ssl.keyStore\", propertiesMap.get(\"ranger.keystore.file\"));\n\t\tSystem.setProperty(\"javax.net.ssl.keyStoreType\", KeyStore.getDefaultType());\n\t\tPath keyStoreFile = Paths.get(propertiesMap.get(\"ranger.keystore.file\"));\n\t\tif (!Files.exists(keyStoreFile) || !Files.isReadable(keyStoreFile)) {\n\t\t\tlogger.debug(\"Could not find or read keystore file '\"+propertiesMap.get(\"ranger.keystore.file\")+\"'\");\n\t\t}else{\n\t\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\")){\n\t\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\t\tString keyStoreAlias=getProperty(\"ranger.keystore.alias\",\"keyStoreAlias\");\n\t\t\t\tif(path!=null && keyStoreAlias!=null){\n\t\t\t\t\tString keyStorePassword=CredentialReader.getDecryptedString(path.trim(), keyStoreAlias.trim(), storeType);\n\t\t\t\t\tif(keyStorePassword!=null&& !keyStorePassword.trim().isEmpty() && !keyStorePassword.trim().equalsIgnoreCase(\"none\")){\n\t\t\t\t\t\tpropertiesMap.put(\"ranger.keystore.password\", keyStorePassword);\n\t\t\t\t\t\tprops.put(\"ranger.keystore.password\", keyStorePassword);\n\t\t\t\t\t}else{\n\t\t\t\t\t\tlogger.info(\"keyStorePassword password not applied; clear text password shall be applicable\");\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tSystem.setProperty(\"javax.net.ssl.keyStorePassword\", propertiesMap.get(\"ranger.keystore.password\"));\n}\n\n//update unixauth keystore and truststore credentials\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\")){\n\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\tif(path!=null){\n\t\tString unixAuthKeyStoreAlias=getProperty(\"ranger.unixauth.keystore.alias\",\"unixAuthKeyStoreAlias\");\n\t\tif(unixAuthKeyStoreAlias!=null){\n\t\t\tString unixAuthKeyStorePass=CredentialReader.getDecryptedString(path.trim(),unixAuthKeyStoreAlias.trim(), storeType);\n\t\t\tif(unixAuthKeyStorePass!=null&& !unixAuthKeyStorePass.trim().isEmpty() &&!unixAuthKeyStorePass.trim().equalsIgnoreCase(\"none\")){\n\t\t\t\tpropertiesMap.put(\"ranger.unixauth.keystore.password\", unixAuthKeyStorePass);\n\t\t\t\tprops.put(\"ranger.unixauth.keystore.password\", unixAuthKeyStorePass);\n\t\t\t}else{\n\t\t\t\tlogger.info(\"unixauth keystore password not applied; clear text password shall be applicable\");\n\t\t\t}\n\t\t}\n\t\t//\n\t\tString unixAuthTrustStoreAlias=getProperty(\"ranger.unixauth.truststore.alias\",\"unixAuthTrustStoreAlias\");\n\t\tif(unixAuthTrustStoreAlias!=null){\n\t\t\tString unixAuthTrustStorePass=CredentialReader.getDecryptedString(path.trim(),unixAuthTrustStoreAlias.trim(), storeType);\n\t\t\tif(unixAuthTrustStorePass!=null&& !unixAuthTrustStorePass.trim().isEmpty() &&!unixAuthTrustStorePass.trim().equalsIgnoreCase(\"none\")){\n\t\t\t\tpropertiesMap.put(\"ranger.unixauth.truststore.password\", unixAuthTrustStorePass);\n\t\t\t\tprops.put(\"ranger.unixauth.truststore.password\", unixAuthTrustStorePass);\n\t\t\t}else{\n\t\t\t\tlogger.info(\"unixauth truststore password not applied; clear text password shall be applicable\");\n\t\t\t}\n\t\t}\n\t}\n}\n\n//update credential from keystore\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.jpa.jdbc.credential.alias\")){\n\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\tString alias=propertiesMap.get(\"ranger.jpa.jdbc.credential.alias\");\n\tif(path!=null && alias!=null){\n\t\tString xaDBPassword=CredentialReader.getDecryptedString(path.trim(),alias.trim(), storeType);\n\t\tif(xaDBPassword!=null&& !xaDBPassword.trim().isEmpty() &&\n\t\t\t\t!\"none\".equalsIgnoreCase(xaDBPassword.trim())){\n\t\t\tpropertiesMap.put(\"ranger.jpa.jdbc.password\", xaDBPassword);\n\t\t\tprops.put(\"ranger.jpa.jdbc.password\", xaDBPassword);\n\t\t}else{\n\t\t\tlogger.info(\"Credential keystore password not applied for Ranger DB; clear text password shall be applicable\");\n\t\t}\n\t}\n}\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.jpa.audit.jdbc.credential.alias\")){\n\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\tString alias=propertiesMap.get(\"ranger.jpa.audit.jdbc.credential.alias\");\n\tif(path!=null && alias!=null){\n\t\tString auditDBPassword=CredentialReader.getDecryptedString(path.trim(), alias.trim(), storeType);\n\t\tif(auditDBPassword!=null&& !auditDBPassword.trim().isEmpty() &&\n\t\t\t\t!\"none\".equalsIgnoreCase(auditDBPassword.trim())){\n\t\t\tpropertiesMap.put(\"ranger.jpa.audit.jdbc.password\", auditDBPassword);\n\t\t\tprops.put(\"ranger.jpa.audit.jdbc.password\", auditDBPassword);\n\t\t}else{\n\t\t\tlogger.info(\"Credential keystore password not applied for Audit DB; clear text password shall be applicable\");\n\t\t}\n\t}\n}\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.authentication.method\")){\n\tString authenticationMethod=propertiesMap.get(\"ranger.authentication.method\");\n\tif(authenticationMethod!=null && (\"ACTIVE_DIRECTORY\".equalsIgnoreCase(authenticationMethod)||\"AD\".equalsIgnoreCase(authenticationMethod))){\n\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.ldap.ad.binddn.credential.alias\")){\n\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\tString alias=propertiesMap.get(\"ranger.ldap.ad.binddn.credential.alias\");\n\t\t\tif(path!=null && alias!=null){\n\t\t\t\tString bindDNPassword=CredentialReader.getDecryptedString(path.trim(), alias.trim(), storeType);\n\t\t\t\tif(bindDNPassword!=null&& !bindDNPassword.trim().isEmpty() &&\n\t\t\t\t\t\t!\"none\".equalsIgnoreCase(bindDNPassword.trim())){\n\t\t\t\t\tpropertiesMap.put(\"ranger.ldap.ad.bind.password\", bindDNPassword);\n\t\t\t\t\tprops.put(\"ranger.ldap.ad.bind.password\", bindDNPassword);\n\t\t\t\t}else{\n\t\t\t\t\tlogger.info(\"Credential keystore password not applied for AD Bind DN; clear text password shall be applicable\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.authentication.method\")){\n\tString authenticationMethod=propertiesMap.get(\"ranger.authentication.method\");\n\tif(authenticationMethod!=null && (\"LDAP\".equalsIgnoreCase(authenticationMethod))){\n\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.ldap.binddn.credential.alias\")){\n\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\tString alias=propertiesMap.get(\"ranger.ldap.binddn.credential.alias\");\n\t\t\tif(path!=null && alias!=null){\n\t\t\t\tString bindDNPassword=CredentialReader.getDecryptedString(path.trim(), alias.trim(), storeType);\n\t\t\t\tif(bindDNPassword!=null&& !bindDNPassword.trim().isEmpty() &&\n\t\t\t\t\t\t!\"none\".equalsIgnoreCase(bindDNPassword.trim())){\n\t\t\t\t\tpropertiesMap.put(\"ranger.ldap.bind.password\", bindDNPassword);\n\t\t\t\t\tprops.put(\"ranger.ldap.bind.password\", bindDNPassword);\n\t\t\t\t}else{\n\t\t\t\t\tlogger.info(\"Credential keystore password not applied for LDAP Bind DN; clear text password shall be applicable\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.audit.source.type\")){\n\tString auditStore=propertiesMap.get(\"ranger.audit.source.type\");\n\tif(auditStore!=null && (\"solr\".equalsIgnoreCase(auditStore))){\n\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.solr.audit.credential.alias\")){\n\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\tString alias=propertiesMap.get(\"ranger.solr.audit.credential.alias\");\n\t\t\tif(path!=null && alias!=null){\n\t\t\t\tString solrAuditPassword=CredentialReader.getDecryptedString(path.trim(), alias.trim(), storeType);\n\t\t\t\tif(solrAuditPassword!=null&& !solrAuditPassword.trim().isEmpty() &&\n\t\t\t\t\t\t!\"none\".equalsIgnoreCase(solrAuditPassword.trim())){\n\t\t\t\t\tpropertiesMap.put(\"ranger.solr.audit.user.password\", solrAuditPassword);\n\t\t\t\t\tprops.put(\"ranger.solr.audit.user.password\", solrAuditPassword);\n\t\t\t\t}else{\n\t\t\t\t\tlogger.info(\"Credential keystore password not applied for Solr; clear text password shall be applicable\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\nif(propertiesMap!=null){\n\tString sha256PasswordUpdateDisable=\"false\";\n\tif(propertiesMap.containsKey(\"ranger.sha256Password.update.disable\")){\n\t\tsha256PasswordUpdateDisable=propertiesMap.get(\"ranger.sha256Password.update.disable\");\n\t\tif(sha256PasswordUpdateDisable==null || sha256PasswordUpdateDisable.trim().isEmpty()|| !\"true\".equalsIgnoreCase(sha256PasswordUpdateDisable)){\n\t\t\tsha256PasswordUpdateDisable=\"false\";\n\t\t}\n\t}\n\tpropertiesMap.put(\"ranger.sha256Password.update.disable\", sha256PasswordUpdateDisable);\n\tprops.put(\"ranger.sha256Password.update.disable\", sha256PasswordUpdateDisable);\n}\nif(RangerBizUtil.getDBFlavor()==AppConstants.DB_FLAVOR_MYSQL || RangerBizUtil.getDBFlavor()==AppConstants.DB_FLAVOR_POSTGRES){\n\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.db.ssl.enabled\")){\n\t\tString db_ssl_enabled=propertiesMap.get(\"ranger.db.ssl.enabled\");\n\t\tif(StringUtils.isEmpty(db_ssl_enabled)|| !\"true\".equalsIgnoreCase(db_ssl_enabled)){\n\t\t\tdb_ssl_enabled=\"false\";\n\t\t}\n\t\tdb_ssl_enabled=db_ssl_enabled.toLowerCase();\n\t\tif(\"true\".equalsIgnoreCase(db_ssl_enabled)){\n\t\t\tString db_ssl_required=propertiesMap.get(\"ranger.db.ssl.required\");\n\t\t\tif(StringUtils.isEmpty(db_ssl_required)|| !\"true\".equalsIgnoreCase(db_ssl_required)){\n\t\t\t\tdb_ssl_required=\"false\";\n\t\t\t}\n\t\t\tdb_ssl_required=db_ssl_required.toLowerCase();\n\t\t\tString db_ssl_verifyServerCertificate=propertiesMap.get(\"ranger.db.ssl.verifyServerCertificate\");\n\t\t\tif(StringUtils.isEmpty(db_ssl_verifyServerCertificate)|| !\"true\".equalsIgnoreCase(db_ssl_verifyServerCertificate)){\n\t\t\t\tdb_ssl_verifyServerCertificate=\"false\";\n\t\t\t}\n\t\t\tdb_ssl_verifyServerCertificate=db_ssl_verifyServerCertificate.toLowerCase();\n\t\t\tString db_ssl_auth_type=propertiesMap.get(\"ranger.db.ssl.auth.type\");\n\t\t\tif(StringUtils.isEmpty(db_ssl_auth_type)|| !\"1-way\".equalsIgnoreCase(db_ssl_auth_type)){\n\t\t\t\tdb_ssl_auth_type=\"2-way\";\n\t\t\t}\n\t\t\tpropertiesMap.put(\"ranger.db.ssl.enabled\", db_ssl_enabled);\n\t\t\tprops.put(\"ranger.db.ssl.enabled\", db_ssl_enabled);\n\t\t\tpropertiesMap.put(\"ranger.db.ssl.required\", db_ssl_required);\n\t\t\tprops.put(\"ranger.db.ssl.required\", db_ssl_required);\n\t\t\tpropertiesMap.put(\"ranger.db.ssl.verifyServerCertificate\", db_ssl_verifyServerCertificate);\n\t\t\tprops.put(\"ranger.db.ssl.verifyServerCertificate\", db_ssl_verifyServerCertificate);\n\t\t\tpropertiesMap.put(\"ranger.db.ssl.auth.type\", db_ssl_auth_type);\n\t\t\tprops.put(\"ranger.db.ssl.auth.type\", db_ssl_auth_type);\n\t\t\tString ranger_jpa_jdbc_url=propertiesMap.get(\"ranger.jpa.jdbc.url\");\n\t\t\tif(StringUtils.isNotEmpty(ranger_jpa_jdbc_url) && !ranger_jpa_jdbc_url.contains(\"?\")){\n\t\t\t\tStringBuffer ranger_jpa_jdbc_url_ssl=new StringBuffer(ranger_jpa_jdbc_url);\n\t\t\t\tif (RangerBizUtil.getDBFlavor()==AppConstants.DB_FLAVOR_MYSQL) {\n\t\t\t\t\tranger_jpa_jdbc_url_ssl.append(\"?useSSL=\"+db_ssl_enabled+\"&requireSSL=\"+db_ssl_required+\"&verifyServerCertificate=\"+db_ssl_verifyServerCertificate);\n\t\t\t\t}else if(RangerBizUtil.getDBFlavor()==AppConstants.DB_FLAVOR_POSTGRES) {\n\t\t\t\t\tString db_ssl_certificate_file = propertiesMap.get(\"ranger.db.ssl.certificateFile\");\n\t\t\t\t\tif(StringUtils.isNotEmpty(db_ssl_certificate_file)) {\n\t\t\t\t\t\tranger_jpa_jdbc_url_ssl.append(\"?ssl=\"+db_ssl_enabled+\"&sslmode=verify-full\"+\"&sslrootcert=\"+db_ssl_certificate_file);\n\t\t\t\t\t} else if (\"true\".equalsIgnoreCase(db_ssl_verifyServerCertificate) || \"true\".equalsIgnoreCase(db_ssl_required)) {\n\t\t\t\t\t\tranger_jpa_jdbc_url_ssl.append(\"?ssl=\"+db_ssl_enabled+\"&sslmode=verify-full\"+\"&sslfactory=org.postgresql.ssl.DefaultJavaSSLFactory\");\n\t\t\t\t\t} else {\n\t\t\t\t\t\tranger_jpa_jdbc_url_ssl.append(\"?ssl=\"+db_ssl_enabled);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tpropertiesMap.put(\"ranger.jpa.jdbc.url\", ranger_jpa_jdbc_url_ssl.toString());\n\t\t\t}\n\t\t\tranger_jpa_jdbc_url=propertiesMap.get(\"ranger.jpa.jdbc.url\");\n\t\t\tif(StringUtils.isNotEmpty(ranger_jpa_jdbc_url)) {\n\t\t\t\tprops.put(\"ranger.jpa.jdbc.url\", ranger_jpa_jdbc_url);\n\t\t\t}\n\t\t\tlogger.info(\"ranger.jpa.jdbc.url=\"+ranger_jpa_jdbc_url);\n\t\t}\n\t}\n}\n\nif (propertiesMap != null && propertiesMap.containsKey(RangerCommonConstants.PROP_COOKIE_NAME)) {\n\tString cookieName = propertiesMap.get(RangerCommonConstants.PROP_COOKIE_NAME);\n\tif (StringUtils.isBlank(cookieName)) {\n\t\tcookieName = RangerCommonConstants.DEFAULT_COOKIE_NAME;\n\t}\n\tpropertiesMap.put(RangerCommonConstants.PROP_COOKIE_NAME, cookieName);\n\tprops.put(RangerCommonConstants.PROP_COOKIE_NAME, cookieName);\n}\n\nkeySet = props.keySet();\nfor (Object key : keySet) {\n\tString keyStr = key.toString();\n}\n\nsuper.processProperties(beanFactory, props);\n   }",
        "accept_response": "@Override\n   protected void processProperties(\n    ConfigurableListableBeanFactory beanFactory, Properties props)\n    throws BeansException {\n\n   // First let's add the system properties\nSet<Object> keySet = System.getProperties().keySet();\nfor (Object key : keySet) {\n    String keyStr = key.toString();\n    propertiesMap.put(keyStr, System.getProperties()\n\t    .getProperty(keyStr).trim());\n}\n\n// Let's add our properties now\nkeySet = props.keySet();\nfor (Object key : keySet) {\n    String keyStr = key.toString();\n    propertiesMap.put(keyStr, props.getProperty(keyStr).trim());\n}\n\nString storeType = propertiesMap.get(\"ranger.keystore.file.type\");\n// update system trust store path with custom trust store.\nif (propertiesMap!=null && propertiesMap.containsKey(\"ranger.truststore.file\")) {\n\tif(!StringUtils.isEmpty(propertiesMap.get(\"ranger.truststore.file\"))){\n\t\tSystem.setProperty(\"javax.net.ssl.trustStore\", propertiesMap.get(\"ranger.truststore.file\"));\n\t\tSystem.setProperty(\"javax.net.ssl.trustStoreType\", KeyStore.getDefaultType());\n\t\tPath trustStoreFile = Paths.get(propertiesMap.get(\"ranger.truststore.file\"));\n\t\tif (!Files.exists(trustStoreFile) || !Files.isReadable(trustStoreFile)) {\n\t\t\tlogger.debug(\"Could not find or read truststore file '\"+propertiesMap.get(\"ranger.truststore.file\")+\"'\");\n\t\t}else{\n\t\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\")){\n\t\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\t\tString trustStoreAlias=getProperty(\"ranger.truststore.alias\",\"trustStoreAlias\");\n\t\t\t\tif(path!=null && trustStoreAlias!=null){\n\t\t\t\t\tString trustStorePassword=CredentialReader.getDecryptedString(path.trim(), trustStoreAlias.trim(), storeType);\n\t\t\t\t\tif(trustStorePassword!=null&& !trustStorePassword.trim().isEmpty() && !trustStorePassword.trim().equalsIgnoreCase(\"none\")){\n\t\t\t\t\t\tpropertiesMap.put(\"ranger.truststore.password\", trustStorePassword);\n\t\t\t\t\t\tprops.put(\"ranger.truststore.password\", trustStorePassword);\n\t\t\t\t\t}else{\n\t\t\t\t\t\tlogger.info(\"trustStorePassword password not applied; clear text password shall be applicable\");\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tSystem.setProperty(\"javax.net.ssl.trustStorePassword\", propertiesMap.get(\"ranger.truststore.password\"));\n}\n\n// update system key store path with custom key store.\nif (propertiesMap!=null && propertiesMap.containsKey(\"ranger.keystore.file\")) {\n\tif(!StringUtils.isEmpty(propertiesMap.get(\"ranger.keystore.file\"))){\n\t\tSystem.setProperty(\"javax.net.ssl.keyStore\", propertiesMap.get(\"ranger.keystore.file\"));\n\t\tSystem.setProperty(\"javax.net.ssl.keyStoreType\", KeyStore.getDefaultType());\n\t\tPath keyStoreFile = Paths.get(propertiesMap.get(\"ranger.keystore.file\"));\n\t\tif (!Files.exists(keyStoreFile) || !Files.isReadable(keyStoreFile)) {\n\t\t\tlogger.debug(\"Could not find or read keystore file '\"+propertiesMap.get(\"ranger.keystore.file\")+\"'\");\n\t\t}else{\n\t\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\")){\n\t\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\t\tString keyStoreAlias=getProperty(\"ranger.keystore.alias\",\"keyStoreAlias\");\n\t\t\t\tif(path!=null && keyStoreAlias!=null){\n\t\t\t\t\tString keyStorePassword=CredentialReader.getDecryptedString(path.trim(), keyStoreAlias.trim(), storeType);\n\t\t\t\t\tif(keyStorePassword!=null&& !keyStorePassword.trim().isEmpty() && !keyStorePassword.trim().equalsIgnoreCase(\"none\")){\n\t\t\t\t\t\tpropertiesMap.put(\"ranger.keystore.password\", keyStorePassword);\n\t\t\t\t\t\tprops.put(\"ranger.keystore.password\", keyStorePassword);\n\t\t\t\t\t}else{\n\t\t\t\t\t\tlogger.info(\"keyStorePassword password not applied; clear text password shall be applicable\");\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tSystem.setProperty(\"javax.net.ssl.keyStorePassword\", propertiesMap.get(\"ranger.keystore.password\"));\n}\n\n//update unixauth keystore and truststore credentials\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\")){\n\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\tif(path!=null){\n\t\tString unixAuthKeyStoreAlias=getProperty(\"ranger.unixauth.keystore.alias\",\"unixAuthKeyStoreAlias\");\n\t\tif(unixAuthKeyStoreAlias!=null){\n\t\t\tString unixAuthKeyStorePass=CredentialReader.getDecryptedString(path.trim(),unixAuthKeyStoreAlias.trim(), storeType);\n\t\t\tif(unixAuthKeyStorePass!=null&& !unixAuthKeyStorePass.trim().isEmpty() &&!unixAuthKeyStorePass.trim().equalsIgnoreCase(\"none\")){\n\t\t\t\tpropertiesMap.put(\"ranger.unixauth.keystore.password\", unixAuthKeyStorePass);\n\t\t\t\tprops.put(\"ranger.unixauth.keystore.password\", unixAuthKeyStorePass);\n\t\t\t}else{\n\t\t\t\tlogger.info(\"unixauth keystore password not applied; clear text password shall be applicable\");\n\t\t\t}\n\t\t}\n\t\t//\n\t\tString unixAuthTrustStoreAlias=getProperty(\"ranger.unixauth.truststore.alias\",\"unixAuthTrustStoreAlias\");\n\t\tif(unixAuthTrustStoreAlias!=null){\n\t\t\tString unixAuthTrustStorePass=CredentialReader.getDecryptedString(path.trim(),unixAuthTrustStoreAlias.trim(), storeType);\n\t\t\tif(unixAuthTrustStorePass!=null&& !unixAuthTrustStorePass.trim().isEmpty() &&!unixAuthTrustStorePass.trim().equalsIgnoreCase(\"none\")){\n\t\t\t\tpropertiesMap.put(\"ranger.unixauth.truststore.password\", unixAuthTrustStorePass);\n\t\t\t\tprops.put(\"ranger.unixauth.truststore.password\", unixAuthTrustStorePass);\n\t\t\t}else{\n\t\t\t\tlogger.info(\"unixauth truststore password not applied; clear text password shall be applicable\");\n\t\t\t}\n\t\t}\n\t}\n}\n\n//update credential from keystore\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.jpa.jdbc.credential.alias\")){\n\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\tString alias=propertiesMap.get(\"ranger.jpa.jdbc.credential.alias\");\n\tif(path!=null && alias!=null){\n\t\tString xaDBPassword=CredentialReader.getDecryptedString(path.trim(),alias.trim(), storeType);\n\t\tif(xaDBPassword!=null&& !xaDBPassword.trim().isEmpty() &&\n\t\t\t\t!\"none\".equalsIgnoreCase(xaDBPassword.trim())){\n\t\t\tpropertiesMap.put(\"ranger.jpa.jdbc.password\", xaDBPassword);\n\t\t\tprops.put(\"ranger.jpa.jdbc.password\", xaDBPassword);\n\t\t}else{\n\t\t\tlogger.info(\"Credential keystore password not applied for Ranger DB; clear text password shall be applicable\");\n\t\t}\n\t}\n}\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.jpa.audit.jdbc.credential.alias\")){\n\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\tString alias=propertiesMap.get(\"ranger.jpa.audit.jdbc.credential.alias\");\n\tif(path!=null && alias!=null){\n\t\tString auditDBPassword=CredentialReader.getDecryptedString(path.trim(), alias.trim(), storeType);\n\t\tif(auditDBPassword!=null&& !auditDBPassword.trim().isEmpty() &&\n\t\t\t\t!\"none\".equalsIgnoreCase(auditDBPassword.trim())){\n\t\t\tpropertiesMap.put(\"ranger.jpa.audit.jdbc.password\", auditDBPassword);\n\t\t\tprops.put(\"ranger.jpa.audit.jdbc.password\", auditDBPassword);\n\t\t}else{\n\t\t\tlogger.info(\"Credential keystore password not applied for Audit DB; clear text password shall be applicable\");\n\t\t}\n\t}\n}\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.authentication.method\")){\n\tString authenticationMethod=propertiesMap.get(\"ranger.authentication.method\");\n\tif(authenticationMethod!=null && (\"ACTIVE_DIRECTORY\".equalsIgnoreCase(authenticationMethod)||\"AD\".equalsIgnoreCase(authenticationMethod))){\n\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.ldap.ad.binddn.credential.alias\")){\n\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\tString alias=propertiesMap.get(\"ranger.ldap.ad.binddn.credential.alias\");\n\t\t\tif(path!=null && alias!=null){\n\t\t\t\tString bindDNPassword=CredentialReader.getDecryptedString(path.trim(), alias.trim(), storeType);\n\t\t\t\tif(bindDNPassword!=null&& !bindDNPassword.trim().isEmpty() &&\n\t\t\t\t\t\t!\"none\".equalsIgnoreCase(bindDNPassword.trim())){\n\t\t\t\t\tpropertiesMap.put(\"ranger.ldap.ad.bind.password\", bindDNPassword);\n\t\t\t\t\tprops.put(\"ranger.ldap.ad.bind.password\", bindDNPassword);\n\t\t\t\t}else{\n\t\t\t\t\tlogger.info(\"Credential keystore password not applied for AD Bind DN; clear text password shall be applicable\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.authentication.method\")){\n\tString authenticationMethod=propertiesMap.get(\"ranger.authentication.method\");\n\tif(authenticationMethod!=null && (\"LDAP\".equalsIgnoreCase(authenticationMethod))){\n\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.ldap.binddn.credential.alias\")){\n\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\tString alias=propertiesMap.get(\"ranger.ldap.binddn.credential.alias\");\n\t\t\tif(path!=null && alias!=null){\n\t\t\t\tString bindDNPassword=CredentialReader.getDecryptedString(path.trim(), alias.trim(), storeType);\n\t\t\t\tif(bindDNPassword!=null&& !bindDNPassword.trim().isEmpty() &&\n\t\t\t\t\t\t!\"none\".equalsIgnoreCase(bindDNPassword.trim())){\n\t\t\t\t\tpropertiesMap.put(\"ranger.ldap.bind.password\", bindDNPassword);\n\t\t\t\t\tprops.put(\"ranger.ldap.bind.password\", bindDNPassword);\n\t\t\t\t}else{\n\t\t\t\t\tlogger.info(\"Credential keystore password not applied for LDAP Bind DN; clear text password shall be applicable\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.audit.source.type\")){\n\tString auditStore=propertiesMap.get(\"ranger.audit.source.type\");\n\tif(auditStore!=null && (\"solr\".equalsIgnoreCase(auditStore))){\n\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.solr.audit.credential.alias\")){\n\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\tString alias=propertiesMap.get(\"ranger.solr.audit.credential.alias\");\n\t\t\tif(path!=null && alias!=null){\n\t\t\t\tString solrAuditPassword=CredentialReader.getDecryptedString(path.trim(), alias.trim(), storeType);\n\t\t\t\tif(solrAuditPassword!=null&& !solrAuditPassword.trim().isEmpty() &&\n\t\t\t\t\t\t!\"none\".equalsIgnoreCase(solrAuditPassword.trim())){\n\t\t\t\t\tpropertiesMap.put(\"ranger.solr.audit.user.password\", solrAuditPassword);\n\t\t\t\t\tprops.put(\"ranger.solr.audit.user.password\", solrAuditPassword);\n\t\t\t\t}else{\n\t\t\t\t\tlogger.info(\"Credential keystore password not applied for Solr; clear text password shall be applicable\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\nif(propertiesMap!=null){\n\tString sha256PasswordUpdateDisable=\"false\";\n\tif(propertiesMap.containsKey(\"ranger.sha256Password.update.disable\")){\n\t\tsha256PasswordUpdateDisable=propertiesMap.get(\"ranger.sha256Password.update.disable\");\n\t\tif(sha256PasswordUpdateDisable==null || sha256PasswordUpdateDisable.trim().isEmpty()|| !\"true\".equalsIgnoreCase(sha256PasswordUpdateDisable)){\n\t\t\tsha256PasswordUpdateDisable=\"false\";\n\t\t}\n\t}\n\tpropertiesMap.put(\"ranger.sha256Password.update.disable\", sha256PasswordUpdateDisable);\n\tprops.put(\"ranger.sha256Password.update.disable\", sha256PasswordUpdateDisable);\n}\nif(RangerBizUtil.getDBFlavor()==AppConstants.DB_FLAVOR_MYSQL || RangerBizUtil.getDBFlavor()==AppConstants.DB_FLAVOR_POSTGRES){\n\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.db.ssl.enabled\")){\n\t\tString db_ssl_enabled=propertiesMap.get(\"ranger.db.ssl.enabled\");\n\t\tif(StringUtils.isEmpty(db_ssl_enabled)|| !\"true\".equalsIgnoreCase(db_ssl_enabled)){\n\t\t\tdb_ssl_enabled=\"false\";\n\t\t}\n\t\tdb_ssl_enabled=db_ssl_enabled.toLowerCase();\n\t\tif(\"true\".equalsIgnoreCase(db_ssl_enabled)){\n\t\t\tString db_ssl_required=propertiesMap.get(\"ranger.db.ssl.required\");\n\t\t\tif(StringUtils.isEmpty(db_ssl_required)|| !\"true\".equalsIgnoreCase(db_ssl_required)){\n\t\t\t\tdb_ssl_required=\"false\";\n\t\t\t}\n\t\t\tdb_ssl_required=db_ssl_required.toLowerCase();\n\t\t\tString db_ssl_verifyServerCertificate=propertiesMap.get(\"ranger.db.ssl.verifyServerCertificate\");\n\t\t\tif(StringUtils.isEmpty(db_ssl_verifyServerCertificate)|| !\"true\".equalsIgnoreCase(db_ssl_verifyServerCertificate)){\n\t\t\t\tdb_ssl_verifyServerCertificate=\"false\";\n\t\t\t}\n\t\t\tdb_ssl_verifyServerCertificate=db_ssl_verifyServerCertificate.toLowerCase();\n\t\t\tString db_ssl_auth_type=propertiesMap.get(\"ranger.db.ssl.auth.type\");\n\t\t\tif(StringUtils.isEmpty(db_ssl_auth_type)|| !\"1-way\".equalsIgnoreCase(db_ssl_auth_type)){\n\t\t\t\tdb_ssl_auth_type=\"2-way\";\n\t\t\t}\n\t\t\tpropertiesMap.put(\"ranger.db.ssl.enabled\", db_ssl_enabled);\n\t\t\tprops.put(\"ranger.db.ssl.enabled\", db_ssl_enabled);\n\t\t\tpropertiesMap.put(\"ranger.db.ssl.required\", db_ssl_required);\n\t\t\tprops.put(\"ranger.db.ssl.required\", db_ssl_required);\n\t\t\tpropertiesMap.put(\"ranger.db.ssl.verifyServerCertificate\", db_ssl_verifyServerCertificate);\n\t\t\tprops.put(\"ranger.db.ssl.verifyServerCertificate\", db_ssl_verifyServerCertificate);\n\t\t\tpropertiesMap.put(\"ranger.db.ssl.auth.type\", db_ssl_auth_type);\n\t\t\tprops.put(\"ranger.db.ssl.auth.type\", db_ssl_auth_type);\n\t\t\tString ranger_jpa_jdbc_url=propertiesMap.get(\"ranger.jpa.jdbc.url\");\n\t\t\tif(StringUtils.isNotEmpty(ranger_jpa_jdbc_url) && !ranger_jpa_jdbc_url.contains(\"?\")){\n\t\t\t\tStringBuffer ranger_jpa_jdbc_url_ssl=new StringBuffer(ranger_jpa_jdbc_url);\n\t\t\t\tif (RangerBizUtil.getDBFlavor()==AppConstants.DB_FLAVOR_MYSQL) {\n\t\t\t\t\tranger_jpa_jdbc_url_ssl.append(\"?useSSL=\"+db_ssl_enabled+\"&requireSSL=\"+db_ssl_required+\"&verifyServerCertificate=\"+db_ssl_verifyServerCertificate);\n\t\t\t\t}else if(RangerBizUtil.getDBFlavor()==AppConstants.DB_FLAVOR_POSTGRES) {\n\t\t\t\t\tString db_ssl_certificate_file = propertiesMap.get(\"ranger.db.ssl.certificateFile\");\n\t\t\t\t\tif(StringUtils.isNotEmpty(db_ssl_certificate_file)) {\n\t\t\t\t\t\tranger_jpa_jdbc_url_ssl.append(\"?ssl=\"+db_ssl_enabled+\"&sslmode=verify-full\"+\"&sslrootcert=\"+db_ssl_certificate_file);\n\t\t\t\t\t} else if (\"true\".equalsIgnoreCase(db_ssl_verifyServerCertificate) || \"true\".equalsIgnoreCase(db_ssl_required)) {\n\t\t\t\t\t\tranger_jpa_jdbc_url_ssl.append(\"?ssl=\"+db_ssl_enabled+\"&sslmode=verify-full\"+\"&sslfactory=org.postgresql.ssl.DefaultJavaSSLFactory\");\n\t\t\t\t\t} else {\n\t\t\t\t\t\tranger_jpa_jdbc_url_ssl.append(\"?ssl=\"+db_ssl_enabled);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tpropertiesMap.put(\"ranger.jpa.jdbc.url\", ranger_jpa_jdbc_url_ssl.toString());\n\t\t\t}\n\t\t\tranger_jpa_jdbc_url=propertiesMap.get(\"ranger.jpa.jdbc.url\");\n\t\t\tif(StringUtils.isNotEmpty(ranger_jpa_jdbc_url)) {\n\t\t\t\tprops.put(\"ranger.jpa.jdbc.url\", ranger_jpa_jdbc_url);\n\t\t\t}\n\t\t\tlogger.info(\"ranger.jpa.jdbc.url=\"+ranger_jpa_jdbc_url);\n\t\t}\n\t}\n}\n\nif (propertiesMap != null && propertiesMap.containsKey(RangerCommonConstants.PROP_COOKIE_NAME)) {\n\tString cookieName = propertiesMap.get(RangerCommonConstants.PROP_COOKIE_NAME);\n\tif (StringUtils.isBlank(cookieName)) {\n\t\tcookieName = RangerCommonConstants.DEFAULT_COOKIE_NAME;\n\t}\n\tpropertiesMap.put(RangerCommonConstants.PROP_COOKIE_NAME, cookieName);\n\tprops.put(RangerCommonConstants.PROP_COOKIE_NAME, cookieName);\n}\n\nkeySet = props.keySet();\nfor (Object key : keySet) {\n\tString keyStr = key.toString();\n               logger.debug(\"PropertiesUtil:[\" + keyStr + \"][\" +\n                   (keyStr.contains(\"password\") || keyStr.contains(\"keystore.pass\")   ? \"********]\" : props.get(keyStr)) + \"]\");\n}\n\nsuper.processProperties(beanFactory, props);\n   }",
        "reject_response": "@Override\n   protected void processProperties(\n    ConfigurableListableBeanFactory beanFactory, Properties props)\n    throws BeansException {\n\n   // First let's add the system properties\nSet<Object> keySet = System.getProperties().keySet();\nfor (Object key : keySet) {\n    String keyStr = key.toString();\n    propertiesMap.put(keyStr, System.getProperties()\n\t    .getProperty(keyStr).trim());\n}\n\n// Let's add our properties now\nkeySet = props.keySet();\nfor (Object key : keySet) {\n    String keyStr = key.toString();\n    propertiesMap.put(keyStr, props.getProperty(keyStr).trim());\n}\n\nString storeType = propertiesMap.get(\"ranger.keystore.file.type\");\n// update system trust store path with custom trust store.\nif (propertiesMap!=null && propertiesMap.containsKey(\"ranger.truststore.file\")) {\n\tif(!StringUtils.isEmpty(propertiesMap.get(\"ranger.truststore.file\"))){\n\t\tSystem.setProperty(\"javax.net.ssl.trustStore\", propertiesMap.get(\"ranger.truststore.file\"));\n\t\tSystem.setProperty(\"javax.net.ssl.trustStoreType\", KeyStore.getDefaultType());\n\t\tPath trustStoreFile = Paths.get(propertiesMap.get(\"ranger.truststore.file\"));\n\t\tif (!Files.exists(trustStoreFile) || !Files.isReadable(trustStoreFile)) {\n\t\t\tlogger.debug(\"Could not find or read truststore file '\"+propertiesMap.get(\"ranger.truststore.file\")+\"'\");\n\t\t}else{\n\t\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\")){\n\t\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\t\tString trustStoreAlias=getProperty(\"ranger.truststore.alias\",\"trustStoreAlias\");\n\t\t\t\tif(path!=null && trustStoreAlias!=null){\n\t\t\t\t\tString trustStorePassword=CredentialReader.getDecryptedString(path.trim(), trustStoreAlias.trim(), storeType);\n\t\t\t\t\tif(trustStorePassword!=null&& !trustStorePassword.trim().isEmpty() && !trustStorePassword.trim().equalsIgnoreCase(\"none\")){\n\t\t\t\t\t\tpropertiesMap.put(\"ranger.truststore.password\", trustStorePassword);\n\t\t\t\t\t\tprops.put(\"ranger.truststore.password\", trustStorePassword);\n\t\t\t\t\t}else{\n\t\t\t\t\t\tlogger.info(\"trustStorePassword password not applied; clear text password shall be applicable\");\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tSystem.setProperty(\"javax.net.ssl.trustStorePassword\", propertiesMap.get(\"ranger.truststore.password\"));\n}\n\n// update system key store path with custom key store.\nif (propertiesMap!=null && propertiesMap.containsKey(\"ranger.keystore.file\")) {\n\tif(!StringUtils.isEmpty(propertiesMap.get(\"ranger.keystore.file\"))){\n\t\tSystem.setProperty(\"javax.net.ssl.keyStore\", propertiesMap.get(\"ranger.keystore.file\"));\n\t\tSystem.setProperty(\"javax.net.ssl.keyStoreType\", KeyStore.getDefaultType());\n\t\tPath keyStoreFile = Paths.get(propertiesMap.get(\"ranger.keystore.file\"));\n\t\tif (!Files.exists(keyStoreFile) || !Files.isReadable(keyStoreFile)) {\n\t\t\tlogger.debug(\"Could not find or read keystore file '\"+propertiesMap.get(\"ranger.keystore.file\")+\"'\");\n\t\t}else{\n\t\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\")){\n\t\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\t\tString keyStoreAlias=getProperty(\"ranger.keystore.alias\",\"keyStoreAlias\");\n\t\t\t\tif(path!=null && keyStoreAlias!=null){\n\t\t\t\t\tString keyStorePassword=CredentialReader.getDecryptedString(path.trim(), keyStoreAlias.trim(), storeType);\n\t\t\t\t\tif(keyStorePassword!=null&& !keyStorePassword.trim().isEmpty() && !keyStorePassword.trim().equalsIgnoreCase(\"none\")){\n\t\t\t\t\t\tpropertiesMap.put(\"ranger.keystore.password\", keyStorePassword);\n\t\t\t\t\t\tprops.put(\"ranger.keystore.password\", keyStorePassword);\n\t\t\t\t\t}else{\n\t\t\t\t\t\tlogger.info(\"keyStorePassword password not applied; clear text password shall be applicable\");\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tSystem.setProperty(\"javax.net.ssl.keyStorePassword\", propertiesMap.get(\"ranger.keystore.password\"));\n}\n\n//update unixauth keystore and truststore credentials\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\")){\n\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\tif(path!=null){\n\t\tString unixAuthKeyStoreAlias=getProperty(\"ranger.unixauth.keystore.alias\",\"unixAuthKeyStoreAlias\");\n\t\tif(unixAuthKeyStoreAlias!=null){\n\t\t\tString unixAuthKeyStorePass=CredentialReader.getDecryptedString(path.trim(),unixAuthKeyStoreAlias.trim(), storeType);\n\t\t\tif(unixAuthKeyStorePass!=null&& !unixAuthKeyStorePass.trim().isEmpty() &&!unixAuthKeyStorePass.trim().equalsIgnoreCase(\"none\")){\n\t\t\t\tpropertiesMap.put(\"ranger.unixauth.keystore.password\", unixAuthKeyStorePass);\n\t\t\t\tprops.put(\"ranger.unixauth.keystore.password\", unixAuthKeyStorePass);\n\t\t\t}else{\n\t\t\t\tlogger.info(\"unixauth keystore password not applied; clear text password shall be applicable\");\n\t\t\t}\n\t\t}\n\t\t//\n\t\tString unixAuthTrustStoreAlias=getProperty(\"ranger.unixauth.truststore.alias\",\"unixAuthTrustStoreAlias\");\n\t\tif(unixAuthTrustStoreAlias!=null){\n\t\t\tString unixAuthTrustStorePass=CredentialReader.getDecryptedString(path.trim(),unixAuthTrustStoreAlias.trim(), storeType);\n\t\t\tif(unixAuthTrustStorePass!=null&& !unixAuthTrustStorePass.trim().isEmpty() &&!unixAuthTrustStorePass.trim().equalsIgnoreCase(\"none\")){\n\t\t\t\tpropertiesMap.put(\"ranger.unixauth.truststore.password\", unixAuthTrustStorePass);\n\t\t\t\tprops.put(\"ranger.unixauth.truststore.password\", unixAuthTrustStorePass);\n\t\t\t}else{\n\t\t\t\tlogger.info(\"unixauth truststore password not applied; clear text password shall be applicable\");\n\t\t\t}\n\t\t}\n\t}\n}\n\n//update credential from keystore\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.jpa.jdbc.credential.alias\")){\n\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\tString alias=propertiesMap.get(\"ranger.jpa.jdbc.credential.alias\");\n\tif(path!=null && alias!=null){\n\t\tString xaDBPassword=CredentialReader.getDecryptedString(path.trim(),alias.trim(), storeType);\n\t\tif(xaDBPassword!=null&& !xaDBPassword.trim().isEmpty() &&\n\t\t\t\t!\"none\".equalsIgnoreCase(xaDBPassword.trim())){\n\t\t\tpropertiesMap.put(\"ranger.jpa.jdbc.password\", xaDBPassword);\n\t\t\tprops.put(\"ranger.jpa.jdbc.password\", xaDBPassword);\n\t\t}else{\n\t\t\tlogger.info(\"Credential keystore password not applied for Ranger DB; clear text password shall be applicable\");\n\t\t}\n\t}\n}\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.jpa.audit.jdbc.credential.alias\")){\n\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\tString alias=propertiesMap.get(\"ranger.jpa.audit.jdbc.credential.alias\");\n\tif(path!=null && alias!=null){\n\t\tString auditDBPassword=CredentialReader.getDecryptedString(path.trim(), alias.trim(), storeType);\n\t\tif(auditDBPassword!=null&& !auditDBPassword.trim().isEmpty() &&\n\t\t\t\t!\"none\".equalsIgnoreCase(auditDBPassword.trim())){\n\t\t\tpropertiesMap.put(\"ranger.jpa.audit.jdbc.password\", auditDBPassword);\n\t\t\tprops.put(\"ranger.jpa.audit.jdbc.password\", auditDBPassword);\n\t\t}else{\n\t\t\tlogger.info(\"Credential keystore password not applied for Audit DB; clear text password shall be applicable\");\n\t\t}\n\t}\n}\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.authentication.method\")){\n\tString authenticationMethod=propertiesMap.get(\"ranger.authentication.method\");\n\tif(authenticationMethod!=null && (\"ACTIVE_DIRECTORY\".equalsIgnoreCase(authenticationMethod)||\"AD\".equalsIgnoreCase(authenticationMethod))){\n\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.ldap.ad.binddn.credential.alias\")){\n\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\tString alias=propertiesMap.get(\"ranger.ldap.ad.binddn.credential.alias\");\n\t\t\tif(path!=null && alias!=null){\n\t\t\t\tString bindDNPassword=CredentialReader.getDecryptedString(path.trim(), alias.trim(), storeType);\n\t\t\t\tif(bindDNPassword!=null&& !bindDNPassword.trim().isEmpty() &&\n\t\t\t\t\t\t!\"none\".equalsIgnoreCase(bindDNPassword.trim())){\n\t\t\t\t\tpropertiesMap.put(\"ranger.ldap.ad.bind.password\", bindDNPassword);\n\t\t\t\t\tprops.put(\"ranger.ldap.ad.bind.password\", bindDNPassword);\n\t\t\t\t}else{\n\t\t\t\t\tlogger.info(\"Credential keystore password not applied for AD Bind DN; clear text password shall be applicable\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.authentication.method\")){\n\tString authenticationMethod=propertiesMap.get(\"ranger.authentication.method\");\n\tif(authenticationMethod!=null && (\"LDAP\".equalsIgnoreCase(authenticationMethod))){\n\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.ldap.binddn.credential.alias\")){\n\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\tString alias=propertiesMap.get(\"ranger.ldap.binddn.credential.alias\");\n\t\t\tif(path!=null && alias!=null){\n\t\t\t\tString bindDNPassword=CredentialReader.getDecryptedString(path.trim(), alias.trim(), storeType);\n\t\t\t\tif(bindDNPassword!=null&& !bindDNPassword.trim().isEmpty() &&\n\t\t\t\t\t\t!\"none\".equalsIgnoreCase(bindDNPassword.trim())){\n\t\t\t\t\tpropertiesMap.put(\"ranger.ldap.bind.password\", bindDNPassword);\n\t\t\t\t\tprops.put(\"ranger.ldap.bind.password\", bindDNPassword);\n\t\t\t\t}else{\n\t\t\t\t\tlogger.info(\"Credential keystore password not applied for LDAP Bind DN; clear text password shall be applicable\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\nif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.audit.source.type\")){\n\tString auditStore=propertiesMap.get(\"ranger.audit.source.type\");\n\tif(auditStore!=null && (\"solr\".equalsIgnoreCase(auditStore))){\n\t\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.credential.provider.path\") && propertiesMap.containsKey(\"ranger.solr.audit.credential.alias\")){\n\t\t\tString path=propertiesMap.get(\"ranger.credential.provider.path\");\n\t\t\tString alias=propertiesMap.get(\"ranger.solr.audit.credential.alias\");\n\t\t\tif(path!=null && alias!=null){\n\t\t\t\tString solrAuditPassword=CredentialReader.getDecryptedString(path.trim(), alias.trim(), storeType);\n\t\t\t\tif(solrAuditPassword!=null&& !solrAuditPassword.trim().isEmpty() &&\n\t\t\t\t\t\t!\"none\".equalsIgnoreCase(solrAuditPassword.trim())){\n\t\t\t\t\tpropertiesMap.put(\"ranger.solr.audit.user.password\", solrAuditPassword);\n\t\t\t\t\tprops.put(\"ranger.solr.audit.user.password\", solrAuditPassword);\n\t\t\t\t}else{\n\t\t\t\t\tlogger.info(\"Credential keystore password not applied for Solr; clear text password shall be applicable\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\nif(propertiesMap!=null){\n\tString sha256PasswordUpdateDisable=\"false\";\n\tif(propertiesMap.containsKey(\"ranger.sha256Password.update.disable\")){\n\t\tsha256PasswordUpdateDisable=propertiesMap.get(\"ranger.sha256Password.update.disable\");\n\t\tif(sha256PasswordUpdateDisable==null || sha256PasswordUpdateDisable.trim().isEmpty()|| !\"true\".equalsIgnoreCase(sha256PasswordUpdateDisable)){\n\t\t\tsha256PasswordUpdateDisable=\"false\";\n\t\t}\n\t}\n\tpropertiesMap.put(\"ranger.sha256Password.update.disable\", sha256PasswordUpdateDisable);\n\tprops.put(\"ranger.sha256Password.update.disable\", sha256PasswordUpdateDisable);\n}\nif(RangerBizUtil.getDBFlavor()==AppConstants.DB_FLAVOR_MYSQL || RangerBizUtil.getDBFlavor()==AppConstants.DB_FLAVOR_POSTGRES){\n\tif(propertiesMap!=null && propertiesMap.containsKey(\"ranger.db.ssl.enabled\")){\n\t\tString db_ssl_enabled=propertiesMap.get(\"ranger.db.ssl.enabled\");\n\t\tif(StringUtils.isEmpty(db_ssl_enabled)|| !\"true\".equalsIgnoreCase(db_ssl_enabled)){\n\t\t\tdb_ssl_enabled=\"false\";\n\t\t}\n\t\tdb_ssl_enabled=db_ssl_enabled.toLowerCase();\n\t\tif(\"true\".equalsIgnoreCase(db_ssl_enabled)){\n\t\t\tString db_ssl_required=propertiesMap.get(\"ranger.db.ssl.required\");\n\t\t\tif(StringUtils.isEmpty(db_ssl_required)|| !\"true\".equalsIgnoreCase(db_ssl_required)){\n\t\t\t\tdb_ssl_required=\"false\";\n\t\t\t}\n\t\t\tdb_ssl_required=db_ssl_required.toLowerCase();\n\t\t\tString db_ssl_verifyServerCertificate=propertiesMap.get(\"ranger.db.ssl.verifyServerCertificate\");\n\t\t\tif(StringUtils.isEmpty(db_ssl_verifyServerCertificate)|| !\"true\".equalsIgnoreCase(db_ssl_verifyServerCertificate)){\n\t\t\t\tdb_ssl_verifyServerCertificate=\"false\";\n\t\t\t}\n\t\t\tdb_ssl_verifyServerCertificate=db_ssl_verifyServerCertificate.toLowerCase();\n\t\t\tString db_ssl_auth_type=propertiesMap.get(\"ranger.db.ssl.auth.type\");\n\t\t\tif(StringUtils.isEmpty(db_ssl_auth_type)|| !\"1-way\".equalsIgnoreCase(db_ssl_auth_type)){\n\t\t\t\tdb_ssl_auth_type=\"2-way\";\n\t\t\t}\n\t\t\tpropertiesMap.put(\"ranger.db.ssl.enabled\", db_ssl_enabled);\n\t\t\tprops.put(\"ranger.db.ssl.enabled\", db_ssl_enabled);\n\t\t\tpropertiesMap.put(\"ranger.db.ssl.required\", db_ssl_required);\n\t\t\tprops.put(\"ranger.db.ssl.required\", db_ssl_required);\n\t\t\tpropertiesMap.put(\"ranger.db.ssl.verifyServerCertificate\", db_ssl_verifyServerCertificate);\n\t\t\tprops.put(\"ranger.db.ssl.verifyServerCertificate\", db_ssl_verifyServerCertificate);\n\t\t\tpropertiesMap.put(\"ranger.db.ssl.auth.type\", db_ssl_auth_type);\n\t\t\tprops.put(\"ranger.db.ssl.auth.type\", db_ssl_auth_type);\n\t\t\tString ranger_jpa_jdbc_url=propertiesMap.get(\"ranger.jpa.jdbc.url\");\n\t\t\tif(StringUtils.isNotEmpty(ranger_jpa_jdbc_url) && !ranger_jpa_jdbc_url.contains(\"?\")){\n\t\t\t\tStringBuffer ranger_jpa_jdbc_url_ssl=new StringBuffer(ranger_jpa_jdbc_url);\n\t\t\t\tif (RangerBizUtil.getDBFlavor()==AppConstants.DB_FLAVOR_MYSQL) {\n\t\t\t\t\tranger_jpa_jdbc_url_ssl.append(\"?useSSL=\"+db_ssl_enabled+\"&requireSSL=\"+db_ssl_required+\"&verifyServerCertificate=\"+db_ssl_verifyServerCertificate);\n\t\t\t\t}else if(RangerBizUtil.getDBFlavor()==AppConstants.DB_FLAVOR_POSTGRES) {\n\t\t\t\t\tString db_ssl_certificate_file = propertiesMap.get(\"ranger.db.ssl.certificateFile\");\n\t\t\t\t\tif(StringUtils.isNotEmpty(db_ssl_certificate_file)) {\n\t\t\t\t\t\tranger_jpa_jdbc_url_ssl.append(\"?ssl=\"+db_ssl_enabled+\"&sslmode=verify-full\"+\"&sslrootcert=\"+db_ssl_certificate_file);\n\t\t\t\t\t} else if (\"true\".equalsIgnoreCase(db_ssl_verifyServerCertificate) || \"true\".equalsIgnoreCase(db_ssl_required)) {\n\t\t\t\t\t\tranger_jpa_jdbc_url_ssl.append(\"?ssl=\"+db_ssl_enabled+\"&sslmode=verify-full\"+\"&sslfactory=org.postgresql.ssl.DefaultJavaSSLFactory\");\n\t\t\t\t\t} else {\n\t\t\t\t\t\tranger_jpa_jdbc_url_ssl.append(\"?ssl=\"+db_ssl_enabled);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tpropertiesMap.put(\"ranger.jpa.jdbc.url\", ranger_jpa_jdbc_url_ssl.toString());\n\t\t\t}\n\t\t\tranger_jpa_jdbc_url=propertiesMap.get(\"ranger.jpa.jdbc.url\");\n\t\t\tif(StringUtils.isNotEmpty(ranger_jpa_jdbc_url)) {\n\t\t\t\tprops.put(\"ranger.jpa.jdbc.url\", ranger_jpa_jdbc_url);\n\t\t\t}\n\t\t\tlogger.info(\"ranger.jpa.jdbc.url=\"+ranger_jpa_jdbc_url);\n\t\t}\n\t}\n}\n\nif (propertiesMap != null && propertiesMap.containsKey(RangerCommonConstants.PROP_COOKIE_NAME)) {\n\tString cookieName = propertiesMap.get(RangerCommonConstants.PROP_COOKIE_NAME);\n\tif (StringUtils.isBlank(cookieName)) {\n\t\tcookieName = RangerCommonConstants.DEFAULT_COOKIE_NAME;\n\t}\n\tpropertiesMap.put(RangerCommonConstants.PROP_COOKIE_NAME, cookieName);\n\tprops.put(RangerCommonConstants.PROP_COOKIE_NAME, cookieName);\n}\n\nkeySet = props.keySet();\nfor (Object key : keySet) {\n\tString keyStr = key.toString();\n\tlogger.debug(\"PropertiesUtil:[\" + keyStr + \"][\" + props.get(keyStr) + \"]\");\n}\n\nsuper.processProperties(beanFactory, props);\n   }",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2901,
        "instruction": "public void onException(Exception exception, SPFSession session)\n        throws PermErrorException, NoneException, TempErrorException,\n        NeutralException {\n\n    String result;\n    if (exception instanceof SPFResultException) {\n        result = ((SPFResultException) exception).getResult();\n        if (!SPFErrorConstants.NEUTRAL_CONV.equals(result)) {\n            LOGGER.warn(exception.getMessage(),exception);\n        }\n    } else {\n        // this should never happen at all. But anyway we will set the\n        // result to neutral. Safety first ..\n        result = SPFErrorConstants.NEUTRAL_CONV;\n    }\n    session.setCurrentResultExpanded(result);\n}",
        "accept_response": "public void onException(Exception exception, SPFSession session)\n        throws PermErrorException, NoneException, TempErrorException,\n        NeutralException {\n\n    String result;\n    if (exception instanceof SPFResultException) {\n        result = ((SPFResultException) exception).getResult();\n        if (!SPFErrorConstants.NEUTRAL_CONV.equals(result)) {\n            LOGGER.warn(exception.getMessage(),exception);\n        }\n    } else {\n        // this should never happen at all. But anyway we will set the\n        // result to neutral. Safety first ..\n        LOGGER.error(exception.getMessage(),exception);\n        result = SPFErrorConstants.NEUTRAL_CONV;\n    }\n    session.setCurrentResultExpanded(result);\n}",
        "reject_response": "public void onException(Exception exception, SPFSession session)\n        throws PermErrorException, NoneException, TempErrorException,\n        NeutralException {\n\n    String result;\n    if (exception instanceof SPFResultException) {\n        result = ((SPFResultException) exception).getResult();\n        if (!SPFErrorConstants.NEUTRAL_CONV.equals(result)) {\n            LOGGER.warn(exception.getMessage(),exception);\n        }\n    } else {\n        // this should never happen at all. But anyway we will set the\n        // result to neutral. Safety first ..\n        log.error(exception.getMessage(),exception);\n        result = SPFErrorConstants.NEUTRAL_CONV;\n    }\n    session.setCurrentResultExpanded(result);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2954,
        "instruction": "@POST\n@Consumes(MediaType.TEXT_PLAIN)\n@Produces(MediaType.TEXT_PLAIN)\n@Path(\"/kerberos\")\n@ApiOperation(\n        value = \"Creates a token for accessing the REST API via Kerberos ticket exchange / SPNEGO negotiation\",\n        notes = \"The token returned is formatted as a JSON Web Token (JWT). The token is base64 encoded and comprised of three parts. The header, \" +\n                \"the body, and the signature. The expiration of the token is a contained within the body. The token can be used in the Authorization header \" +\n                \"in the format 'Authorization: Bearer <token>'.\",\n        response = String.class\n)\n@ApiResponses(\n        value = {\n                @ApiResponse(code = 400, message = \"NiFi was unable to complete the request because it was invalid. The request should not be retried without modification.\"),\n                @ApiResponse(code = 401, message = \"NiFi was unable to complete the request because it did not contain a valid Kerberos \" +\n                        \"ticket in the Authorization header. Retry this request after initializing a ticket with kinit and \" +\n                        \"ensuring your browser is configured to support SPNEGO.\"),\n                @ApiResponse(code = 409, message = \"Unable to create access token because NiFi is not in the appropriate state. (i.e. may not be configured to support Kerberos login.\"),\n                @ApiResponse(code = 500, message = \"Unable to create access token because an unexpected error occurred.\")\n        }\n)\npublic Response createAccessTokenFromTicket(@Context HttpServletRequest httpServletRequest) {\n\n    // only support access tokens when communicating over HTTPS\n    if (!httpServletRequest.isSecure()) {\n        throw new AuthenticationNotSupportedException(\"Access tokens are only issued over HTTPS.\");\n    }\n\n    // If Kerberos Service Principal and keytab location not configured, throws exception\n    if (!properties.isKerberosSpnegoSupportEnabled() || kerberosService == null) {\n        final String message = \"Kerberos ticket login not supported by this NiFi.\";\n        return Response.status(Response.Status.CONFLICT).entity(message).build();\n    }\n\n    String authorizationHeaderValue = httpServletRequest.getHeader(KerberosService.AUTHORIZATION_HEADER_NAME);\n\n    if (!kerberosService.isValidKerberosHeader(authorizationHeaderValue)) {\n        final Response response = generateNotAuthorizedResponse().header(KerberosService.AUTHENTICATION_CHALLENGE_HEADER_NAME, KerberosService.AUTHORIZATION_NEGOTIATE).build();\n        return response;\n    } else {\n        try {\n            // attempt to authenticate\n            Authentication authentication = kerberosService.validateKerberosTicket(httpServletRequest);\n\n            if (authentication == null) {\n                throw new IllegalArgumentException(\"Request is not HTTPS or Kerberos ticket missing or malformed\");\n            }\n\n            final String expirationFromProperties = properties.getKerberosAuthenticationExpiration();\n            long expiration = FormatUtils.getTimeDuration(expirationFromProperties, TimeUnit.MILLISECONDS);\n            final String rawIdentity = authentication.getName();\n            String mappedIdentity = IdentityMappingUtil.mapIdentity(rawIdentity, IdentityMappingUtil.getIdentityMappings(properties));\n            expiration = validateTokenExpiration(expiration, mappedIdentity);\n\n            // create the authentication token\n            final LoginAuthenticationToken loginAuthenticationToken = new LoginAuthenticationToken(mappedIdentity, expiration, \"KerberosService\");\n\n            // generate JWT for response\n            final String token = jwtService.generateSignedToken(loginAuthenticationToken);\n\n            // build the response\n            final URI uri = URI.create(generateResourceUri(\"access\", \"kerberos\"));\n            return generateCreatedResponse(uri, token).build();\n        } catch (final AuthenticationException e) {\n            throw new AccessDeniedException(e.getMessage(), e);\n        }\n    }\n}",
        "accept_response": "@POST\n@Consumes(MediaType.TEXT_PLAIN)\n@Produces(MediaType.TEXT_PLAIN)\n@Path(\"/kerberos\")\n@ApiOperation(\n        value = \"Creates a token for accessing the REST API via Kerberos ticket exchange / SPNEGO negotiation\",\n        notes = \"The token returned is formatted as a JSON Web Token (JWT). The token is base64 encoded and comprised of three parts. The header, \" +\n                \"the body, and the signature. The expiration of the token is a contained within the body. The token can be used in the Authorization header \" +\n                \"in the format 'Authorization: Bearer <token>'.\",\n        response = String.class\n)\n@ApiResponses(\n        value = {\n                @ApiResponse(code = 400, message = \"NiFi was unable to complete the request because it was invalid. The request should not be retried without modification.\"),\n                @ApiResponse(code = 401, message = \"NiFi was unable to complete the request because it did not contain a valid Kerberos \" +\n                        \"ticket in the Authorization header. Retry this request after initializing a ticket with kinit and \" +\n                        \"ensuring your browser is configured to support SPNEGO.\"),\n                @ApiResponse(code = 409, message = \"Unable to create access token because NiFi is not in the appropriate state. (i.e. may not be configured to support Kerberos login.\"),\n                @ApiResponse(code = 500, message = \"Unable to create access token because an unexpected error occurred.\")\n        }\n)\npublic Response createAccessTokenFromTicket(@Context HttpServletRequest httpServletRequest) {\n\n    // only support access tokens when communicating over HTTPS\n    if (!httpServletRequest.isSecure()) {\n        throw new AuthenticationNotSupportedException(\"Access tokens are only issued over HTTPS.\");\n    }\n\n    // If Kerberos Service Principal and keytab location not configured, throws exception\n    if (!properties.isKerberosSpnegoSupportEnabled() || kerberosService == null) {\n        final String message = \"Kerberos ticket login not supported by this NiFi.\";\n        logger.debug(message);\n        return Response.status(Response.Status.CONFLICT).entity(message).build();\n    }\n\n    String authorizationHeaderValue = httpServletRequest.getHeader(KerberosService.AUTHORIZATION_HEADER_NAME);\n\n    if (!kerberosService.isValidKerberosHeader(authorizationHeaderValue)) {\n        final Response response = generateNotAuthorizedResponse().header(KerberosService.AUTHENTICATION_CHALLENGE_HEADER_NAME, KerberosService.AUTHORIZATION_NEGOTIATE).build();\n        return response;\n    } else {\n        try {\n            // attempt to authenticate\n            Authentication authentication = kerberosService.validateKerberosTicket(httpServletRequest);\n\n            if (authentication == null) {\n                throw new IllegalArgumentException(\"Request is not HTTPS or Kerberos ticket missing or malformed\");\n            }\n\n            final String expirationFromProperties = properties.getKerberosAuthenticationExpiration();\n            long expiration = FormatUtils.getTimeDuration(expirationFromProperties, TimeUnit.MILLISECONDS);\n            final String rawIdentity = authentication.getName();\n            String mappedIdentity = IdentityMappingUtil.mapIdentity(rawIdentity, IdentityMappingUtil.getIdentityMappings(properties));\n            expiration = validateTokenExpiration(expiration, mappedIdentity);\n\n            // create the authentication token\n            final LoginAuthenticationToken loginAuthenticationToken = new LoginAuthenticationToken(mappedIdentity, expiration, \"KerberosService\");\n\n            // generate JWT for response\n            final String token = jwtService.generateSignedToken(loginAuthenticationToken);\n\n            // build the response\n            final URI uri = URI.create(generateResourceUri(\"access\", \"kerberos\"));\n            return generateCreatedResponse(uri, token).build();\n        } catch (final AuthenticationException e) {\n            throw new AccessDeniedException(e.getMessage(), e);\n        }\n    }\n}",
        "reject_response": "@POST\n@Consumes(MediaType.TEXT_PLAIN)\n@Produces(MediaType.TEXT_PLAIN)\n@Path(\"/kerberos\")\n@ApiOperation(\n        value = \"Creates a token for accessing the REST API via Kerberos ticket exchange / SPNEGO negotiation\",\n        notes = \"The token returned is formatted as a JSON Web Token (JWT). The token is base64 encoded and comprised of three parts. The header, \" +\n                \"the body, and the signature. The expiration of the token is a contained within the body. The token can be used in the Authorization header \" +\n                \"in the format 'Authorization: Bearer <token>'.\",\n        response = String.class\n)\n@ApiResponses(\n        value = {\n                @ApiResponse(code = 400, message = \"NiFi was unable to complete the request because it was invalid. The request should not be retried without modification.\"),\n                @ApiResponse(code = 401, message = \"NiFi was unable to complete the request because it did not contain a valid Kerberos \" +\n                        \"ticket in the Authorization header. Retry this request after initializing a ticket with kinit and \" +\n                        \"ensuring your browser is configured to support SPNEGO.\"),\n                @ApiResponse(code = 409, message = \"Unable to create access token because NiFi is not in the appropriate state. (i.e. may not be configured to support Kerberos login.\"),\n                @ApiResponse(code = 500, message = \"Unable to create access token because an unexpected error occurred.\")\n        }\n)\npublic Response createAccessTokenFromTicket(@Context HttpServletRequest httpServletRequest) {\n\n    // only support access tokens when communicating over HTTPS\n    if (!httpServletRequest.isSecure()) {\n        throw new AuthenticationNotSupportedException(\"Access tokens are only issued over HTTPS.\");\n    }\n\n    // If Kerberos Service Principal and keytab location not configured, throws exception\n    if (!properties.isKerberosSpnegoSupportEnabled() || kerberosService == null) {\n        final String message = \"Kerberos ticket login not supported by this NiFi.\";\n        logger.warn(message);\n        return Response.status(Response.Status.CONFLICT).entity(message).build();\n    }\n\n    String authorizationHeaderValue = httpServletRequest.getHeader(KerberosService.AUTHORIZATION_HEADER_NAME);\n\n    if (!kerberosService.isValidKerberosHeader(authorizationHeaderValue)) {\n        final Response response = generateNotAuthorizedResponse().header(KerberosService.AUTHENTICATION_CHALLENGE_HEADER_NAME, KerberosService.AUTHORIZATION_NEGOTIATE).build();\n        return response;\n    } else {\n        try {\n            // attempt to authenticate\n            Authentication authentication = kerberosService.validateKerberosTicket(httpServletRequest);\n\n            if (authentication == null) {\n                throw new IllegalArgumentException(\"Request is not HTTPS or Kerberos ticket missing or malformed\");\n            }\n\n            final String expirationFromProperties = properties.getKerberosAuthenticationExpiration();\n            long expiration = FormatUtils.getTimeDuration(expirationFromProperties, TimeUnit.MILLISECONDS);\n            final String rawIdentity = authentication.getName();\n            String mappedIdentity = IdentityMappingUtil.mapIdentity(rawIdentity, IdentityMappingUtil.getIdentityMappings(properties));\n            expiration = validateTokenExpiration(expiration, mappedIdentity);\n\n            // create the authentication token\n            final LoginAuthenticationToken loginAuthenticationToken = new LoginAuthenticationToken(mappedIdentity, expiration, \"KerberosService\");\n\n            // generate JWT for response\n            final String token = jwtService.generateSignedToken(loginAuthenticationToken);\n\n            // build the response\n            final URI uri = URI.create(generateResourceUri(\"access\", \"kerberos\"));\n            return generateCreatedResponse(uri, token).build();\n        } catch (final AuthenticationException e) {\n            throw new AccessDeniedException(e.getMessage(), e);\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2652,
        "instruction": "@Override\npublic void contextInitialized(ServletContextEvent event) {\n\n  messagesToBeLogged = messagesToBeLogged\n      .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_CONTEXT_INITIALIZED\")));\n\n  // Load Pulse version details\n  loadPulseVersionDetails();\n\n  // Load Pulse Properties\n  pulseProperties = loadProperties(PulseConstants.PULSE_PROPERTIES_FILE);\n\n  if (pulseProperties.isEmpty()) {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_NOT_FOUND\")));\n  } else {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_FOUND\")));\n\n    // set Pulse product support into the Pulse controller for access from\n    // client side\n    // to display the appropriate ui depending on which product is supported\n    // in present deployment\n    String pulseProduct =\n        pulseProperties.getProperty(PulseConstants.APPLICATION_PROPERTY_PULSE_PRODUCTSUPPORT);\n  }\n\n  pulseSecurityProperties = loadProperties(PulseConstants.PULSE_SECURITY_PROPERTIES_FILE);\n\n  // Initialize logger\n  initializeLogger();\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n  logger.info(resourceBundle.getString(\"LOG_MSG_CHECK_APP_RUNNING_MODE\"));\n\n  boolean sysIsEmbedded = Boolean.getBoolean(PulseConstants.SYSTEM_PROPERTY_PULSE_EMBEDDED);\n\n  if (sysIsEmbedded) {\n    // Application Pulse is running in Embedded Mode\n    repository.setIsEmbeddedMode(true);\n\n    sysPulseUseLocator = Boolean.FALSE;\n    try {\n      // Get host name of machine running pulse in embedded mode\n      sysPulseHost = InetAddress.getLocalHost().getCanonicalHostName();\n    } catch (Exception e) {\n      logger.debug(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_UNKNOWN_HOST\"), e);\n      // Set default host name\n      sysPulseHost = PulseConstants.GEMFIRE_DEFAULT_HOST;\n    }\n    sysPulsePort = System.getProperty(PulseConstants.SYSTEM_PROPERTY_PULSE_PORT);\n    if (StringUtils.isBlank(sysPulsePort)) {\n      sysPulsePort = PulseConstants.GEMFIRE_DEFAULT_PORT;\n    }\n\n  } else {\n    // Application Pulse is running in Non-Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_NONEMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(false);\n\n    // Load JMX User Details\n    loadJMXUserDetails();\n    // Load locator and/or manager details\n    loadLocatorManagerDetails();\n\n    useGemFireCredentials = areWeUsingGemFireSecurityProfile(event);\n  }\n\n  // Set user details in repository\n  repository.setJmxUserName(jmxUserName);\n  repository.setJmxUserPassword(jmxUserPassword);\n\n  // Set locator/Manager details in repository\n  repository.setJmxUseLocator(sysPulseUseLocator);\n  repository.setJmxHost(sysPulseHost);\n  repository.setJmxPort(sysPulsePort);\n\n  // set SSL info\n  initializeSSL();\n  repository.setUseSSLLocator(sysPulseUseSSLLocator);\n  repository.setUseSSLManager(sysPulseUseSSLManager);\n\n  repository.setUseGemFireCredentials(useGemFireCredentials);\n\n}",
        "accept_response": "@Override\npublic void contextInitialized(ServletContextEvent event) {\n\n  messagesToBeLogged = messagesToBeLogged\n      .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_CONTEXT_INITIALIZED\")));\n\n  // Load Pulse version details\n  loadPulseVersionDetails();\n\n  // Load Pulse Properties\n  pulseProperties = loadProperties(PulseConstants.PULSE_PROPERTIES_FILE);\n\n  if (pulseProperties.isEmpty()) {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_NOT_FOUND\")));\n  } else {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_FOUND\")));\n\n    // set Pulse product support into the Pulse controller for access from\n    // client side\n    // to display the appropriate ui depending on which product is supported\n    // in present deployment\n    String pulseProduct =\n        pulseProperties.getProperty(PulseConstants.APPLICATION_PROPERTY_PULSE_PRODUCTSUPPORT);\n  }\n\n  pulseSecurityProperties = loadProperties(PulseConstants.PULSE_SECURITY_PROPERTIES_FILE);\n\n  // Initialize logger\n  initializeLogger();\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n  logger.info(resourceBundle.getString(\"LOG_MSG_CHECK_APP_RUNNING_MODE\"));\n\n  boolean sysIsEmbedded = Boolean.getBoolean(PulseConstants.SYSTEM_PROPERTY_PULSE_EMBEDDED);\n\n  if (sysIsEmbedded) {\n    // Application Pulse is running in Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_EMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(true);\n\n    sysPulseUseLocator = Boolean.FALSE;\n    try {\n      // Get host name of machine running pulse in embedded mode\n      sysPulseHost = InetAddress.getLocalHost().getCanonicalHostName();\n    } catch (Exception e) {\n      logger.debug(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_UNKNOWN_HOST\"), e);\n      // Set default host name\n      sysPulseHost = PulseConstants.GEMFIRE_DEFAULT_HOST;\n    }\n    sysPulsePort = System.getProperty(PulseConstants.SYSTEM_PROPERTY_PULSE_PORT);\n    if (StringUtils.isBlank(sysPulsePort)) {\n      sysPulsePort = PulseConstants.GEMFIRE_DEFAULT_PORT;\n    }\n\n  } else {\n    // Application Pulse is running in Non-Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_NONEMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(false);\n\n    // Load JMX User Details\n    loadJMXUserDetails();\n    // Load locator and/or manager details\n    loadLocatorManagerDetails();\n\n    useGemFireCredentials = areWeUsingGemFireSecurityProfile(event);\n  }\n\n  // Set user details in repository\n  repository.setJmxUserName(jmxUserName);\n  repository.setJmxUserPassword(jmxUserPassword);\n\n  // Set locator/Manager details in repository\n  repository.setJmxUseLocator(sysPulseUseLocator);\n  repository.setJmxHost(sysPulseHost);\n  repository.setJmxPort(sysPulsePort);\n\n  // set SSL info\n  initializeSSL();\n  repository.setUseSSLLocator(sysPulseUseSSLLocator);\n  repository.setUseSSLManager(sysPulseUseSSLManager);\n\n  repository.setUseGemFireCredentials(useGemFireCredentials);\n\n}",
        "reject_response": "@Override\npublic void contextInitialized(ServletContextEvent event) {\n\n  messagesToBeLogged = messagesToBeLogged\n      .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_CONTEXT_INITIALIZED\")));\n\n  // Load Pulse version details\n  loadPulseVersionDetails();\n\n  // Load Pulse Properties\n  pulseProperties = loadProperties(PulseConstants.PULSE_PROPERTIES_FILE);\n\n  if (pulseProperties.isEmpty()) {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_NOT_FOUND\")));\n  } else {\n    messagesToBeLogged = messagesToBeLogged\n        .concat(formatLogString(resourceBundle.getString(\"LOG_MSG_PROPERTIES_FOUND\")));\n\n    // set Pulse product support into the Pulse controller for access from\n    // client side\n    // to display the appropriate ui depending on which product is supported\n    // in present deployment\n    String pulseProduct =\n        pulseProperties.getProperty(PulseConstants.APPLICATION_PROPERTY_PULSE_PRODUCTSUPPORT);\n  }\n\n  pulseSecurityProperties = loadProperties(PulseConstants.PULSE_SECURITY_PROPERTIES_FILE);\n\n  // Initialize logger\n  initializeLogger();\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n  logger.info(resourceBundle.getString(\"LOG_MSG_CHECK_APP_RUNNING_MODE\"));\n\n  boolean sysIsEmbedded = Boolean.getBoolean(PulseConstants.SYSTEM_PROPERTY_PULSE_EMBEDDED);\n\n  if (sysIsEmbedded) {\n    // Application Pulse is running in Embedded Mode\n    if (LOGGER.infoEnabled()) {\n      LOGGER.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_EMBEDDED_MODE\"));\n    }\n    repository.setIsEmbeddedMode(true);\n\n    sysPulseUseLocator = Boolean.FALSE;\n    try {\n      // Get host name of machine running pulse in embedded mode\n      sysPulseHost = InetAddress.getLocalHost().getCanonicalHostName();\n    } catch (Exception e) {\n      logger.debug(resourceBundle.getString(\"LOG_MSG_JMX_CONNECTION_UNKNOWN_HOST\"), e);\n      // Set default host name\n      sysPulseHost = PulseConstants.GEMFIRE_DEFAULT_HOST;\n    }\n    sysPulsePort = System.getProperty(PulseConstants.SYSTEM_PROPERTY_PULSE_PORT);\n    if (StringUtils.isBlank(sysPulsePort)) {\n      sysPulsePort = PulseConstants.GEMFIRE_DEFAULT_PORT;\n    }\n\n  } else {\n    // Application Pulse is running in Non-Embedded Mode\n    logger.info(resourceBundle.getString(\"LOG_MSG_APP_RUNNING_NONEMBEDDED_MODE\"));\n    repository.setIsEmbeddedMode(false);\n\n    // Load JMX User Details\n    loadJMXUserDetails();\n    // Load locator and/or manager details\n    loadLocatorManagerDetails();\n\n    useGemFireCredentials = areWeUsingGemFireSecurityProfile(event);\n  }\n\n  // Set user details in repository\n  repository.setJmxUserName(jmxUserName);\n  repository.setJmxUserPassword(jmxUserPassword);\n\n  // Set locator/Manager details in repository\n  repository.setJmxUseLocator(sysPulseUseLocator);\n  repository.setJmxHost(sysPulseHost);\n  repository.setJmxPort(sysPulsePort);\n\n  // set SSL info\n  initializeSSL();\n  repository.setUseSSLLocator(sysPulseUseSSLLocator);\n  repository.setUseSSLManager(sysPulseUseSSLManager);\n\n  repository.setUseGemFireCredentials(useGemFireCredentials);\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2542,
        "instruction": "LogReader getInputStreamInternal(DLSN fromDLSN, Optional<Long> fromTxnId)\n        throws IOException {\n    checkClosedOrInError(\"getInputStream\");\n    return new BKSyncLogReader(\n            conf,\n            this,\n            fromDLSN,\n            fromTxnId,\n            statsLogger);\n}",
        "accept_response": "LogReader getInputStreamInternal(DLSN fromDLSN, Optional<Long> fromTxnId)\n        throws IOException {\n    LOG.info(\"Create sync reader starting from {}\", fromDLSN);\n    checkClosedOrInError(\"getInputStream\");\n    return new BKSyncLogReader(\n            conf,\n            this,\n            fromDLSN,\n            fromTxnId,\n            statsLogger);\n}",
        "reject_response": "LogReader getInputStreamInternal(DLSN fromDLSN, Optional<Long> fromTxnId)\n        throws IOException {\n    LOG.info(\"Create async reader starting from {}\", fromDLSN);\n    checkClosedOrInError(\"getInputStream\");\n    return new BKSyncLogReader(\n            conf,\n            this,\n            fromDLSN,\n            fromTxnId,\n            statsLogger);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3137,
        "instruction": "@Override\npublic void transition(TaskAttemptImpl ta, TaskAttemptEvent event) {\n\n  // If TaskAttempt is recovered to SUCCEEDED, send events generated by this TaskAttempt to vertex\n  // for its downstream consumers. For normal dag execution, the events are sent by TaskAttmeptListener\n  // for performance consideration.\n  if (ta.recoveryData != null && ta.recoveryData.isTaskAttemptSucceeded()) {\n    TaskAttemptFinishedEvent taFinishedEvent = ta.recoveryData\n        .getTaskAttemptFinishedEvent();\n    ta.reportedStatus.counters = taFinishedEvent.getCounters();\n    List<TezEvent> tezEvents = taFinishedEvent.getTAGeneratedEvents();\n    if (tezEvents != null && !tezEvents.isEmpty()) {\n      ta.sendEvent(new VertexEventRouteEvent(ta.getVertexID(), tezEvents));\n    }\n    ta.finishTime = taFinishedEvent.getFinishTime();\n    ta.isRecoveredDuration = true;\n  } else {\n    ta.setFinishTime();\n    // Send out history event.\n    ta.logJobHistoryAttemptFinishedEvent(TaskAttemptStateInternal.SUCCEEDED);\n  }\n\n  ta.sendEvent(createDAGCounterUpdateEventTAFinished(ta,\n      TaskAttemptState.SUCCEEDED));\n\n  // Inform the Scheduler.\n  ta.sendEvent(new AMSchedulerEventTAEnded(ta, ta.containerId,\n      TaskAttemptState.SUCCEEDED, null, null, ta.getVertex().getTaskSchedulerIdentifier()));\n\n  // Inform the task.\n  ta.sendEvent(new TaskEventTASucceeded(ta.attemptId));\n\n  // Unregister from the TaskHeartbeatHandler.\n  ta.taskHeartbeatHandler.unregister(ta.attemptId);\n\n  ta.reportedStatus.state = TaskAttemptState.SUCCEEDED;\n  ta.reportedStatus.progress = 1.0f;\n\n  if (ta.isSpeculationEnabled()) {\n    ta.sendEvent(new SpeculatorEventTaskAttemptStatusUpdate(ta.attemptId, TaskAttemptState.SUCCEEDED,\n        ta.clock.getTime()));\n  }\n\n  // TODO maybe. For reuse ... Stacking pulls for a reduce task, even if the\n  // TA finishes independently. // Will likely be the Job's responsibility.\n\n}",
        "accept_response": "@Override\npublic void transition(TaskAttemptImpl ta, TaskAttemptEvent event) {\n\n  // If TaskAttempt is recovered to SUCCEEDED, send events generated by this TaskAttempt to vertex\n  // for its downstream consumers. For normal dag execution, the events are sent by TaskAttmeptListener\n  // for performance consideration.\n  if (ta.recoveryData != null && ta.recoveryData.isTaskAttemptSucceeded()) {\n    TaskAttemptFinishedEvent taFinishedEvent = ta.recoveryData\n        .getTaskAttemptFinishedEvent();\n    LOG.debug(\"TaskAttempt is recovered to SUCCEEDED, attemptId={}\", ta.attemptId);\n    ta.reportedStatus.counters = taFinishedEvent.getCounters();\n    List<TezEvent> tezEvents = taFinishedEvent.getTAGeneratedEvents();\n    if (tezEvents != null && !tezEvents.isEmpty()) {\n      ta.sendEvent(new VertexEventRouteEvent(ta.getVertexID(), tezEvents));\n    }\n    ta.finishTime = taFinishedEvent.getFinishTime();\n    ta.isRecoveredDuration = true;\n  } else {\n    ta.setFinishTime();\n    // Send out history event.\n    ta.logJobHistoryAttemptFinishedEvent(TaskAttemptStateInternal.SUCCEEDED);\n  }\n\n  ta.sendEvent(createDAGCounterUpdateEventTAFinished(ta,\n      TaskAttemptState.SUCCEEDED));\n\n  // Inform the Scheduler.\n  ta.sendEvent(new AMSchedulerEventTAEnded(ta, ta.containerId,\n      TaskAttemptState.SUCCEEDED, null, null, ta.getVertex().getTaskSchedulerIdentifier()));\n\n  // Inform the task.\n  ta.sendEvent(new TaskEventTASucceeded(ta.attemptId));\n\n  // Unregister from the TaskHeartbeatHandler.\n  ta.taskHeartbeatHandler.unregister(ta.attemptId);\n\n  ta.reportedStatus.state = TaskAttemptState.SUCCEEDED;\n  ta.reportedStatus.progress = 1.0f;\n\n  if (ta.isSpeculationEnabled()) {\n    ta.sendEvent(new SpeculatorEventTaskAttemptStatusUpdate(ta.attemptId, TaskAttemptState.SUCCEEDED,\n        ta.clock.getTime()));\n  }\n\n  // TODO maybe. For reuse ... Stacking pulls for a reduce task, even if the\n  // TA finishes independently. // Will likely be the Job's responsibility.\n\n}",
        "reject_response": "@Override\npublic void transition(TaskAttemptImpl ta, TaskAttemptEvent event) {\n\n  // If TaskAttempt is recovered to SUCCEEDED, send events generated by this TaskAttempt to vertex\n  // for its downstream consumers. For normal dag execution, the events are sent by TaskAttmeptListener\n  // for performance consideration.\n  if (ta.recoveryData != null && ta.recoveryData.isTaskAttemptSucceeded()) {\n    TaskAttemptFinishedEvent taFinishedEvent = ta.recoveryData\n        .getTaskAttemptFinishedEvent();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"TaskAttempt is recovered to SUCCEEDED, attemptId=\" + ta.attemptId);\n    }\n    ta.reportedStatus.counters = taFinishedEvent.getCounters();\n    List<TezEvent> tezEvents = taFinishedEvent.getTAGeneratedEvents();\n    if (tezEvents != null && !tezEvents.isEmpty()) {\n      ta.sendEvent(new VertexEventRouteEvent(ta.getVertexID(), tezEvents));\n    }\n    ta.finishTime = taFinishedEvent.getFinishTime();\n    ta.isRecoveredDuration = true;\n  } else {\n    ta.setFinishTime();\n    // Send out history event.\n    ta.logJobHistoryAttemptFinishedEvent(TaskAttemptStateInternal.SUCCEEDED);\n  }\n\n  ta.sendEvent(createDAGCounterUpdateEventTAFinished(ta,\n      TaskAttemptState.SUCCEEDED));\n\n  // Inform the Scheduler.\n  ta.sendEvent(new AMSchedulerEventTAEnded(ta, ta.containerId,\n      TaskAttemptState.SUCCEEDED, null, null, ta.getVertex().getTaskSchedulerIdentifier()));\n\n  // Inform the task.\n  ta.sendEvent(new TaskEventTASucceeded(ta.attemptId));\n\n  // Unregister from the TaskHeartbeatHandler.\n  ta.taskHeartbeatHandler.unregister(ta.attemptId);\n\n  ta.reportedStatus.state = TaskAttemptState.SUCCEEDED;\n  ta.reportedStatus.progress = 1.0f;\n\n  if (ta.isSpeculationEnabled()) {\n    ta.sendEvent(new SpeculatorEventTaskAttemptStatusUpdate(ta.attemptId, TaskAttemptState.SUCCEEDED,\n        ta.clock.getTime()));\n  }\n\n  // TODO maybe. For reuse ... Stacking pulls for a reduce task, even if the\n  // TA finishes independently. // Will likely be the Job's responsibility.\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3064,
        "instruction": "public synchronized void waitForConnected(long waitForConnection)\n    throws TimeoutException {\n  log.debug(\"Waiting for client to connect to ZooKeeper\");\n  long expire = System.nanoTime() + TimeUnit.NANOSECONDS.convert(waitForConnection, TimeUnit.MILLISECONDS);\n  long left = 1;\n  while (!connected && left > 0) {\n    if (isClosed) {\n      break;\n    }\n    try {\n      wait(500);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      break;\n    }\n    left = expire - System.nanoTime();\n  }\n  if (!connected) {\n    throw new TimeoutException(\"Could not connect to ZooKeeper \" + zkServerAddress + \" within \" + waitForConnection + \" ms\");\n  }\n}",
        "accept_response": "public synchronized void waitForConnected(long waitForConnection)\n    throws TimeoutException {\n  log.debug(\"Waiting for client to connect to ZooKeeper\");\n  long expire = System.nanoTime() + TimeUnit.NANOSECONDS.convert(waitForConnection, TimeUnit.MILLISECONDS);\n  long left = 1;\n  while (!connected && left > 0) {\n    if (isClosed) {\n      break;\n    }\n    try {\n      wait(500);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      break;\n    }\n    left = expire - System.nanoTime();\n  }\n  if (!connected) {\n    throw new TimeoutException(\"Could not connect to ZooKeeper \" + zkServerAddress + \" within \" + waitForConnection + \" ms\");\n  }\n  log.debug(\"Client is connected to ZooKeeper\");\n}",
        "reject_response": "public synchronized void waitForConnected(long waitForConnection)\n    throws TimeoutException {\n  log.debug(\"Waiting for client to connect to ZooKeeper\");\n  long expire = System.nanoTime() + TimeUnit.NANOSECONDS.convert(waitForConnection, TimeUnit.MILLISECONDS);\n  long left = 1;\n  while (!connected && left > 0) {\n    if (isClosed) {\n      break;\n    }\n    try {\n      wait(500);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      break;\n    }\n    left = expire - System.nanoTime();\n  }\n  if (!connected) {\n    throw new TimeoutException(\"Could not connect to ZooKeeper \" + zkServerAddress + \" within \" + waitForConnection + \" ms\");\n  }\n  log.info(\"Client is connected to ZooKeeper\");\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2540,
        "instruction": "@SuppressWarnings(\"unchecked\")\n@Override\npublic void aroundWriteTo(WriterInterceptorContext c) throws IOException, WebApplicationException {\n\n    if (LOG.isLoggable(Level.FINE)) {\n        String writerClassName = writer == null ? \"null\" : writer.getClass().getName();\n    }\n\n    MultivaluedMap<String, Object> headers = c.getHeaders();\n    Object mtObject = headers.getFirst(HttpHeaders.CONTENT_TYPE);\n    MediaType entityMt = mtObject == null ? c.getMediaType() : JAXRSUtils.toMediaType(mtObject.toString());\n    m.put(Message.CONTENT_TYPE, entityMt.toString());\n\n    Class<?> entityCls = c.getType();\n    Type entityType = c.getGenericType();\n    Annotation[] entityAnns = c.getAnnotations();\n\n    if (writer == null\n        || m.get(ProviderFactory.PROVIDER_SELECTION_PROPERTY_CHANGED) == Boolean.TRUE\n        && !writer.isWriteable(entityCls, entityType, entityAnns, entityMt)) {\n\n        writer = (MessageBodyWriter<Object>)ProviderFactory.getInstance(m)\n            .createMessageBodyWriter(entityCls, entityType, entityAnns, entityMt, m);\n        if (writer == null) {\n            String errorMessage = JAXRSUtils.logMessageHandlerProblem(\"NO_MSG_WRITER\", entityCls, entityMt);\n            throw new ProcessingException(errorMessage);\n        }\n    }\n\n    HttpUtils.convertHeaderValuesToString(headers, true);\n\n    writer.writeTo(c.getEntity(),\n                   c.getType(),\n                   c.getGenericType(),\n                   c.getAnnotations(),\n                   entityMt,\n                   headers,\n                   c.getOutputStream());\n}",
        "accept_response": "@SuppressWarnings(\"unchecked\")\n@Override\npublic void aroundWriteTo(WriterInterceptorContext c) throws IOException, WebApplicationException {\n\n    if (LOG.isLoggable(Level.FINE)) {\n        String writerClassName = writer == null ? \"null\" : writer.getClass().getName();\n        LOG.fine(\"Response EntityProvider is: \" + writerClassName);\n    }\n\n    MultivaluedMap<String, Object> headers = c.getHeaders();\n    Object mtObject = headers.getFirst(HttpHeaders.CONTENT_TYPE);\n    MediaType entityMt = mtObject == null ? c.getMediaType() : JAXRSUtils.toMediaType(mtObject.toString());\n    m.put(Message.CONTENT_TYPE, entityMt.toString());\n\n    Class<?> entityCls = c.getType();\n    Type entityType = c.getGenericType();\n    Annotation[] entityAnns = c.getAnnotations();\n\n    if (writer == null\n        || m.get(ProviderFactory.PROVIDER_SELECTION_PROPERTY_CHANGED) == Boolean.TRUE\n        && !writer.isWriteable(entityCls, entityType, entityAnns, entityMt)) {\n\n        writer = (MessageBodyWriter<Object>)ProviderFactory.getInstance(m)\n            .createMessageBodyWriter(entityCls, entityType, entityAnns, entityMt, m);\n        if (writer == null) {\n            String errorMessage = JAXRSUtils.logMessageHandlerProblem(\"NO_MSG_WRITER\", entityCls, entityMt);\n            throw new ProcessingException(errorMessage);\n        }\n    }\n\n    HttpUtils.convertHeaderValuesToString(headers, true);\n\n    writer.writeTo(c.getEntity(),\n                   c.getType(),\n                   c.getGenericType(),\n                   c.getAnnotations(),\n                   entityMt,\n                   headers,\n                   c.getOutputStream());\n}",
        "reject_response": "@SuppressWarnings(\"unchecked\")\n@Override\npublic void aroundWriteTo(WriterInterceptorContext c) throws IOException, WebApplicationException {\n\n    if (LOG.isLoggable(Level.FINE)) {\n        String writerClassName = writer == null ? \"null\" : writer.getClass().getName();\n        LOG.fine(\"Response EntityProvider is: \" + writer.getClass().getName());\n    }\n\n    MultivaluedMap<String, Object> headers = c.getHeaders();\n    Object mtObject = headers.getFirst(HttpHeaders.CONTENT_TYPE);\n    MediaType entityMt = mtObject == null ? c.getMediaType() : JAXRSUtils.toMediaType(mtObject.toString());\n    m.put(Message.CONTENT_TYPE, entityMt.toString());\n\n    Class<?> entityCls = c.getType();\n    Type entityType = c.getGenericType();\n    Annotation[] entityAnns = c.getAnnotations();\n\n    if (writer == null\n        || m.get(ProviderFactory.PROVIDER_SELECTION_PROPERTY_CHANGED) == Boolean.TRUE\n        && !writer.isWriteable(entityCls, entityType, entityAnns, entityMt)) {\n\n        writer = (MessageBodyWriter<Object>)ProviderFactory.getInstance(m)\n            .createMessageBodyWriter(entityCls, entityType, entityAnns, entityMt, m);\n        if (writer == null) {\n            String errorMessage = JAXRSUtils.logMessageHandlerProblem(\"NO_MSG_WRITER\", entityCls, entityMt);\n            throw new ProcessingException(errorMessage);\n        }\n    }\n\n    HttpUtils.convertHeaderValuesToString(headers, true);\n\n    writer.writeTo(c.getEntity(),\n                   c.getType(),\n                   c.getGenericType(),\n                   c.getAnnotations(),\n                   entityMt,\n                   headers,\n                   c.getOutputStream());\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3249,
        "instruction": "private Packet findSendablePacket(LinkedBlockingDeque<Packet> outgoingQueue, boolean tunneledAuthInProgres) {\n    if (outgoingQueue.isEmpty()) {\n        return null;\n    }\n    // If we've already starting sending the first packet, we better finish\n    if (outgoingQueue.getFirst().bb != null || !tunneledAuthInProgres) {\n        return outgoingQueue.getFirst();\n    }\n    // Since client's authentication with server is in progress,\n    // send only the null-header packet queued by primeConnection().\n    // This packet must be sent so that the SASL authentication process\n    // can proceed, but all other packets should wait until\n    // SASL authentication completes.\n    Iterator<Packet> iter = outgoingQueue.iterator();\n    while (iter.hasNext()) {\n        Packet p = iter.next();\n        if (p.requestHeader == null) {\n            // We've found the priming-packet. Move it to the beginning of the queue.\n            iter.remove();\n            outgoingQueue.addFirst(p);\n            return p;\n        } else {\n            // Non-priming packet: defer it until later, leaving it in the queue\n            // until authentication completes.\n        }\n    }\n    return null;\n}",
        "accept_response": "private Packet findSendablePacket(LinkedBlockingDeque<Packet> outgoingQueue, boolean tunneledAuthInProgres) {\n    if (outgoingQueue.isEmpty()) {\n        return null;\n    }\n    // If we've already starting sending the first packet, we better finish\n    if (outgoingQueue.getFirst().bb != null || !tunneledAuthInProgres) {\n        return outgoingQueue.getFirst();\n    }\n    // Since client's authentication with server is in progress,\n    // send only the null-header packet queued by primeConnection().\n    // This packet must be sent so that the SASL authentication process\n    // can proceed, but all other packets should wait until\n    // SASL authentication completes.\n    Iterator<Packet> iter = outgoingQueue.iterator();\n    while (iter.hasNext()) {\n        Packet p = iter.next();\n        if (p.requestHeader == null) {\n            // We've found the priming-packet. Move it to the beginning of the queue.\n            iter.remove();\n            outgoingQueue.addFirst(p);\n            return p;\n        } else {\n            // Non-priming packet: defer it until later, leaving it in the queue\n            // until authentication completes.\n            LOG.debug(\"Deferring non-priming packet {} until SASL authentication completes.\", p);\n        }\n    }\n    return null;\n}",
        "reject_response": "private Packet findSendablePacket(LinkedBlockingDeque<Packet> outgoingQueue, boolean tunneledAuthInProgres) {\n    if (outgoingQueue.isEmpty()) {\n        return null;\n    }\n    // If we've already starting sending the first packet, we better finish\n    if (outgoingQueue.getFirst().bb != null || !tunneledAuthInProgres) {\n        return outgoingQueue.getFirst();\n    }\n    // Since client's authentication with server is in progress,\n    // send only the null-header packet queued by primeConnection().\n    // This packet must be sent so that the SASL authentication process\n    // can proceed, but all other packets should wait until\n    // SASL authentication completes.\n    Iterator<Packet> iter = outgoingQueue.iterator();\n    while (iter.hasNext()) {\n        Packet p = iter.next();\n        if (p.requestHeader == null) {\n            // We've found the priming-packet. Move it to the beginning of the queue.\n            iter.remove();\n            outgoingQueue.addFirst(p);\n            return p;\n        } else {\n            // Non-priming packet: defer it until later, leaving it in the queue\n            // until authentication completes.\n            LOG.debug(\"deferring non-priming packet {} until SASL authentation completes.\", p);\n        }\n    }\n    return null;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3206,
        "instruction": "@SuppressWarnings(\"unchecked\")\n@Override\npublic void run() {\n  try {\n    doAppLogAggregation();\n  } catch (Exception e) {\n    // do post clean up of log directories on any exception\n    LOG.error(\"Error occurred while aggregating the log for the application \"\n        + appId, e);\n    doAppLogAggregationPostCleanUp();\n  } finally {\n    if (!this.appAggregationFinished.get() && !this.aborted.get()) {\n      this.dispatcher.getEventHandler().handle(\n          new ApplicationEvent(this.appId,\n              ApplicationEventType.APPLICATION_LOG_HANDLING_FAILED));\n    }\n    this.appAggregationFinished.set(true);\n  }\n}",
        "accept_response": "@SuppressWarnings(\"unchecked\")\n@Override\npublic void run() {\n  try {\n    doAppLogAggregation();\n  } catch (Exception e) {\n    // do post clean up of log directories on any exception\n    LOG.error(\"Error occurred while aggregating the log for the application \"\n        + appId, e);\n    doAppLogAggregationPostCleanUp();\n  } finally {\n    if (!this.appAggregationFinished.get() && !this.aborted.get()) {\n      LOG.warn(\"Log aggregation did not complete for application \" + appId);\n      this.dispatcher.getEventHandler().handle(\n          new ApplicationEvent(this.appId,\n              ApplicationEventType.APPLICATION_LOG_HANDLING_FAILED));\n    }\n    this.appAggregationFinished.set(true);\n  }\n}",
        "reject_response": "@SuppressWarnings(\"unchecked\")\n@Override\npublic void run() {\n  try {\n    doAppLogAggregation();\n  } catch (Exception e) {\n    // do post clean up of log directories on any exception\n    LOG.error(\"Error occurred while aggregating the log for the application \"\n        + appId, e);\n    doAppLogAggregationPostCleanUp();\n  } finally {\n    if (!this.appAggregationFinished.get() && !this.aborted.get()) {\n    if (!this.appAggregationFinished.get()) {\n      LOG.warn(\"Aggregation did not complete for application \" + appId);\n      this.dispatcher.getEventHandler().handle(\n          new ApplicationEvent(this.appId,\n              ApplicationEventType.APPLICATION_LOG_HANDLING_FAILED));\n    }\n    this.appAggregationFinished.set(true);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2874,
        "instruction": "@Override\npublic Node get1(Var var)\n{\n    try {\n        Node n = cacheGet(var) ;\n        if ( n != null )\n            return n ;\n\n        NodeId id = idBinding.get(var) ;\n        if ( id == null )\n            return null ;\n        n = nodeTable.getNodeForNodeId(id) ;\n        // Update cache.\n        cachePut(var, n) ;\n        return n ;\n    } catch (Exception ex)\n    {\n        return null ;\n    }\n}",
        "accept_response": "@Override\npublic Node get1(Var var)\n{\n    try {\n        Node n = cacheGet(var) ;\n        if ( n != null )\n            return n ;\n\n        NodeId id = idBinding.get(var) ;\n        if ( id == null )\n            return null ;\n        n = nodeTable.getNodeForNodeId(id) ;\n        // Update cache.\n        cachePut(var, n) ;\n        return n ;\n    } catch (Exception ex)\n    {\n        Log.error(this, String.format(\"get1(%s)\", var), ex) ;\n        return null ;\n    }\n}",
        "reject_response": "@Override\npublic Node get1(Var var)\n{\n    try {\n        Node n = cacheGet(var) ;\n        if ( n != null )\n            return n ;\n\n        NodeId id = idBinding.get(var) ;\n        if ( id == null )\n            return null ;\n        n = nodeTable.getNodeForNodeId(id) ;\n        // Update cache.\n        cachePut(var, n) ;\n        return n ;\n    } catch (Exception ex)\n    {\n        Log.fatal(this, String.format(\"get1(%s)\", var), ex) ;\n        return null ;\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2923,
        "instruction": "void commitImpl(final boolean startNewTransaction) {\n    metrics.metrics.measureLatencyNs(\n        time,\n        new Runnable() {\n            @Override\n            public void run() {\n                flushState();\n                if (!eosEnabled) {\n                    stateMgr.checkpoint(recordCollectorOffsets());\n                }\n                commitOffsets(startNewTransaction);\n            }\n        },\n        metrics.taskCommitTimeSensor);\n\n    commitRequested = false;\n}",
        "accept_response": "void commitImpl(final boolean startNewTransaction) {\n    log.debug(\"{} Committing\", logPrefix);\n    metrics.metrics.measureLatencyNs(\n        time,\n        new Runnable() {\n            @Override\n            public void run() {\n                flushState();\n                if (!eosEnabled) {\n                    stateMgr.checkpoint(recordCollectorOffsets());\n                }\n                commitOffsets(startNewTransaction);\n            }\n        },\n        metrics.taskCommitTimeSensor);\n\n    commitRequested = false;\n}",
        "reject_response": "void commitImpl(final boolean startNewTransaction) {\n    log.trace(\"{} Committing\", logPrefix);\n    metrics.metrics.measureLatencyNs(\n        time,\n        new Runnable() {\n            @Override\n            public void run() {\n                flushState();\n                if (!eosEnabled) {\n                    stateMgr.checkpoint(recordCollectorOffsets());\n                }\n                commitOffsets(startNewTransaction);\n            }\n        },\n        metrics.taskCommitTimeSensor);\n\n    commitRequested = false;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2723,
        "instruction": "public boolean rebuildMeta() throws IOException, InterruptedException {\n  // TODO check to make sure hbase is offline. (or at least the table\n  // currently being worked on is off line)\n\n  // Determine what's on HDFS\n  loadHdfsRegionDirs(); // populating regioninfo table.\n\n  int errs = errors.getErrorList().size();\n  tablesInfo = loadHdfsRegionInfos(); // update tableInfos based on region info in fs.\n  checkHdfsIntegrity(false, false);\n\n  // make sure ok.\n  if (errors.getErrorList().size() != errs) {\n    // While in error state, iterate until no more fixes possible\n    while(true) {\n      fixes = 0;\n      suggestFixes(tablesInfo);\n      errors.clear();\n      loadHdfsRegionInfos(); // update tableInfos based on region info in fs.\n      checkHdfsIntegrity(shouldFixHdfsHoles(), shouldFixHdfsOverlaps());\n\n      int errCount = errors.getErrorList().size();\n\n      if (fixes == 0) {\n        if (errCount > 0) {\n          return false; // failed to fix problems.\n        } else {\n          break; // no fixes and no problems? drop out and fix stuff!\n        }\n      }\n    }\n  }\n\n  // we can rebuild, move old meta out of the way and start\n  LOG.info(\"HDFS regioninfo's seems good.  Sidelining old hbase:meta\");\n  Path backupDir = sidelineOldMeta();\n  HRegion meta = createNewMeta();\n  // Save off the waldir we're writing to so we can delete it when done.\n  Path waldir = ((AbstractFSWAL)meta.getWAL()).getCurrentFileName().getParent();\n  try {\n    // populate meta\n    List<Put> puts = generatePuts(tablesInfo);\n    if (puts == null) {\n      LOG.error(\"Problem encountered when creating new hbase:meta entries.  \" +\n          \"You may need to restore the previously sidelined hbase:meta\");\n      return false;\n    }\n    meta.batchMutate(puts.toArray(new Put[puts.size()]));\n  } finally {\n    HBaseTestingUtility.closeRegionAndWAL(meta);\n    // Clean out the WAL we created and used here.\n    boolean deleteWalDir = HBCKFsUtils.delete(waldir.getFileSystem(getConf()), waldir, true);\n  }\n  LOG.info(\"Success! hbase:meta table rebuilt. Old hbase:meta moved into \" + backupDir);\n  return true;\n}",
        "accept_response": "public boolean rebuildMeta() throws IOException, InterruptedException {\n  // TODO check to make sure hbase is offline. (or at least the table\n  // currently being worked on is off line)\n\n  // Determine what's on HDFS\n  loadHdfsRegionDirs(); // populating regioninfo table.\n\n  int errs = errors.getErrorList().size();\n  tablesInfo = loadHdfsRegionInfos(); // update tableInfos based on region info in fs.\n  checkHdfsIntegrity(false, false);\n\n  // make sure ok.\n  if (errors.getErrorList().size() != errs) {\n    // While in error state, iterate until no more fixes possible\n    while(true) {\n      fixes = 0;\n      suggestFixes(tablesInfo);\n      errors.clear();\n      loadHdfsRegionInfos(); // update tableInfos based on region info in fs.\n      checkHdfsIntegrity(shouldFixHdfsHoles(), shouldFixHdfsOverlaps());\n\n      int errCount = errors.getErrorList().size();\n\n      if (fixes == 0) {\n        if (errCount > 0) {\n          return false; // failed to fix problems.\n        } else {\n          break; // no fixes and no problems? drop out and fix stuff!\n        }\n      }\n    }\n  }\n\n  // we can rebuild, move old meta out of the way and start\n  LOG.info(\"HDFS regioninfo's seems good.  Sidelining old hbase:meta\");\n  Path backupDir = sidelineOldMeta();\n  HRegion meta = createNewMeta();\n  // Save off the waldir we're writing to so we can delete it when done.\n  Path waldir = ((AbstractFSWAL)meta.getWAL()).getCurrentFileName().getParent();\n  try {\n    // populate meta\n    List<Put> puts = generatePuts(tablesInfo);\n    if (puts == null) {\n      LOG.error(\"Problem encountered when creating new hbase:meta entries.  \" +\n          \"You may need to restore the previously sidelined hbase:meta\");\n      return false;\n    }\n    meta.batchMutate(puts.toArray(new Put[puts.size()]));\n  } finally {\n    HBaseTestingUtility.closeRegionAndWAL(meta);\n    // Clean out the WAL we created and used here.\n    boolean deleteWalDir = HBCKFsUtils.delete(waldir.getFileSystem(getConf()), waldir, true);\n    LOG.info(\"Deleting WAL directory {}, result={}\", waldir, deleteWalDir);\n  }\n  LOG.info(\"Success! hbase:meta table rebuilt. Old hbase:meta moved into \" + backupDir);\n  return true;\n}",
        "reject_response": "public boolean rebuildMeta() throws IOException, InterruptedException {\n  // TODO check to make sure hbase is offline. (or at least the table\n  // currently being worked on is off line)\n\n  // Determine what's on HDFS\n  loadHdfsRegionDirs(); // populating regioninfo table.\n\n  int errs = errors.getErrorList().size();\n  tablesInfo = loadHdfsRegionInfos(); // update tableInfos based on region info in fs.\n  checkHdfsIntegrity(false, false);\n\n  // make sure ok.\n  if (errors.getErrorList().size() != errs) {\n    // While in error state, iterate until no more fixes possible\n    while(true) {\n      fixes = 0;\n      suggestFixes(tablesInfo);\n      errors.clear();\n      loadHdfsRegionInfos(); // update tableInfos based on region info in fs.\n      checkHdfsIntegrity(shouldFixHdfsHoles(), shouldFixHdfsOverlaps());\n\n      int errCount = errors.getErrorList().size();\n\n      if (fixes == 0) {\n        if (errCount > 0) {\n          return false; // failed to fix problems.\n        } else {\n          break; // no fixes and no problems? drop out and fix stuff!\n        }\n      }\n    }\n  }\n\n  // we can rebuild, move old meta out of the way and start\n  LOG.info(\"HDFS regioninfo's seems good.  Sidelining old hbase:meta\");\n  Path backupDir = sidelineOldMeta();\n  HRegion meta = createNewMeta();\n  // Save off the waldir we're writing to so we can delete it when done.\n  Path waldir = ((AbstractFSWAL)meta.getWAL()).getCurrentFileName().getParent();\n  try {\n    // populate meta\n    List<Put> puts = generatePuts(tablesInfo);\n    if (puts == null) {\n      LOG.error(\"Problem encountered when creating new hbase:meta entries.  \" +\n          \"You may need to restore the previously sidelined hbase:meta\");\n      return false;\n    }\n    meta.batchMutate(puts.toArray(new Put[puts.size()]));\n  } finally {\n    HBaseTestingUtility.closeRegionAndWAL(meta);\n    // Clean out the WAL we created and used here.\n    boolean deleteWalDir = HBCKFsUtils.delete(waldir.getFileSystem(getConf()), waldir, true);\n    LOG.info(\"Deleting {}, result={}\", waldir,\n        HBCKFsUtils.delete(waldir.getFileSystem(getConf()), waldir, true));\n  }\n  LOG.info(\"Success! hbase:meta table rebuilt. Old hbase:meta moved into \" + backupDir);\n  return true;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2872,
        "instruction": "@Override\npublic QueryIterator execute(BasicPattern pattern, QueryIterator input, ExecutionContext execCxt) {\n    if ( input == null )\n\n    Graph graph = execCxt.getActiveGraph() ;\n\n    // Choose reorder transformation and execution strategy.\n\n    ReorderTransformation reorder = reorderFixed ;\n    StageGenerator executor = StageBuilder.executeInline ;\n\n    return execute(pattern, reorder, executor, input, execCxt) ;\n}",
        "accept_response": "@Override\npublic QueryIterator execute(BasicPattern pattern, QueryIterator input, ExecutionContext execCxt) {\n    if ( input == null )\n        Log.error(this, \"Null input to \" + Lib.classShortName(this.getClass())) ;\n\n    Graph graph = execCxt.getActiveGraph() ;\n\n    // Choose reorder transformation and execution strategy.\n\n    ReorderTransformation reorder = reorderFixed ;\n    StageGenerator executor = StageBuilder.executeInline ;\n\n    return execute(pattern, reorder, executor, input, execCxt) ;\n}",
        "reject_response": "@Override\npublic QueryIterator execute(BasicPattern pattern, QueryIterator input, ExecutionContext execCxt) {\n    if ( input == null )\n        Log.fatal(this, \"Null input to \" + Lib.classShortName(this.getClass())) ;\n\n    Graph graph = execCxt.getActiveGraph() ;\n\n    // Choose reorder transformation and execution strategy.\n\n    ReorderTransformation reorder = reorderFixed ;\n    StageGenerator executor = StageBuilder.executeInline ;\n\n    return execute(pattern, reorder, executor, input, execCxt) ;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2785,
        "instruction": "ClassicHttpResponse cacheAndReturnResponse(\n        final HttpHost target,\n        final HttpRequest request,\n        final ClassicHttpResponse backendResponse,\n        final ExecChain.Scope scope,\n        final Date requestSent,\n        final Date responseReceived) throws IOException {\n    LOG.debug(\"Caching backend response\");\n    final ByteArrayBuffer buf;\n    final HttpEntity entity = backendResponse.getEntity();\n    if (entity != null) {\n        buf = new ByteArrayBuffer(1024);\n        final InputStream inStream = entity.getContent();\n        final byte[] tmp = new byte[2048];\n        long total = 0;\n        int l;\n        while ((l = inStream.read(tmp)) != -1) {\n            buf.append(tmp, 0, l);\n            total += l;\n            if (total > cacheConfig.getMaxObjectSize()) {\n                LOG.debug(\"Backend response content length exceeds maximum\");\n                backendResponse.setEntity(new CombinedEntity(entity, buf));\n                return backendResponse;\n            }\n        }\n    } else {\n        buf = null;\n    }\n    backendResponse.close();\n\n    final HttpCacheEntry cacheEntry;\n    if (cacheConfig.isFreshnessCheckEnabled()) {\n        final HttpCacheEntry existingEntry = responseCache.getCacheEntry(target, request);\n        if (DateUtils.isAfter(existingEntry, backendResponse, HttpHeaders.DATE)) {\n            cacheEntry = existingEntry;\n        } else {\n            cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n            LOG.debug(\"Backend response successfully cached\");\n        }\n    } else {\n        cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n        LOG.debug(\"Backend response successfully cached (freshness check skipped)\");\n    }\n    return convert(responseGenerator.generateResponse(request, cacheEntry), scope);\n}",
        "accept_response": "ClassicHttpResponse cacheAndReturnResponse(\n        final HttpHost target,\n        final HttpRequest request,\n        final ClassicHttpResponse backendResponse,\n        final ExecChain.Scope scope,\n        final Date requestSent,\n        final Date responseReceived) throws IOException {\n    LOG.debug(\"Caching backend response\");\n    final ByteArrayBuffer buf;\n    final HttpEntity entity = backendResponse.getEntity();\n    if (entity != null) {\n        buf = new ByteArrayBuffer(1024);\n        final InputStream inStream = entity.getContent();\n        final byte[] tmp = new byte[2048];\n        long total = 0;\n        int l;\n        while ((l = inStream.read(tmp)) != -1) {\n            buf.append(tmp, 0, l);\n            total += l;\n            if (total > cacheConfig.getMaxObjectSize()) {\n                LOG.debug(\"Backend response content length exceeds maximum\");\n                backendResponse.setEntity(new CombinedEntity(entity, buf));\n                return backendResponse;\n            }\n        }\n    } else {\n        buf = null;\n    }\n    backendResponse.close();\n\n    final HttpCacheEntry cacheEntry;\n    if (cacheConfig.isFreshnessCheckEnabled()) {\n        final HttpCacheEntry existingEntry = responseCache.getCacheEntry(target, request);\n        if (DateUtils.isAfter(existingEntry, backendResponse, HttpHeaders.DATE)) {\n            LOG.debug(\"Backend already contains fresher cache entry\");\n            cacheEntry = existingEntry;\n        } else {\n            cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n            LOG.debug(\"Backend response successfully cached\");\n        }\n    } else {\n        cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n        LOG.debug(\"Backend response successfully cached (freshness check skipped)\");\n    }\n    return convert(responseGenerator.generateResponse(request, cacheEntry), scope);\n}",
        "reject_response": "ClassicHttpResponse cacheAndReturnResponse(\n        final HttpHost target,\n        final HttpRequest request,\n        final ClassicHttpResponse backendResponse,\n        final ExecChain.Scope scope,\n        final Date requestSent,\n        final Date responseReceived) throws IOException {\n    LOG.debug(\"Caching backend response\");\n    final ByteArrayBuffer buf;\n    final HttpEntity entity = backendResponse.getEntity();\n    if (entity != null) {\n        buf = new ByteArrayBuffer(1024);\n        final InputStream inStream = entity.getContent();\n        final byte[] tmp = new byte[2048];\n        long total = 0;\n        int l;\n        while ((l = inStream.read(tmp)) != -1) {\n            buf.append(tmp, 0, l);\n            total += l;\n            if (total > cacheConfig.getMaxObjectSize()) {\n                LOG.debug(\"Backend response content length exceeds maximum\");\n                backendResponse.setEntity(new CombinedEntity(entity, buf));\n                return backendResponse;\n            }\n        }\n    } else {\n        buf = null;\n    }\n    backendResponse.close();\n\n    final HttpCacheEntry cacheEntry;\n    if (cacheConfig.isFreshnessCheckEnabled()) {\n        final HttpCacheEntry existingEntry = responseCache.getCacheEntry(target, request);\n        if (DateUtils.isAfter(existingEntry, backendResponse, HttpHeaders.DATE)) {\n            log.debug(\"Backend already contains fresher cache entry\");\n            cacheEntry = existingEntry;\n        } else {\n            cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n            LOG.debug(\"Backend response successfully cached\");\n        }\n    } else {\n        cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n        LOG.debug(\"Backend response successfully cached (freshness check skipped)\");\n    }\n    return convert(responseGenerator.generateResponse(request, cacheEntry), scope);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2727,
        "instruction": "void doDrain(final List<RAMQueueEntry> entries) throws InterruptedException {\n  if (entries.isEmpty()) {\n    return;\n  }\n  // This method is a little hard to follow. We run through the passed in entries and for each\n  // successful add, we add a non-null BucketEntry to the below bucketEntries.  Later we must\n  // do cleanup making sure we've cleared ramCache of all entries regardless of whether we\n  // successfully added the item to the bucketcache; if we don't do the cleanup, we'll OOME by\n  // filling ramCache.  We do the clean up by again running through the passed in entries\n  // doing extra work when we find a non-null bucketEntries corresponding entry.\n  final int size = entries.size();\n  BucketEntry[] bucketEntries = new BucketEntry[size];\n  // Index updated inside loop if success or if we can't succeed. We retry if cache is full\n  // when we go to add an entry by going around the loop again without upping the index.\n  int index = 0;\n  while (cacheEnabled && index < size) {\n    RAMQueueEntry re = null;\n    try {\n      re = entries.get(index);\n      if (re == null) {\n        LOG.warn(\"Couldn't get entry or changed on us; who else is messing with it?\");\n        index++;\n        continue;\n      }\n      BucketEntry bucketEntry = re.writeToCache(ioEngine, bucketAllocator, realCacheSize);\n      // Successfully added. Up index and add bucketEntry. Clear io exceptions.\n      bucketEntries[index] = bucketEntry;\n      if (ioErrorStartTime > 0) {\n        ioErrorStartTime = -1;\n      }\n      index++;\n    } catch (BucketAllocatorException fle) {\n      long currTE = System.currentTimeMillis()/1000; // Current time since Epoch in seconds.\n      if (prevRecTE == 0) {\n        // The very first exception.\n        LOG.warn(\"Failed allocation for \" + (re == null ? \"\" : re.getKey()) + \"; \" + fle);\n      } else {\n        // Record the warning.\n        sinceLastTraceAllocFailCount.increment();\n        sinceLastTraceAllocFailSize.add(re.getData().getSerializedLength());\n        if (currTE - prevRecTE > 60) {\n          LOG.warn(\"Most recent failed allocation: key: \" + (re == null ? \"\" : re.getKey()) + \"; \" + fle);\n          sinceLastTraceAllocFailCount.reset();\n          sinceLastTraceAllocFailSize.reset();\n        }\n      }\n      // Presume can't add. Too big? Move index on. Entry will be cleared from ramCache below.\n      bucketEntries[index] = null;\n      index++;\n      prevRecTE = currTE;\n    } catch (CacheFullException cfe) {\n      // Cache full when we tried to add. Try freeing space and then retrying (don't up index)\n      if (!freeInProgress) {\n        freeSpace(\"Full!\");\n      } else {\n        Thread.sleep(50);\n      }\n    } catch (IOException ioex) {\n      // Hopefully transient. Retry. checkIOErrorIsTolerated disables cache if problem.\n      LOG.error(\"Failed writing to bucket cache\", ioex);\n      checkIOErrorIsTolerated();\n    }\n  }\n\n  // Make sure data pages are written on media before we update maps.\n  try {\n    ioEngine.sync();\n  } catch (IOException ioex) {\n    LOG.error(\"Failed syncing IO engine\", ioex);\n    checkIOErrorIsTolerated();\n    // Since we failed sync, free the blocks in bucket allocator\n    for (int i = 0; i < entries.size(); ++i) {\n      if (bucketEntries[i] != null) {\n        bucketAllocator.freeBlock(bucketEntries[i].offset());\n        bucketEntries[i] = null;\n      }\n    }\n  }\n\n  // Now add to backingMap if successfully added to bucket cache.  Remove from ramCache if\n  // success or error.\n  for (int i = 0; i < size; ++i) {\n    BlockCacheKey key = entries.get(i).getKey();\n    // Only add if non-null entry.\n    if (bucketEntries[i] != null) {\n      putIntoBackingMap(key, bucketEntries[i]);\n    }\n    // Always remove from ramCache even if we failed adding it to the block cache above.\n    boolean existed = ramCache.remove(key, re -> {\n      if (re != null) {\n        heapSize.add(-1 * re.getData().heapSize());\n      }\n    });\n    if (!existed && bucketEntries[i] != null) {\n      // Block should have already been evicted. Remove it and free space.\n      final BucketEntry bucketEntry = bucketEntries[i];\n      bucketEntry.withWriteLock(offsetLock, () -> {\n        if (backingMap.remove(key, bucketEntry)) {\n          blockEvicted(key, bucketEntry, false);\n        }\n        return null;\n      });\n    }\n  }\n\n  long used = bucketAllocator.getUsedSize();\n  if (used > acceptableSize()) {\n    freeSpace(\"Used=\" + used + \" > acceptable=\" + acceptableSize());\n  }\n  return;\n}",
        "accept_response": "void doDrain(final List<RAMQueueEntry> entries) throws InterruptedException {\n  if (entries.isEmpty()) {\n    return;\n  }\n  // This method is a little hard to follow. We run through the passed in entries and for each\n  // successful add, we add a non-null BucketEntry to the below bucketEntries.  Later we must\n  // do cleanup making sure we've cleared ramCache of all entries regardless of whether we\n  // successfully added the item to the bucketcache; if we don't do the cleanup, we'll OOME by\n  // filling ramCache.  We do the clean up by again running through the passed in entries\n  // doing extra work when we find a non-null bucketEntries corresponding entry.\n  final int size = entries.size();\n  BucketEntry[] bucketEntries = new BucketEntry[size];\n  // Index updated inside loop if success or if we can't succeed. We retry if cache is full\n  // when we go to add an entry by going around the loop again without upping the index.\n  int index = 0;\n  while (cacheEnabled && index < size) {\n    RAMQueueEntry re = null;\n    try {\n      re = entries.get(index);\n      if (re == null) {\n        LOG.warn(\"Couldn't get entry or changed on us; who else is messing with it?\");\n        index++;\n        continue;\n      }\n      BucketEntry bucketEntry = re.writeToCache(ioEngine, bucketAllocator, realCacheSize);\n      // Successfully added. Up index and add bucketEntry. Clear io exceptions.\n      bucketEntries[index] = bucketEntry;\n      if (ioErrorStartTime > 0) {\n        ioErrorStartTime = -1;\n      }\n      index++;\n    } catch (BucketAllocatorException fle) {\n      long currTE = System.currentTimeMillis()/1000; // Current time since Epoch in seconds.\n      if (prevRecTE == 0) {\n        // The very first exception.\n        LOG.warn(\"Failed allocation for \" + (re == null ? \"\" : re.getKey()) + \"; \" + fle);\n      } else {\n        // Record the warning.\n        sinceLastTraceAllocFailCount.increment();\n        sinceLastTraceAllocFailSize.add(re.getData().getSerializedLength());\n        if (currTE - prevRecTE > 60) {\n          LOG.warn(\"Failed allocations since last trace: \" + \" count: \" + sinceLastTraceAllocFailCount\n            + \" size: \" + sinceLastTraceAllocFailSize);\n          LOG.warn(\"Most recent failed allocation: key: \" + (re == null ? \"\" : re.getKey()) + \"; \" + fle);\n          sinceLastTraceAllocFailCount.reset();\n          sinceLastTraceAllocFailSize.reset();\n        }\n      }\n      // Presume can't add. Too big? Move index on. Entry will be cleared from ramCache below.\n      bucketEntries[index] = null;\n      index++;\n      prevRecTE = currTE;\n    } catch (CacheFullException cfe) {\n      // Cache full when we tried to add. Try freeing space and then retrying (don't up index)\n      if (!freeInProgress) {\n        freeSpace(\"Full!\");\n      } else {\n        Thread.sleep(50);\n      }\n    } catch (IOException ioex) {\n      // Hopefully transient. Retry. checkIOErrorIsTolerated disables cache if problem.\n      LOG.error(\"Failed writing to bucket cache\", ioex);\n      checkIOErrorIsTolerated();\n    }\n  }\n\n  // Make sure data pages are written on media before we update maps.\n  try {\n    ioEngine.sync();\n  } catch (IOException ioex) {\n    LOG.error(\"Failed syncing IO engine\", ioex);\n    checkIOErrorIsTolerated();\n    // Since we failed sync, free the blocks in bucket allocator\n    for (int i = 0; i < entries.size(); ++i) {\n      if (bucketEntries[i] != null) {\n        bucketAllocator.freeBlock(bucketEntries[i].offset());\n        bucketEntries[i] = null;\n      }\n    }\n  }\n\n  // Now add to backingMap if successfully added to bucket cache.  Remove from ramCache if\n  // success or error.\n  for (int i = 0; i < size; ++i) {\n    BlockCacheKey key = entries.get(i).getKey();\n    // Only add if non-null entry.\n    if (bucketEntries[i] != null) {\n      putIntoBackingMap(key, bucketEntries[i]);\n    }\n    // Always remove from ramCache even if we failed adding it to the block cache above.\n    boolean existed = ramCache.remove(key, re -> {\n      if (re != null) {\n        heapSize.add(-1 * re.getData().heapSize());\n      }\n    });\n    if (!existed && bucketEntries[i] != null) {\n      // Block should have already been evicted. Remove it and free space.\n      final BucketEntry bucketEntry = bucketEntries[i];\n      bucketEntry.withWriteLock(offsetLock, () -> {\n        if (backingMap.remove(key, bucketEntry)) {\n          blockEvicted(key, bucketEntry, false);\n        }\n        return null;\n      });\n    }\n  }\n\n  long used = bucketAllocator.getUsedSize();\n  if (used > acceptableSize()) {\n    freeSpace(\"Used=\" + used + \" > acceptable=\" + acceptableSize());\n  }\n  return;\n}",
        "reject_response": "void doDrain(final List<RAMQueueEntry> entries) throws InterruptedException {\n  if (entries.isEmpty()) {\n    return;\n  }\n  // This method is a little hard to follow. We run through the passed in entries and for each\n  // successful add, we add a non-null BucketEntry to the below bucketEntries.  Later we must\n  // do cleanup making sure we've cleared ramCache of all entries regardless of whether we\n  // successfully added the item to the bucketcache; if we don't do the cleanup, we'll OOME by\n  // filling ramCache.  We do the clean up by again running through the passed in entries\n  // doing extra work when we find a non-null bucketEntries corresponding entry.\n  final int size = entries.size();\n  BucketEntry[] bucketEntries = new BucketEntry[size];\n  // Index updated inside loop if success or if we can't succeed. We retry if cache is full\n  // when we go to add an entry by going around the loop again without upping the index.\n  int index = 0;\n  while (cacheEnabled && index < size) {\n    RAMQueueEntry re = null;\n    try {\n      re = entries.get(index);\n      if (re == null) {\n        LOG.warn(\"Couldn't get entry or changed on us; who else is messing with it?\");\n        index++;\n        continue;\n      }\n      BucketEntry bucketEntry = re.writeToCache(ioEngine, bucketAllocator, realCacheSize);\n      // Successfully added. Up index and add bucketEntry. Clear io exceptions.\n      bucketEntries[index] = bucketEntry;\n      if (ioErrorStartTime > 0) {\n        ioErrorStartTime = -1;\n      }\n      index++;\n    } catch (BucketAllocatorException fle) {\n      long currTE = System.currentTimeMillis()/1000; // Current time since Epoch in seconds.\n      if (prevRecTE == 0) {\n        // The very first exception.\n        LOG.warn(\"Failed allocation for \" + (re == null ? \"\" : re.getKey()) + \"; \" + fle);\n      } else {\n        // Record the warning.\n        sinceLastTraceAllocFailCount.increment();\n        sinceLastTraceAllocFailSize.add(re.getData().getSerializedLength());\n        if (currTE - prevRecTE > 60) {\n      LOG.warn(\"Failed allocation for \" + (re == null ? \"\" : re.getKey()) + \"; \" + fle);\n          LOG.warn(\"Most recent failed allocation: key: \" + (re == null ? \"\" : re.getKey()) + \"; \" + fle);\n          sinceLastTraceAllocFailCount.reset();\n          sinceLastTraceAllocFailSize.reset();\n        }\n      }\n      // Presume can't add. Too big? Move index on. Entry will be cleared from ramCache below.\n      bucketEntries[index] = null;\n      index++;\n      prevRecTE = currTE;\n    } catch (CacheFullException cfe) {\n      // Cache full when we tried to add. Try freeing space and then retrying (don't up index)\n      if (!freeInProgress) {\n        freeSpace(\"Full!\");\n      } else {\n        Thread.sleep(50);\n      }\n    } catch (IOException ioex) {\n      // Hopefully transient. Retry. checkIOErrorIsTolerated disables cache if problem.\n      LOG.error(\"Failed writing to bucket cache\", ioex);\n      checkIOErrorIsTolerated();\n    }\n  }\n\n  // Make sure data pages are written on media before we update maps.\n  try {\n    ioEngine.sync();\n  } catch (IOException ioex) {\n    LOG.error(\"Failed syncing IO engine\", ioex);\n    checkIOErrorIsTolerated();\n    // Since we failed sync, free the blocks in bucket allocator\n    for (int i = 0; i < entries.size(); ++i) {\n      if (bucketEntries[i] != null) {\n        bucketAllocator.freeBlock(bucketEntries[i].offset());\n        bucketEntries[i] = null;\n      }\n    }\n  }\n\n  // Now add to backingMap if successfully added to bucket cache.  Remove from ramCache if\n  // success or error.\n  for (int i = 0; i < size; ++i) {\n    BlockCacheKey key = entries.get(i).getKey();\n    // Only add if non-null entry.\n    if (bucketEntries[i] != null) {\n      putIntoBackingMap(key, bucketEntries[i]);\n    }\n    // Always remove from ramCache even if we failed adding it to the block cache above.\n    boolean existed = ramCache.remove(key, re -> {\n      if (re != null) {\n        heapSize.add(-1 * re.getData().heapSize());\n      }\n    });\n    if (!existed && bucketEntries[i] != null) {\n      // Block should have already been evicted. Remove it and free space.\n      final BucketEntry bucketEntry = bucketEntries[i];\n      bucketEntry.withWriteLock(offsetLock, () -> {\n        if (backingMap.remove(key, bucketEntry)) {\n          blockEvicted(key, bucketEntry, false);\n        }\n        return null;\n      });\n    }\n  }\n\n  long used = bucketAllocator.getUsedSize();\n  if (used > acceptableSize()) {\n    freeSpace(\"Used=\" + used + \" > acceptable=\" + acceptableSize());\n  }\n  return;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2790,
        "instruction": "boolean mayCallBackend(final HttpRequest request) {\n    final Iterator<HeaderElement> it = MessageSupport.iterate(request, HeaderConstants.CACHE_CONTROL);\n    while (it.hasNext()) {\n        final HeaderElement elt = it.next();\n        if (\"only-if-cached\".equals(elt.getName())) {\n            return false;\n        }\n    }\n    return true;\n}",
        "accept_response": "boolean mayCallBackend(final HttpRequest request) {\n    final Iterator<HeaderElement> it = MessageSupport.iterate(request, HeaderConstants.CACHE_CONTROL);\n    while (it.hasNext()) {\n        final HeaderElement elt = it.next();\n        if (\"only-if-cached\".equals(elt.getName())) {\n            LOG.debug(\"Request marked only-if-cached\");\n            return false;\n        }\n    }\n    return true;\n}",
        "reject_response": "boolean mayCallBackend(final HttpRequest request) {\n    final Iterator<HeaderElement> it = MessageSupport.iterate(request, HeaderConstants.CACHE_CONTROL);\n    while (it.hasNext()) {\n        final HeaderElement elt = it.next();\n        if (\"only-if-cached\".equals(elt.getName())) {\n            log.debug(\"Request marked only-if-cached\");\n            return false;\n        }\n    }\n    return true;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2442,
        "instruction": "@Override\nprotected Schema readSchema() throws IOException {\n  if (footer == null) {\n    if (in.size() <= (ArrowMagic.MAGIC_LENGTH * 2 + 4)) {\n      throw new InvalidArrowFileException(\"file too small: \" + in.size());\n    }\n    ByteBuffer buffer = ByteBuffer.allocate(4 + ArrowMagic.MAGIC_LENGTH);\n    long footerLengthOffset = in.size() - buffer.remaining();\n    in.setPosition(footerLengthOffset);\n    in.readFully(buffer);\n    buffer.flip();\n    byte[] array = buffer.array();\n    if (!ArrowMagic.validateMagic(Arrays.copyOfRange(array, 4, array.length))) {\n      throw new InvalidArrowFileException(\"missing Magic number \" + Arrays.toString(buffer.array()));\n    }\n    int footerLength = MessageSerializer.bytesToInt(array);\n    if (footerLength <= 0 || footerLength + ArrowMagic.MAGIC_LENGTH * 2 + 4 > in.size()) {\n      throw new InvalidArrowFileException(\"invalid footer length: \" + footerLength);\n    }\n    long footerOffset = footerLengthOffset - footerLength;\n    ByteBuffer footerBuffer = ByteBuffer.allocate(footerLength);\n    in.setPosition(footerOffset);\n    in.readFully(footerBuffer);\n    footerBuffer.flip();\n    Footer footerFB = Footer.getRootAsFooter(footerBuffer);\n    this.footer = new ArrowFooter(footerFB);\n  }\n  return footer.getSchema();\n}",
        "accept_response": "@Override\nprotected Schema readSchema() throws IOException {\n  if (footer == null) {\n    if (in.size() <= (ArrowMagic.MAGIC_LENGTH * 2 + 4)) {\n      throw new InvalidArrowFileException(\"file too small: \" + in.size());\n    }\n    ByteBuffer buffer = ByteBuffer.allocate(4 + ArrowMagic.MAGIC_LENGTH);\n    long footerLengthOffset = in.size() - buffer.remaining();\n    in.setPosition(footerLengthOffset);\n    in.readFully(buffer);\n    buffer.flip();\n    byte[] array = buffer.array();\n    if (!ArrowMagic.validateMagic(Arrays.copyOfRange(array, 4, array.length))) {\n      throw new InvalidArrowFileException(\"missing Magic number \" + Arrays.toString(buffer.array()));\n    }\n    int footerLength = MessageSerializer.bytesToInt(array);\n    if (footerLength <= 0 || footerLength + ArrowMagic.MAGIC_LENGTH * 2 + 4 > in.size()) {\n      throw new InvalidArrowFileException(\"invalid footer length: \" + footerLength);\n    }\n    long footerOffset = footerLengthOffset - footerLength;\n    LOGGER.debug(\"Footer starts at {}, length: {}\", footerOffset, footerLength);\n    ByteBuffer footerBuffer = ByteBuffer.allocate(footerLength);\n    in.setPosition(footerOffset);\n    in.readFully(footerBuffer);\n    footerBuffer.flip();\n    Footer footerFB = Footer.getRootAsFooter(footerBuffer);\n    this.footer = new ArrowFooter(footerFB);\n  }\n  return footer.getSchema();\n}",
        "reject_response": "@Override\nprotected Schema readSchema() throws IOException {\n  if (footer == null) {\n    if (in.size() <= (ArrowMagic.MAGIC_LENGTH * 2 + 4)) {\n      throw new InvalidArrowFileException(\"file too small: \" + in.size());\n    }\n    ByteBuffer buffer = ByteBuffer.allocate(4 + ArrowMagic.MAGIC_LENGTH);\n    long footerLengthOffset = in.size() - buffer.remaining();\n    in.setPosition(footerLengthOffset);\n    in.readFully(buffer);\n    buffer.flip();\n    byte[] array = buffer.array();\n    if (!ArrowMagic.validateMagic(Arrays.copyOfRange(array, 4, array.length))) {\n      throw new InvalidArrowFileException(\"missing Magic number \" + Arrays.toString(buffer.array()));\n    }\n    int footerLength = MessageSerializer.bytesToInt(array);\n    if (footerLength <= 0 || footerLength + ArrowMagic.MAGIC_LENGTH * 2 + 4 > in.size()) {\n      throw new InvalidArrowFileException(\"invalid footer length: \" + footerLength);\n    }\n    long footerOffset = footerLengthOffset - footerLength;\n    LOGGER.debug(String.format(\"Footer starts at %d, length: %d\", footerOffset, footerLength));\n    ByteBuffer footerBuffer = ByteBuffer.allocate(footerLength);\n    in.setPosition(footerOffset);\n    in.readFully(footerBuffer);\n    footerBuffer.flip();\n    Footer footerFB = Footer.getRootAsFooter(footerBuffer);\n    this.footer = new ArrowFooter(footerFB);\n  }\n  return footer.getSchema();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3037,
        "instruction": "@Override\npublic boolean isValidHref(String url) {\n    if (StringUtils.isEmpty(url)) {\n        return true;\n    }\n    try {\n        String decodedURL = URLDecoder.decode(url, StandardCharsets.UTF_8.name());\n        /*\n            StringEscapeUtils is deprecated starting with version 3.6 of commons-lang3, however the indicated replacement comes from\n            commons-text, which is not an OSGi bundle\n         */\n        String xmlDecodedURL = StringEscapeUtils.unescapeXml(decodedURL);\n        if (xmlDecodedURL.equals(url) || xmlDecodedURL.equals(decodedURL)) {\n            return runHrefValidation(url);\n        }\n        return runHrefValidation(xmlDecodedURL);\n    } catch (UnsupportedEncodingException e) {\n        logger.warn(\"Unable to decode url.\", e);\n    }\n    return false;\n}",
        "accept_response": "@Override\npublic boolean isValidHref(String url) {\n    if (StringUtils.isEmpty(url)) {\n        return true;\n    }\n    try {\n        String decodedURL = URLDecoder.decode(url, StandardCharsets.UTF_8.name());\n        /*\n            StringEscapeUtils is deprecated starting with version 3.6 of commons-lang3, however the indicated replacement comes from\n            commons-text, which is not an OSGi bundle\n         */\n        String xmlDecodedURL = StringEscapeUtils.unescapeXml(decodedURL);\n        if (xmlDecodedURL.equals(url) || xmlDecodedURL.equals(decodedURL)) {\n            return runHrefValidation(url);\n        }\n        return runHrefValidation(xmlDecodedURL);\n    } catch (UnsupportedEncodingException e) {\n        logger.warn(\"Unable to decode url.\", e);\n        logger.debug(\"URL input: {}\", url);\n    }\n    return false;\n}",
        "reject_response": "@Override\npublic boolean isValidHref(String url) {\n    if (StringUtils.isEmpty(url)) {\n        return true;\n    }\n    try {\n        String decodedURL = URLDecoder.decode(url, StandardCharsets.UTF_8.name());\n        /*\n            StringEscapeUtils is deprecated starting with version 3.6 of commons-lang3, however the indicated replacement comes from\n            commons-text, which is not an OSGi bundle\n         */\n        String xmlDecodedURL = StringEscapeUtils.unescapeXml(decodedURL);\n        if (xmlDecodedURL.equals(url) || xmlDecodedURL.equals(decodedURL)) {\n            return runHrefValidation(url);\n        }\n        return runHrefValidation(xmlDecodedURL);\n    } catch (UnsupportedEncodingException e) {\n        logger.warn(\"Unable to decode url.\", e);\n        logger.error(\"Unable to decode url: {}.\", url);\n    }\n    return false;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2704,
        "instruction": "private ProtocolData getProtocolData(final ServletContext servletContext,\n                                     final HttpHeaders headers,\n                                     final String path) throws Exception {\n\n    if (LOG.isDebugEnabled()) {\n        StringBuilder startMsg = new StringBuilder(\n                \"FRAGMENTER started for path \\\"\" + path + \"\\\"\");\n        for (String header : headers.getRequestHeaders().keySet()) {\n            startMsg.append(\" Header: \").append(header).append(\" Value: \").append(\n                    headers.getRequestHeader(header));\n        }\n    }\n\n    /* Convert headers into a case-insensitive regular map */\n    Map<String, String> params = convertToCaseInsensitiveMap(headers.getRequestHeaders());\n\n    /* Store protocol level properties and verify */\n    ProtocolData protData = new ProtocolData(params);\n    if (protData.getFragmenter() == null) {\n        protData.protocolViolation(\"fragmenter\");\n    }\n    SecuredHDFS.verifyToken(protData, servletContext);\n\n    return protData;\n}",
        "accept_response": "private ProtocolData getProtocolData(final ServletContext servletContext,\n                                     final HttpHeaders headers,\n                                     final String path) throws Exception {\n\n    if (LOG.isDebugEnabled()) {\n        StringBuilder startMsg = new StringBuilder(\n                \"FRAGMENTER started for path \\\"\" + path + \"\\\"\");\n        for (String header : headers.getRequestHeaders().keySet()) {\n            startMsg.append(\" Header: \").append(header).append(\" Value: \").append(\n                    headers.getRequestHeader(header));\n        }\n        LOG.debug(startMsg);\n    }\n\n    /* Convert headers into a case-insensitive regular map */\n    Map<String, String> params = convertToCaseInsensitiveMap(headers.getRequestHeaders());\n\n    /* Store protocol level properties and verify */\n    ProtocolData protData = new ProtocolData(params);\n    if (protData.getFragmenter() == null) {\n        protData.protocolViolation(\"fragmenter\");\n    }\n    SecuredHDFS.verifyToken(protData, servletContext);\n\n    return protData;\n}",
        "reject_response": "private ProtocolData getProtocolData(final ServletContext servletContext,\n                                     final HttpHeaders headers,\n                                     final String path) throws Exception {\n\n    if (LOG.isDebugEnabled()) {\n        StringBuilder startMsg = new StringBuilder(\n                \"FRAGMENTER started for path \\\"\" + path + \"\\\"\");\n        for (String header : headers.getRequestHeaders().keySet()) {\n            startMsg.append(\" Header: \").append(header).append(\" Value: \").append(\n                    headers.getRequestHeader(header));\n        }\n        Log.debug(startMsg);\n    }\n\n    /* Convert headers into a case-insensitive regular map */\n    Map<String, String> params = convertToCaseInsensitiveMap(headers.getRequestHeaders());\n\n    /* Store protocol level properties and verify */\n    ProtocolData protData = new ProtocolData(params);\n    if (protData.getFragmenter() == null) {\n        protData.protocolViolation(\"fragmenter\");\n    }\n    SecuredHDFS.verifyToken(protData, servletContext);\n\n    return protData;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3220,
        "instruction": "public static boolean deleteRecursive(ZooKeeper zk, final String pathRoot, final int batchSize)\n    throws InterruptedException, KeeperException\n{\n    PathUtils.validatePath(pathRoot);\n\n    List<String> tree = listSubTreeBFS(zk, pathRoot);\n\n    return deleteInBatch(zk, tree, batchSize);\n}",
        "accept_response": "public static boolean deleteRecursive(ZooKeeper zk, final String pathRoot, final int batchSize)\n    throws InterruptedException, KeeperException\n{\n    PathUtils.validatePath(pathRoot);\n\n    List<String> tree = listSubTreeBFS(zk, pathRoot);\n    LOG.debug(\"Deleting tree: {}\", tree);\n\n    return deleteInBatch(zk, tree, batchSize);\n}",
        "reject_response": "public static boolean deleteRecursive(ZooKeeper zk, final String pathRoot, final int batchSize)\n    throws InterruptedException, KeeperException\n{\n    PathUtils.validatePath(pathRoot);\n\n    List<String> tree = listSubTreeBFS(zk, pathRoot);\n    LOG.debug(\"Deleting {}\",tree);\n    LOG.debug(\"Deleting {} subnodes \",tree.size());\n\n    return deleteInBatch(zk, tree, batchSize);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2456,
        "instruction": "private void clearUpTo(final int lastReceivedCommandID) {\n   final int numberToClear = 1 + lastReceivedCommandID - firstStoredCommandID;\n\n   if (logger.isTraceEnabled()) {\n      logger.trace(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" ChannelImpl::clearUpTo lastReceived commandID=\" + lastReceivedCommandID + \" first commandID=\" + firstStoredCommandID + \" number to clear \" + numberToClear);\n   }\n\n   for (int i = 0; i < numberToClear; i++) {\n      final Packet packet = resendCache.poll();\n\n      if (packet == null) {\n         ActiveMQClientLogger.LOGGER.cannotFindPacketToClear(lastReceivedCommandID, firstStoredCommandID);\n         firstStoredCommandID = lastReceivedCommandID + 1;\n         return;\n      }\n      if (commandConfirmationHandler != null) {\n         commandConfirmationHandler.commandConfirmed(packet);\n      }\n   }\n\n   firstStoredCommandID += numberToClear;\n}",
        "accept_response": "private void clearUpTo(final int lastReceivedCommandID) {\n   final int numberToClear = 1 + lastReceivedCommandID - firstStoredCommandID;\n\n   if (logger.isTraceEnabled()) {\n      logger.trace(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" ChannelImpl::clearUpTo lastReceived commandID=\" + lastReceivedCommandID + \" first commandID=\" + firstStoredCommandID + \" number to clear \" + numberToClear);\n   }\n\n   for (int i = 0; i < numberToClear; i++) {\n      final Packet packet = resendCache.poll();\n\n      if (packet == null) {\n         ActiveMQClientLogger.LOGGER.cannotFindPacketToClear(lastReceivedCommandID, firstStoredCommandID);\n         firstStoredCommandID = lastReceivedCommandID + 1;\n         return;\n      }\n\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"RemotingConnectionID=\" + connection.getID() + \" ChannelImpl::clearUpTo confirming \" + packet + \" towards \" + commandConfirmationHandler);\n      }\n      if (commandConfirmationHandler != null) {\n         commandConfirmationHandler.commandConfirmed(packet);\n      }\n   }\n\n   firstStoredCommandID += numberToClear;\n}",
        "reject_response": "private void clearUpTo(final int lastReceivedCommandID) {\n   final int numberToClear = 1 + lastReceivedCommandID - firstStoredCommandID;\n\n   if (logger.isTraceEnabled()) {\n      logger.trace(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" ChannelImpl::clearUpTo lastReceived commandID=\" + lastReceivedCommandID + \" first commandID=\" + firstStoredCommandID + \" number to clear \" + numberToClear);\n   }\n\n   for (int i = 0; i < numberToClear; i++) {\n      final Packet packet = resendCache.poll();\n\n      if (packet == null) {\n         ActiveMQClientLogger.LOGGER.cannotFindPacketToClear(lastReceivedCommandID, firstStoredCommandID);\n         firstStoredCommandID = lastReceivedCommandID + 1;\n         return;\n      }\n\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"ChannelImpl::clearUpTo confirming \" + packet + \" towards \" + commandConfirmationHandler);\n      }\n      if (commandConfirmationHandler != null) {\n         commandConfirmationHandler.commandConfirmed(packet);\n      }\n   }\n\n   firstStoredCommandID += numberToClear;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2970,
        "instruction": "private WebSocketBehavior newWsBehavior() {\n\treturn new WebSocketBehavior() {\n\t\tprivate static final long serialVersionUID = 1L;\n\n\t\t@Override\n\t\tpublic void renderHead(Component component, IHeaderResponse response) {\n\t\t\tsuper.renderHead(component, response);\n\t\t\tresponse.render(JavaScriptHeaderItem.forScript(\n\t\t\t\t\tString.format(\"Wicket.Event.subscribe(Wicket.Event.Topic.WebSocket.Opened, function() {Wicket.WebSocket.send('%s');});\",CONNECTED_MSG)\n\t\t\t\t\t, \"ws-connected-script\"));\n\t\t}\n\n\t\t@Override\n\t\tprotected void onConnect(ConnectedMessage message) {\n\t\t\tsuper.onConnect(message);\n\t\t\tOmWebSocketPanel.this.onConnect(message);\n\t\t}\n\n\t\t@Override\n\t\tprotected void onMessage(WebSocketRequestHandler handler, TextMessage msg) {\n\t\t\tif (CONNECTED_MSG.equals(msg.getText())) {\n\t\t\t\tif (connected.compareAndSet(false, true)) {\n\t\t\t\t\tOmWebSocketPanel.this.onConnect(handler);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfinal JSONObject m;\n\t\t\t\ttry {\n\t\t\t\t\tm = new JSONObject(msg.getText());\n\t\t\t\t\tswitch(m.optString(\"type\", \"\")) {\n\t\t\t\t\t\tcase KURENTO_TYPE:\n\t\t\t\t\t\t\tkHandler.onMessage(getWsClient(), m);\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase \"mic\":\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tIWsClient curClient = getWsClient();\n\t\t\t\t\t\t\tif (!(curClient instanceof Client)) {\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tClient c = (Client)curClient;\n\t\t\t\t\t\t\tif (c.getRoomId() == null) {\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tWebSocketHelper.sendRoomOthers(c.getRoomId(), c.getUid(), m.put(\"uid\", c.getUid()));\n\t\t\t\t\t\t}\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase \"ping\":\n\t\t\t\t\t\t\thandler.appendJavaScript(\"OmUtil.ping();\");\n\t\t\t\t\t\t\tWebSocketHelper.sendClient(getWsClient(), new byte[]{getUserId() == null ? 0 : getUserId().byteValue()});\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\tOmWebSocketPanel.this.onMessage(handler, m);\n\t\t\t\t\t}\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tlog.error(\"Error while processing incoming message\", e);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t@Override\n\t\tprotected void onAbort(AbortedMessage msg) {\n\t\t\tcloseHandler(msg);\n\t\t}\n\n\t\t@Override\n\t\tprotected void onClose(ClosedMessage msg) {\n\t\t\tcloseHandler(msg);\n\t\t}\n\n\t\t@Override\n\t\tprotected void onError(WebSocketRequestHandler handler, ErrorMessage msg) {\n\t\t\tcloseHandler(msg);\n\t\t}\n\t};\n}",
        "accept_response": "private WebSocketBehavior newWsBehavior() {\n\treturn new WebSocketBehavior() {\n\t\tprivate static final long serialVersionUID = 1L;\n\n\t\t@Override\n\t\tpublic void renderHead(Component component, IHeaderResponse response) {\n\t\t\tsuper.renderHead(component, response);\n\t\t\tresponse.render(JavaScriptHeaderItem.forScript(\n\t\t\t\t\tString.format(\"Wicket.Event.subscribe(Wicket.Event.Topic.WebSocket.Opened, function() {Wicket.WebSocket.send('%s');});\",CONNECTED_MSG)\n\t\t\t\t\t, \"ws-connected-script\"));\n\t\t}\n\n\t\t@Override\n\t\tprotected void onConnect(ConnectedMessage message) {\n\t\t\tsuper.onConnect(message);\n\t\t\tOmWebSocketPanel.this.onConnect(message);\n\t\t}\n\n\t\t@Override\n\t\tprotected void onMessage(WebSocketRequestHandler handler, TextMessage msg) {\n\t\t\tif (CONNECTED_MSG.equals(msg.getText())) {\n\t\t\t\tif (connected.compareAndSet(false, true)) {\n\t\t\t\t\tOmWebSocketPanel.this.onConnect(handler);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfinal JSONObject m;\n\t\t\t\ttry {\n\t\t\t\t\tm = new JSONObject(msg.getText());\n\t\t\t\t\tswitch(m.optString(\"type\", \"\")) {\n\t\t\t\t\t\tcase KURENTO_TYPE:\n\t\t\t\t\t\t\tkHandler.onMessage(getWsClient(), m);\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase \"mic\":\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tIWsClient curClient = getWsClient();\n\t\t\t\t\t\t\tif (!(curClient instanceof Client)) {\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tClient c = (Client)curClient;\n\t\t\t\t\t\t\tif (c.getRoomId() == null) {\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tWebSocketHelper.sendRoomOthers(c.getRoomId(), c.getUid(), m.put(\"uid\", c.getUid()));\n\t\t\t\t\t\t}\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase \"ping\":\n\t\t\t\t\t\t\tlog.trace(\"Sending WebSocket PING\");\n\t\t\t\t\t\t\thandler.appendJavaScript(\"OmUtil.ping();\");\n\t\t\t\t\t\t\tWebSocketHelper.sendClient(getWsClient(), new byte[]{getUserId() == null ? 0 : getUserId().byteValue()});\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\tOmWebSocketPanel.this.onMessage(handler, m);\n\t\t\t\t\t}\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tlog.error(\"Error while processing incoming message\", e);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t@Override\n\t\tprotected void onAbort(AbortedMessage msg) {\n\t\t\tcloseHandler(msg);\n\t\t}\n\n\t\t@Override\n\t\tprotected void onClose(ClosedMessage msg) {\n\t\t\tcloseHandler(msg);\n\t\t}\n\n\t\t@Override\n\t\tprotected void onError(WebSocketRequestHandler handler, ErrorMessage msg) {\n\t\t\tcloseHandler(msg);\n\t\t}\n\t};\n}",
        "reject_response": "private WebSocketBehavior newWsBehavior() {\n\treturn new WebSocketBehavior() {\n\t\tprivate static final long serialVersionUID = 1L;\n\n\t\t@Override\n\t\tpublic void renderHead(Component component, IHeaderResponse response) {\n\t\t\tsuper.renderHead(component, response);\n\t\t\tresponse.render(JavaScriptHeaderItem.forScript(\n\t\t\t\t\tString.format(\"Wicket.Event.subscribe(Wicket.Event.Topic.WebSocket.Opened, function() {Wicket.WebSocket.send('%s');});\",CONNECTED_MSG)\n\t\t\t\t\t, \"ws-connected-script\"));\n\t\t}\n\n\t\t@Override\n\t\tprotected void onConnect(ConnectedMessage message) {\n\t\t\tsuper.onConnect(message);\n\t\t\tOmWebSocketPanel.this.onConnect(message);\n\t\t}\n\n\t\t@Override\n\t\tprotected void onMessage(WebSocketRequestHandler handler, TextMessage msg) {\n\t\t\tif (CONNECTED_MSG.equals(msg.getText())) {\n\t\t\t\tif (connected.compareAndSet(false, true)) {\n\t\t\t\t\tOmWebSocketPanel.this.onConnect(handler);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfinal JSONObject m;\n\t\t\t\ttry {\n\t\t\t\t\tm = new JSONObject(msg.getText());\n\t\t\t\t\tswitch(m.optString(\"type\", \"\")) {\n\t\t\t\t\t\tcase KURENTO_TYPE:\n\t\t\t\t\t\t\tkHandler.onMessage(getWsClient(), m);\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase \"mic\":\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tIWsClient curClient = getWsClient();\n\t\t\t\t\t\t\tif (!(curClient instanceof Client)) {\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tClient c = (Client)curClient;\n\t\t\t\t\t\t\tif (c.getRoomId() == null) {\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tWebSocketHelper.sendRoomOthers(c.getRoomId(), c.getUid(), m.put(\"uid\", c.getUid()));\n\t\t\t\t\t\t}\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase \"ping\":\n\t\t\t\t\t\t\tlog.debug(\"Sending WebSocket PING\");\n\t\t\t\t\t\t\thandler.appendJavaScript(\"OmUtil.ping();\");\n\t\t\t\t\t\t\tWebSocketHelper.sendClient(getWsClient(), new byte[]{getUserId() == null ? 0 : getUserId().byteValue()});\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\tOmWebSocketPanel.this.onMessage(handler, m);\n\t\t\t\t\t}\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tlog.error(\"Error while processing incoming message\", e);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t@Override\n\t\tprotected void onAbort(AbortedMessage msg) {\n\t\t\tcloseHandler(msg);\n\t\t}\n\n\t\t@Override\n\t\tprotected void onClose(ClosedMessage msg) {\n\t\t\tcloseHandler(msg);\n\t\t}\n\n\t\t@Override\n\t\tprotected void onError(WebSocketRequestHandler handler, ErrorMessage msg) {\n\t\t\tcloseHandler(msg);\n\t\t}\n\t};\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2671,
        "instruction": "public List<String> getGroups() {\n  ensureInitialized();\n  try {\n    return groups.getGroups(getShortUserName());\n  } catch (IOException ie) {\n    return Collections.emptyList();\n  }\n}",
        "accept_response": "public List<String> getGroups() {\n  ensureInitialized();\n  try {\n    return groups.getGroups(getShortUserName());\n  } catch (IOException ie) {\n    LOG.debug(\"Failed to get groups for user {}\", getShortUserName(), ie);\n    return Collections.emptyList();\n  }\n}",
        "reject_response": "public List<String> getGroups() {\n  ensureInitialized();\n  try {\n    return groups.getGroups(getShortUserName());\n  } catch (IOException ie) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Failed to get groups for user \" + getShortUserName()\n          + \" by \" + ie);\n      LOG.trace(\"TRACE\", ie);\n    }\n    return Collections.emptyList();\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2951,
        "instruction": "protected void shutdown() {\n    this.shutdown = true;\n\n    LOGGER.info(\"Application Server shutdown started\");\n    if (nifiServer != null) {\n        nifiServer.stop();\n    }\n    if (bootstrapListener != null) {\n        bootstrapListener.stop();\n    }\n}",
        "accept_response": "protected void shutdown() {\n    this.shutdown = true;\n\n    LOGGER.info(\"Application Server shutdown started\");\n    if (nifiServer != null) {\n        nifiServer.stop();\n    }\n    if (bootstrapListener != null) {\n        bootstrapListener.stop();\n    }\n    LOGGER.info(\"Application Server shutdown completed\");\n}",
        "reject_response": "protected void shutdown() {\n    this.shutdown = true;\n\n    LOGGER.info(\"Application Server shutdown started\");\n    if (nifiServer != null) {\n        nifiServer.stop();\n    }\n    if (bootstrapListener != null) {\n        bootstrapListener.stop();\n    }\n    LOGGER.info(\"Jetty web server shutdown completed (nicely or otherwise).\");\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2940,
        "instruction": "@Override\npublic void enqueueEvent(final LogEvent event, final AsyncLoggerConfig asyncLoggerConfig) {\n    // LOG4J2-639: catch NPE if disruptor field was set to null after our check above\n    try {\n        final LogEvent logEvent = prepareEvent(event);\n        enqueue(logEvent, asyncLoggerConfig);\n    } catch (final NullPointerException npe) {\n        // Note: NPE prevents us from adding a log event to the disruptor after it was shut down,\n        // which could cause the publishEvent method to hang and never return.\n    }\n}",
        "accept_response": "@Override\npublic void enqueueEvent(final LogEvent event, final AsyncLoggerConfig asyncLoggerConfig) {\n    // LOG4J2-639: catch NPE if disruptor field was set to null after our check above\n    try {\n        final LogEvent logEvent = prepareEvent(event);\n        enqueue(logEvent, asyncLoggerConfig);\n    } catch (final NullPointerException npe) {\n        // Note: NPE prevents us from adding a log event to the disruptor after it was shut down,\n        // which could cause the publishEvent method to hang and never return.\n        LOGGER.warn(\"Ignoring log event after log4j was shut down: {} [{}] {}\", event.getLevel(),\n                event.getLoggerName(), event.getMessage().getFormattedMessage()\n                        + (event.getThrown() == null ? \"\" : Throwables.toStringList(event.getThrown())));\n    }\n}",
        "reject_response": "@Override\npublic void enqueueEvent(final LogEvent event, final AsyncLoggerConfig asyncLoggerConfig) {\n    // LOG4J2-639: catch NPE if disruptor field was set to null after our check above\n    try {\n        final LogEvent logEvent = prepareEvent(event);\n        enqueue(logEvent, asyncLoggerConfig);\n    } catch (final NullPointerException npe) {\n        // Note: NPE prevents us from adding a log event to the disruptor after it was shut down,\n        // which could cause the publishEvent method to hang and never return.\n        LOGGER.warn(\"Ignoring log event after log4j was shut down.\");\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2422,
        "instruction": "@Override\npublic void init(Component component, Map<String, String> properties, String clusterName) throws Exception {\n  this.properties = properties;\n\n  String root = MapUtils.getString(properties, ZK_ROOT_NODE_PROPERTY, DEFAULT_ZK_ROOT);\n  client = CuratorFrameworkFactory.builder()\n      .connectString(properties.get(ZK_CONNECT_STRING_PROPERTY) + root)\n      .retryPolicy(new ExponentialBackoffRetry(1000, 3))\n      .connectionTimeoutMs(CONNECTION_TIMEOUT)\n      .sessionTimeoutMs(SESSION_TIMEOUT)\n      .build();\n  client.start();\n\n\n  if (component == Component.SERVER) {\n    if (client.checkExists().forPath(\"/\") == null) {\n      client.create().creatingParentContainersIfNeeded().forPath(\"/\");\n    }\n    cache = new TreeCache(client, \"/\");\n    cache.start();\n  } else {\n    while (client.checkExists().forPath(\"/\") == null) {\n      LOG.info(\"Root node is not present yet, going to sleep for \" + WAIT_FOR_ROOT_SLEEP_SECONDS + \" seconds\");\n      Thread.sleep(WAIT_FOR_ROOT_SLEEP_SECONDS * 1000);\n    }\n    cache = new TreeCache(client, String.format(\"/%s\", clusterName));\n  }\n\n  gson = new GsonBuilder().setDateFormat(DATE_FORMAT).create();\n}",
        "accept_response": "@Override\npublic void init(Component component, Map<String, String> properties, String clusterName) throws Exception {\n  this.properties = properties;\n\n  String root = MapUtils.getString(properties, ZK_ROOT_NODE_PROPERTY, DEFAULT_ZK_ROOT);\n  LOG.info(\"Connecting to ZooKeeper at \" + properties.get(ZK_CONNECT_STRING_PROPERTY) + root);\n  client = CuratorFrameworkFactory.builder()\n      .connectString(properties.get(ZK_CONNECT_STRING_PROPERTY) + root)\n      .retryPolicy(new ExponentialBackoffRetry(1000, 3))\n      .connectionTimeoutMs(CONNECTION_TIMEOUT)\n      .sessionTimeoutMs(SESSION_TIMEOUT)\n      .build();\n  client.start();\n\n\n  if (component == Component.SERVER) {\n    if (client.checkExists().forPath(\"/\") == null) {\n      client.create().creatingParentContainersIfNeeded().forPath(\"/\");\n    }\n    cache = new TreeCache(client, \"/\");\n    cache.start();\n  } else {\n    while (client.checkExists().forPath(\"/\") == null) {\n      LOG.info(\"Root node is not present yet, going to sleep for \" + WAIT_FOR_ROOT_SLEEP_SECONDS + \" seconds\");\n      Thread.sleep(WAIT_FOR_ROOT_SLEEP_SECONDS * 1000);\n    }\n    cache = new TreeCache(client, String.format(\"/%s\", clusterName));\n  }\n\n  gson = new GsonBuilder().setDateFormat(DATE_FORMAT).create();\n}",
        "reject_response": "@Override\npublic void init(Component component, Map<String, String> properties, String clusterName) throws Exception {\n  this.properties = properties;\n\n  String root = MapUtils.getString(properties, ZK_ROOT_NODE_PROPERTY, DEFAULT_ZK_ROOT);\n  LOG.info(\"Connecting to ZooKeeper at \" + properties.get(ZK_CONNECT_STRING_PROPERTY));\n  client = CuratorFrameworkFactory.builder()\n      .connectString(properties.get(ZK_CONNECT_STRING_PROPERTY) + root)\n      .retryPolicy(new ExponentialBackoffRetry(1000, 3))\n      .connectionTimeoutMs(CONNECTION_TIMEOUT)\n      .sessionTimeoutMs(SESSION_TIMEOUT)\n      .build();\n  client.start();\n\n\n  if (component == Component.SERVER) {\n    if (client.checkExists().forPath(\"/\") == null) {\n      client.create().creatingParentContainersIfNeeded().forPath(\"/\");\n    }\n    cache = new TreeCache(client, \"/\");\n    cache.start();\n  } else {\n    while (client.checkExists().forPath(\"/\") == null) {\n      LOG.info(\"Root node is not present yet, going to sleep for \" + WAIT_FOR_ROOT_SLEEP_SECONDS + \" seconds\");\n      Thread.sleep(WAIT_FOR_ROOT_SLEEP_SECONDS * 1000);\n    }\n    cache = new TreeCache(client, String.format(\"/%s\", clusterName));\n  }\n\n  gson = new GsonBuilder().setDateFormat(DATE_FORMAT).create();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2702,
        "instruction": "@GET\n@Path(\"getNodesInfo\")\n@Produces(\"application/json\")\npublic Response read() throws Exception {\n    LOG.debug(\"getNodesInfo started\");\n    StringBuilder jsonOutput = new StringBuilder(\"{\\\"regions\\\":[\");\n    try {\n        /*\n         * 1. Initialize the HADOOP client side API for a distributed file\n         * system\n         */\n        Configuration conf = new Configuration();\n        FileSystem fs = FileSystem.get(conf);\n        DistributedFileSystem dfs = (DistributedFileSystem) fs;\n\n        /*\n         * 2. Query the namenode for the datanodes info. Only live nodes are\n         * returned - in accordance with the results returned by\n         * org.apache.hadoop.hdfs.tools.DFSAdmin#report().\n         */\n        DatanodeInfo[] liveNodes = dfs.getDataNodeStats(DatanodeReportType.LIVE);\n\n        /*\n         * 3. Pack the datanodes info in a JSON text format and write it to\n         * the HTTP output stream.\n         */\n        String prefix = \"\";\n        for (DatanodeInfo node : liveNodes) {\n            verifyNode(node);\n            // write one node to the HTTP stream\n            jsonOutput.append(prefix).append(writeNode(node));\n            prefix = \",\";\n        }\n        jsonOutput.append(\"]}\");\n        LOG.debug(\"getNodesCluster output: \" + jsonOutput);\n    } catch (NodeDataException e) {\n        LOG.error(\"Nodes verification failed\", e);\n        throw e;\n    } catch (ClientAbortException e) {\n        LOG.error(\"Remote connection closed by HAWQ\", e);\n        throw e;\n    } catch (java.io.IOException e) {\n        throw e;\n    }\n\n    return Response.ok(jsonOutput.toString(),\n            MediaType.APPLICATION_JSON_TYPE).build();\n}",
        "accept_response": "@GET\n@Path(\"getNodesInfo\")\n@Produces(\"application/json\")\npublic Response read() throws Exception {\n    LOG.debug(\"getNodesInfo started\");\n    StringBuilder jsonOutput = new StringBuilder(\"{\\\"regions\\\":[\");\n    try {\n        /*\n         * 1. Initialize the HADOOP client side API for a distributed file\n         * system\n         */\n        Configuration conf = new Configuration();\n        FileSystem fs = FileSystem.get(conf);\n        DistributedFileSystem dfs = (DistributedFileSystem) fs;\n\n        /*\n         * 2. Query the namenode for the datanodes info. Only live nodes are\n         * returned - in accordance with the results returned by\n         * org.apache.hadoop.hdfs.tools.DFSAdmin#report().\n         */\n        DatanodeInfo[] liveNodes = dfs.getDataNodeStats(DatanodeReportType.LIVE);\n\n        /*\n         * 3. Pack the datanodes info in a JSON text format and write it to\n         * the HTTP output stream.\n         */\n        String prefix = \"\";\n        for (DatanodeInfo node : liveNodes) {\n            verifyNode(node);\n            // write one node to the HTTP stream\n            jsonOutput.append(prefix).append(writeNode(node));\n            prefix = \",\";\n        }\n        jsonOutput.append(\"]}\");\n        LOG.debug(\"getNodesCluster output: \" + jsonOutput);\n    } catch (NodeDataException e) {\n        LOG.error(\"Nodes verification failed\", e);\n        throw e;\n    } catch (ClientAbortException e) {\n        LOG.error(\"Remote connection closed by HAWQ\", e);\n        throw e;\n    } catch (java.io.IOException e) {\n        LOG.error(\"Unhandled exception thrown\", e);\n        throw e;\n    }\n\n    return Response.ok(jsonOutput.toString(),\n            MediaType.APPLICATION_JSON_TYPE).build();\n}",
        "reject_response": "@GET\n@Path(\"getNodesInfo\")\n@Produces(\"application/json\")\npublic Response read() throws Exception {\n    LOG.debug(\"getNodesInfo started\");\n    StringBuilder jsonOutput = new StringBuilder(\"{\\\"regions\\\":[\");\n    try {\n        /*\n         * 1. Initialize the HADOOP client side API for a distributed file\n         * system\n         */\n        Configuration conf = new Configuration();\n        FileSystem fs = FileSystem.get(conf);\n        DistributedFileSystem dfs = (DistributedFileSystem) fs;\n\n        /*\n         * 2. Query the namenode for the datanodes info. Only live nodes are\n         * returned - in accordance with the results returned by\n         * org.apache.hadoop.hdfs.tools.DFSAdmin#report().\n         */\n        DatanodeInfo[] liveNodes = dfs.getDataNodeStats(DatanodeReportType.LIVE);\n\n        /*\n         * 3. Pack the datanodes info in a JSON text format and write it to\n         * the HTTP output stream.\n         */\n        String prefix = \"\";\n        for (DatanodeInfo node : liveNodes) {\n            verifyNode(node);\n            // write one node to the HTTP stream\n            jsonOutput.append(prefix).append(writeNode(node));\n            prefix = \",\";\n        }\n        jsonOutput.append(\"]}\");\n        LOG.debug(\"getNodesCluster output: \" + jsonOutput);\n    } catch (NodeDataException e) {\n        LOG.error(\"Nodes verification failed\", e);\n        throw e;\n    } catch (ClientAbortException e) {\n        LOG.error(\"Remote connection closed by HAWQ\", e);\n        throw e;\n    } catch (java.io.IOException e) {\n        Log.error(\"Unhandled exception thrown\", e);\n        throw e;\n    }\n\n    return Response.ok(jsonOutput.toString(),\n            MediaType.APPLICATION_JSON_TYPE).build();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3240,
        "instruction": "private Thread runElectionSupportThread(\n    final CountDownLatch latch,\n    final AtomicInteger failureCounter,\n    final long sleepDuration) {\n    final LeaderElectionSupport electionSupport = createLeaderElectionSupport();\n\n    Thread t = new Thread(() -> {\n        try {\n            electionSupport.start();\n            Thread.sleep(sleepDuration);\n            electionSupport.stop();\n\n            latch.countDown();\n        } catch (Exception e) {\n            failureCounter.incrementAndGet();\n        }\n    });\n\n    t.start();\n\n    return t;\n}",
        "accept_response": "private Thread runElectionSupportThread(\n    final CountDownLatch latch,\n    final AtomicInteger failureCounter,\n    final long sleepDuration) {\n    final LeaderElectionSupport electionSupport = createLeaderElectionSupport();\n\n    Thread t = new Thread(() -> {\n        try {\n            electionSupport.start();\n            Thread.sleep(sleepDuration);\n            electionSupport.stop();\n\n            latch.countDown();\n        } catch (Exception e) {\n            LOGGER.warn(\"Failed to run leader election.\", e);\n            failureCounter.incrementAndGet();\n        }\n    });\n\n    t.start();\n\n    return t;\n}",
        "reject_response": "private Thread runElectionSupportThread(\n    final CountDownLatch latch,\n    final AtomicInteger failureCounter,\n    final long sleepDuration) {\n    final LeaderElectionSupport electionSupport = createLeaderElectionSupport();\n\n    Thread t = new Thread(() -> {\n        try {\n            electionSupport.start();\n            Thread.sleep(sleepDuration);\n            electionSupport.stop();\n\n            latch.countDown();\n        } catch (Exception e) {\n            LOGGER.warn(\n                \"Failed to run leader election due to: {}\",\n                e.getMessage());\n            failureCounter.incrementAndGet();\n        }\n    });\n\n    t.start();\n\n    return t;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2673,
        "instruction": "private void loadConf(String fileName, boolean isMandatory) {\n    URL url = getClassLoader().getResource(fileName);\n    if (url == null) {\n        if (isMandatory) {\n            throw new ProfileConfException(PROFILES_FILE_NOT_FOUND, fileName);\n        }\n        return;\n    }\n    try {\n        XMLConfiguration conf = new XMLConfiguration(url);\n        loadMap(conf);\n    } catch (ConfigurationException e) {\n        throw new ProfileConfException(PROFILES_FILE_LOAD_ERR, url.getFile(), String.valueOf(e.getCause()));\n    }\n}",
        "accept_response": "private void loadConf(String fileName, boolean isMandatory) {\n    URL url = getClassLoader().getResource(fileName);\n    if (url == null) {\n        LOG.warn(fileName + \" not found in the classpath\");\n        if (isMandatory) {\n            throw new ProfileConfException(PROFILES_FILE_NOT_FOUND, fileName);\n        }\n        return;\n    }\n    try {\n        XMLConfiguration conf = new XMLConfiguration(url);\n        loadMap(conf);\n    } catch (ConfigurationException e) {\n        throw new ProfileConfException(PROFILES_FILE_LOAD_ERR, url.getFile(), String.valueOf(e.getCause()));\n    }\n}",
        "reject_response": "private void loadConf(String fileName, boolean isMandatory) {\n    URL url = getClassLoader().getResource(fileName);\n    if (url == null) {\n        log.warn(fileName + \" not found in the classpath\");\n        if (isMandatory) {\n            throw new ProfileConfException(PROFILES_FILE_NOT_FOUND, fileName);\n        }\n        return;\n    }\n    try {\n        XMLConfiguration conf = new XMLConfiguration(url);\n        loadMap(conf);\n    } catch (ConfigurationException e) {\n        throw new ProfileConfException(PROFILES_FILE_LOAD_ERR, url.getFile(), String.valueOf(e.getCause()));\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2772,
        "instruction": "@Override\npublic void execute(\n        final HttpRequest request,\n        final AsyncEntityProducer entityProducer,\n        final AsyncExecChain.Scope scope,\n        final AsyncExecChain chain,\n        final AsyncExecCallback asyncExecCallback) throws HttpException, IOException {\n    final String exchangeId = scope.exchangeId;\n    final HttpRoute route = scope.route;\n    final CancellableDependency operation = scope.cancellableDependency;\n    final HttpClientContext clientContext = scope.clientContext;\n    final AsyncExecRuntime execRuntime = scope.execRuntime;\n\n    final AtomicInteger messageCountDown = new AtomicInteger(2);\n    final AsyncClientExchangeHandler internalExchangeHandler = new AsyncClientExchangeHandler() {\n\n        private final AtomicReference<AsyncDataConsumer> entityConsumerRef = new AtomicReference<>(null);\n\n        @Override\n        public void releaseResources() {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.getAndSet(null);\n            if (entityConsumer != null) {\n                entityConsumer.releaseResources();\n            }\n        }\n\n        @Override\n        public void failed(final Exception cause) {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.getAndSet(null);\n            if (entityConsumer != null) {\n                entityConsumer.releaseResources();\n            }\n            execRuntime.markConnectionNonReusable();\n            asyncExecCallback.failed(cause);\n        }\n\n        @Override\n        public void cancel() {\n            failed(new InterruptedIOException());\n        }\n\n        @Override\n        public void produceRequest(\n                final RequestChannel channel,\n                final HttpContext context) throws HttpException, IOException {\n            channel.sendRequest(request, entityProducer, context);\n            if (entityProducer == null) {\n                messageCountDown.decrementAndGet();\n            }\n        }\n\n        @Override\n        public int available() {\n            return entityProducer.available();\n        }\n\n        @Override\n        public void produce(final DataStreamChannel channel) throws IOException {\n            entityProducer.produce(new DataStreamChannel() {\n\n                @Override\n                public void requestOutput() {\n                    channel.requestOutput();\n                }\n\n                @Override\n                public int write(final ByteBuffer src) throws IOException {\n                    return channel.write(src);\n                }\n\n                @Override\n                public void endStream(final List<? extends Header> trailers) throws IOException {\n                    channel.endStream(trailers);\n                    if (messageCountDown.decrementAndGet() <= 0) {\n                        asyncExecCallback.completed();\n                    }\n                }\n\n                @Override\n                public void endStream() throws IOException {\n                    channel.endStream();\n                    if (messageCountDown.decrementAndGet() <= 0) {\n                        asyncExecCallback.completed();\n                    }\n                }\n\n            });\n        }\n\n        @Override\n        public void consumeInformation(\n                final HttpResponse response,\n                final HttpContext context) throws HttpException, IOException {\n            asyncExecCallback.handleInformationResponse(response);\n        }\n\n        @Override\n        public void consumeResponse(\n                final HttpResponse response,\n                final EntityDetails entityDetails,\n                final HttpContext context) throws HttpException, IOException {\n            entityConsumerRef.set(asyncExecCallback.handleResponse(response, entityDetails));\n            if (response.getCode() >= HttpStatus.SC_CLIENT_ERROR) {\n                messageCountDown.decrementAndGet();\n            }\n            final TimeValue keepAliveDuration = keepAliveStrategy.getKeepAliveDuration(response, clientContext);\n            Object userToken = clientContext.getUserToken();\n            if (userToken == null) {\n                userToken = userTokenHandler.getUserToken(route, clientContext);\n                clientContext.setAttribute(HttpClientContext.USER_TOKEN, userToken);\n            }\n            execRuntime.markConnectionReusable(userToken, keepAliveDuration);\n            if (entityDetails == null) {\n                execRuntime.validateConnection();\n                if (messageCountDown.decrementAndGet() <= 0) {\n                    asyncExecCallback.completed();\n                }\n            }\n        }\n\n        @Override\n        public void updateCapacity(final CapacityChannel capacityChannel) throws IOException {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.get();\n            if (entityConsumer != null) {\n                entityConsumer.updateCapacity(capacityChannel);\n            } else {\n                capacityChannel.update(Integer.MAX_VALUE);\n            }\n        }\n\n        @Override\n        public void consume(final ByteBuffer src) throws IOException {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.get();\n            if (entityConsumer != null) {\n                entityConsumer.consume(src);\n            }\n        }\n\n        @Override\n        public void streamEnd(final List<? extends Header> trailers) throws HttpException, IOException {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.getAndSet(null);\n            if (entityConsumer != null) {\n                entityConsumer.streamEnd(trailers);\n            } else {\n                execRuntime.validateConnection();\n            }\n            if (messageCountDown.decrementAndGet() <= 0) {\n                asyncExecCallback.completed();\n            }\n        }\n\n    };\n\n    if (log.isDebugEnabled()) {\n        operation.setDependency(execRuntime.execute(\n                exchangeId,\n                new LoggingAsyncClientExchangeHandler(log, exchangeId, internalExchangeHandler),\n                clientContext));\n    } else {\n        operation.setDependency(execRuntime.execute(exchangeId, internalExchangeHandler, clientContext));\n    }\n}",
        "accept_response": "@Override\npublic void execute(\n        final HttpRequest request,\n        final AsyncEntityProducer entityProducer,\n        final AsyncExecChain.Scope scope,\n        final AsyncExecChain chain,\n        final AsyncExecCallback asyncExecCallback) throws HttpException, IOException {\n    final String exchangeId = scope.exchangeId;\n    final HttpRoute route = scope.route;\n    final CancellableDependency operation = scope.cancellableDependency;\n    final HttpClientContext clientContext = scope.clientContext;\n    final AsyncExecRuntime execRuntime = scope.execRuntime;\n\n    if (log.isDebugEnabled()) {\n        log.debug(\"{}: executing {}\", exchangeId, new RequestLine(request));\n    }\n\n    final AtomicInteger messageCountDown = new AtomicInteger(2);\n    final AsyncClientExchangeHandler internalExchangeHandler = new AsyncClientExchangeHandler() {\n\n        private final AtomicReference<AsyncDataConsumer> entityConsumerRef = new AtomicReference<>(null);\n\n        @Override\n        public void releaseResources() {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.getAndSet(null);\n            if (entityConsumer != null) {\n                entityConsumer.releaseResources();\n            }\n        }\n\n        @Override\n        public void failed(final Exception cause) {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.getAndSet(null);\n            if (entityConsumer != null) {\n                entityConsumer.releaseResources();\n            }\n            execRuntime.markConnectionNonReusable();\n            asyncExecCallback.failed(cause);\n        }\n\n        @Override\n        public void cancel() {\n            failed(new InterruptedIOException());\n        }\n\n        @Override\n        public void produceRequest(\n                final RequestChannel channel,\n                final HttpContext context) throws HttpException, IOException {\n            channel.sendRequest(request, entityProducer, context);\n            if (entityProducer == null) {\n                messageCountDown.decrementAndGet();\n            }\n        }\n\n        @Override\n        public int available() {\n            return entityProducer.available();\n        }\n\n        @Override\n        public void produce(final DataStreamChannel channel) throws IOException {\n            entityProducer.produce(new DataStreamChannel() {\n\n                @Override\n                public void requestOutput() {\n                    channel.requestOutput();\n                }\n\n                @Override\n                public int write(final ByteBuffer src) throws IOException {\n                    return channel.write(src);\n                }\n\n                @Override\n                public void endStream(final List<? extends Header> trailers) throws IOException {\n                    channel.endStream(trailers);\n                    if (messageCountDown.decrementAndGet() <= 0) {\n                        asyncExecCallback.completed();\n                    }\n                }\n\n                @Override\n                public void endStream() throws IOException {\n                    channel.endStream();\n                    if (messageCountDown.decrementAndGet() <= 0) {\n                        asyncExecCallback.completed();\n                    }\n                }\n\n            });\n        }\n\n        @Override\n        public void consumeInformation(\n                final HttpResponse response,\n                final HttpContext context) throws HttpException, IOException {\n            asyncExecCallback.handleInformationResponse(response);\n        }\n\n        @Override\n        public void consumeResponse(\n                final HttpResponse response,\n                final EntityDetails entityDetails,\n                final HttpContext context) throws HttpException, IOException {\n            entityConsumerRef.set(asyncExecCallback.handleResponse(response, entityDetails));\n            if (response.getCode() >= HttpStatus.SC_CLIENT_ERROR) {\n                messageCountDown.decrementAndGet();\n            }\n            final TimeValue keepAliveDuration = keepAliveStrategy.getKeepAliveDuration(response, clientContext);\n            Object userToken = clientContext.getUserToken();\n            if (userToken == null) {\n                userToken = userTokenHandler.getUserToken(route, clientContext);\n                clientContext.setAttribute(HttpClientContext.USER_TOKEN, userToken);\n            }\n            execRuntime.markConnectionReusable(userToken, keepAliveDuration);\n            if (entityDetails == null) {\n                execRuntime.validateConnection();\n                if (messageCountDown.decrementAndGet() <= 0) {\n                    asyncExecCallback.completed();\n                }\n            }\n        }\n\n        @Override\n        public void updateCapacity(final CapacityChannel capacityChannel) throws IOException {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.get();\n            if (entityConsumer != null) {\n                entityConsumer.updateCapacity(capacityChannel);\n            } else {\n                capacityChannel.update(Integer.MAX_VALUE);\n            }\n        }\n\n        @Override\n        public void consume(final ByteBuffer src) throws IOException {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.get();\n            if (entityConsumer != null) {\n                entityConsumer.consume(src);\n            }\n        }\n\n        @Override\n        public void streamEnd(final List<? extends Header> trailers) throws HttpException, IOException {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.getAndSet(null);\n            if (entityConsumer != null) {\n                entityConsumer.streamEnd(trailers);\n            } else {\n                execRuntime.validateConnection();\n            }\n            if (messageCountDown.decrementAndGet() <= 0) {\n                asyncExecCallback.completed();\n            }\n        }\n\n    };\n\n    if (log.isDebugEnabled()) {\n        operation.setDependency(execRuntime.execute(\n                exchangeId,\n                new LoggingAsyncClientExchangeHandler(log, exchangeId, internalExchangeHandler),\n                clientContext));\n    } else {\n        operation.setDependency(execRuntime.execute(exchangeId, internalExchangeHandler, clientContext));\n    }\n}",
        "reject_response": "@Override\npublic void execute(\n        final HttpRequest request,\n        final AsyncEntityProducer entityProducer,\n        final AsyncExecChain.Scope scope,\n        final AsyncExecChain chain,\n        final AsyncExecCallback asyncExecCallback) throws HttpException, IOException {\n    final String exchangeId = scope.exchangeId;\n    final HttpRoute route = scope.route;\n    final CancellableDependency operation = scope.cancellableDependency;\n    final HttpClientContext clientContext = scope.clientContext;\n    final AsyncExecRuntime execRuntime = scope.execRuntime;\n\n    if (log.isDebugEnabled()) {\n        log.debug(exchangeId + \": executing \" + new RequestLine(request));\n    }\n\n    final AtomicInteger messageCountDown = new AtomicInteger(2);\n    final AsyncClientExchangeHandler internalExchangeHandler = new AsyncClientExchangeHandler() {\n\n        private final AtomicReference<AsyncDataConsumer> entityConsumerRef = new AtomicReference<>(null);\n\n        @Override\n        public void releaseResources() {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.getAndSet(null);\n            if (entityConsumer != null) {\n                entityConsumer.releaseResources();\n            }\n        }\n\n        @Override\n        public void failed(final Exception cause) {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.getAndSet(null);\n            if (entityConsumer != null) {\n                entityConsumer.releaseResources();\n            }\n            execRuntime.markConnectionNonReusable();\n            asyncExecCallback.failed(cause);\n        }\n\n        @Override\n        public void cancel() {\n            failed(new InterruptedIOException());\n        }\n\n        @Override\n        public void produceRequest(\n                final RequestChannel channel,\n                final HttpContext context) throws HttpException, IOException {\n            channel.sendRequest(request, entityProducer, context);\n            if (entityProducer == null) {\n                messageCountDown.decrementAndGet();\n            }\n        }\n\n        @Override\n        public int available() {\n            return entityProducer.available();\n        }\n\n        @Override\n        public void produce(final DataStreamChannel channel) throws IOException {\n            entityProducer.produce(new DataStreamChannel() {\n\n                @Override\n                public void requestOutput() {\n                    channel.requestOutput();\n                }\n\n                @Override\n                public int write(final ByteBuffer src) throws IOException {\n                    return channel.write(src);\n                }\n\n                @Override\n                public void endStream(final List<? extends Header> trailers) throws IOException {\n                    channel.endStream(trailers);\n                    if (messageCountDown.decrementAndGet() <= 0) {\n                        asyncExecCallback.completed();\n                    }\n                }\n\n                @Override\n                public void endStream() throws IOException {\n                    channel.endStream();\n                    if (messageCountDown.decrementAndGet() <= 0) {\n                        asyncExecCallback.completed();\n                    }\n                }\n\n            });\n        }\n\n        @Override\n        public void consumeInformation(\n                final HttpResponse response,\n                final HttpContext context) throws HttpException, IOException {\n            asyncExecCallback.handleInformationResponse(response);\n        }\n\n        @Override\n        public void consumeResponse(\n                final HttpResponse response,\n                final EntityDetails entityDetails,\n                final HttpContext context) throws HttpException, IOException {\n            entityConsumerRef.set(asyncExecCallback.handleResponse(response, entityDetails));\n            if (response.getCode() >= HttpStatus.SC_CLIENT_ERROR) {\n                messageCountDown.decrementAndGet();\n            }\n            final TimeValue keepAliveDuration = keepAliveStrategy.getKeepAliveDuration(response, clientContext);\n            Object userToken = clientContext.getUserToken();\n            if (userToken == null) {\n                userToken = userTokenHandler.getUserToken(route, clientContext);\n                clientContext.setAttribute(HttpClientContext.USER_TOKEN, userToken);\n            }\n            execRuntime.markConnectionReusable(userToken, keepAliveDuration);\n            if (entityDetails == null) {\n                execRuntime.validateConnection();\n                if (messageCountDown.decrementAndGet() <= 0) {\n                    asyncExecCallback.completed();\n                }\n            }\n        }\n\n        @Override\n        public void updateCapacity(final CapacityChannel capacityChannel) throws IOException {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.get();\n            if (entityConsumer != null) {\n                entityConsumer.updateCapacity(capacityChannel);\n            } else {\n                capacityChannel.update(Integer.MAX_VALUE);\n            }\n        }\n\n        @Override\n        public void consume(final ByteBuffer src) throws IOException {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.get();\n            if (entityConsumer != null) {\n                entityConsumer.consume(src);\n            }\n        }\n\n        @Override\n        public void streamEnd(final List<? extends Header> trailers) throws HttpException, IOException {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.getAndSet(null);\n            if (entityConsumer != null) {\n                entityConsumer.streamEnd(trailers);\n            } else {\n                execRuntime.validateConnection();\n            }\n            if (messageCountDown.decrementAndGet() <= 0) {\n                asyncExecCallback.completed();\n            }\n        }\n\n    };\n\n    if (log.isDebugEnabled()) {\n        operation.setDependency(execRuntime.execute(\n                exchangeId,\n                new LoggingAsyncClientExchangeHandler(log, exchangeId, internalExchangeHandler),\n                clientContext));\n    } else {\n        operation.setDependency(execRuntime.execute(exchangeId, internalExchangeHandler, clientContext));\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2884,
        "instruction": "public void reset() {\n  LOG.info(\"Resetting the JoshuaConfiguration to its defaults ...\");\n  LOG.info(\"\\n\\tResetting the StatefullFF global state index ...\");\n  StatefulFF.resetGlobalStateIndex();\n  tms = new ArrayList<String>();\n  weights_file = \"\";\n  default_non_terminal = \"[X]\";\n  oovList = new ArrayList<OOVItem>();\n  oovList.add(new OOVItem(default_non_terminal, 1.0f));\n  goal_symbol = \"[GOAL]\";\n  amortized_sorting = true;\n  constrain_parse = false;\n  use_pos_labels = false;\n  true_oovs_only = false;\n  filter_grammar = false;\n  pop_limit = 100;\n  maxlen = 200;\n  use_unique_nbest = false;\n  include_align_index = false;\n  topN = 1;\n  outputFormat = \"%i ||| %s ||| %f ||| %c\";\n  num_parallel_decoders = 1;\n  hypergraphFilePattern = \"\";\n  mark_oovs = false;\n  // oracleFile = null;\n  parse = false; // perform synchronous parsing\n  features = new ArrayList<String>();\n  weights = new ArrayList<String>();\n  server_port = 0;\n\n  reordering_limit = 8;\n  num_translation_options = 20;\n  LOG.info(\"...done\");\n}",
        "accept_response": "public void reset() {\n  LOG.info(\"Resetting the JoshuaConfiguration to its defaults ...\");\n  LOG.info(\"\\n\\tResetting the StatefullFF global state index ...\");\n  LOG.info(\"\\n\\t...done\");\n  StatefulFF.resetGlobalStateIndex();\n  tms = new ArrayList<String>();\n  weights_file = \"\";\n  default_non_terminal = \"[X]\";\n  oovList = new ArrayList<OOVItem>();\n  oovList.add(new OOVItem(default_non_terminal, 1.0f));\n  goal_symbol = \"[GOAL]\";\n  amortized_sorting = true;\n  constrain_parse = false;\n  use_pos_labels = false;\n  true_oovs_only = false;\n  filter_grammar = false;\n  pop_limit = 100;\n  maxlen = 200;\n  use_unique_nbest = false;\n  include_align_index = false;\n  topN = 1;\n  outputFormat = \"%i ||| %s ||| %f ||| %c\";\n  num_parallel_decoders = 1;\n  hypergraphFilePattern = \"\";\n  mark_oovs = false;\n  // oracleFile = null;\n  parse = false; // perform synchronous parsing\n  features = new ArrayList<String>();\n  weights = new ArrayList<String>();\n  server_port = 0;\n\n  reordering_limit = 8;\n  num_translation_options = 20;\n  LOG.info(\"...done\");\n}",
        "reject_response": "public void reset() {\n  LOG.info(\"Resetting the JoshuaConfiguration to its defaults ...\");\n  LOG.info(\"\\n\\tResetting the StatefullFF global state index ...\");\n  logger.info(\"Resetting the JoshuaConfiguration to its defaults ...\");\n  logger.info(\"\\n\\tResetting the StatefullFF global state index ...\");\n  logger.info(\"\\n\\t...done\");\n  StatefulFF.resetGlobalStateIndex();\n  tms = new ArrayList<String>();\n  weights_file = \"\";\n  default_non_terminal = \"[X]\";\n  oovList = new ArrayList<OOVItem>();\n  oovList.add(new OOVItem(default_non_terminal, 1.0f));\n  goal_symbol = \"[GOAL]\";\n  amortized_sorting = true;\n  constrain_parse = false;\n  use_pos_labels = false;\n  true_oovs_only = false;\n  filter_grammar = false;\n  pop_limit = 100;\n  maxlen = 200;\n  use_unique_nbest = false;\n  include_align_index = false;\n  topN = 1;\n  outputFormat = \"%i ||| %s ||| %f ||| %c\";\n  num_parallel_decoders = 1;\n  hypergraphFilePattern = \"\";\n  mark_oovs = false;\n  // oracleFile = null;\n  parse = false; // perform synchronous parsing\n  features = new ArrayList<String>();\n  weights = new ArrayList<String>();\n  server_port = 0;\n\n  reordering_limit = 8;\n  num_translation_options = 20;\n  LOG.info(\"...done\");\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2755,
        "instruction": "@Deprecated\npublic void cancelDelegationToken(Token<DelegationTokenIdentifier> token)\n    throws IOException {\n  try {\n    token.cancel(conf);\n  } catch (InterruptedException ie) {\n    throw new RuntimeException(\"caught interrupted\", ie);\n  } catch (RemoteException re) {\n    throw re.unwrapRemoteException(InvalidToken.class,\n        AccessControlException.class);\n  }\n}",
        "accept_response": "@Deprecated\npublic void cancelDelegationToken(Token<DelegationTokenIdentifier> token)\n    throws IOException {\n  LOG.info(\"Cancelling {}\", DelegationTokenIdentifier.stringifyToken(token));\n  try {\n    token.cancel(conf);\n  } catch (InterruptedException ie) {\n    throw new RuntimeException(\"caught interrupted\", ie);\n  } catch (RemoteException re) {\n    throw re.unwrapRemoteException(InvalidToken.class,\n        AccessControlException.class);\n  }\n}",
        "reject_response": "@Deprecated\npublic void cancelDelegationToken(Token<DelegationTokenIdentifier> token)\n    throws IOException {\n  LOG.info(\"Cancelling \" + DelegationTokenIdentifier.stringifyToken(token));\n  try {\n    token.cancel(conf);\n  } catch (InterruptedException ie) {\n    throw new RuntimeException(\"caught interrupted\", ie);\n  } catch (RemoteException re) {\n    throw re.unwrapRemoteException(InvalidToken.class,\n        AccessControlException.class);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2438,
        "instruction": "private ObjectName createTransportLoggerObjectName()  {\n    try {\n        return new ObjectName(\n                createTransportLoggerObjectNameRoot(this.managementContext)\n                + JMXSupport.encodeObjectNamePart(TransportLogger.class.getSimpleName()\n                        + \" \" + this.id + \";\" + this.nextTransportName));\n    } catch (Exception e) {\n        return null;\n    }\n}",
        "accept_response": "private ObjectName createTransportLoggerObjectName()  {\n    try {\n        return new ObjectName(\n                createTransportLoggerObjectNameRoot(this.managementContext)\n                + JMXSupport.encodeObjectNamePart(TransportLogger.class.getSimpleName()\n                        + \" \" + this.id + \";\" + this.nextTransportName));\n    } catch (Exception e) {\n        log.error(\"Could not create ObjectName for TransportLoggerView {}, reason: {}\", id, e, e);\n        return null;\n    }\n}",
        "reject_response": "private ObjectName createTransportLoggerObjectName()  {\n    try {\n        return new ObjectName(\n                createTransportLoggerObjectNameRoot(this.managementContext)\n                + JMXSupport.encodeObjectNamePart(TransportLogger.class.getSimpleName()\n                        + \" \" + this.id + \";\" + this.nextTransportName));\n    } catch (Exception e) {\n        log.error(\"Could not create ObjectName for TransportLoggerView \" + id + \", reason: \" + e, e);\n        return null;\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2913,
        "instruction": "private synchronized KerberosTicket getTGT() {\n    Set<KerberosTicket> tickets = subject.getPrivateCredentials(KerberosTicket.class);\n    for (KerberosTicket ticket : tickets) {\n        KerberosPrincipal server = ticket.getServer();\n        if (server.getName().equals(\"krbtgt/\" + server.getRealm() + \"@\" + server.getRealm())) {\n            return ticket;\n        }\n    }\n    return null;\n}",
        "accept_response": "private synchronized KerberosTicket getTGT() {\n    Set<KerberosTicket> tickets = subject.getPrivateCredentials(KerberosTicket.class);\n    for (KerberosTicket ticket : tickets) {\n        KerberosPrincipal server = ticket.getServer();\n        if (server.getName().equals(\"krbtgt/\" + server.getRealm() + \"@\" + server.getRealm())) {\n            log.debug(\"Found TGT with client principal '{}' and server principal '{}'.\", ticket.getClient().getName(),\n                    ticket.getServer().getName());\n            return ticket;\n        }\n    }\n    return null;\n}",
        "reject_response": "private synchronized KerberosTicket getTGT() {\n    Set<KerberosTicket> tickets = subject.getPrivateCredentials(KerberosTicket.class);\n    for (KerberosTicket ticket : tickets) {\n        KerberosPrincipal server = ticket.getServer();\n        if (server.getName().equals(\"krbtgt/\" + server.getRealm() + \"@\" + server.getRealm())) {\n            log.debug(\"Found TGT {}.\", ticket);\n            return ticket;\n        }\n    }\n    return null;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2907,
        "instruction": "@SuppressWarnings(\"unchecked\")\npublic DNSLookupContinuation onDNSResponse(DNSResponse response, SPFSession spfSession)\n       throws PermErrorException, TempErrorException, NoneException, NeutralException {\n       try {\n\n           List<String> records = (List<String>) spfSession.getAttribute(ATTRIBUTE_CHECK_RECORDS);\n           List<String> mxR = (List<String>) spfSession.getAttribute(ATTRIBUTE_MX_RECORDS);\n\n           if (records == null) {\n\n               records = response.getResponse();\n\n               if (records == null) {\n                   // no mx record found\n                   spfSession.setAttribute(Directive.ATTRIBUTE_MECHANISM_RESULT, Boolean.FALSE);\n                   return null;\n               }\n\n               spfSession.setAttribute(ATTRIBUTE_CHECK_RECORDS, records);\n\n           } else {\n\n               List<String> res = response.getResponse();\n\n               if (res != null) {\n                   if (mxR == null) {\n                       mxR = new ArrayList<String>();\n                       spfSession.setAttribute(ATTRIBUTE_MX_RECORDS, mxR);\n                   }\n                   mxR.addAll(res);\n               }\n\n           }\n\n           // if the remote IP is an ipv6 we check ipv6 addresses, otherwise ip4\n           boolean isIPv6 = IPAddr.isIPV6(spfSession.getIpAddress());\n\n           String mx;\n           while (records.size() > 0 && (mx = records.remove(0)) != null && mx.length() > 0) {\n\n               return new DNSLookupContinuation(new DNSRequest(mx, isIPv6 ? DNSRequest.AAAA : DNSRequest.A), MXMechanism.this);\n\n           }\n\n           // no mx record found\n           if (mxR == null || mxR.size() == 0) {\n               spfSession.setAttribute(Directive.ATTRIBUTE_MECHANISM_RESULT, Boolean.FALSE);\n               return null;\n           }\n\n           // get the ipAddress\n           IPAddr checkAddress;\n           checkAddress = IPAddr.getAddress(spfSession.getIpAddress(), isIPv6 ? getIp6cidr() : getIp4cidr());\n\n           // clean up attributes\n           spfSession.removeAttribute(ATTRIBUTE_CHECK_RECORDS);\n           spfSession.removeAttribute(ATTRIBUTE_MX_RECORDS);\n           spfSession.setAttribute(Directive.ATTRIBUTE_MECHANISM_RESULT, Boolean.valueOf(checkAddressList(checkAddress, mxR, getIp4cidr())));\n           return null;\n\n       } catch (TimeoutException e) {\n           spfSession.setAttribute(ATTRIBUTE_CHECK_RECORDS, null);\n           spfSession.setAttribute(ATTRIBUTE_MX_RECORDS, null);\n           throw new TempErrorException(\"Timeout querying the dns server\");\n       }\n   }",
        "accept_response": "@SuppressWarnings(\"unchecked\")\npublic DNSLookupContinuation onDNSResponse(DNSResponse response, SPFSession spfSession)\n       throws PermErrorException, TempErrorException, NoneException, NeutralException {\n       try {\n\n           List<String> records = (List<String>) spfSession.getAttribute(ATTRIBUTE_CHECK_RECORDS);\n           List<String> mxR = (List<String>) spfSession.getAttribute(ATTRIBUTE_MX_RECORDS);\n\n           if (records == null) {\n\n               records = response.getResponse();\n\n               if (records == null) {\n                   // no mx record found\n                   spfSession.setAttribute(Directive.ATTRIBUTE_MECHANISM_RESULT, Boolean.FALSE);\n                   return null;\n               }\n\n               spfSession.setAttribute(ATTRIBUTE_CHECK_RECORDS, records);\n\n           } else {\n\n               List<String> res = response.getResponse();\n\n               if (res != null) {\n                   if (mxR == null) {\n                       mxR = new ArrayList<String>();\n                       spfSession.setAttribute(ATTRIBUTE_MX_RECORDS, mxR);\n                   }\n                   mxR.addAll(res);\n               }\n\n           }\n\n           // if the remote IP is an ipv6 we check ipv6 addresses, otherwise ip4\n           boolean isIPv6 = IPAddr.isIPV6(spfSession.getIpAddress());\n\n           String mx;\n           while (records.size() > 0 && (mx = records.remove(0)) != null && mx.length() > 0) {\n               LOGGER.debug(\"Add MX-Record {} to list\", mx);\n\n               return new DNSLookupContinuation(new DNSRequest(mx, isIPv6 ? DNSRequest.AAAA : DNSRequest.A), MXMechanism.this);\n\n           }\n\n           // no mx record found\n           if (mxR == null || mxR.size() == 0) {\n               spfSession.setAttribute(Directive.ATTRIBUTE_MECHANISM_RESULT, Boolean.FALSE);\n               return null;\n           }\n\n           // get the ipAddress\n           IPAddr checkAddress;\n           checkAddress = IPAddr.getAddress(spfSession.getIpAddress(), isIPv6 ? getIp6cidr() : getIp4cidr());\n\n           // clean up attributes\n           spfSession.removeAttribute(ATTRIBUTE_CHECK_RECORDS);\n           spfSession.removeAttribute(ATTRIBUTE_MX_RECORDS);\n           spfSession.setAttribute(Directive.ATTRIBUTE_MECHANISM_RESULT, Boolean.valueOf(checkAddressList(checkAddress, mxR, getIp4cidr())));\n           return null;\n\n       } catch (TimeoutException e) {\n           spfSession.setAttribute(ATTRIBUTE_CHECK_RECORDS, null);\n           spfSession.setAttribute(ATTRIBUTE_MX_RECORDS, null);\n           throw new TempErrorException(\"Timeout querying the dns server\");\n       }\n   }",
        "reject_response": "@SuppressWarnings(\"unchecked\")\npublic DNSLookupContinuation onDNSResponse(DNSResponse response, SPFSession spfSession)\n       throws PermErrorException, TempErrorException, NoneException, NeutralException {\n       try {\n\n           List<String> records = (List<String>) spfSession.getAttribute(ATTRIBUTE_CHECK_RECORDS);\n           List<String> mxR = (List<String>) spfSession.getAttribute(ATTRIBUTE_MX_RECORDS);\n\n           if (records == null) {\n\n               records = response.getResponse();\n\n               if (records == null) {\n                   // no mx record found\n                   spfSession.setAttribute(Directive.ATTRIBUTE_MECHANISM_RESULT, Boolean.FALSE);\n                   return null;\n               }\n\n               spfSession.setAttribute(ATTRIBUTE_CHECK_RECORDS, records);\n\n           } else {\n\n               List<String> res = response.getResponse();\n\n               if (res != null) {\n                   if (mxR == null) {\n                       mxR = new ArrayList<String>();\n                       spfSession.setAttribute(ATTRIBUTE_MX_RECORDS, mxR);\n                   }\n                   mxR.addAll(res);\n               }\n\n           }\n\n           // if the remote IP is an ipv6 we check ipv6 addresses, otherwise ip4\n           boolean isIPv6 = IPAddr.isIPV6(spfSession.getIpAddress());\n\n           String mx;\n           while (records.size() > 0 && (mx = records.remove(0)) != null && mx.length() > 0) {\n               log.debug(\"Add MX-Record \" + mx + \" to list\");\n\n               return new DNSLookupContinuation(new DNSRequest(mx, isIPv6 ? DNSRequest.AAAA : DNSRequest.A), MXMechanism.this);\n\n           }\n\n           // no mx record found\n           if (mxR == null || mxR.size() == 0) {\n               spfSession.setAttribute(Directive.ATTRIBUTE_MECHANISM_RESULT, Boolean.FALSE);\n               return null;\n           }\n\n           // get the ipAddress\n           IPAddr checkAddress;\n           checkAddress = IPAddr.getAddress(spfSession.getIpAddress(), isIPv6 ? getIp6cidr() : getIp4cidr());\n\n           // clean up attributes\n           spfSession.removeAttribute(ATTRIBUTE_CHECK_RECORDS);\n           spfSession.removeAttribute(ATTRIBUTE_MX_RECORDS);\n           spfSession.setAttribute(Directive.ATTRIBUTE_MECHANISM_RESULT, Boolean.valueOf(checkAddressList(checkAddress, mxR, getIp4cidr())));\n           return null;\n\n       } catch (TimeoutException e) {\n           spfSession.setAttribute(ATTRIBUTE_CHECK_RECORDS, null);\n           spfSession.setAttribute(ATTRIBUTE_MX_RECORDS, null);\n           throw new TempErrorException(\"Timeout querying the dns server\");\n       }\n   }",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3055,
        "instruction": "private void updateCommits(List<? extends IndexCommit> commits) {\n  // to be safe, we should only call delete on a commit point passed to us\n  // in this specific call (may be across diff IndexWriter instances).\n  // this will happen rarely, so just synchronize everything\n  // for safety and to avoid race conditions\n\n  synchronized (this) {\n    long maxCommitAgeTimeStamp = -1L;\n    IndexCommit newest = commits.get(commits.size() - 1);\n    int singleSegKept = (newest.getSegmentCount() == 1) ? 1 : 0;\n    int totalKept = 1;\n\n    // work our way from newest to oldest, skipping the first since we always want to keep it.\n    for (int i=commits.size()-2; i>=0; i--) {\n      IndexCommit commit = commits.get(i);\n\n      // delete anything too old, regardless of other policies\n      try {\n        if (maxCommitAge != null) {\n          if (maxCommitAgeTimeStamp==-1) {\n            DateMathParser dmp = new DateMathParser(DateMathParser.UTC);\n            maxCommitAgeTimeStamp = dmp.parseMath(maxCommitAge).getTime();\n          }\n          if (IndexDeletionPolicyWrapper.getCommitTimestamp(commit) < maxCommitAgeTimeStamp) {\n            commit.delete();\n            continue;\n          }\n        }\n      } catch (Exception e) {\n        log.warn(\"Exception while checking commit point's age for deletion\", e);\n      }\n\n      if (singleSegKept < maxOptimizedCommitsToKeep && commit.getSegmentCount() == 1) {\n        totalKept++;\n        singleSegKept++;\n        continue;\n      }\n\n      if (totalKept < maxCommitsToKeep) {\n        totalKept++;\n        continue;\n      }\n\n      commit.delete();\n    }\n\n  } // end synchronized\n}",
        "accept_response": "private void updateCommits(List<? extends IndexCommit> commits) {\n  // to be safe, we should only call delete on a commit point passed to us\n  // in this specific call (may be across diff IndexWriter instances).\n  // this will happen rarely, so just synchronize everything\n  // for safety and to avoid race conditions\n\n  synchronized (this) {\n    long maxCommitAgeTimeStamp = -1L;\n    IndexCommit newest = commits.get(commits.size() - 1);\n    log.debug(\"newest commit generation = {}\", newest.getGeneration());\n    int singleSegKept = (newest.getSegmentCount() == 1) ? 1 : 0;\n    int totalKept = 1;\n\n    // work our way from newest to oldest, skipping the first since we always want to keep it.\n    for (int i=commits.size()-2; i>=0; i--) {\n      IndexCommit commit = commits.get(i);\n\n      // delete anything too old, regardless of other policies\n      try {\n        if (maxCommitAge != null) {\n          if (maxCommitAgeTimeStamp==-1) {\n            DateMathParser dmp = new DateMathParser(DateMathParser.UTC);\n            maxCommitAgeTimeStamp = dmp.parseMath(maxCommitAge).getTime();\n          }\n          if (IndexDeletionPolicyWrapper.getCommitTimestamp(commit) < maxCommitAgeTimeStamp) {\n            commit.delete();\n            continue;\n          }\n        }\n      } catch (Exception e) {\n        log.warn(\"Exception while checking commit point's age for deletion\", e);\n      }\n\n      if (singleSegKept < maxOptimizedCommitsToKeep && commit.getSegmentCount() == 1) {\n        totalKept++;\n        singleSegKept++;\n        continue;\n      }\n\n      if (totalKept < maxCommitsToKeep) {\n        totalKept++;\n        continue;\n      }\n\n      commit.delete();\n    }\n\n  } // end synchronized\n}",
        "reject_response": "private void updateCommits(List<? extends IndexCommit> commits) {\n  // to be safe, we should only call delete on a commit point passed to us\n  // in this specific call (may be across diff IndexWriter instances).\n  // this will happen rarely, so just synchronize everything\n  // for safety and to avoid race conditions\n\n  synchronized (this) {\n    long maxCommitAgeTimeStamp = -1L;\n    IndexCommit newest = commits.get(commits.size() - 1);\n    log.debug(\"newest commit generation = \" + newest.getGeneration());\n    int singleSegKept = (newest.getSegmentCount() == 1) ? 1 : 0;\n    int totalKept = 1;\n\n    // work our way from newest to oldest, skipping the first since we always want to keep it.\n    for (int i=commits.size()-2; i>=0; i--) {\n      IndexCommit commit = commits.get(i);\n\n      // delete anything too old, regardless of other policies\n      try {\n        if (maxCommitAge != null) {\n          if (maxCommitAgeTimeStamp==-1) {\n            DateMathParser dmp = new DateMathParser(DateMathParser.UTC);\n            maxCommitAgeTimeStamp = dmp.parseMath(maxCommitAge).getTime();\n          }\n          if (IndexDeletionPolicyWrapper.getCommitTimestamp(commit) < maxCommitAgeTimeStamp) {\n            commit.delete();\n            continue;\n          }\n        }\n      } catch (Exception e) {\n        log.warn(\"Exception while checking commit point's age for deletion\", e);\n      }\n\n      if (singleSegKept < maxOptimizedCommitsToKeep && commit.getSegmentCount() == 1) {\n        totalKept++;\n        singleSegKept++;\n        continue;\n      }\n\n      if (totalKept < maxCommitsToKeep) {\n        totalKept++;\n        continue;\n      }\n\n      commit.delete();\n    }\n\n  } // end synchronized\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2435,
        "instruction": "private Transport createRemoteTransport(final Transport local) throws Exception {\n    Transport transport = TransportFactory.compositeConnect(remote);\n    CompositeTransport ct = transport.narrow(CompositeTransport.class);\n    if (ct != null && localUri != null && proxyToLocalBroker) {\n        ct.add(false, new URI[] { localUri });\n    }\n\n    // Add a transport filter so that we can track the transport life cycle\n    transport = new TransportFilter(transport) {\n        @Override\n        public void stop() throws Exception {\n            LOG.info(\"Stopping proxy.\");\n            super.stop();\n            ProxyConnection dummy = new ProxyConnection(local, this);\n            connections.remove(dummy);\n        }\n    };\n    return transport;\n}",
        "accept_response": "private Transport createRemoteTransport(final Transport local) throws Exception {\n    Transport transport = TransportFactory.compositeConnect(remote);\n    CompositeTransport ct = transport.narrow(CompositeTransport.class);\n    if (ct != null && localUri != null && proxyToLocalBroker) {\n        ct.add(false, new URI[] { localUri });\n    }\n\n    // Add a transport filter so that we can track the transport life cycle\n    transport = new TransportFilter(transport) {\n        @Override\n        public void stop() throws Exception {\n            LOG.info(\"Stopping proxy.\");\n            super.stop();\n            ProxyConnection dummy = new ProxyConnection(local, this);\n            LOG.debug(\"Removing proxyConnection {}\", dummy);\n            connections.remove(dummy);\n        }\n    };\n    return transport;\n}",
        "reject_response": "private Transport createRemoteTransport(final Transport local) throws Exception {\n    Transport transport = TransportFactory.compositeConnect(remote);\n    CompositeTransport ct = transport.narrow(CompositeTransport.class);\n    if (ct != null && localUri != null && proxyToLocalBroker) {\n        ct.add(false, new URI[] { localUri });\n    }\n\n    // Add a transport filter so that we can track the transport life cycle\n    transport = new TransportFilter(transport) {\n        @Override\n        public void stop() throws Exception {\n            LOG.info(\"Stopping proxy.\");\n            super.stop();\n            ProxyConnection dummy = new ProxyConnection(local, this);\n            LOG.debug(\"Removing proxyConnection {}\", dummy.toString());\n            connections.remove(dummy);\n        }\n    };\n    return transport;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2956,
        "instruction": "private static NiFiProperties initializeProperties(final String[] args, final ClassLoader boostrapLoader) {\n    // Try to get key\n    // If key doesn't exist, instantiate without\n    // Load properties\n    // If properties are protected and key missing, throw RuntimeException\n\n    final ClassLoader contextClassLoader = Thread.currentThread().getContextClassLoader();\n    final String key;\n    try {\n        key = loadFormattedKey(args);\n        // The key might be empty or null when it is passed to the loader\n    } catch (IllegalArgumentException e) {\n        final String msg = \"The bootstrap process did not provide a valid key\";\n        throw new IllegalArgumentException(msg, e);\n    }\n    Thread.currentThread().setContextClassLoader(boostrapLoader);\n\n    try {\n        final Class<?> propsLoaderClass = Class.forName(\"org.apache.nifi.properties.NiFiPropertiesLoader\", true, boostrapLoader);\n        final Method withKeyMethod = propsLoaderClass.getMethod(\"withKey\", String.class);\n        final Object loaderInstance = withKeyMethod.invoke(null, key);\n        final Method getMethod = propsLoaderClass.getMethod(\"get\");\n        final NiFiProperties properties = (NiFiProperties) getMethod.invoke(loaderInstance);\n        return properties;\n    } catch (InvocationTargetException wrappedException) {\n        final String msg = \"There was an issue decrypting protected properties\";\n        throw new IllegalArgumentException(msg, wrappedException.getCause() == null ? wrappedException : wrappedException.getCause());\n    } catch (final IllegalAccessException | NoSuchMethodException | ClassNotFoundException reex) {\n        final String msg = \"Unable to access properties loader in the expected manner - apparent classpath or build issue\";\n        throw new IllegalArgumentException(msg, reex);\n    } catch (final RuntimeException e) {\n        final String msg = \"There was an issue decrypting protected properties\";\n        throw new IllegalArgumentException(msg, e);\n    } finally {\n        Thread.currentThread().setContextClassLoader(contextClassLoader);\n    }\n}",
        "accept_response": "private static NiFiProperties initializeProperties(final String[] args, final ClassLoader boostrapLoader) {\n    // Try to get key\n    // If key doesn't exist, instantiate without\n    // Load properties\n    // If properties are protected and key missing, throw RuntimeException\n\n    final ClassLoader contextClassLoader = Thread.currentThread().getContextClassLoader();\n    final String key;\n    try {\n        key = loadFormattedKey(args);\n        // The key might be empty or null when it is passed to the loader\n    } catch (IllegalArgumentException e) {\n        final String msg = \"The bootstrap process did not provide a valid key\";\n        throw new IllegalArgumentException(msg, e);\n    }\n    Thread.currentThread().setContextClassLoader(boostrapLoader);\n\n    try {\n        final Class<?> propsLoaderClass = Class.forName(\"org.apache.nifi.properties.NiFiPropertiesLoader\", true, boostrapLoader);\n        final Method withKeyMethod = propsLoaderClass.getMethod(\"withKey\", String.class);\n        final Object loaderInstance = withKeyMethod.invoke(null, key);\n        final Method getMethod = propsLoaderClass.getMethod(\"get\");\n        final NiFiProperties properties = (NiFiProperties) getMethod.invoke(loaderInstance);\n        LOGGER.info(\"Application Properties loaded [{}]\", properties.size());\n        return properties;\n    } catch (InvocationTargetException wrappedException) {\n        final String msg = \"There was an issue decrypting protected properties\";\n        throw new IllegalArgumentException(msg, wrappedException.getCause() == null ? wrappedException : wrappedException.getCause());\n    } catch (final IllegalAccessException | NoSuchMethodException | ClassNotFoundException reex) {\n        final String msg = \"Unable to access properties loader in the expected manner - apparent classpath or build issue\";\n        throw new IllegalArgumentException(msg, reex);\n    } catch (final RuntimeException e) {\n        final String msg = \"There was an issue decrypting protected properties\";\n        throw new IllegalArgumentException(msg, e);\n    } finally {\n        Thread.currentThread().setContextClassLoader(contextClassLoader);\n    }\n}",
        "reject_response": "private static NiFiProperties initializeProperties(final String[] args, final ClassLoader boostrapLoader) {\n    // Try to get key\n    // If key doesn't exist, instantiate without\n    // Load properties\n    // If properties are protected and key missing, throw RuntimeException\n\n    final ClassLoader contextClassLoader = Thread.currentThread().getContextClassLoader();\n    final String key;\n    try {\n        key = loadFormattedKey(args);\n        // The key might be empty or null when it is passed to the loader\n    } catch (IllegalArgumentException e) {\n        final String msg = \"The bootstrap process did not provide a valid key\";\n        throw new IllegalArgumentException(msg, e);\n    }\n    Thread.currentThread().setContextClassLoader(boostrapLoader);\n\n    try {\n        final Class<?> propsLoaderClass = Class.forName(\"org.apache.nifi.properties.NiFiPropertiesLoader\", true, boostrapLoader);\n        final Method withKeyMethod = propsLoaderClass.getMethod(\"withKey\", String.class);\n        final Object loaderInstance = withKeyMethod.invoke(null, key);\n        final Method getMethod = propsLoaderClass.getMethod(\"get\");\n        final NiFiProperties properties = (NiFiProperties) getMethod.invoke(loaderInstance);\n        LOGGER.info(\"Loaded {} properties\", properties.size());\n        return properties;\n    } catch (InvocationTargetException wrappedException) {\n        final String msg = \"There was an issue decrypting protected properties\";\n        throw new IllegalArgumentException(msg, wrappedException.getCause() == null ? wrappedException : wrappedException.getCause());\n    } catch (final IllegalAccessException | NoSuchMethodException | ClassNotFoundException reex) {\n        final String msg = \"Unable to access properties loader in the expected manner - apparent classpath or build issue\";\n        throw new IllegalArgumentException(msg, reex);\n    } catch (final RuntimeException e) {\n        final String msg = \"There was an issue decrypting protected properties\";\n        throw new IllegalArgumentException(msg, e);\n    } finally {\n        Thread.currentThread().setContextClassLoader(contextClassLoader);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2407,
        "instruction": "private void loadOutputs() {\n  for (Map<String, Object> map : outputConfigList) {\n    if (map == null) {\n      continue;\n    }\n    mergeBlocks(globalConfigs, map);\n\n    String value = (String) map.get(\"destination\");\n    if (StringUtils.isEmpty(value)) {\n      LOG.error(\"Output block doesn't have destination element\");\n      continue;\n    }\n    Output output = (Output) AliasUtil.getClassInstance(value, AliasType.OUTPUT);\n    if (output == null) {\n      continue;\n    }\n    output.setDestination(value);\n    output.loadConfig(map);\n\n    // We will only check for is_enabled out here. Down below we will check whether this output is enabled for the input\n    if (output.getBooleanValue(\"is_enabled\", true)) {\n      output.logConfgs(Level.INFO);\n      outputManager.add(output);\n    } else {\n      LOG.info(\"Output is disabled. So ignoring it. \" + output.getShortDescription());\n    }\n  }\n}",
        "accept_response": "private void loadOutputs() {\n  for (Map<String, Object> map : outputConfigList) {\n    if (map == null) {\n      continue;\n    }\n    mergeBlocks(globalConfigs, map);\n\n    String value = (String) map.get(\"destination\");\n    if (StringUtils.isEmpty(value)) {\n      LOG.error(\"Output block doesn't have destination element\");\n      continue;\n    }\n    Output output = (Output) AliasUtil.getClassInstance(value, AliasType.OUTPUT);\n    if (output == null) {\n      LOG.error(\"Output object could not be found\");\n      continue;\n    }\n    output.setDestination(value);\n    output.loadConfig(map);\n\n    // We will only check for is_enabled out here. Down below we will check whether this output is enabled for the input\n    if (output.getBooleanValue(\"is_enabled\", true)) {\n      output.logConfgs(Level.INFO);\n      outputManager.add(output);\n    } else {\n      LOG.info(\"Output is disabled. So ignoring it. \" + output.getShortDescription());\n    }\n  }\n}",
        "reject_response": "private void loadOutputs() {\n  for (Map<String, Object> map : outputConfigList) {\n    if (map == null) {\n      continue;\n    }\n    mergeBlocks(globalConfigs, map);\n\n    String value = (String) map.get(\"destination\");\n    if (StringUtils.isEmpty(value)) {\n      LOG.error(\"Output block doesn't have destination element\");\n      continue;\n    }\n    Output output = (Output) AliasUtil.getClassInstance(value, AliasType.OUTPUT);\n    if (output == null) {\n      logger.error(\"Destination Object is null\");\n      continue;\n    }\n    output.setDestination(value);\n    output.loadConfig(map);\n\n    // We will only check for is_enabled out here. Down below we will check whether this output is enabled for the input\n    if (output.getBooleanValue(\"is_enabled\", true)) {\n      output.logConfgs(Level.INFO);\n      outputManager.add(output);\n    } else {\n      LOG.info(\"Output is disabled. So ignoring it. \" + output.getShortDescription());\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2896,
        "instruction": "public void setSPFResult(SPFSession session) {\n    Iterator<IFutureSPFResultListener> listenerIt = null;\n    synchronized (this) {\n        if (!isReady) {\n            setSPFSession(session);\n            isReady = true;\n            if (waiters > 0) {\n                notifyAll();\n            }\n            if (listeners != null) {\n                listenerIt = listeners.iterator();\n                listeners = null;\n            }\n        }\n    }\n    if (listenerIt != null) {\n        while (listenerIt.hasNext()) {\n            IFutureSPFResultListener listener = listenerIt.next();\n            try {\n                listener.onSPFResult(this);\n            } catch (Throwable e) {\n                // catch exception. See JSPF-95\n            }\n        }\n        listenerIt = null;\n    }\n}",
        "accept_response": "public void setSPFResult(SPFSession session) {\n    Iterator<IFutureSPFResultListener> listenerIt = null;\n    synchronized (this) {\n        if (!isReady) {\n            setSPFSession(session);\n            isReady = true;\n            if (waiters > 0) {\n                notifyAll();\n            }\n            if (listeners != null) {\n                listenerIt = listeners.iterator();\n                listeners = null;\n            }\n        }\n    }\n    if (listenerIt != null) {\n        while (listenerIt.hasNext()) {\n            IFutureSPFResultListener listener = listenerIt.next();\n            try {\n                listener.onSPFResult(this);\n            } catch (Throwable e) {\n                // catch exception. See JSPF-95\n                LOGGER.warn(\"An exception was thrown by the listener {}\", listener, e);\n            }\n        }\n        listenerIt = null;\n    }\n}",
        "reject_response": "public void setSPFResult(SPFSession session) {\n    Iterator<IFutureSPFResultListener> listenerIt = null;\n    synchronized (this) {\n        if (!isReady) {\n            setSPFSession(session);\n            isReady = true;\n            if (waiters > 0) {\n                notifyAll();\n            }\n            if (listeners != null) {\n                listenerIt = listeners.iterator();\n                listeners = null;\n            }\n        }\n    }\n    if (listenerIt != null) {\n        while (listenerIt.hasNext()) {\n            IFutureSPFResultListener listener = listenerIt.next();\n            try {\n                listener.onSPFResult(this);\n            } catch (Throwable e) {\n                // catch exception. See JSPF-95\n                if (log != null) {\n                    log.warn(\"An exception was thrown by the listener \" + listener, e);\n                }\n            }\n        }\n        listenerIt = null;\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2794,
        "instruction": "public boolean isResponseCacheable(final HttpRequest request, final HttpResponse response) {\n    final ProtocolVersion version = request.getVersion() != null ? request.getVersion() : HttpVersion.DEFAULT;\n    if (version.compareToVersion(HttpVersion.HTTP_1_1) > 0) {\n        return false;\n    }\n\n    final String[] uncacheableRequestDirectives = { HeaderConstants.CACHE_CONTROL_NO_STORE };\n    if (hasCacheControlParameterFrom(request,uncacheableRequestDirectives)) {\n        LOG.debug(\"Response is explcitily non-cacheable per cache control directive\");\n        return false;\n    }\n\n    if (request.getRequestUri().contains(\"?\")) {\n        if (neverCache1_0ResponsesWithQueryString && from1_0Origin(response)) {\n            LOG.debug(\"Response is not cacheable as it had a query string\");\n            return false;\n        } else if (!isExplicitlyCacheable(response)) {\n            LOG.debug(\"Response is not cacheable as it is missing explicit caching headers\");\n            return false;\n        }\n    }\n\n    if (expiresHeaderLessOrEqualToDateHeaderAndNoCacheControl(response)) {\n        LOG.debug(\"Expires header less or equal to Date header and no cache control directives\");\n        return false;\n    }\n\n    if (sharedCache) {\n        if (request.countHeaders(HeaderConstants.AUTHORIZATION) > 0\n                && !hasCacheControlParameterFrom(response, AUTH_CACHEABLE_PARAMS)) {\n            LOG.debug(\"Request contains private credentials\");\n            return false;\n        }\n    }\n\n    final String method = request.getMethod();\n    return isResponseCacheable(method, response);\n}",
        "accept_response": "public boolean isResponseCacheable(final HttpRequest request, final HttpResponse response) {\n    final ProtocolVersion version = request.getVersion() != null ? request.getVersion() : HttpVersion.DEFAULT;\n    if (version.compareToVersion(HttpVersion.HTTP_1_1) > 0) {\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Protocol version {} is non-cacheable\", version);\n        }\n        return false;\n    }\n\n    final String[] uncacheableRequestDirectives = { HeaderConstants.CACHE_CONTROL_NO_STORE };\n    if (hasCacheControlParameterFrom(request,uncacheableRequestDirectives)) {\n        LOG.debug(\"Response is explcitily non-cacheable per cache control directive\");\n        return false;\n    }\n\n    if (request.getRequestUri().contains(\"?\")) {\n        if (neverCache1_0ResponsesWithQueryString && from1_0Origin(response)) {\n            LOG.debug(\"Response is not cacheable as it had a query string\");\n            return false;\n        } else if (!isExplicitlyCacheable(response)) {\n            LOG.debug(\"Response is not cacheable as it is missing explicit caching headers\");\n            return false;\n        }\n    }\n\n    if (expiresHeaderLessOrEqualToDateHeaderAndNoCacheControl(response)) {\n        LOG.debug(\"Expires header less or equal to Date header and no cache control directives\");\n        return false;\n    }\n\n    if (sharedCache) {\n        if (request.countHeaders(HeaderConstants.AUTHORIZATION) > 0\n                && !hasCacheControlParameterFrom(response, AUTH_CACHEABLE_PARAMS)) {\n            LOG.debug(\"Request contains private credentials\");\n            return false;\n        }\n    }\n\n    final String method = request.getMethod();\n    return isResponseCacheable(method, response);\n}",
        "reject_response": "public boolean isResponseCacheable(final HttpRequest request, final HttpResponse response) {\n    final ProtocolVersion version = request.getVersion() != null ? request.getVersion() : HttpVersion.DEFAULT;\n    if (version.compareToVersion(HttpVersion.HTTP_1_1) > 0) {\n        if (LOG.isDebugEnabled()) {\n        if (log.isDebugEnabled()) {\n            log.debug(\"Protocol version {} is non-cacheable\", version);\n        }\n        return false;\n    }\n\n    final String[] uncacheableRequestDirectives = { HeaderConstants.CACHE_CONTROL_NO_STORE };\n    if (hasCacheControlParameterFrom(request,uncacheableRequestDirectives)) {\n        LOG.debug(\"Response is explcitily non-cacheable per cache control directive\");\n        return false;\n    }\n\n    if (request.getRequestUri().contains(\"?\")) {\n        if (neverCache1_0ResponsesWithQueryString && from1_0Origin(response)) {\n            LOG.debug(\"Response is not cacheable as it had a query string\");\n            return false;\n        } else if (!isExplicitlyCacheable(response)) {\n            LOG.debug(\"Response is not cacheable as it is missing explicit caching headers\");\n            return false;\n        }\n    }\n\n    if (expiresHeaderLessOrEqualToDateHeaderAndNoCacheControl(response)) {\n        LOG.debug(\"Expires header less or equal to Date header and no cache control directives\");\n        return false;\n    }\n\n    if (sharedCache) {\n        if (request.countHeaders(HeaderConstants.AUTHORIZATION) > 0\n                && !hasCacheControlParameterFrom(response, AUTH_CACHEABLE_PARAMS)) {\n            LOG.debug(\"Request contains private credentials\");\n            return false;\n        }\n    }\n\n    final String method = request.getMethod();\n    return isResponseCacheable(method, response);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2855,
        "instruction": "@Override\npublic synchronized void onBeginRequest(RequestCycle requestCycle) {\n\n    log.debug(\"onBeginRequest in\");\n\n    if (!Session.exists()) {\n        // Track if session was created from an expired one to notify user of the refresh.\n        // If there is no remember me cookie, user will be redirected to sign in and no need to notify.\n        if (userHasSessionWithRememberMe(requestCycle)) {\n            requestCycle.setMetaData(SESSION_LIFECYCLE_PHASE_KEY, SessionLifecyclePhase.EXPIRED);\n            log.debug(\"flagging the RequestCycle as expired (rememberMe feature is active for the current user)\");\n        }\n        log.debug(\"onBeginRequest out - session was not opened (because no Session)\");\n        return;\n    }\n\n    val commonContext = getCommonContext();\n    val interactionService = commonContext.lookupServiceElseFail(InteractionService.class);\n\n    // if there is an interactionContext already (as some authentication mechanisms setup in filters, eg\n    // SpringSecurityFilter), then just use it, taking into account whether impersonation\n\n    // otherwise, take the value cached on AuthenticatedWebSessionForIsis.\n    val interactionContextIfAny = interactionService.currentInteractionContext();\n    val impersonatedUserHolder = commonContext.lookupServiceElseFail(ImpersonatedUserHolder.class);\n    val userMementoImpersonatedIfAny = impersonatedUserHolder.getUserMemento();\n\n    if(interactionContextIfAny.isPresent()) {\n        val interactionContext = interactionContextIfAny.get();\n        if(userMementoImpersonatedIfAny.isPresent()){\n            val useruserMementoImpersonated = userMementoImpersonatedIfAny.get();\n            interactionService.openInteraction(interactionContext.withUser(useruserMementoImpersonated));\n        }\n    } else {\n        if(userMementoImpersonatedIfAny.isPresent()){\n            val userMementoImpersonated = userMementoImpersonatedIfAny.get();\n            interactionService.openInteraction(InteractionContext.ofUserWithSystemDefaults(userMementoImpersonated));\n        } else {\n            // fallback to using that cached by Wicket viewer\n            val interactionContext = AuthenticatedWebSessionForIsis.get().getAuthentication();\n            if (interactionContext == null) {\n                return;\n            }\n            interactionService.openInteraction(interactionContext);\n        }\n    }\n\n    // as a side-effect, sync authentication to Wicket viewer\n    AuthenticatedWebSessionForIsis.get().getAuthentication();\n\n    log.debug(\"onBeginRequest out - session was opened\");\n}",
        "accept_response": "@Override\npublic synchronized void onBeginRequest(RequestCycle requestCycle) {\n\n    log.debug(\"onBeginRequest in\");\n\n    if (!Session.exists()) {\n        // Track if session was created from an expired one to notify user of the refresh.\n        // If there is no remember me cookie, user will be redirected to sign in and no need to notify.\n        if (userHasSessionWithRememberMe(requestCycle)) {\n            requestCycle.setMetaData(SESSION_LIFECYCLE_PHASE_KEY, SessionLifecyclePhase.EXPIRED);\n            log.debug(\"flagging the RequestCycle as expired (rememberMe feature is active for the current user)\");\n        }\n        log.debug(\"onBeginRequest out - session was not opened (because no Session)\");\n        return;\n    }\n\n    val commonContext = getCommonContext();\n    val interactionService = commonContext.lookupServiceElseFail(InteractionService.class);\n\n    // if there is an interactionContext already (as some authentication mechanisms setup in filters, eg\n    // SpringSecurityFilter), then just use it, taking into account whether impersonation\n\n    // otherwise, take the value cached on AuthenticatedWebSessionForIsis.\n    val interactionContextIfAny = interactionService.currentInteractionContext();\n    val impersonatedUserHolder = commonContext.lookupServiceElseFail(ImpersonatedUserHolder.class);\n    val userMementoImpersonatedIfAny = impersonatedUserHolder.getUserMemento();\n\n    if(interactionContextIfAny.isPresent()) {\n        val interactionContext = interactionContextIfAny.get();\n        if(userMementoImpersonatedIfAny.isPresent()){\n            val useruserMementoImpersonated = userMementoImpersonatedIfAny.get();\n            interactionService.openInteraction(interactionContext.withUser(useruserMementoImpersonated));\n        }\n    } else {\n        if(userMementoImpersonatedIfAny.isPresent()){\n            val userMementoImpersonated = userMementoImpersonatedIfAny.get();\n            interactionService.openInteraction(InteractionContext.ofUserWithSystemDefaults(userMementoImpersonated));\n        } else {\n            // fallback to using that cached by Wicket viewer\n            val interactionContext = AuthenticatedWebSessionForIsis.get().getAuthentication();\n            if (interactionContext == null) {\n                log.warn(\"onBeginRequest out - session was not opened (because no authentication)\");\n                return;\n            }\n            interactionService.openInteraction(interactionContext);\n        }\n    }\n\n    // as a side-effect, sync authentication to Wicket viewer\n    AuthenticatedWebSessionForIsis.get().getAuthentication();\n\n    log.debug(\"onBeginRequest out - session was opened\");\n}",
        "reject_response": "@Override\npublic synchronized void onBeginRequest(RequestCycle requestCycle) {\n\n    log.debug(\"onBeginRequest in\");\n\n    if (!Session.exists()) {\n        // Track if session was created from an expired one to notify user of the refresh.\n        // If there is no remember me cookie, user will be redirected to sign in and no need to notify.\n        if (userHasSessionWithRememberMe(requestCycle)) {\n            requestCycle.setMetaData(SESSION_LIFECYCLE_PHASE_KEY, SessionLifecyclePhase.EXPIRED);\n            log.debug(\"flagging the RequestCycle as expired (rememberMe feature is active for the current user)\");\n        }\n        log.debug(\"onBeginRequest out - session was not opened (because no Session)\");\n        return;\n    }\n\n    val commonContext = getCommonContext();\n    val interactionService = commonContext.lookupServiceElseFail(InteractionService.class);\n\n    // if there is an interactionContext already (as some authentication mechanisms setup in filters, eg\n    // SpringSecurityFilter), then just use it, taking into account whether impersonation\n\n    // otherwise, take the value cached on AuthenticatedWebSessionForIsis.\n    val interactionContextIfAny = interactionService.currentInteractionContext();\n    val impersonatedUserHolder = commonContext.lookupServiceElseFail(ImpersonatedUserHolder.class);\n    val userMementoImpersonatedIfAny = impersonatedUserHolder.getUserMemento();\n\n    if(interactionContextIfAny.isPresent()) {\n        val interactionContext = interactionContextIfAny.get();\n        if(userMementoImpersonatedIfAny.isPresent()){\n            val useruserMementoImpersonated = userMementoImpersonatedIfAny.get();\n            interactionService.openInteraction(interactionContext.withUser(useruserMementoImpersonated));\n        }\n    } else {\n        if(userMementoImpersonatedIfAny.isPresent()){\n            val userMementoImpersonated = userMementoImpersonatedIfAny.get();\n            interactionService.openInteraction(InteractionContext.ofUserWithSystemDefaults(userMementoImpersonated));\n        } else {\n            // fallback to using that cached by Wicket viewer\n            val interactionContext = AuthenticatedWebSessionForIsis.get().getAuthentication();\n            if (interactionContext == null) {\n                log.debug(\"onBeginRequest out - session was not opened (because no authentication)\");\n                return;\n            }\n            interactionService.openInteraction(interactionContext);\n        }\n    }\n\n    // as a side-effect, sync authentication to Wicket viewer\n    AuthenticatedWebSessionForIsis.get().getAuthentication();\n\n    log.debug(\"onBeginRequest out - session was opened\");\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2569,
        "instruction": "private Collection<AuditData> retrieveEntries(final String useType, final SQLBuilder extraCriteria, final String groupAndOrderBySQL, final boolean includeJson) {\n\n    if ((!useType.equals(\"audit\") && !useType.equals(\"makerchecker\"))) { throw new PlatformDataIntegrityException(\n            \"error.msg.invalid.auditSearchTemplate.useType\", \"Invalid Audit Search Template UseType: \" + useType); }\n\n    final AppUser currentUser = this.context.authenticatedUser();\n    final String hierarchy = currentUser.getOffice().getHierarchy();\n\n    final AuditMapper rm = new AuditMapper();\n    String sql = \"select \" + rm.schema(includeJson, hierarchy);\n\n    Boolean isLimitedChecker = false;\n    if (useType.equals(\"makerchecker\")) {\n        if (currentUser.hasNotPermissionForAnyOf(\"ALL_FUNCTIONS\", \"CHECKER_SUPER_USER\")) {\n            isLimitedChecker = true;\n        }\n    }\n\n    if (isLimitedChecker) {\n        sql += \" join m_permission p on REPLACE(p.action_name, '_CHECKER', '')  = aud.action_name and p.entity_name = aud.entity_name and p.code like '%\\\\_CHECKER'\"\n                + \" join m_role_permission rp on rp.permission_id = p.id\" + \" join m_role r on r.id = rp.role_id \"\n                + \" join m_appuser_role ur on ur.role_id = r.id and ur.appuser_id = \" + currentUser.getId();\n    }\n    sql += extraCriteria.getSQLTemplate();\n    sql += groupAndOrderBySQL;\n\n    return this.jdbcTemplate.query(sql, rm, extraCriteria.getArguments());\n}",
        "accept_response": "private Collection<AuditData> retrieveEntries(final String useType, final SQLBuilder extraCriteria, final String groupAndOrderBySQL, final boolean includeJson) {\n\n    if ((!useType.equals(\"audit\") && !useType.equals(\"makerchecker\"))) { throw new PlatformDataIntegrityException(\n            \"error.msg.invalid.auditSearchTemplate.useType\", \"Invalid Audit Search Template UseType: \" + useType); }\n\n    final AppUser currentUser = this.context.authenticatedUser();\n    final String hierarchy = currentUser.getOffice().getHierarchy();\n\n    final AuditMapper rm = new AuditMapper();\n    String sql = \"select \" + rm.schema(includeJson, hierarchy);\n\n    Boolean isLimitedChecker = false;\n    if (useType.equals(\"makerchecker\")) {\n        if (currentUser.hasNotPermissionForAnyOf(\"ALL_FUNCTIONS\", \"CHECKER_SUPER_USER\")) {\n            isLimitedChecker = true;\n        }\n    }\n\n    if (isLimitedChecker) {\n        sql += \" join m_permission p on REPLACE(p.action_name, '_CHECKER', '')  = aud.action_name and p.entity_name = aud.entity_name and p.code like '%\\\\_CHECKER'\"\n                + \" join m_role_permission rp on rp.permission_id = p.id\" + \" join m_role r on r.id = rp.role_id \"\n                + \" join m_appuser_role ur on ur.role_id = r.id and ur.appuser_id = \" + currentUser.getId();\n    }\n    sql += extraCriteria.getSQLTemplate();\n    sql += groupAndOrderBySQL;\n    logger.info(\"sql: {}\", sql);\n\n    return this.jdbcTemplate.query(sql, rm, extraCriteria.getArguments());\n}",
        "reject_response": "private Collection<AuditData> retrieveEntries(final String useType, final SQLBuilder extraCriteria, final String groupAndOrderBySQL, final boolean includeJson) {\n\n    if ((!useType.equals(\"audit\") && !useType.equals(\"makerchecker\"))) { throw new PlatformDataIntegrityException(\n            \"error.msg.invalid.auditSearchTemplate.useType\", \"Invalid Audit Search Template UseType: \" + useType); }\n\n    final AppUser currentUser = this.context.authenticatedUser();\n    final String hierarchy = currentUser.getOffice().getHierarchy();\n\n    final AuditMapper rm = new AuditMapper();\n    String sql = \"select \" + rm.schema(includeJson, hierarchy);\n\n    Boolean isLimitedChecker = false;\n    if (useType.equals(\"makerchecker\")) {\n        if (currentUser.hasNotPermissionForAnyOf(\"ALL_FUNCTIONS\", \"CHECKER_SUPER_USER\")) {\n            isLimitedChecker = true;\n        }\n    }\n\n    if (isLimitedChecker) {\n        sql += \" join m_permission p on REPLACE(p.action_name, '_CHECKER', '')  = aud.action_name and p.entity_name = aud.entity_name and p.code like '%\\\\_CHECKER'\"\n                + \" join m_role_permission rp on rp.permission_id = p.id\" + \" join m_role r on r.id = rp.role_id \"\n                + \" join m_appuser_role ur on ur.role_id = r.id and ur.appuser_id = \" + currentUser.getId();\n    }\n    sql += extraCriteria.getSQLTemplate();\n    sql += groupAndOrderBySQL;\n    logger.info(\"sql: \" + sql);\n\n    return this.jdbcTemplate.query(sql, rm, extraCriteria.getArguments());\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2543,
        "instruction": "@Override\npublic void close() {\n    if (!closed.compareAndSet(false, true)) {\n        return;\n    }\n    // shutdown the driver\n    Utils.close(driver);\n    // close the write limiter\n    this.writeLimiter.close();\n    // Shutdown the schedulers\n    SchedulerUtils.shutdownScheduler(scheduler, conf.getSchedulerShutdownTimeoutMs(),\n            TimeUnit.MILLISECONDS);\n}",
        "accept_response": "@Override\npublic void close() {\n    if (!closed.compareAndSet(false, true)) {\n        return;\n    }\n    // shutdown the driver\n    Utils.close(driver);\n    // close the write limiter\n    this.writeLimiter.close();\n    // Shutdown the schedulers\n    SchedulerUtils.shutdownScheduler(scheduler, conf.getSchedulerShutdownTimeoutMs(),\n            TimeUnit.MILLISECONDS);\n    LOG.info(\"Executor Service Stopped.\");\n}",
        "reject_response": "@Override\npublic void close() {\n    if (!closed.compareAndSet(false, true)) {\n        return;\n    }\n    // shutdown the driver\n    Utils.close(driver);\n    // close the write limiter\n    this.writeLimiter.close();\n    // Shutdown the schedulers\n    SchedulerUtils.shutdownScheduler(scheduler, conf.getSchedulerShutdownTimeoutMs(),\n            TimeUnit.MILLISECONDS);\n        LOG.info(\"ReadAhead Executor Service Stopped.\");\n    }\n\n    writerBKC.close();\n    readerBKC.close();\n    sharedWriterZKCForDL.close();\n    sharedReaderZKCForDL.close();\n\n    // Close shared zookeeper clients for bk\n    if (null != writerZKC) {\n        writerZKC.close();\n    }\n    if (null != readerZKC) {\n        readerZKC.close();\n    }\n    channelFactory.releaseExternalResources();\n    LOG.info(\"Release external resources used by channel factory.\");\n    requestTimer.stop();\n    LOG.info(\"Stopped request timer\");\n    SchedulerUtils.shutdownScheduler(lockStateExecutor, 5000, TimeUnit.MILLISECONDS);\n    LOG.info(\"Stopped lock state executor\");\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2959,
        "instruction": "@Override\npublic Map<String, Object> run(Map<String, Object> args, String crawlId) throws Exception {\n\n  Map<String, Object> results = new HashMap<String, Object>();\n\n  Path segment;\n  if(args.containsKey(Nutch.ARG_SEGMENT)) {\n    Object seg = args.get(Nutch.ARG_SEGMENT);\n    if(seg instanceof Path) {\n      segment = (Path) seg;\n    }\n    else {\n      segment = new Path(seg.toString());\n    }\n  }\n  else {\n    String segment_dir = crawlId+\"/segments\";\n    File segmentsDir = new File(segment_dir);\n    File[] segmentsList = segmentsDir.listFiles();\n    Arrays.sort(segmentsList, new Comparator<File>(){\n      @Override\n      public int compare(File f1, File f2) {\n        if(f1.lastModified()>f2.lastModified())\n          return -1;\n        else\n          return 0;\n      }\n    });\n    segment = new Path(segmentsList[0].getPath());\n  }\n\n\n  int threads = getConf().getInt(\"fetcher.threads.fetch\", 10);\n\n  // parse command line\n  if (args.containsKey(\"threads\")) { // found -threads option\n    threads = Integer.parseInt((String)args.get(\"threads\"));\n  }\n  getConf().setInt(\"fetcher.threads.fetch\", threads);\n\n  try {\n    fetch(segment, threads);\n    results.put(Nutch.VAL_RESULT, Integer.toString(0));\n    return results;\n  } catch (Exception e) {\n    results.put(Nutch.VAL_RESULT, Integer.toString(-1));\n    return results;\n  }\n}",
        "accept_response": "@Override\npublic Map<String, Object> run(Map<String, Object> args, String crawlId) throws Exception {\n\n  Map<String, Object> results = new HashMap<String, Object>();\n\n  Path segment;\n  if(args.containsKey(Nutch.ARG_SEGMENT)) {\n    Object seg = args.get(Nutch.ARG_SEGMENT);\n    if(seg instanceof Path) {\n      segment = (Path) seg;\n    }\n    else {\n      segment = new Path(seg.toString());\n    }\n  }\n  else {\n    String segment_dir = crawlId+\"/segments\";\n    File segmentsDir = new File(segment_dir);\n    File[] segmentsList = segmentsDir.listFiles();\n    Arrays.sort(segmentsList, new Comparator<File>(){\n      @Override\n      public int compare(File f1, File f2) {\n        if(f1.lastModified()>f2.lastModified())\n          return -1;\n        else\n          return 0;\n      }\n    });\n    segment = new Path(segmentsList[0].getPath());\n  }\n\n\n  int threads = getConf().getInt(\"fetcher.threads.fetch\", 10);\n\n  // parse command line\n  if (args.containsKey(\"threads\")) { // found -threads option\n    threads = Integer.parseInt((String)args.get(\"threads\"));\n  }\n  getConf().setInt(\"fetcher.threads.fetch\", threads);\n\n  try {\n    fetch(segment, threads);\n    results.put(Nutch.VAL_RESULT, Integer.toString(0));\n    return results;\n  } catch (Exception e) {\n    LOG.error(\"Fetcher: {}\", StringUtils.stringifyException(e));\n    results.put(Nutch.VAL_RESULT, Integer.toString(-1));\n    return results;\n  }\n}",
        "reject_response": "@Override\npublic Map<String, Object> run(Map<String, Object> args, String crawlId) throws Exception {\n\n  Map<String, Object> results = new HashMap<String, Object>();\n\n  Path segment;\n  if(args.containsKey(Nutch.ARG_SEGMENT)) {\n    Object seg = args.get(Nutch.ARG_SEGMENT);\n    if(seg instanceof Path) {\n      segment = (Path) seg;\n    }\n    else {\n      segment = new Path(seg.toString());\n    }\n  }\n  else {\n    String segment_dir = crawlId+\"/segments\";\n    File segmentsDir = new File(segment_dir);\n    File[] segmentsList = segmentsDir.listFiles();\n    Arrays.sort(segmentsList, new Comparator<File>(){\n      @Override\n      public int compare(File f1, File f2) {\n        if(f1.lastModified()>f2.lastModified())\n          return -1;\n        else\n          return 0;\n      }\n    });\n    segment = new Path(segmentsList[0].getPath());\n  }\n\n\n  int threads = getConf().getInt(\"fetcher.threads.fetch\", 10);\n\n  // parse command line\n  if (args.containsKey(\"threads\")) { // found -threads option\n    threads = Integer.parseInt((String)args.get(\"threads\"));\n  }\n  getConf().setInt(\"fetcher.threads.fetch\", threads);\n\n  try {\n    fetch(segment, threads);\n    results.put(Nutch.VAL_RESULT, Integer.toString(0));\n    return results;\n  } catch (Exception e) {\n    LOG.error(\"Fetcher: \" + StringUtils.stringifyException(e));\n    results.put(Nutch.VAL_RESULT, Integer.toString(-1));\n    return results;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2639,
        "instruction": "private Integer getIntegerAttribute(Object object, String name) {\n  if (object == null) {\n    return Integer.valueOf(0);\n  }\n\n  try {\n    if (!(object.getClass().equals(Integer.class))) {\n      return Integer.valueOf(0);\n    } else {\n      return (Integer) object;\n    }\n  } catch (Exception e) {\n    logger.info(\"Exception occurred: \", e);\n    return Integer.valueOf(0);\n  }\n}",
        "accept_response": "private Integer getIntegerAttribute(Object object, String name) {\n  if (object == null) {\n    return Integer.valueOf(0);\n  }\n\n  try {\n    if (!(object.getClass().equals(Integer.class))) {\n      logger.info(\n          \"************************Unexpected type for attribute: {}; Expected type: {}; Received type: {}************************\",\n          name, Integer.class.getName(), object.getClass().getName());\n      return Integer.valueOf(0);\n    } else {\n      return (Integer) object;\n    }\n  } catch (Exception e) {\n    logger.info(\"Exception occurred: \", e);\n    return Integer.valueOf(0);\n  }\n}",
        "reject_response": "private Integer getIntegerAttribute(Object object, String name) {\n  if (object == null) {\n    return Integer.valueOf(0);\n  }\n\n  try {\n    if (!(object.getClass().equals(Integer.class))) {\n      if (LOGGER.infoEnabled()) {\n        LOGGER.info(\"************************Unexpected type for attribute: \" + name\n            + \" Expected type: \" + Integer.class.getName() + \" Received type: \"\n            + object.getClass().getName() + \"************************\");\n      }\n      return Integer.valueOf(0);\n    } else {\n      return (Integer) object;\n    }\n  } catch (Exception e) {\n    logger.info(\"Exception occurred: \", e);\n    return Integer.valueOf(0);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2405,
        "instruction": "private void loadConfigsUsingFile(File configFile) throws Exception {\n  try {\n    String configData = FileUtils.readFileToString(configFile);\n    loadConfigs(configData);\n  } catch (Exception t) {\n    throw t;\n  }\n}",
        "accept_response": "private void loadConfigsUsingFile(File configFile) throws Exception {\n  try {\n    String configData = FileUtils.readFileToString(configFile);\n    loadConfigs(configData);\n  } catch (Exception t) {\n    LOG.error(\"Error opening config file. configFilePath=\" + configFile.getAbsolutePath());\n    throw t;\n  }\n}",
        "reject_response": "private void loadConfigsUsingFile(File configFile) throws Exception {\n  try {\n    String configData = FileUtils.readFileToString(configFile);\n    loadConfigs(configData);\n  } catch (Exception t) {\n    logger.error(\"Error opening config file. configFilePath=\"\n      + configFile.getAbsolutePath());\n    throw t;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2632,
        "instruction": "private void refresh(Member m) {\n\n  Random r = new Random(System.currentTimeMillis());\n\n  m.uptime = System.currentTimeMillis();\n  m.queueBacklog = \"\" + Math.abs(r.nextInt(500));\n  m.currentHeapSize = Math.abs(r.nextInt(Math.abs((int) m.maxHeapSize)));\n  m.OffHeapFreeSize = Math.abs(r.nextInt(Math.abs((int) m.maxHeapSize)));\n  m.OffHeapUsedSize = Math.abs(r.nextInt(Math.abs((int) m.maxHeapSize)));\n  m.totalDiskUsage = Math.abs(r.nextInt(100));\n\n  double cpuUsage = r.nextDouble() * 100;\n  m.cpuUsageSamples.add(cpuUsage);\n  m.cpuUsage = cpuUsage;\n  m.hostCpuUsage = r.nextDouble() * 200;\n\n  m.heapUsageSamples.add(m.currentHeapSize);\n  m.loadAverage = (double) Math.abs(r.nextInt(100));\n  m.numThreads = Math.abs(r.nextInt(100));\n  m.garbageCollectionCount = (long) Math.abs(r.nextInt(100));\n  m.garbageCollectionSamples.add(m.garbageCollectionCount);\n\n  m.totalFileDescriptorOpen = (long) Math.abs(r.nextInt(100));\n\n  m.throughputWrites = Math.abs(r.nextInt(10));\n  m.throughputWritesTrend.add(m.throughputWrites);\n\n  m.throughputReads = Math.abs(r.nextInt(10));\n  m.throughputReadsTrend.add(m.throughputReads);\n\n  m.getsRate = Math.abs(r.nextInt(5000));\n  m.getsPerSecond.add(m.getsRate);\n\n  m.putsRate = Math.abs(r.nextInt(5000));\n  m.putsPerSecond.add(m.putsRate);\n\n  if (r.nextBoolean()) {\n    // Generate alerts\n    if (r.nextBoolean()) {\n      if (r.nextInt(10) > 5) {\n        alertsList.add(createAlert(Alert.SEVERE, m.name, alertsList.size()));\n        if (alertsList.size() > ALERTS_MAX_SIZE) {\n          alertsList.remove(0);\n        }\n      }\n    }\n\n    if (r.nextBoolean()) {\n      if (r.nextInt(10) > 5) {\n        alertsList.add(createAlert(Alert.ERROR, m.name, alertsList.size()));\n        if (alertsList.size() > ALERTS_MAX_SIZE) {\n          alertsList.remove(0);\n        }\n      }\n    }\n\n    if (r.nextBoolean()) {\n      if (r.nextInt(10) > 5) {\n        alertsList.add(createAlert(Alert.WARNING, m.name, alertsList.size()));\n        if (alertsList.size() > ALERTS_MAX_SIZE) {\n          alertsList.remove(0);\n        }\n      }\n    }\n\n    if (r.nextBoolean()) {\n      if (r.nextInt(10) > 5) {\n        alertsList.add(createAlert(Alert.INFO, m.name, alertsList.size()));\n        if (alertsList.size() > ALERTS_MAX_SIZE) {\n          alertsList.remove(0);\n        }\n      }\n    }\n  }\n}",
        "accept_response": "private void refresh(Member m) {\n  logger.info(\"{} : {}\", resourceBundle.getString(\"LOG_MSG_REFRESHING_MEMBER_DATA\"), m.name);\n\n  Random r = new Random(System.currentTimeMillis());\n\n  m.uptime = System.currentTimeMillis();\n  m.queueBacklog = \"\" + Math.abs(r.nextInt(500));\n  m.currentHeapSize = Math.abs(r.nextInt(Math.abs((int) m.maxHeapSize)));\n  m.OffHeapFreeSize = Math.abs(r.nextInt(Math.abs((int) m.maxHeapSize)));\n  m.OffHeapUsedSize = Math.abs(r.nextInt(Math.abs((int) m.maxHeapSize)));\n  m.totalDiskUsage = Math.abs(r.nextInt(100));\n\n  double cpuUsage = r.nextDouble() * 100;\n  m.cpuUsageSamples.add(cpuUsage);\n  m.cpuUsage = cpuUsage;\n  m.hostCpuUsage = r.nextDouble() * 200;\n\n  m.heapUsageSamples.add(m.currentHeapSize);\n  m.loadAverage = (double) Math.abs(r.nextInt(100));\n  m.numThreads = Math.abs(r.nextInt(100));\n  m.garbageCollectionCount = (long) Math.abs(r.nextInt(100));\n  m.garbageCollectionSamples.add(m.garbageCollectionCount);\n\n  m.totalFileDescriptorOpen = (long) Math.abs(r.nextInt(100));\n\n  m.throughputWrites = Math.abs(r.nextInt(10));\n  m.throughputWritesTrend.add(m.throughputWrites);\n\n  m.throughputReads = Math.abs(r.nextInt(10));\n  m.throughputReadsTrend.add(m.throughputReads);\n\n  m.getsRate = Math.abs(r.nextInt(5000));\n  m.getsPerSecond.add(m.getsRate);\n\n  m.putsRate = Math.abs(r.nextInt(5000));\n  m.putsPerSecond.add(m.putsRate);\n\n  if (r.nextBoolean()) {\n    // Generate alerts\n    if (r.nextBoolean()) {\n      if (r.nextInt(10) > 5) {\n        alertsList.add(createAlert(Alert.SEVERE, m.name, alertsList.size()));\n        if (alertsList.size() > ALERTS_MAX_SIZE) {\n          alertsList.remove(0);\n        }\n      }\n    }\n\n    if (r.nextBoolean()) {\n      if (r.nextInt(10) > 5) {\n        alertsList.add(createAlert(Alert.ERROR, m.name, alertsList.size()));\n        if (alertsList.size() > ALERTS_MAX_SIZE) {\n          alertsList.remove(0);\n        }\n      }\n    }\n\n    if (r.nextBoolean()) {\n      if (r.nextInt(10) > 5) {\n        alertsList.add(createAlert(Alert.WARNING, m.name, alertsList.size()));\n        if (alertsList.size() > ALERTS_MAX_SIZE) {\n          alertsList.remove(0);\n        }\n      }\n    }\n\n    if (r.nextBoolean()) {\n      if (r.nextInt(10) > 5) {\n        alertsList.add(createAlert(Alert.INFO, m.name, alertsList.size()));\n        if (alertsList.size() > ALERTS_MAX_SIZE) {\n          alertsList.remove(0);\n        }\n      }\n    }\n  }\n}",
        "reject_response": "private void refresh(Member m) {\n  if (LOGGER.infoEnabled()) {\n    LOGGER.info(resourceBundle.getString(\"LOG_MSG_REFRESHING_MEMBER_DATA\") + \" : \" + m.name);\n  }\n\n  Random r = new Random(System.currentTimeMillis());\n\n  m.uptime = System.currentTimeMillis();\n  m.queueBacklog = \"\" + Math.abs(r.nextInt(500));\n  m.currentHeapSize = Math.abs(r.nextInt(Math.abs((int) m.maxHeapSize)));\n  m.OffHeapFreeSize = Math.abs(r.nextInt(Math.abs((int) m.maxHeapSize)));\n  m.OffHeapUsedSize = Math.abs(r.nextInt(Math.abs((int) m.maxHeapSize)));\n  m.totalDiskUsage = Math.abs(r.nextInt(100));\n\n  double cpuUsage = r.nextDouble() * 100;\n  m.cpuUsageSamples.add(cpuUsage);\n  m.cpuUsage = cpuUsage;\n  m.hostCpuUsage = r.nextDouble() * 200;\n\n  m.heapUsageSamples.add(m.currentHeapSize);\n  m.loadAverage = (double) Math.abs(r.nextInt(100));\n  m.numThreads = Math.abs(r.nextInt(100));\n  m.garbageCollectionCount = (long) Math.abs(r.nextInt(100));\n  m.garbageCollectionSamples.add(m.garbageCollectionCount);\n\n  m.totalFileDescriptorOpen = (long) Math.abs(r.nextInt(100));\n\n  m.throughputWrites = Math.abs(r.nextInt(10));\n  m.throughputWritesTrend.add(m.throughputWrites);\n\n  m.throughputReads = Math.abs(r.nextInt(10));\n  m.throughputReadsTrend.add(m.throughputReads);\n\n  m.getsRate = Math.abs(r.nextInt(5000));\n  m.getsPerSecond.add(m.getsRate);\n\n  m.putsRate = Math.abs(r.nextInt(5000));\n  m.putsPerSecond.add(m.putsRate);\n\n  if (r.nextBoolean()) {\n    // Generate alerts\n    if (r.nextBoolean()) {\n      if (r.nextInt(10) > 5) {\n        alertsList.add(createAlert(Alert.SEVERE, m.name, alertsList.size()));\n        if (alertsList.size() > ALERTS_MAX_SIZE) {\n          alertsList.remove(0);\n        }\n      }\n    }\n\n    if (r.nextBoolean()) {\n      if (r.nextInt(10) > 5) {\n        alertsList.add(createAlert(Alert.ERROR, m.name, alertsList.size()));\n        if (alertsList.size() > ALERTS_MAX_SIZE) {\n          alertsList.remove(0);\n        }\n      }\n    }\n\n    if (r.nextBoolean()) {\n      if (r.nextInt(10) > 5) {\n        alertsList.add(createAlert(Alert.WARNING, m.name, alertsList.size()));\n        if (alertsList.size() > ALERTS_MAX_SIZE) {\n          alertsList.remove(0);\n        }\n      }\n    }\n\n    if (r.nextBoolean()) {\n      if (r.nextInt(10) > 5) {\n        alertsList.add(createAlert(Alert.INFO, m.name, alertsList.size()));\n        if (alertsList.size() > ALERTS_MAX_SIZE) {\n          alertsList.remove(0);\n        }\n      }\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2449,
        "instruction": "@Override\npublic Packet sendBlocking(final Packet packet,\n                           final int reconnectID,\n                           byte expectedPacket) throws ActiveMQException {\n   String interceptionResult = invokeInterceptors(packet, interceptors, connection);\n\n   if (interceptionResult != null) {\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" interceptionResult=\" + interceptionResult);\n      }\n      // if we don't throw an exception here the client might not unblock\n      throw ActiveMQClientMessageBundle.BUNDLE.interceptorRejectedPacket(interceptionResult);\n   }\n\n   if (closed) {\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" closed.\");\n      }\n      throw ActiveMQClientMessageBundle.BUNDLE.connectionDestroyed();\n   }\n\n   if (connection.getBlockingCallTimeout() == -1) {\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" Cannot do a blocking call timeout on a server side connection\");\n      }\n      throw new IllegalStateException(\"Cannot do a blocking call timeout on a server side connection\");\n   }\n\n   // Synchronized since can't be called concurrently by more than one thread and this can occur\n   // E.g. blocking acknowledge() from inside a message handler at some time as other operation on main thread\n   synchronized (sendBlockingLock) {\n      packet.setChannelID(id);\n\n      final ActiveMQBuffer buffer = packet.encode(connection);\n\n      lock.lock();\n\n      try {\n         if (failingOver) {\n            waitForFailOver(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" timed-out waiting for fail-over condition on blocking send\");\n         }\n\n         response = null;\n\n         if (resendCache != null && packet.isRequiresConfirmations()) {\n            addResendPacket(packet);\n         }\n\n         checkReconnectID(reconnectID);\n\n         connection.getTransportConnection().write(buffer, false, false);\n\n         long toWait = connection.getBlockingCallTimeout();\n\n         long start = System.currentTimeMillis();\n\n         while (!closed && (response == null || (response.getType() != PacketImpl.EXCEPTION && response.getType() != expectedPacket)) && toWait > 0) {\n            try {\n               sendCondition.await(toWait, TimeUnit.MILLISECONDS);\n            } catch (InterruptedException e) {\n               throw new ActiveMQInterruptedException(e);\n            }\n\n            if (response != null && response.getType() != PacketImpl.EXCEPTION && response.getType() != expectedPacket) {\n               ActiveMQClientLogger.LOGGER.packetOutOfOrder(response, new Exception(\"trace\"));\n            }\n\n            if (closed) {\n               break;\n            }\n\n            final long now = System.currentTimeMillis();\n\n            toWait -= now - start;\n\n            start = now;\n         }\n\n         if (closed && toWait > 0 && response == null) {\n            Throwable cause = ActiveMQClientMessageBundle.BUNDLE.connectionDestroyed();\n            throw ActiveMQClientMessageBundle.BUNDLE.unblockingACall(cause);\n         }\n\n         if (response == null) {\n            throw ActiveMQClientMessageBundle.BUNDLE.timedOutSendingPacket(connection.getBlockingCallTimeout(), packet.getType());\n         }\n\n         if (response.getType() == PacketImpl.EXCEPTION) {\n            final ActiveMQExceptionMessage mem = (ActiveMQExceptionMessage) response;\n\n            ActiveMQException e = mem.getException();\n\n            e.fillInStackTrace();\n\n            throw e;\n         }\n      } finally {\n         lock.unlock();\n      }\n\n      return response;\n   }\n}",
        "accept_response": "@Override\npublic Packet sendBlocking(final Packet packet,\n                           final int reconnectID,\n                           byte expectedPacket) throws ActiveMQException {\n   String interceptionResult = invokeInterceptors(packet, interceptors, connection);\n\n   if (interceptionResult != null) {\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" interceptionResult=\" + interceptionResult);\n      }\n      // if we don't throw an exception here the client might not unblock\n      throw ActiveMQClientMessageBundle.BUNDLE.interceptorRejectedPacket(interceptionResult);\n   }\n\n   if (closed) {\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" closed.\");\n      }\n      throw ActiveMQClientMessageBundle.BUNDLE.connectionDestroyed();\n   }\n\n   if (connection.getBlockingCallTimeout() == -1) {\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" Cannot do a blocking call timeout on a server side connection\");\n      }\n      throw new IllegalStateException(\"Cannot do a blocking call timeout on a server side connection\");\n   }\n\n   // Synchronized since can't be called concurrently by more than one thread and this can occur\n   // E.g. blocking acknowledge() from inside a message handler at some time as other operation on main thread\n   synchronized (sendBlockingLock) {\n      packet.setChannelID(id);\n\n      final ActiveMQBuffer buffer = packet.encode(connection);\n\n      lock.lock();\n\n      try {\n         if (failingOver) {\n            waitForFailOver(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" timed-out waiting for fail-over condition on blocking send\");\n         }\n\n         response = null;\n\n         if (resendCache != null && packet.isRequiresConfirmations()) {\n            addResendPacket(packet);\n         }\n\n         checkReconnectID(reconnectID);\n\n         if (logger.isTraceEnabled()) {\n            logger.trace(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" Sending blocking \" + packet);\n         }\n\n         connection.getTransportConnection().write(buffer, false, false);\n\n         long toWait = connection.getBlockingCallTimeout();\n\n         long start = System.currentTimeMillis();\n\n         while (!closed && (response == null || (response.getType() != PacketImpl.EXCEPTION && response.getType() != expectedPacket)) && toWait > 0) {\n            try {\n               sendCondition.await(toWait, TimeUnit.MILLISECONDS);\n            } catch (InterruptedException e) {\n               throw new ActiveMQInterruptedException(e);\n            }\n\n            if (response != null && response.getType() != PacketImpl.EXCEPTION && response.getType() != expectedPacket) {\n               ActiveMQClientLogger.LOGGER.packetOutOfOrder(response, new Exception(\"trace\"));\n            }\n\n            if (closed) {\n               break;\n            }\n\n            final long now = System.currentTimeMillis();\n\n            toWait -= now - start;\n\n            start = now;\n         }\n\n         if (closed && toWait > 0 && response == null) {\n            Throwable cause = ActiveMQClientMessageBundle.BUNDLE.connectionDestroyed();\n            throw ActiveMQClientMessageBundle.BUNDLE.unblockingACall(cause);\n         }\n\n         if (response == null) {\n            throw ActiveMQClientMessageBundle.BUNDLE.timedOutSendingPacket(connection.getBlockingCallTimeout(), packet.getType());\n         }\n\n         if (response.getType() == PacketImpl.EXCEPTION) {\n            final ActiveMQExceptionMessage mem = (ActiveMQExceptionMessage) response;\n\n            ActiveMQException e = mem.getException();\n\n            e.fillInStackTrace();\n\n            throw e;\n         }\n      } finally {\n         lock.unlock();\n      }\n\n      return response;\n   }\n}",
        "reject_response": "@Override\npublic Packet sendBlocking(final Packet packet,\n                           final int reconnectID,\n                           byte expectedPacket) throws ActiveMQException {\n   String interceptionResult = invokeInterceptors(packet, interceptors, connection);\n\n   if (interceptionResult != null) {\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" interceptionResult=\" + interceptionResult);\n      }\n      // if we don't throw an exception here the client might not unblock\n      throw ActiveMQClientMessageBundle.BUNDLE.interceptorRejectedPacket(interceptionResult);\n   }\n\n   if (closed) {\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" closed.\");\n      }\n      throw ActiveMQClientMessageBundle.BUNDLE.connectionDestroyed();\n   }\n\n   if (connection.getBlockingCallTimeout() == -1) {\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" Cannot do a blocking call timeout on a server side connection\");\n      }\n      throw new IllegalStateException(\"Cannot do a blocking call timeout on a server side connection\");\n   }\n\n   // Synchronized since can't be called concurrently by more than one thread and this can occur\n   // E.g. blocking acknowledge() from inside a message handler at some time as other operation on main thread\n   synchronized (sendBlockingLock) {\n      packet.setChannelID(id);\n\n      final ActiveMQBuffer buffer = packet.encode(connection);\n\n      lock.lock();\n\n      try {\n         if (failingOver) {\n            waitForFailOver(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" timed-out waiting for fail-over condition on blocking send\");\n         }\n\n         response = null;\n\n         if (resendCache != null && packet.isRequiresConfirmations()) {\n            addResendPacket(packet);\n         }\n\n         checkReconnectID(reconnectID);\n\n         if (logger.isTraceEnabled()) {\n            logger.trace(\"Sending blocking \" + packet);\n         }\n\n         connection.getTransportConnection().write(buffer, false, false);\n\n         long toWait = connection.getBlockingCallTimeout();\n\n         long start = System.currentTimeMillis();\n\n         while (!closed && (response == null || (response.getType() != PacketImpl.EXCEPTION && response.getType() != expectedPacket)) && toWait > 0) {\n            try {\n               sendCondition.await(toWait, TimeUnit.MILLISECONDS);\n            } catch (InterruptedException e) {\n               throw new ActiveMQInterruptedException(e);\n            }\n\n            if (response != null && response.getType() != PacketImpl.EXCEPTION && response.getType() != expectedPacket) {\n               ActiveMQClientLogger.LOGGER.packetOutOfOrder(response, new Exception(\"trace\"));\n            }\n\n            if (closed) {\n               break;\n            }\n\n            final long now = System.currentTimeMillis();\n\n            toWait -= now - start;\n\n            start = now;\n         }\n\n         if (closed && toWait > 0 && response == null) {\n            Throwable cause = ActiveMQClientMessageBundle.BUNDLE.connectionDestroyed();\n            throw ActiveMQClientMessageBundle.BUNDLE.unblockingACall(cause);\n         }\n\n         if (response == null) {\n            throw ActiveMQClientMessageBundle.BUNDLE.timedOutSendingPacket(connection.getBlockingCallTimeout(), packet.getType());\n         }\n\n         if (response.getType() == PacketImpl.EXCEPTION) {\n            final ActiveMQExceptionMessage mem = (ActiveMQExceptionMessage) response;\n\n            ActiveMQException e = mem.getException();\n\n            e.fillInStackTrace();\n\n            throw e;\n         }\n      } finally {\n         lock.unlock();\n      }\n\n      return response;\n   }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2567,
        "instruction": "@Override\npublic Page<AuditData> retrievePaginatedAuditEntries(final SQLBuilder extraCriteria, final boolean includeJson,\n        final PaginationParameters parameters) {\n\n    this.paginationParametersDataValidator.validateParameterValues(parameters, supportedOrderByValues, \"audits\");\n    final AppUser currentUser = this.context.authenticatedUser();\n    final String hierarchy = currentUser.getOffice().getHierarchy();\n\n    final AuditMapper rm = new AuditMapper();\n    final StringBuilder sqlBuilder = new StringBuilder(200);\n    sqlBuilder.append(\"select SQL_CALC_FOUND_ROWS \");\n    sqlBuilder.append(rm.schema(includeJson, hierarchy));\n    sqlBuilder.append(' ').append(extraCriteria.getSQLTemplate());\n    if (parameters.isOrderByRequested()) {\n        sqlBuilder.append(' ').append(parameters.orderBySql());\n        this.columnValidator.validateSqlInjection(sqlBuilder.toString(), parameters.orderBySql());\n    } else {\n        sqlBuilder.append(' ').append(' ').append(\" order by aud.id DESC\");\n    }\n\n    if (parameters.isLimited()) {\n        sqlBuilder.append(' ').append(parameters.limitSql());\n        this.columnValidator.validateSqlInjection(sqlBuilder.toString(), parameters.limitSql());\n    }\n\n\n    final String sqlCountRows = \"SELECT FOUND_ROWS()\";\n    return this.paginationHelper.fetchPage(this.jdbcTemplate, sqlCountRows, sqlBuilder.toString(), extraCriteria.getArguments(), rm);\n}",
        "accept_response": "@Override\npublic Page<AuditData> retrievePaginatedAuditEntries(final SQLBuilder extraCriteria, final boolean includeJson,\n        final PaginationParameters parameters) {\n\n    this.paginationParametersDataValidator.validateParameterValues(parameters, supportedOrderByValues, \"audits\");\n    final AppUser currentUser = this.context.authenticatedUser();\n    final String hierarchy = currentUser.getOffice().getHierarchy();\n\n    final AuditMapper rm = new AuditMapper();\n    final StringBuilder sqlBuilder = new StringBuilder(200);\n    sqlBuilder.append(\"select SQL_CALC_FOUND_ROWS \");\n    sqlBuilder.append(rm.schema(includeJson, hierarchy));\n    sqlBuilder.append(' ').append(extraCriteria.getSQLTemplate());\n    if (parameters.isOrderByRequested()) {\n        sqlBuilder.append(' ').append(parameters.orderBySql());\n        this.columnValidator.validateSqlInjection(sqlBuilder.toString(), parameters.orderBySql());\n    } else {\n        sqlBuilder.append(' ').append(' ').append(\" order by aud.id DESC\");\n    }\n\n    if (parameters.isLimited()) {\n        sqlBuilder.append(' ').append(parameters.limitSql());\n        this.columnValidator.validateSqlInjection(sqlBuilder.toString(), parameters.limitSql());\n    }\n\n    logger.info(\"sql: {}\", sqlBuilder);\n\n    final String sqlCountRows = \"SELECT FOUND_ROWS()\";\n    return this.paginationHelper.fetchPage(this.jdbcTemplate, sqlCountRows, sqlBuilder.toString(), extraCriteria.getArguments(), rm);\n}",
        "reject_response": "@Override\npublic Page<AuditData> retrievePaginatedAuditEntries(final SQLBuilder extraCriteria, final boolean includeJson,\n        final PaginationParameters parameters) {\n\n    this.paginationParametersDataValidator.validateParameterValues(parameters, supportedOrderByValues, \"audits\");\n    final AppUser currentUser = this.context.authenticatedUser();\n    final String hierarchy = currentUser.getOffice().getHierarchy();\n\n    final AuditMapper rm = new AuditMapper();\n    final StringBuilder sqlBuilder = new StringBuilder(200);\n    sqlBuilder.append(\"select SQL_CALC_FOUND_ROWS \");\n    sqlBuilder.append(rm.schema(includeJson, hierarchy));\n    sqlBuilder.append(' ').append(extraCriteria.getSQLTemplate());\n    if (parameters.isOrderByRequested()) {\n        sqlBuilder.append(' ').append(parameters.orderBySql());\n        this.columnValidator.validateSqlInjection(sqlBuilder.toString(), parameters.orderBySql());\n    } else {\n        sqlBuilder.append(' ').append(' ').append(\" order by aud.id DESC\");\n    }\n\n    if (parameters.isLimited()) {\n        sqlBuilder.append(' ').append(parameters.limitSql());\n        this.columnValidator.validateSqlInjection(sqlBuilder.toString(), parameters.limitSql());\n    }\n\n    logger.info(\"sql: \" + sqlBuilder.toString());\n\n    final String sqlCountRows = \"SELECT FOUND_ROWS()\";\n    return this.paginationHelper.fetchPage(this.jdbcTemplate, sqlCountRows, sqlBuilder.toString(), extraCriteria.getArguments(), rm);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2703,
        "instruction": "@GET\n@Path(\"getFragmentsStats\")\n@Produces(\"application/json\")\npublic Response getFragmentsStats(@Context final ServletContext servletContext,\n                                  @Context final HttpHeaders headers,\n                                  @QueryParam(\"path\") final String path)\n        throws Exception {\n\n    ProtocolData protData = getProtocolData(servletContext, headers, path);\n\n    /* Create a fragmenter instance with API level parameters */\n    final Fragmenter fragmenter = FragmenterFactory.create(protData);\n\n    FragmentsStats fragmentsStats = fragmenter.getFragmentsStats();\n    String response = FragmentsStats.dataToJSON(fragmentsStats);\n\n    return Response.ok(response, MediaType.APPLICATION_JSON_TYPE).build();\n}",
        "accept_response": "@GET\n@Path(\"getFragmentsStats\")\n@Produces(\"application/json\")\npublic Response getFragmentsStats(@Context final ServletContext servletContext,\n                                  @Context final HttpHeaders headers,\n                                  @QueryParam(\"path\") final String path)\n        throws Exception {\n\n    ProtocolData protData = getProtocolData(servletContext, headers, path);\n\n    /* Create a fragmenter instance with API level parameters */\n    final Fragmenter fragmenter = FragmenterFactory.create(protData);\n\n    FragmentsStats fragmentsStats = fragmenter.getFragmentsStats();\n    String response = FragmentsStats.dataToJSON(fragmentsStats);\n    if (LOG.isDebugEnabled()) {\n        LOG.debug(FragmentsStats.dataToString(fragmentsStats, path));\n    }\n\n    return Response.ok(response, MediaType.APPLICATION_JSON_TYPE).build();\n}",
        "reject_response": "@GET\n@Path(\"getFragmentsStats\")\n@Produces(\"application/json\")\npublic Response getFragmentsStats(@Context final ServletContext servletContext,\n                                  @Context final HttpHeaders headers,\n                                  @QueryParam(\"path\") final String path)\n        throws Exception {\n\n    ProtocolData protData = getProtocolData(servletContext, headers, path);\n\n    /* Create a fragmenter instance with API level parameters */\n    final Fragmenter fragmenter = FragmenterFactory.create(protData);\n\n    FragmentsStats fragmentsStats = fragmenter.getFragmentsStats();\n    String response = FragmentsStats.dataToJSON(fragmentsStats);\n    if (LOG.isDebugEnabled()) {\n    if (Log.isDebugEnabled()) {\n        Log.debug(FragmentsStats.dataToString(fragmentsStats, path));\n    }\n\n    return Response.ok(response, MediaType.APPLICATION_JSON_TYPE).build();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2805,
        "instruction": "private boolean undeploy(ClassLoader ldr, GridCacheEntryEx e, GridCacheAdapter cache) {\n    KeyCacheObject key = e.key();\n\n    GridCacheEntryEx entry = cache.peekEx(key);\n\n    if (entry == null)\n        return false;\n\n    Object key0;\n    Object val0;\n\n    try {\n        CacheObject v = entry.peek(null);\n\n        key0 = key.value(cache.context().cacheObjectContext(), false);\n\n        assert key0 != null : \"Key cannot be null for cache entry: \" + e;\n\n        val0 = CU.value(v, cache.context(), false);\n    }\n    catch (GridCacheEntryRemovedException ignore) {\n        return false;\n    }\n    catch (BinaryInvalidTypeException ex) {\n        log.error(\"An attempt to undeploy cache with binary objects.\", ex);\n\n        return false;\n    }\n    catch (IgniteCheckedException | IgniteException ignore) {\n        // Peek can throw runtime exception if unmarshalling failed.\n        return true;\n    }\n\n    ClassLoader keyLdr = U.detectObjectClassLoader(key0);\n    ClassLoader valLdr = U.detectObjectClassLoader(val0);\n\n    boolean res = F.eq(ldr, keyLdr) || F.eq(ldr, valLdr);\n\n    if (log.isDebugEnabled())\n\n    return res;\n}",
        "accept_response": "private boolean undeploy(ClassLoader ldr, GridCacheEntryEx e, GridCacheAdapter cache) {\n    KeyCacheObject key = e.key();\n\n    GridCacheEntryEx entry = cache.peekEx(key);\n\n    if (entry == null)\n        return false;\n\n    Object key0;\n    Object val0;\n\n    try {\n        CacheObject v = entry.peek(null);\n\n        key0 = key.value(cache.context().cacheObjectContext(), false);\n\n        assert key0 != null : \"Key cannot be null for cache entry: \" + e;\n\n        val0 = CU.value(v, cache.context(), false);\n    }\n    catch (GridCacheEntryRemovedException ignore) {\n        return false;\n    }\n    catch (BinaryInvalidTypeException ex) {\n        log.error(\"An attempt to undeploy cache with binary objects.\", ex);\n\n        return false;\n    }\n    catch (IgniteCheckedException | IgniteException ignore) {\n        // Peek can throw runtime exception if unmarshalling failed.\n        return true;\n    }\n\n    ClassLoader keyLdr = U.detectObjectClassLoader(key0);\n    ClassLoader valLdr = U.detectObjectClassLoader(val0);\n\n    boolean res = F.eq(ldr, keyLdr) || F.eq(ldr, valLdr);\n\n    if (log.isDebugEnabled())\n        log.debug(S.toString(\"Finished examining entry\",\n            \"entryCls\", e.getClass(), true,\n            \"key\", key0, true,\n            \"keyCls\", key0.getClass(), true,\n            \"valCls\", (val0 != null ? val0.getClass() : \"null\"), true,\n            \"keyLdr\", keyLdr, false,\n            \"valLdr\", valLdr, false,\n            \"res\", res, false));\n\n    return res;\n}",
        "reject_response": "private boolean undeploy(ClassLoader ldr, GridCacheEntryEx e, GridCacheAdapter cache) {\n    KeyCacheObject key = e.key();\n\n    GridCacheEntryEx entry = cache.peekEx(key);\n\n    if (entry == null)\n        return false;\n\n    Object key0;\n    Object val0;\n\n    try {\n        CacheObject v = entry.peek(null);\n\n        key0 = key.value(cache.context().cacheObjectContext(), false);\n\n        assert key0 != null : \"Key cannot be null for cache entry: \" + e;\n\n        val0 = CU.value(v, cache.context(), false);\n    }\n    catch (GridCacheEntryRemovedException ignore) {\n        return false;\n    }\n    catch (BinaryInvalidTypeException ex) {\n        log.error(\"An attempt to undeploy cache with binary objects.\", ex);\n\n        return false;\n    }\n    catch (IgniteCheckedException | IgniteException ignore) {\n        // Peek can throw runtime exception if unmarshalling failed.\n        return true;\n    }\n\n    ClassLoader keyLdr = U.detectObjectClassLoader(key0);\n    ClassLoader valLdr = U.detectObjectClassLoader(val0);\n\n    boolean res = F.eq(ldr, keyLdr) || F.eq(ldr, valLdr);\n\n    if (log.isDebugEnabled())\n        log.debug(\"Finished examining entry [entryCls=\" + e.getClass() +\n            \", key=\" + key0 + \", keyCls=\" + key0.getClass() +\n            \", valCls=\" + (val0 != null ? val0.getClass() : \"null\") +\n            \", keyLdr=\" + keyLdr + \", valLdr=\" + valLdr + \", res=\" + res + ']');\n\n    return res;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3253,
        "instruction": "public synchronized void close() throws InterruptedException {\n    if (!cnxn.getState().isAlive()) {\n        LOG.debug(\"Close called on already closed client\");\n        return;\n    }\n\n    LOG.debug(\"Closing session: 0x\" + Long.toHexString(getSessionId()));\n\n    try {\n        cnxn.close();\n    } catch (IOException e) {\n        LOG.debug(\"Ignoring unexpected exception during close\", e);\n    }\n\n}",
        "accept_response": "public synchronized void close() throws InterruptedException {\n    if (!cnxn.getState().isAlive()) {\n        LOG.debug(\"Close called on already closed client\");\n        return;\n    }\n\n    LOG.debug(\"Closing session: 0x\" + Long.toHexString(getSessionId()));\n\n    try {\n        cnxn.close();\n    } catch (IOException e) {\n        LOG.debug(\"Ignoring unexpected exception during close\", e);\n    }\n\n    LOG.info(\"Session: 0x{} closed\", Long.toHexString(getSessionId()));\n}",
        "reject_response": "public synchronized void close() throws InterruptedException {\n    if (!cnxn.getState().isAlive()) {\n        LOG.debug(\"Close called on already closed client\");\n        return;\n    }\n\n    LOG.debug(\"Closing session: 0x\" + Long.toHexString(getSessionId()));\n\n    try {\n        cnxn.close();\n    } catch (IOException e) {\n        LOG.debug(\"Ignoring unexpected exception during close\", e);\n    }\n\n    LOG.info(\"Session: 0x\" + Long.toHexString(getSessionId()) + \" closed\");\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2997,
        "instruction": "public List<String> getCatalogList(String needle, final List<String> catalogs) throws HadoopException {\n  final String ndl = needle;\n  final List<String> catList = catalogs;\n\n  List<String> dbs = Subject.doAs(getLoginSubject(), new PrivilegedAction<List<String>>() {\n    @Override\n    public List<String> run() {\n      List<String> ret = null;\n      try {\n        ret = getCatalogs(ndl, catList);\n      } catch (HadoopException he) {\n        throw he;\n      }\n      return ret;\n    }\n  });\n\n  return dbs;\n}",
        "accept_response": "public List<String> getCatalogList(String needle, final List<String> catalogs) throws HadoopException {\n  final String ndl = needle;\n  final List<String> catList = catalogs;\n\n  List<String> dbs = Subject.doAs(getLoginSubject(), new PrivilegedAction<List<String>>() {\n    @Override\n    public List<String> run() {\n      List<String> ret = null;\n      try {\n        ret = getCatalogs(ndl, catList);\n      } catch (HadoopException he) {\n        LOG.error(\"<== PrestoClient.getCatalogList() :Unable to get the Database List\", he);\n        throw he;\n      }\n      return ret;\n    }\n  });\n\n  return dbs;\n}",
        "reject_response": "public List<String> getCatalogList(String needle, final List<String> catalogs) throws HadoopException {\n  final String ndl = needle;\n  final List<String> catList = catalogs;\n\n  List<String> dbs = Subject.doAs(getLoginSubject(), new PrivilegedAction<List<String>>() {\n    @Override\n    public List<String> run() {\n      List<String> ret = null;\n      try {\n        ret = getCatalogs(ndl, catList);\n      } catch (HadoopException he) {\n        LOG.error(\"<== PrestoClient getCatalogList() :Unable to get the Database List\", he);\n        throw he;\n      }\n      return ret;\n    }\n  });\n\n  return dbs;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2437,
        "instruction": "protected MemoryMessageStore asMemoryMessageStore(Object value) {\n    if (value instanceof MemoryMessageStore) {\n        return (MemoryMessageStore) value;\n    }\n    if (value instanceof ProxyMessageStore) {\n        MessageStore delegate = ((ProxyMessageStore) value).getDelegate();\n        if (delegate instanceof MemoryMessageStore) {\n            return (MemoryMessageStore) delegate;\n        }\n    }\n    return null;\n}",
        "accept_response": "protected MemoryMessageStore asMemoryMessageStore(Object value) {\n    if (value instanceof MemoryMessageStore) {\n        return (MemoryMessageStore) value;\n    }\n    if (value instanceof ProxyMessageStore) {\n        MessageStore delegate = ((ProxyMessageStore) value).getDelegate();\n        if (delegate instanceof MemoryMessageStore) {\n            return (MemoryMessageStore) delegate;\n        }\n    }\n    LOG.warn(\"Expected an instance of MemoryMessageStore but was: {}\", value);\n    return null;\n}",
        "reject_response": "protected MemoryMessageStore asMemoryMessageStore(Object value) {\n    if (value instanceof MemoryMessageStore) {\n        return (MemoryMessageStore) value;\n    }\n    if (value instanceof ProxyMessageStore) {\n        MessageStore delegate = ((ProxyMessageStore) value).getDelegate();\n        if (delegate instanceof MemoryMessageStore) {\n            return (MemoryMessageStore) delegate;\n        }\n    }\n    LOG.warn(\"Expected an instance of MemoryMessageStore but was: \" + value);\n    return null;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2756,
        "instruction": "@SuppressWarnings(\"unchecked\")\n@Override\npublic void cancel(Token<?> token, Configuration conf) throws IOException {\n  Token<DelegationTokenIdentifier> delToken =\n      (Token<DelegationTokenIdentifier>) token;\n  ClientProtocol nn = getNNProxy(delToken, conf);\n  try {\n    nn.cancelDelegationToken(delToken);\n  } catch (RemoteException re) {\n    throw re.unwrapRemoteException(InvalidToken.class,\n        AccessControlException.class);\n  }\n}",
        "accept_response": "@SuppressWarnings(\"unchecked\")\n@Override\npublic void cancel(Token<?> token, Configuration conf) throws IOException {\n  Token<DelegationTokenIdentifier> delToken =\n      (Token<DelegationTokenIdentifier>) token;\n  LOG.info(\"Cancelling {}\", DelegationTokenIdentifier.stringifyToken(delToken));\n  ClientProtocol nn = getNNProxy(delToken, conf);\n  try {\n    nn.cancelDelegationToken(delToken);\n  } catch (RemoteException re) {\n    throw re.unwrapRemoteException(InvalidToken.class,\n        AccessControlException.class);\n  }\n}",
        "reject_response": "@SuppressWarnings(\"unchecked\")\n@Override\npublic void cancel(Token<?> token, Configuration conf) throws IOException {\n  Token<DelegationTokenIdentifier> delToken =\n      (Token<DelegationTokenIdentifier>) token;\n  LOG.info(\"Cancelling \" +\n      DelegationTokenIdentifier.stringifyToken(delToken));\n  ClientProtocol nn = getNNProxy(delToken, conf);\n  try {\n    nn.cancelDelegationToken(delToken);\n  } catch (RemoteException re) {\n    throw re.unwrapRemoteException(InvalidToken.class,\n        AccessControlException.class);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2403,
        "instruction": "private void loadConfigFiles() throws Exception {\n  List<String> configFiles = getConfigFiles();\n  for (String configFileName : configFiles) {\n    LOG.info(\"Going to load config file:\" + configFileName);\n    configFileName = configFileName.replace(\"\\\\ \", \"%20\");\n    File configFile = new File(configFileName);\n    if (configFile.exists() && configFile.isFile()) {\n      loadConfigsUsingFile(configFile);\n    } else {\n      LOG.info(\"Trying to load config file from classloader: \" + configFileName);\n      loadConfigsUsingClassLoader(configFileName);\n      LOG.info(\"Loaded config file from classloader: \" + configFileName);\n    }\n  }\n}",
        "accept_response": "private void loadConfigFiles() throws Exception {\n  List<String> configFiles = getConfigFiles();\n  for (String configFileName : configFiles) {\n    LOG.info(\"Going to load config file:\" + configFileName);\n    configFileName = configFileName.replace(\"\\\\ \", \"%20\");\n    File configFile = new File(configFileName);\n    if (configFile.exists() && configFile.isFile()) {\n      LOG.info(\"Config file exists in path.\" + configFile.getAbsolutePath());\n      loadConfigsUsingFile(configFile);\n    } else {\n      LOG.info(\"Trying to load config file from classloader: \" + configFileName);\n      loadConfigsUsingClassLoader(configFileName);\n      LOG.info(\"Loaded config file from classloader: \" + configFileName);\n    }\n  }\n}",
        "reject_response": "private void loadConfigFiles() throws Exception {\n  List<String> configFiles = getConfigFiles();\n  for (String configFileName : configFiles) {\n    LOG.info(\"Going to load config file:\" + configFileName);\n    configFileName = configFileName.replace(\"\\\\ \", \"%20\");\n    File configFile = new File(configFileName);\n    if (configFile.exists() && configFile.isFile()) {\n      logger.info(\"Config file exists in path.\"\n        + configFile.getAbsolutePath());\n      loadConfigsUsingFile(configFile);\n    } else {\n      LOG.info(\"Trying to load config file from classloader: \" + configFileName);\n      loadConfigsUsingClassLoader(configFileName);\n      LOG.info(\"Loaded config file from classloader: \" + configFileName);\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2745,
        "instruction": "void addDatanode(final DatanodeDescriptor node) {\n  // To keep host2DatanodeMap consistent with datanodeMap,\n  // remove  from host2DatanodeMap the datanodeDescriptor removed\n  // from datanodeMap before adding node to host2DatanodeMap.\n  synchronized(this) {\n    host2DatanodeMap.remove(datanodeMap.put(node.getDatanodeUuid(), node));\n  }\n\n  networktopology.add(node); // may throw InvalidTopologyException\n  host2DatanodeMap.add(node);\n  checkIfClusterIsNowMultiRack(node);\n  resolveUpgradeDomain(node);\n}",
        "accept_response": "void addDatanode(final DatanodeDescriptor node) {\n  // To keep host2DatanodeMap consistent with datanodeMap,\n  // remove  from host2DatanodeMap the datanodeDescriptor removed\n  // from datanodeMap before adding node to host2DatanodeMap.\n  synchronized(this) {\n    host2DatanodeMap.remove(datanodeMap.put(node.getDatanodeUuid(), node));\n  }\n\n  networktopology.add(node); // may throw InvalidTopologyException\n  host2DatanodeMap.add(node);\n  checkIfClusterIsNowMultiRack(node);\n  resolveUpgradeDomain(node);\n\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"{}.addDatanode: node {} is added to datanodeMap.\",\n        getClass().getSimpleName(), node);\n  }\n}",
        "reject_response": "void addDatanode(final DatanodeDescriptor node) {\n  // To keep host2DatanodeMap consistent with datanodeMap,\n  // remove  from host2DatanodeMap the datanodeDescriptor removed\n  // from datanodeMap before adding node to host2DatanodeMap.\n  synchronized(this) {\n    host2DatanodeMap.remove(datanodeMap.put(node.getDatanodeUuid(), node));\n  }\n\n  networktopology.add(node); // may throw InvalidTopologyException\n  host2DatanodeMap.add(node);\n  checkIfClusterIsNowMultiRack(node);\n  resolveUpgradeDomain(node);\n\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(getClass().getSimpleName() + \".addDatanode: \"\n        + \"node \" + node + \" is added to datanodeMap.\");\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3067,
        "instruction": "@Override\npublic void inform(SolrCore core) {\n  if (initParams != null) {\n    LOG.info(\"Initializing spell checkers\");\n    boolean hasDefault = false;\n    for (int i = 0; i < initParams.size(); i++) {\n      if (initParams.getName(i).equals(\"spellchecker\")) {\n        Object cfg = initParams.getVal(i);\n        if (cfg instanceof NamedList) {\n          addSpellChecker(core, hasDefault, (NamedList) cfg);\n        } else if (cfg instanceof Map) {\n          addSpellChecker(core, hasDefault, new NamedList((Map) cfg));\n        } else if (cfg instanceof List) {\n          for (Object o : (List) cfg) {\n            if (o instanceof Map) {\n              addSpellChecker(core, hasDefault, new NamedList((Map) o));\n            }\n          }\n        }\n      }\n    }\n\n    Map<String, QueryConverter> queryConverters = new HashMap<>();\n    core.initPlugins(queryConverters,QueryConverter.class);\n\n    //ensure that there is at least one query converter defined\n    if (queryConverters.size() == 0) {\n      queryConverters.put(\"queryConverter\", new SpellingQueryConverter());\n    }\n\n    //there should only be one\n    if (queryConverters.size() == 1) {\n      queryConverter = queryConverters.values().iterator().next();\n      IndexSchema schema = core.getLatestSchema();\n      String fieldTypeName = (String) initParams.get(\"queryAnalyzerFieldType\");\n      FieldType fieldType = schema.getFieldTypes().get(fieldTypeName);\n      Analyzer analyzer = fieldType == null ? new WhitespaceAnalyzer()\n              : fieldType.getQueryAnalyzer();\n      //TODO: There's got to be a better way!  Where's Spring when you need it?\n      queryConverter.setAnalyzer(analyzer);\n    }\n  }\n}",
        "accept_response": "@Override\npublic void inform(SolrCore core) {\n  if (initParams != null) {\n    LOG.info(\"Initializing spell checkers\");\n    boolean hasDefault = false;\n    for (int i = 0; i < initParams.size(); i++) {\n      if (initParams.getName(i).equals(\"spellchecker\")) {\n        Object cfg = initParams.getVal(i);\n        if (cfg instanceof NamedList) {\n          addSpellChecker(core, hasDefault, (NamedList) cfg);\n        } else if (cfg instanceof Map) {\n          addSpellChecker(core, hasDefault, new NamedList((Map) cfg));\n        } else if (cfg instanceof List) {\n          for (Object o : (List) cfg) {\n            if (o instanceof Map) {\n              addSpellChecker(core, hasDefault, new NamedList((Map) o));\n            }\n          }\n        }\n      }\n    }\n\n    Map<String, QueryConverter> queryConverters = new HashMap<>();\n    core.initPlugins(queryConverters,QueryConverter.class);\n\n    //ensure that there is at least one query converter defined\n    if (queryConverters.size() == 0) {\n      LOG.trace(\"No queryConverter defined, using default converter\");\n      queryConverters.put(\"queryConverter\", new SpellingQueryConverter());\n    }\n\n    //there should only be one\n    if (queryConverters.size() == 1) {\n      queryConverter = queryConverters.values().iterator().next();\n      IndexSchema schema = core.getLatestSchema();\n      String fieldTypeName = (String) initParams.get(\"queryAnalyzerFieldType\");\n      FieldType fieldType = schema.getFieldTypes().get(fieldTypeName);\n      Analyzer analyzer = fieldType == null ? new WhitespaceAnalyzer()\n              : fieldType.getQueryAnalyzer();\n      //TODO: There's got to be a better way!  Where's Spring when you need it?\n      queryConverter.setAnalyzer(analyzer);\n    }\n  }\n}",
        "reject_response": "@Override\npublic void inform(SolrCore core) {\n  if (initParams != null) {\n    LOG.info(\"Initializing spell checkers\");\n    boolean hasDefault = false;\n    for (int i = 0; i < initParams.size(); i++) {\n      if (initParams.getName(i).equals(\"spellchecker\")) {\n        Object cfg = initParams.getVal(i);\n        if (cfg instanceof NamedList) {\n          addSpellChecker(core, hasDefault, (NamedList) cfg);\n        } else if (cfg instanceof Map) {\n          addSpellChecker(core, hasDefault, new NamedList((Map) cfg));\n        } else if (cfg instanceof List) {\n          for (Object o : (List) cfg) {\n            if (o instanceof Map) {\n              addSpellChecker(core, hasDefault, new NamedList((Map) o));\n            }\n          }\n        }\n      }\n    }\n\n    Map<String, QueryConverter> queryConverters = new HashMap<>();\n    core.initPlugins(queryConverters,QueryConverter.class);\n\n    //ensure that there is at least one query converter defined\n    if (queryConverters.size() == 0) {\n      LOG.info(\"No queryConverter defined, using default converter\");\n      queryConverters.put(\"queryConverter\", new SpellingQueryConverter());\n    }\n\n    //there should only be one\n    if (queryConverters.size() == 1) {\n      queryConverter = queryConverters.values().iterator().next();\n      IndexSchema schema = core.getLatestSchema();\n      String fieldTypeName = (String) initParams.get(\"queryAnalyzerFieldType\");\n      FieldType fieldType = schema.getFieldTypes().get(fieldTypeName);\n      Analyzer analyzer = fieldType == null ? new WhitespaceAnalyzer()\n              : fieldType.getQueryAnalyzer();\n      //TODO: There's got to be a better way!  Where's Spring when you need it?\n      queryConverter.setAnalyzer(analyzer);\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2770,
        "instruction": "@Override\npublic void execute(\n        final HttpRequest request,\n        final AsyncEntityProducer entityProducer,\n        final AsyncExecChain.Scope scope,\n        final AsyncExecChain chain,\n        final AsyncExecCallback asyncExecCallback) throws HttpException, IOException {\n    Args.notNull(request, \"HTTP request\");\n    Args.notNull(scope, \"Scope\");\n\n    final String exchangeId = scope.exchangeId;\n    final HttpRoute route = scope.route;\n    final CancellableDependency cancellableDependency = scope.cancellableDependency;\n    final HttpClientContext clientContext = scope.clientContext;\n    final AsyncExecRuntime execRuntime = scope.execRuntime;\n    final State state = new State(route);\n\n    if (!execRuntime.isEndpointAcquired()) {\n        final Object userToken = clientContext.getUserToken();\n        cancellableDependency.setDependency(execRuntime.acquireEndpoint(\n                exchangeId, route, userToken, clientContext, new FutureCallback<AsyncExecRuntime>() {\n\n                    @Override\n                    public void completed(final AsyncExecRuntime execRuntime) {\n                        if (execRuntime.isEndpointConnected()) {\n                            try {\n                                chain.proceed(request, entityProducer, scope, asyncExecCallback);\n                            } catch (final HttpException | IOException ex) {\n                                asyncExecCallback.failed(ex);\n                            }\n                        } else {\n                            proceedToNextHop(state, request, entityProducer, scope, chain, asyncExecCallback);\n                        }\n                    }\n\n                    @Override\n                    public void failed(final Exception ex) {\n                        asyncExecCallback.failed(ex);\n                    }\n\n                    @Override\n                    public void cancelled() {\n                        asyncExecCallback.failed(new InterruptedIOException());\n                    }\n\n                }));\n    } else {\n        if (execRuntime.isEndpointConnected()) {\n            try {\n                chain.proceed(request, entityProducer, scope, asyncExecCallback);\n            } catch (final HttpException | IOException ex) {\n                asyncExecCallback.failed(ex);\n            }\n        } else {\n            proceedToNextHop(state, request, entityProducer, scope, chain, asyncExecCallback);\n        }\n    }\n\n}",
        "accept_response": "@Override\npublic void execute(\n        final HttpRequest request,\n        final AsyncEntityProducer entityProducer,\n        final AsyncExecChain.Scope scope,\n        final AsyncExecChain chain,\n        final AsyncExecCallback asyncExecCallback) throws HttpException, IOException {\n    Args.notNull(request, \"HTTP request\");\n    Args.notNull(scope, \"Scope\");\n\n    final String exchangeId = scope.exchangeId;\n    final HttpRoute route = scope.route;\n    final CancellableDependency cancellableDependency = scope.cancellableDependency;\n    final HttpClientContext clientContext = scope.clientContext;\n    final AsyncExecRuntime execRuntime = scope.execRuntime;\n    final State state = new State(route);\n\n    if (!execRuntime.isEndpointAcquired()) {\n        final Object userToken = clientContext.getUserToken();\n        if (log.isDebugEnabled()) {\n            log.debug(\"{}: acquiring connection with route {}\", exchangeId, route);\n        }\n        cancellableDependency.setDependency(execRuntime.acquireEndpoint(\n                exchangeId, route, userToken, clientContext, new FutureCallback<AsyncExecRuntime>() {\n\n                    @Override\n                    public void completed(final AsyncExecRuntime execRuntime) {\n                        if (execRuntime.isEndpointConnected()) {\n                            try {\n                                chain.proceed(request, entityProducer, scope, asyncExecCallback);\n                            } catch (final HttpException | IOException ex) {\n                                asyncExecCallback.failed(ex);\n                            }\n                        } else {\n                            proceedToNextHop(state, request, entityProducer, scope, chain, asyncExecCallback);\n                        }\n                    }\n\n                    @Override\n                    public void failed(final Exception ex) {\n                        asyncExecCallback.failed(ex);\n                    }\n\n                    @Override\n                    public void cancelled() {\n                        asyncExecCallback.failed(new InterruptedIOException());\n                    }\n\n                }));\n    } else {\n        if (execRuntime.isEndpointConnected()) {\n            try {\n                chain.proceed(request, entityProducer, scope, asyncExecCallback);\n            } catch (final HttpException | IOException ex) {\n                asyncExecCallback.failed(ex);\n            }\n        } else {\n            proceedToNextHop(state, request, entityProducer, scope, chain, asyncExecCallback);\n        }\n    }\n\n}",
        "reject_response": "@Override\npublic void execute(\n        final HttpRequest request,\n        final AsyncEntityProducer entityProducer,\n        final AsyncExecChain.Scope scope,\n        final AsyncExecChain chain,\n        final AsyncExecCallback asyncExecCallback) throws HttpException, IOException {\n    Args.notNull(request, \"HTTP request\");\n    Args.notNull(scope, \"Scope\");\n\n    final String exchangeId = scope.exchangeId;\n    final HttpRoute route = scope.route;\n    final CancellableDependency cancellableDependency = scope.cancellableDependency;\n    final HttpClientContext clientContext = scope.clientContext;\n    final AsyncExecRuntime execRuntime = scope.execRuntime;\n    final State state = new State(route);\n\n    if (!execRuntime.isEndpointAcquired()) {\n        final Object userToken = clientContext.getUserToken();\n        if (log.isDebugEnabled()) {\n            log.debug(exchangeId + \": acquiring connection with route \" + route);\n        }\n        cancellableDependency.setDependency(execRuntime.acquireEndpoint(\n                exchangeId, route, userToken, clientContext, new FutureCallback<AsyncExecRuntime>() {\n\n                    @Override\n                    public void completed(final AsyncExecRuntime execRuntime) {\n                        if (execRuntime.isEndpointConnected()) {\n                            try {\n                                chain.proceed(request, entityProducer, scope, asyncExecCallback);\n                            } catch (final HttpException | IOException ex) {\n                                asyncExecCallback.failed(ex);\n                            }\n                        } else {\n                            proceedToNextHop(state, request, entityProducer, scope, chain, asyncExecCallback);\n                        }\n                    }\n\n                    @Override\n                    public void failed(final Exception ex) {\n                        asyncExecCallback.failed(ex);\n                    }\n\n                    @Override\n                    public void cancelled() {\n                        asyncExecCallback.failed(new InterruptedIOException());\n                    }\n\n                }));\n    } else {\n        if (execRuntime.isEndpointConnected()) {\n            try {\n                chain.proceed(request, entityProducer, scope, asyncExecCallback);\n            } catch (final HttpException | IOException ex) {\n                asyncExecCallback.failed(ex);\n            }\n        } else {\n            proceedToNextHop(state, request, entityProducer, scope, chain, asyncExecCallback);\n        }\n    }\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3247,
        "instruction": "public void close() throws IOException {\n\n    try {\n        RequestHeader h = new RequestHeader();\n        h.setType(ZooDefs.OpCode.closeSession);\n\n        submitRequest(h, null, null, null);\n    } catch (InterruptedException e) {\n        // ignore, close the send/event threads\n    } finally {\n        disconnect();\n    }\n}",
        "accept_response": "public void close() throws IOException {\n    LOG.debug(\"Closing client for session: 0x{}\", Long.toHexString(getSessionId()));\n\n    try {\n        RequestHeader h = new RequestHeader();\n        h.setType(ZooDefs.OpCode.closeSession);\n\n        submitRequest(h, null, null, null);\n    } catch (InterruptedException e) {\n        // ignore, close the send/event threads\n    } finally {\n        disconnect();\n    }\n}",
        "reject_response": "public void close() throws IOException {\n    if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Closing client for session: 0x\" + Long.toHexString(getSessionId()));\n    }\n\n    try {\n        RequestHeader h = new RequestHeader();\n        h.setType(ZooDefs.OpCode.closeSession);\n\n        submitRequest(h, null, null, null);\n    } catch (InterruptedException e) {\n        // ignore, close the send/event threads\n    } finally {\n        disconnect();\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2946,
        "instruction": "private Object getMutex(File file) {\n    // The interned string of path is (mis)used as mutex, to exclude different threads going for same file,\n    // as JVM file locking happens on JVM not on Thread level. This is how original code did it  \u00af\\_(\u30c4)_/\u00af\n    /*\n     * NOTE: Locks held by one JVM must not overlap and using the canonical path is our best bet, still another\n     * piece of code might have locked the same file (unlikely though) or the canonical path fails to capture file\n     * identity sufficiently as is the case with Java 1.6 and symlinks on Windows.\n     */\n    try {\n        return file.getCanonicalPath().intern();\n    } catch (IOException e) {\n        // TODO This is code smell and deprecated\n        return file.getAbsolutePath().intern();\n    }\n}",
        "accept_response": "private Object getMutex(File file) {\n    // The interned string of path is (mis)used as mutex, to exclude different threads going for same file,\n    // as JVM file locking happens on JVM not on Thread level. This is how original code did it  \u00af\\_(\u30c4)_/\u00af\n    /*\n     * NOTE: Locks held by one JVM must not overlap and using the canonical path is our best bet, still another\n     * piece of code might have locked the same file (unlikely though) or the canonical path fails to capture file\n     * identity sufficiently as is the case with Java 1.6 and symlinks on Windows.\n     */\n    try {\n        return file.getCanonicalPath().intern();\n    } catch (IOException e) {\n        LOGGER.warn(\"Failed to canonicalize path {}\", file, e);\n        // TODO This is code smell and deprecated\n        return file.getAbsolutePath().intern();\n    }\n}",
        "reject_response": "private Object getMutex(File file) {\n    // The interned string of path is (mis)used as mutex, to exclude different threads going for same file,\n    // as JVM file locking happens on JVM not on Thread level. This is how original code did it  \u00af\\_(\u30c4)_/\u00af\n    /*\n     * NOTE: Locks held by one JVM must not overlap and using the canonical path is our best bet, still another\n     * piece of code might have locked the same file (unlikely though) or the canonical path fails to capture file\n     * identity sufficiently as is the case with Java 1.6 and symlinks on Windows.\n     */\n    try {\n        return file.getCanonicalPath().intern();\n    } catch (IOException e) {\n        LOGGER.warn(\"Failed to get real path {}\", file, e);\n        return file.toAbsolutePath().toString().intern();\n        // TODO This is code smell and deprecated\n        return file.getAbsolutePath().intern();\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3061,
        "instruction": "public NodeList getNodeList(String path, boolean errIfMissing) {\n  XPath xpath = xpathFactory.newXPath();\n  String xstr = normalize(path);\n\n  try {\n    NodeList nodeList = (NodeList)xpath.evaluate(xstr, doc, XPathConstants.NODESET);\n\n    if (null == nodeList) {\n      if (errIfMissing) {\n        throw new RuntimeException(name + \" missing \"+path);\n      } else {\n        log.trace(name + \" missing optional \" + path);\n        return null;\n      }\n    }\n\n    return nodeList;\n\n  } catch (XPathExpressionException e) {\n    SolrException.log(log,\"Error in xpath\",e);\n    throw new SolrException( SolrException.ErrorCode.SERVER_ERROR,\"Error in xpath:\" + xstr + \" for \" + name,e);\n  } catch (SolrException e) {\n    throw(e);\n  } catch (Exception e) {\n    SolrException.log(log,\"Error in xpath\",e);\n    throw new SolrException( SolrException.ErrorCode.SERVER_ERROR,\"Error in xpath:\" + xstr+ \" for \" + name,e);\n  }\n}",
        "accept_response": "public NodeList getNodeList(String path, boolean errIfMissing) {\n  XPath xpath = xpathFactory.newXPath();\n  String xstr = normalize(path);\n\n  try {\n    NodeList nodeList = (NodeList)xpath.evaluate(xstr, doc, XPathConstants.NODESET);\n\n    if (null == nodeList) {\n      if (errIfMissing) {\n        throw new RuntimeException(name + \" missing \"+path);\n      } else {\n        log.trace(name + \" missing optional \" + path);\n        return null;\n      }\n    }\n\n    log.trace(name + \":\" + path + \"=\" + nodeList);\n    return nodeList;\n\n  } catch (XPathExpressionException e) {\n    SolrException.log(log,\"Error in xpath\",e);\n    throw new SolrException( SolrException.ErrorCode.SERVER_ERROR,\"Error in xpath:\" + xstr + \" for \" + name,e);\n  } catch (SolrException e) {\n    throw(e);\n  } catch (Exception e) {\n    SolrException.log(log,\"Error in xpath\",e);\n    throw new SolrException( SolrException.ErrorCode.SERVER_ERROR,\"Error in xpath:\" + xstr+ \" for \" + name,e);\n  }\n}",
        "reject_response": "public NodeList getNodeList(String path, boolean errIfMissing) {\n  XPath xpath = xpathFactory.newXPath();\n  String xstr = normalize(path);\n\n  try {\n    NodeList nodeList = (NodeList)xpath.evaluate(xstr, doc, XPathConstants.NODESET);\n\n    if (null == nodeList) {\n      if (errIfMissing) {\n        throw new RuntimeException(name + \" missing \"+path);\n      } else {\n        log.trace(name + \" missing optional \" + path);\n        return null;\n      }\n    }\n\n    log.debug(name + \":\" + path + \"=\" + nodeList);\n    return nodeList;\n\n  } catch (XPathExpressionException e) {\n    SolrException.log(log,\"Error in xpath\",e);\n    throw new SolrException( SolrException.ErrorCode.SERVER_ERROR,\"Error in xpath:\" + xstr + \" for \" + name,e);\n  } catch (SolrException e) {\n    throw(e);\n  } catch (Exception e) {\n    SolrException.log(log,\"Error in xpath\",e);\n    throw new SolrException( SolrException.ErrorCode.SERVER_ERROR,\"Error in xpath:\" + xstr+ \" for \" + name,e);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2846,
        "instruction": "private void body0() throws InterruptedException, IgniteCheckedException {\n    long timeout = cctx.gridConfig().getNetworkTimeout();\n\n    long cnt = 0;\n\n    while (!isCancelled()) {\n        onIdle();\n\n        cnt++;\n\n        CachePartitionExchangeWorkerTask task = null;\n\n        try {\n            boolean preloadFinished = true;\n\n            for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                if (grp.isLocal())\n                    continue;\n\n                preloadFinished &= grp.preloader() != null && grp.preloader().syncFuture().isDone();\n\n                if (!preloadFinished)\n                    break;\n            }\n\n            // If not first preloading and no more topology events present.\n            if (!cctx.kernalContext().clientNode() && !hasPendingExchange() && preloadFinished)\n                timeout = cctx.gridConfig().getNetworkTimeout();\n\n            // After workers line up and before preloading starts we initialize all futures.\n            if (log.isTraceEnabled()) {\n                Collection<IgniteInternalFuture> unfinished = new HashSet<>();\n\n                for (GridDhtPartitionsExchangeFuture fut : exchFuts.values()) {\n                    if (!fut.isDone())\n                        unfinished.add(fut);\n                }\n\n            }\n\n            // Take next exchange future.\n            if (isCancelled())\n                Thread.currentThread().interrupt();\n\n            updateHeartbeat();\n\n            task = futQ.poll(timeout, MILLISECONDS);\n\n            updateHeartbeat();\n\n            if (task == null)\n                continue; // Main while loop.\n\n            if (!isExchangeTask(task)) {\n                processCustomTask(task);\n\n                continue;\n            }\n\n            busy = true;\n\n            Map<Integer, GridDhtPreloaderAssignments> assignsMap = null;\n\n            boolean forcePreload = false;\n\n            GridDhtPartitionExchangeId exchId;\n\n            GridDhtPartitionsExchangeFuture exchFut = null;\n\n            AffinityTopologyVersion resVer = null;\n\n            try {\n                if (isCancelled())\n                    break;\n\n                if (task instanceof RebalanceReassignExchangeTask)\n                    exchId = ((RebalanceReassignExchangeTask) task).exchangeId();\n                else if (task instanceof ForceRebalanceExchangeTask) {\n                    forcePreload = true;\n\n                    timeout = 0; // Force refresh.\n\n                    exchId = ((ForceRebalanceExchangeTask)task).exchangeId();\n                }\n                else {\n                    assert task instanceof GridDhtPartitionsExchangeFuture : task;\n\n                    exchFut = (GridDhtPartitionsExchangeFuture)task;\n\n                    exchId = exchFut.exchangeId();\n\n                    lastInitializedFut = exchFut;\n\n                    boolean newCrd = false;\n\n                    if (!crd) {\n                        List<ClusterNode> srvNodes = exchFut.firstEventCache().serverNodes();\n\n                        crd = newCrd = !srvNodes.isEmpty() && srvNodes.get(0).isLocal();\n                    }\n\n                    exchFut.init(newCrd);\n\n                    int dumpCnt = 0;\n\n                    long waitStart = U.currentTimeMillis();\n\n                    // Call rollback logic only for client node, for server nodes\n                    // rollback logic is in GridDhtPartitionsExchangeFuture.\n                    boolean txRolledBack = !cctx.localNode().isClient();\n\n                    IgniteConfiguration cfg = cctx.gridConfig();\n\n                    final long dumpTimeout = 2 * cfg.getNetworkTimeout();\n\n                    long nextDumpTime = 0;\n\n                    while (true) {\n                        // Read txTimeoutOnPME from configuration after every iteration.\n                        long curTimeout = cfg.getTransactionConfiguration().getTxTimeoutOnPartitionMapExchange();\n\n                        try {\n                            long exchTimeout = curTimeout > 0 && !txRolledBack\n                                ? Math.min(curTimeout, dumpTimeout)\n                                : dumpTimeout;\n\n                            blockingSectionEnd();\n\n                            try {\n                                resVer = exchFut.get(exchTimeout, TimeUnit.MILLISECONDS);\n                            } finally {\n                                blockingSectionEnd();\n                            }\n\n                            onIdle();\n\n                            break;\n                        }\n                        catch (IgniteFutureTimeoutCheckedException ignored) {\n                            updateHeartbeat();\n\n                            if (nextDumpTime <= U.currentTimeMillis()) {\n                                U.warn(diagnosticLog, \"Failed to wait for partition map exchange [\" +\n                                    \"topVer=\" + exchFut.initialVersion() +\n                                    \", node=\" + cctx.localNodeId() + \"]. \" +\n                                    (curTimeout <= 0 && !txRolledBack ? \"Consider changing \" +\n                                            \"TransactionConfiguration.txTimeoutOnPartitionMapSynchronization\" +\n                                            \" to non default value to avoid this message. \" : \"\") +\n                                    \"Dumping pending objects that might be the cause: \");\n\n                                try {\n                                    dumpDebugInfo(exchFut);\n                                }\n                                catch (Exception e) {\n                                    U.error(diagnosticLog, \"Failed to dump debug information: \" + e, e);\n                                }\n\n                                nextDumpTime = U.currentTimeMillis() + nextDumpTimeout(dumpCnt++, dumpTimeout);\n                            }\n\n                            if (!txRolledBack && curTimeout > 0 && U.currentTimeMillis() - waitStart >= curTimeout) {\n                                txRolledBack = true; // Try automatic rollback only once.\n\n                                cctx.tm().rollbackOnTopologyChange(exchFut.initialVersion());\n                            }\n                        }\n                        catch (Exception e) {\n                            if (exchFut.reconnectOnError(e))\n                                throw new IgniteNeedReconnectException(cctx.localNode(), e);\n\n                            throw e;\n                        }\n                    }\n\n                    removeMergedFutures(resVer, exchFut);\n\n                    if (log.isTraceEnabled())\n                        log.trace(\"After waiting for exchange future [exchFut=\" + exchFut + \", worker=\" +\n                            this + ']');\n\n                    if (exchFut.exchangeId().nodeId().equals(cctx.localNodeId()))\n                        lastRefresh.compareAndSet(-1, U.currentTimeMillis());\n\n                    // Just pick first worker to do this, so we don't\n                    // invoke topology callback more than once for the\n                    // same event.\n\n                    boolean changed = false;\n\n                    for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                        if (grp.isLocal())\n                            continue;\n\n                        if (grp.preloader().rebalanceRequired(rebTopVer, exchFut))\n                            rebTopVer = AffinityTopologyVersion.NONE;\n\n                        changed |= grp.topology().afterExchange(exchFut);\n                    }\n\n                    if (!cctx.kernalContext().clientNode() && changed && !hasPendingServerExchange())\n                        refreshPartitions();\n                }\n\n                // Schedule rebalance if force rebalance or force reassign occurs.\n                if (exchFut == null)\n                    rebTopVer = AffinityTopologyVersion.NONE;\n\n                if (!cctx.kernalContext().clientNode() && rebTopVer.equals(AffinityTopologyVersion.NONE)) {\n                    assignsMap = new HashMap<>();\n\n                    IgniteCacheSnapshotManager snp = cctx.snapshot();\n\n                    for (final CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                        long delay = grp.config().getRebalanceDelay();\n\n                        boolean disableRebalance = snp.partitionsAreFrozen(grp);\n\n                        GridDhtPreloaderAssignments assigns = null;\n\n                        // Don't delay for dummy reassigns to avoid infinite recursion.\n                        if ((delay == 0 || forcePreload) && !disableRebalance)\n                            assigns = grp.preloader().generateAssignments(exchId, exchFut);\n\n                        assignsMap.put(grp.groupId(), assigns);\n\n                        if (resVer == null && !grp.isLocal())\n                            resVer = grp.topology().readyTopologyVersion();\n                    }\n                }\n\n                if (resVer == null)\n                    resVer = exchId.topologyVersion();\n            }\n            finally {\n                // Must flip busy flag before assignments are given to demand workers.\n                busy = false;\n            }\n\n            if (assignsMap != null && rebTopVer.equals(AffinityTopologyVersion.NONE)) {\n                int size = assignsMap.size();\n\n                NavigableMap<Integer, List<Integer>> orderMap = new TreeMap<>();\n\n                for (Map.Entry<Integer, GridDhtPreloaderAssignments> e : assignsMap.entrySet()) {\n                    int grpId = e.getKey();\n\n                    CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                    int order = grp.config().getRebalanceOrder();\n\n                    if (orderMap.get(order) == null)\n                        orderMap.put(order, new ArrayList<Integer>(size));\n\n                    orderMap.get(order).add(grpId);\n                }\n\n                Runnable r = null;\n\n                List<String> rebList = new LinkedList<>();\n\n                boolean assignsCancelled = false;\n\n                GridCompoundFuture<Boolean, Boolean> forcedRebFut = null;\n\n                if (task instanceof ForceRebalanceExchangeTask)\n                    forcedRebFut = ((ForceRebalanceExchangeTask)task).forcedRebalanceFuture();\n\n                for (Integer order : orderMap.descendingKeySet()) {\n                    for (Integer grpId : orderMap.get(order)) {\n                        CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                        GridDhtPreloaderAssignments assigns = assignsMap.get(grpId);\n\n                        if (assigns != null)\n                            assignsCancelled |= assigns.cancelled();\n\n                        Runnable cur = grp.preloader().addAssignments(assigns,\n                            forcePreload,\n                            cnt,\n                            r,\n                            forcedRebFut);\n\n                        if (cur != null) {\n                            rebList.add(grp.cacheOrGroupName());\n\n                            r = cur;\n                        }\n                    }\n                }\n\n                if (forcedRebFut != null)\n                    forcedRebFut.markInitialized();\n\n                if (assignsCancelled || hasPendingExchange()) {\n                    U.log(log, \"Skipping rebalancing (obsolete exchange ID) \" +\n                        \"[top=\" + resVer + \", evt=\" + exchId.discoveryEventName() +\n                        \", node=\" + exchId.nodeId() + ']');\n                }\n                else if (r != null) {\n                    Collections.reverse(rebList);\n\n                    U.log(log, \"Rebalancing scheduled [order=\" + rebList +\n                        \", top=\" + resVer + \", force=\" + (exchFut == null) +\n                        \", evt=\" + exchId.discoveryEventName() +\n                        \", node=\" + exchId.nodeId() + ']');\n\n                    rebTopVer = resVer;\n\n                    // Start rebalancing cache groups chain. Each group will be rebalanced\n                    // sequentially one by one e.g.:\n                    // ignite-sys-cache -> cacheGroupR1 -> cacheGroupP2 -> cacheGroupR3\n                    r.run();\n                }\n                else\n                    U.log(log, \"Skipping rebalancing (nothing scheduled) \" +\n                        \"[top=\" + resVer + \", force=\" + (exchFut == null) +\n                        \", evt=\" + exchId.discoveryEventName() +\n                        \", node=\" + exchId.nodeId() + ']');\n            }\n            else\n                U.log(log, \"Skipping rebalancing (no affinity changes) \" +\n                    \"[top=\" + resVer +\n                    \", rebTopVer=\" + rebTopVer +\n                    \", evt=\" + exchId.discoveryEventName() +\n                    \", evtNode=\" + exchId.nodeId() +\n                    \", client=\" + cctx.kernalContext().clientNode() + ']');\n        }\n        catch (IgniteInterruptedCheckedException e) {\n            throw e;\n        }\n        catch (IgniteClientDisconnectedCheckedException | IgniteNeedReconnectException e) {\n            if (cctx.discovery().reconnectSupported()) {\n                U.warn(log, \"Local node failed to complete partition map exchange due to \" +\n                    \"exception, will try to reconnect to cluster: \" + e.getMessage(), e);\n\n                cctx.discovery().reconnect();\n\n                reconnectNeeded = true;\n            }\n            else\n                U.warn(log, \"Local node received IgniteClientDisconnectedCheckedException or \" +\n                    \" IgniteNeedReconnectException exception but doesn't support reconnect, stopping node: \" +\n                    e.getMessage(), e);\n\n            return;\n        }\n        catch (IgniteCheckedException e) {\n            U.error(log, \"Failed to wait for completion of partition map exchange \" +\n                \"(preloading will not start): \" + task, e);\n\n            throw e;\n        }\n    }\n}",
        "accept_response": "private void body0() throws InterruptedException, IgniteCheckedException {\n    long timeout = cctx.gridConfig().getNetworkTimeout();\n\n    long cnt = 0;\n\n    while (!isCancelled()) {\n        onIdle();\n\n        cnt++;\n\n        CachePartitionExchangeWorkerTask task = null;\n\n        try {\n            boolean preloadFinished = true;\n\n            for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                if (grp.isLocal())\n                    continue;\n\n                preloadFinished &= grp.preloader() != null && grp.preloader().syncFuture().isDone();\n\n                if (!preloadFinished)\n                    break;\n            }\n\n            // If not first preloading and no more topology events present.\n            if (!cctx.kernalContext().clientNode() && !hasPendingExchange() && preloadFinished)\n                timeout = cctx.gridConfig().getNetworkTimeout();\n\n            // After workers line up and before preloading starts we initialize all futures.\n            if (log.isTraceEnabled()) {\n                Collection<IgniteInternalFuture> unfinished = new HashSet<>();\n\n                for (GridDhtPartitionsExchangeFuture fut : exchFuts.values()) {\n                    if (!fut.isDone())\n                        unfinished.add(fut);\n                }\n\n                log.trace(\"Before waiting for exchange futures [futs\" + unfinished + \", worker=\" + this + ']');\n            }\n\n            // Take next exchange future.\n            if (isCancelled())\n                Thread.currentThread().interrupt();\n\n            updateHeartbeat();\n\n            task = futQ.poll(timeout, MILLISECONDS);\n\n            updateHeartbeat();\n\n            if (task == null)\n                continue; // Main while loop.\n\n            if (!isExchangeTask(task)) {\n                processCustomTask(task);\n\n                continue;\n            }\n\n            busy = true;\n\n            Map<Integer, GridDhtPreloaderAssignments> assignsMap = null;\n\n            boolean forcePreload = false;\n\n            GridDhtPartitionExchangeId exchId;\n\n            GridDhtPartitionsExchangeFuture exchFut = null;\n\n            AffinityTopologyVersion resVer = null;\n\n            try {\n                if (isCancelled())\n                    break;\n\n                if (task instanceof RebalanceReassignExchangeTask)\n                    exchId = ((RebalanceReassignExchangeTask) task).exchangeId();\n                else if (task instanceof ForceRebalanceExchangeTask) {\n                    forcePreload = true;\n\n                    timeout = 0; // Force refresh.\n\n                    exchId = ((ForceRebalanceExchangeTask)task).exchangeId();\n                }\n                else {\n                    assert task instanceof GridDhtPartitionsExchangeFuture : task;\n\n                    exchFut = (GridDhtPartitionsExchangeFuture)task;\n\n                    exchId = exchFut.exchangeId();\n\n                    lastInitializedFut = exchFut;\n\n                    boolean newCrd = false;\n\n                    if (!crd) {\n                        List<ClusterNode> srvNodes = exchFut.firstEventCache().serverNodes();\n\n                        crd = newCrd = !srvNodes.isEmpty() && srvNodes.get(0).isLocal();\n                    }\n\n                    exchFut.init(newCrd);\n\n                    int dumpCnt = 0;\n\n                    long waitStart = U.currentTimeMillis();\n\n                    // Call rollback logic only for client node, for server nodes\n                    // rollback logic is in GridDhtPartitionsExchangeFuture.\n                    boolean txRolledBack = !cctx.localNode().isClient();\n\n                    IgniteConfiguration cfg = cctx.gridConfig();\n\n                    final long dumpTimeout = 2 * cfg.getNetworkTimeout();\n\n                    long nextDumpTime = 0;\n\n                    while (true) {\n                        // Read txTimeoutOnPME from configuration after every iteration.\n                        long curTimeout = cfg.getTransactionConfiguration().getTxTimeoutOnPartitionMapExchange();\n\n                        try {\n                            long exchTimeout = curTimeout > 0 && !txRolledBack\n                                ? Math.min(curTimeout, dumpTimeout)\n                                : dumpTimeout;\n\n                            blockingSectionEnd();\n\n                            try {\n                                resVer = exchFut.get(exchTimeout, TimeUnit.MILLISECONDS);\n                            } finally {\n                                blockingSectionEnd();\n                            }\n\n                            onIdle();\n\n                            break;\n                        }\n                        catch (IgniteFutureTimeoutCheckedException ignored) {\n                            updateHeartbeat();\n\n                            if (nextDumpTime <= U.currentTimeMillis()) {\n                                U.warn(diagnosticLog, \"Failed to wait for partition map exchange [\" +\n                                    \"topVer=\" + exchFut.initialVersion() +\n                                    \", node=\" + cctx.localNodeId() + \"]. \" +\n                                    (curTimeout <= 0 && !txRolledBack ? \"Consider changing \" +\n                                            \"TransactionConfiguration.txTimeoutOnPartitionMapSynchronization\" +\n                                            \" to non default value to avoid this message. \" : \"\") +\n                                    \"Dumping pending objects that might be the cause: \");\n\n                                try {\n                                    dumpDebugInfo(exchFut);\n                                }\n                                catch (Exception e) {\n                                    U.error(diagnosticLog, \"Failed to dump debug information: \" + e, e);\n                                }\n\n                                nextDumpTime = U.currentTimeMillis() + nextDumpTimeout(dumpCnt++, dumpTimeout);\n                            }\n\n                            if (!txRolledBack && curTimeout > 0 && U.currentTimeMillis() - waitStart >= curTimeout) {\n                                txRolledBack = true; // Try automatic rollback only once.\n\n                                cctx.tm().rollbackOnTopologyChange(exchFut.initialVersion());\n                            }\n                        }\n                        catch (Exception e) {\n                            if (exchFut.reconnectOnError(e))\n                                throw new IgniteNeedReconnectException(cctx.localNode(), e);\n\n                            throw e;\n                        }\n                    }\n\n                    removeMergedFutures(resVer, exchFut);\n\n                    if (log.isTraceEnabled())\n                        log.trace(\"After waiting for exchange future [exchFut=\" + exchFut + \", worker=\" +\n                            this + ']');\n\n                    if (exchFut.exchangeId().nodeId().equals(cctx.localNodeId()))\n                        lastRefresh.compareAndSet(-1, U.currentTimeMillis());\n\n                    // Just pick first worker to do this, so we don't\n                    // invoke topology callback more than once for the\n                    // same event.\n\n                    boolean changed = false;\n\n                    for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                        if (grp.isLocal())\n                            continue;\n\n                        if (grp.preloader().rebalanceRequired(rebTopVer, exchFut))\n                            rebTopVer = AffinityTopologyVersion.NONE;\n\n                        changed |= grp.topology().afterExchange(exchFut);\n                    }\n\n                    if (!cctx.kernalContext().clientNode() && changed && !hasPendingServerExchange())\n                        refreshPartitions();\n                }\n\n                // Schedule rebalance if force rebalance or force reassign occurs.\n                if (exchFut == null)\n                    rebTopVer = AffinityTopologyVersion.NONE;\n\n                if (!cctx.kernalContext().clientNode() && rebTopVer.equals(AffinityTopologyVersion.NONE)) {\n                    assignsMap = new HashMap<>();\n\n                    IgniteCacheSnapshotManager snp = cctx.snapshot();\n\n                    for (final CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                        long delay = grp.config().getRebalanceDelay();\n\n                        boolean disableRebalance = snp.partitionsAreFrozen(grp);\n\n                        GridDhtPreloaderAssignments assigns = null;\n\n                        // Don't delay for dummy reassigns to avoid infinite recursion.\n                        if ((delay == 0 || forcePreload) && !disableRebalance)\n                            assigns = grp.preloader().generateAssignments(exchId, exchFut);\n\n                        assignsMap.put(grp.groupId(), assigns);\n\n                        if (resVer == null && !grp.isLocal())\n                            resVer = grp.topology().readyTopologyVersion();\n                    }\n                }\n\n                if (resVer == null)\n                    resVer = exchId.topologyVersion();\n            }\n            finally {\n                // Must flip busy flag before assignments are given to demand workers.\n                busy = false;\n            }\n\n            if (assignsMap != null && rebTopVer.equals(AffinityTopologyVersion.NONE)) {\n                int size = assignsMap.size();\n\n                NavigableMap<Integer, List<Integer>> orderMap = new TreeMap<>();\n\n                for (Map.Entry<Integer, GridDhtPreloaderAssignments> e : assignsMap.entrySet()) {\n                    int grpId = e.getKey();\n\n                    CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                    int order = grp.config().getRebalanceOrder();\n\n                    if (orderMap.get(order) == null)\n                        orderMap.put(order, new ArrayList<Integer>(size));\n\n                    orderMap.get(order).add(grpId);\n                }\n\n                Runnable r = null;\n\n                List<String> rebList = new LinkedList<>();\n\n                boolean assignsCancelled = false;\n\n                GridCompoundFuture<Boolean, Boolean> forcedRebFut = null;\n\n                if (task instanceof ForceRebalanceExchangeTask)\n                    forcedRebFut = ((ForceRebalanceExchangeTask)task).forcedRebalanceFuture();\n\n                for (Integer order : orderMap.descendingKeySet()) {\n                    for (Integer grpId : orderMap.get(order)) {\n                        CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                        GridDhtPreloaderAssignments assigns = assignsMap.get(grpId);\n\n                        if (assigns != null)\n                            assignsCancelled |= assigns.cancelled();\n\n                        Runnable cur = grp.preloader().addAssignments(assigns,\n                            forcePreload,\n                            cnt,\n                            r,\n                            forcedRebFut);\n\n                        if (cur != null) {\n                            rebList.add(grp.cacheOrGroupName());\n\n                            r = cur;\n                        }\n                    }\n                }\n\n                if (forcedRebFut != null)\n                    forcedRebFut.markInitialized();\n\n                if (assignsCancelled || hasPendingExchange()) {\n                    U.log(log, \"Skipping rebalancing (obsolete exchange ID) \" +\n                        \"[top=\" + resVer + \", evt=\" + exchId.discoveryEventName() +\n                        \", node=\" + exchId.nodeId() + ']');\n                }\n                else if (r != null) {\n                    Collections.reverse(rebList);\n\n                    U.log(log, \"Rebalancing scheduled [order=\" + rebList +\n                        \", top=\" + resVer + \", force=\" + (exchFut == null) +\n                        \", evt=\" + exchId.discoveryEventName() +\n                        \", node=\" + exchId.nodeId() + ']');\n\n                    rebTopVer = resVer;\n\n                    // Start rebalancing cache groups chain. Each group will be rebalanced\n                    // sequentially one by one e.g.:\n                    // ignite-sys-cache -> cacheGroupR1 -> cacheGroupP2 -> cacheGroupR3\n                    r.run();\n                }\n                else\n                    U.log(log, \"Skipping rebalancing (nothing scheduled) \" +\n                        \"[top=\" + resVer + \", force=\" + (exchFut == null) +\n                        \", evt=\" + exchId.discoveryEventName() +\n                        \", node=\" + exchId.nodeId() + ']');\n            }\n            else\n                U.log(log, \"Skipping rebalancing (no affinity changes) \" +\n                    \"[top=\" + resVer +\n                    \", rebTopVer=\" + rebTopVer +\n                    \", evt=\" + exchId.discoveryEventName() +\n                    \", evtNode=\" + exchId.nodeId() +\n                    \", client=\" + cctx.kernalContext().clientNode() + ']');\n        }\n        catch (IgniteInterruptedCheckedException e) {\n            throw e;\n        }\n        catch (IgniteClientDisconnectedCheckedException | IgniteNeedReconnectException e) {\n            if (cctx.discovery().reconnectSupported()) {\n                U.warn(log, \"Local node failed to complete partition map exchange due to \" +\n                    \"exception, will try to reconnect to cluster: \" + e.getMessage(), e);\n\n                cctx.discovery().reconnect();\n\n                reconnectNeeded = true;\n            }\n            else\n                U.warn(log, \"Local node received IgniteClientDisconnectedCheckedException or \" +\n                    \" IgniteNeedReconnectException exception but doesn't support reconnect, stopping node: \" +\n                    e.getMessage(), e);\n\n            return;\n        }\n        catch (IgniteCheckedException e) {\n            U.error(log, \"Failed to wait for completion of partition map exchange \" +\n                \"(preloading will not start): \" + task, e);\n\n            throw e;\n        }\n    }\n}",
        "reject_response": "private void body0() throws InterruptedException, IgniteCheckedException {\n    long timeout = cctx.gridConfig().getNetworkTimeout();\n\n    long cnt = 0;\n\n    while (!isCancelled()) {\n        onIdle();\n\n        cnt++;\n\n        CachePartitionExchangeWorkerTask task = null;\n\n        try {\n            boolean preloadFinished = true;\n\n            for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                if (grp.isLocal())\n                    continue;\n\n                preloadFinished &= grp.preloader() != null && grp.preloader().syncFuture().isDone();\n\n                if (!preloadFinished)\n                    break;\n            }\n\n            // If not first preloading and no more topology events present.\n            if (!cctx.kernalContext().clientNode() && !hasPendingExchange() && preloadFinished)\n                timeout = cctx.gridConfig().getNetworkTimeout();\n\n            // After workers line up and before preloading starts we initialize all futures.\n            if (log.isTraceEnabled()) {\n                Collection<IgniteInternalFuture> unfinished = new HashSet<>();\n\n                for (GridDhtPartitionsExchangeFuture fut : exchFuts.values()) {\n                    if (!fut.isDone())\n                        unfinished.add(fut);\n                }\n\n                log.debug(\"Before waiting for exchange futures [futs\" + unfinished + \", worker=\" + this + ']');\n            }\n\n            // Take next exchange future.\n            if (isCancelled())\n                Thread.currentThread().interrupt();\n\n            updateHeartbeat();\n\n            task = futQ.poll(timeout, MILLISECONDS);\n\n            updateHeartbeat();\n\n            if (task == null)\n                continue; // Main while loop.\n\n            if (!isExchangeTask(task)) {\n                processCustomTask(task);\n\n                continue;\n            }\n\n            busy = true;\n\n            Map<Integer, GridDhtPreloaderAssignments> assignsMap = null;\n\n            boolean forcePreload = false;\n\n            GridDhtPartitionExchangeId exchId;\n\n            GridDhtPartitionsExchangeFuture exchFut = null;\n\n            AffinityTopologyVersion resVer = null;\n\n            try {\n                if (isCancelled())\n                    break;\n\n                if (task instanceof RebalanceReassignExchangeTask)\n                    exchId = ((RebalanceReassignExchangeTask) task).exchangeId();\n                else if (task instanceof ForceRebalanceExchangeTask) {\n                    forcePreload = true;\n\n                    timeout = 0; // Force refresh.\n\n                    exchId = ((ForceRebalanceExchangeTask)task).exchangeId();\n                }\n                else {\n                    assert task instanceof GridDhtPartitionsExchangeFuture : task;\n\n                    exchFut = (GridDhtPartitionsExchangeFuture)task;\n\n                    exchId = exchFut.exchangeId();\n\n                    lastInitializedFut = exchFut;\n\n                    boolean newCrd = false;\n\n                    if (!crd) {\n                        List<ClusterNode> srvNodes = exchFut.firstEventCache().serverNodes();\n\n                        crd = newCrd = !srvNodes.isEmpty() && srvNodes.get(0).isLocal();\n                    }\n\n                    exchFut.init(newCrd);\n\n                    int dumpCnt = 0;\n\n                    long waitStart = U.currentTimeMillis();\n\n                    // Call rollback logic only for client node, for server nodes\n                    // rollback logic is in GridDhtPartitionsExchangeFuture.\n                    boolean txRolledBack = !cctx.localNode().isClient();\n\n                    IgniteConfiguration cfg = cctx.gridConfig();\n\n                    final long dumpTimeout = 2 * cfg.getNetworkTimeout();\n\n                    long nextDumpTime = 0;\n\n                    while (true) {\n                        // Read txTimeoutOnPME from configuration after every iteration.\n                        long curTimeout = cfg.getTransactionConfiguration().getTxTimeoutOnPartitionMapExchange();\n\n                        try {\n                            long exchTimeout = curTimeout > 0 && !txRolledBack\n                                ? Math.min(curTimeout, dumpTimeout)\n                                : dumpTimeout;\n\n                            blockingSectionEnd();\n\n                            try {\n                                resVer = exchFut.get(exchTimeout, TimeUnit.MILLISECONDS);\n                            } finally {\n                                blockingSectionEnd();\n                            }\n\n                            onIdle();\n\n                            break;\n                        }\n                        catch (IgniteFutureTimeoutCheckedException ignored) {\n                            updateHeartbeat();\n\n                            if (nextDumpTime <= U.currentTimeMillis()) {\n                                U.warn(diagnosticLog, \"Failed to wait for partition map exchange [\" +\n                                    \"topVer=\" + exchFut.initialVersion() +\n                                    \", node=\" + cctx.localNodeId() + \"]. \" +\n                                    (curTimeout <= 0 && !txRolledBack ? \"Consider changing \" +\n                                            \"TransactionConfiguration.txTimeoutOnPartitionMapSynchronization\" +\n                                            \" to non default value to avoid this message. \" : \"\") +\n                                    \"Dumping pending objects that might be the cause: \");\n\n                                try {\n                                    dumpDebugInfo(exchFut);\n                                }\n                                catch (Exception e) {\n                                    U.error(diagnosticLog, \"Failed to dump debug information: \" + e, e);\n                                }\n\n                                nextDumpTime = U.currentTimeMillis() + nextDumpTimeout(dumpCnt++, dumpTimeout);\n                            }\n\n                            if (!txRolledBack && curTimeout > 0 && U.currentTimeMillis() - waitStart >= curTimeout) {\n                                txRolledBack = true; // Try automatic rollback only once.\n\n                                cctx.tm().rollbackOnTopologyChange(exchFut.initialVersion());\n                            }\n                        }\n                        catch (Exception e) {\n                            if (exchFut.reconnectOnError(e))\n                                throw new IgniteNeedReconnectException(cctx.localNode(), e);\n\n                            throw e;\n                        }\n                    }\n\n                    removeMergedFutures(resVer, exchFut);\n\n                    if (log.isTraceEnabled())\n                        log.trace(\"After waiting for exchange future [exchFut=\" + exchFut + \", worker=\" +\n                            this + ']');\n\n                    if (exchFut.exchangeId().nodeId().equals(cctx.localNodeId()))\n                        lastRefresh.compareAndSet(-1, U.currentTimeMillis());\n\n                    // Just pick first worker to do this, so we don't\n                    // invoke topology callback more than once for the\n                    // same event.\n\n                    boolean changed = false;\n\n                    for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                        if (grp.isLocal())\n                            continue;\n\n                        if (grp.preloader().rebalanceRequired(rebTopVer, exchFut))\n                            rebTopVer = AffinityTopologyVersion.NONE;\n\n                        changed |= grp.topology().afterExchange(exchFut);\n                    }\n\n                    if (!cctx.kernalContext().clientNode() && changed && !hasPendingServerExchange())\n                        refreshPartitions();\n                }\n\n                // Schedule rebalance if force rebalance or force reassign occurs.\n                if (exchFut == null)\n                    rebTopVer = AffinityTopologyVersion.NONE;\n\n                if (!cctx.kernalContext().clientNode() && rebTopVer.equals(AffinityTopologyVersion.NONE)) {\n                    assignsMap = new HashMap<>();\n\n                    IgniteCacheSnapshotManager snp = cctx.snapshot();\n\n                    for (final CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                        long delay = grp.config().getRebalanceDelay();\n\n                        boolean disableRebalance = snp.partitionsAreFrozen(grp);\n\n                        GridDhtPreloaderAssignments assigns = null;\n\n                        // Don't delay for dummy reassigns to avoid infinite recursion.\n                        if ((delay == 0 || forcePreload) && !disableRebalance)\n                            assigns = grp.preloader().generateAssignments(exchId, exchFut);\n\n                        assignsMap.put(grp.groupId(), assigns);\n\n                        if (resVer == null && !grp.isLocal())\n                            resVer = grp.topology().readyTopologyVersion();\n                    }\n                }\n\n                if (resVer == null)\n                    resVer = exchId.topologyVersion();\n            }\n            finally {\n                // Must flip busy flag before assignments are given to demand workers.\n                busy = false;\n            }\n\n            if (assignsMap != null && rebTopVer.equals(AffinityTopologyVersion.NONE)) {\n                int size = assignsMap.size();\n\n                NavigableMap<Integer, List<Integer>> orderMap = new TreeMap<>();\n\n                for (Map.Entry<Integer, GridDhtPreloaderAssignments> e : assignsMap.entrySet()) {\n                    int grpId = e.getKey();\n\n                    CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                    int order = grp.config().getRebalanceOrder();\n\n                    if (orderMap.get(order) == null)\n                        orderMap.put(order, new ArrayList<Integer>(size));\n\n                    orderMap.get(order).add(grpId);\n                }\n\n                Runnable r = null;\n\n                List<String> rebList = new LinkedList<>();\n\n                boolean assignsCancelled = false;\n\n                GridCompoundFuture<Boolean, Boolean> forcedRebFut = null;\n\n                if (task instanceof ForceRebalanceExchangeTask)\n                    forcedRebFut = ((ForceRebalanceExchangeTask)task).forcedRebalanceFuture();\n\n                for (Integer order : orderMap.descendingKeySet()) {\n                    for (Integer grpId : orderMap.get(order)) {\n                        CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                        GridDhtPreloaderAssignments assigns = assignsMap.get(grpId);\n\n                        if (assigns != null)\n                            assignsCancelled |= assigns.cancelled();\n\n                        Runnable cur = grp.preloader().addAssignments(assigns,\n                            forcePreload,\n                            cnt,\n                            r,\n                            forcedRebFut);\n\n                        if (cur != null) {\n                            rebList.add(grp.cacheOrGroupName());\n\n                            r = cur;\n                        }\n                    }\n                }\n\n                if (forcedRebFut != null)\n                    forcedRebFut.markInitialized();\n\n                if (assignsCancelled || hasPendingExchange()) {\n                    U.log(log, \"Skipping rebalancing (obsolete exchange ID) \" +\n                        \"[top=\" + resVer + \", evt=\" + exchId.discoveryEventName() +\n                        \", node=\" + exchId.nodeId() + ']');\n                }\n                else if (r != null) {\n                    Collections.reverse(rebList);\n\n                    U.log(log, \"Rebalancing scheduled [order=\" + rebList +\n                        \", top=\" + resVer + \", force=\" + (exchFut == null) +\n                        \", evt=\" + exchId.discoveryEventName() +\n                        \", node=\" + exchId.nodeId() + ']');\n\n                    rebTopVer = resVer;\n\n                    // Start rebalancing cache groups chain. Each group will be rebalanced\n                    // sequentially one by one e.g.:\n                    // ignite-sys-cache -> cacheGroupR1 -> cacheGroupP2 -> cacheGroupR3\n                    r.run();\n                }\n                else\n                    U.log(log, \"Skipping rebalancing (nothing scheduled) \" +\n                        \"[top=\" + resVer + \", force=\" + (exchFut == null) +\n                        \", evt=\" + exchId.discoveryEventName() +\n                        \", node=\" + exchId.nodeId() + ']');\n            }\n            else\n                U.log(log, \"Skipping rebalancing (no affinity changes) \" +\n                    \"[top=\" + resVer +\n                    \", rebTopVer=\" + rebTopVer +\n                    \", evt=\" + exchId.discoveryEventName() +\n                    \", evtNode=\" + exchId.nodeId() +\n                    \", client=\" + cctx.kernalContext().clientNode() + ']');\n        }\n        catch (IgniteInterruptedCheckedException e) {\n            throw e;\n        }\n        catch (IgniteClientDisconnectedCheckedException | IgniteNeedReconnectException e) {\n            if (cctx.discovery().reconnectSupported()) {\n                U.warn(log, \"Local node failed to complete partition map exchange due to \" +\n                    \"exception, will try to reconnect to cluster: \" + e.getMessage(), e);\n\n                cctx.discovery().reconnect();\n\n                reconnectNeeded = true;\n            }\n            else\n                U.warn(log, \"Local node received IgniteClientDisconnectedCheckedException or \" +\n                    \" IgniteNeedReconnectException exception but doesn't support reconnect, stopping node: \" +\n                    e.getMessage(), e);\n\n            return;\n        }\n        catch (IgniteCheckedException e) {\n            U.error(log, \"Failed to wait for completion of partition map exchange \" +\n                \"(preloading will not start): \" + task, e);\n\n            throw e;\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2597,
        "instruction": "@Override\npublic LocatableInputSplit getNextInputSplit(String host) {\n\t// for a null host, we return an arbitrary split\n\tif (host == null) {\n\n\t\tsynchronized (this.unassigned) {\n\t\t\tIterator<LocatableInputSplit> iter = this.unassigned.iterator();\n\t\t\tif (iter.hasNext()) {\n\t\t\t\tLocatableInputSplit next = iter.next();\n\t\t\t\titer.remove();\n\n\t\t\t\tremoteAssignments++;\n\t\t\t\treturn next;\n\t\t\t} else {\n\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\tLOG.debug(\"No more unassigned input splits remaining.\");\n\t\t\t\t}\n\t\t\t\treturn null;\n\t\t\t}\n\t\t}\n\t}\n\n\thost = host.toLowerCase(Locale.US);\n\n\t// for any non-null host, we take the list of non-null splits\n\tList<LocatableInputSplit> localSplits = this.localPerHost.get(host);\n\n\t// if we have no list for this host yet, create one\n\tif (localSplits == null) {\n\t\tlocalSplits = new ArrayList<LocatableInputSplit>(16);\n\n\t\t// lock the list, to be sure that others have to wait for that host's local list\n\t\tsynchronized (localSplits) {\n\t\t\tList<LocatableInputSplit> prior = this.localPerHost.putIfAbsent(host, localSplits);\n\n\t\t\t// if someone else beat us in the case to create this list, then we do not populate this one, but\n\t\t\t// simply work with that other list\n\t\t\tif (prior == null) {\n\t\t\t\t// we are the first, we populate\n\n\t\t\t\t// first, copy the remaining splits to release the lock on the set early\n\t\t\t\t// because that is shared among threads\n\t\t\t\tLocatableInputSplit[] remaining;\n\t\t\t\tsynchronized (this.unassigned) {\n\t\t\t\t\tremaining = (LocatableInputSplit[]) this.unassigned.toArray(new LocatableInputSplit[this.unassigned.size()]);\n\t\t\t\t}\n\n\t\t\t\tfor (LocatableInputSplit is : remaining) {\n\t\t\t\t\tif (isLocal(host, is.getHostnames())) {\n\t\t\t\t\t\tlocalSplits.add(is);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\telse {\n\t\t\t\t// someone else was faster\n\t\t\t\tlocalSplits = prior;\n\t\t\t}\n\t\t}\n\t}\n\n\n\t// at this point, we have a list of local splits (possibly empty)\n\t// we need to make sure no one else operates in the current list (that protects against\n\t// list creation races) and that the unassigned set is consistent\n\t// NOTE: we need to obtain the locks in this order, strictly!!!\n\tsynchronized (localSplits) {\n\t\tint size = localSplits.size();\n\t\tif (size > 0) {\n\t\t\tsynchronized (this.unassigned) {\n\t\t\t\tdo {\n\t\t\t\t\t--size;\n\t\t\t\t\tLocatableInputSplit split = localSplits.remove(size);\n\t\t\t\t\tif (this.unassigned.remove(split)) {\n\n\t\t\t\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\t\t\t\tLOG.info(\"Assigning local split to host \" + host);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tlocalAssignments++;\n\t\t\t\t\t\treturn split;\n\t\t\t\t\t}\n\t\t\t\t} while (size > 0);\n\t\t\t}\n\t\t}\n\t}\n\n\t// we did not find a local split, return any\n\tsynchronized (this.unassigned) {\n\t\tIterator<LocatableInputSplit> iter = this.unassigned.iterator();\n\t\tif (iter.hasNext()) {\n\t\t\tLocatableInputSplit next = iter.next();\n\t\t\titer.remove();\n\n\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\tLOG.info(\"Assigning remote split to host \" + host);\n\t\t\t}\n\n\t\t\tremoteAssignments++;\n\t\t\treturn next;\n\t\t} else {\n\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n\t\t\t}\n\t\t\treturn null;\n\t\t}\n\t}\n}",
        "accept_response": "@Override\npublic LocatableInputSplit getNextInputSplit(String host) {\n\t// for a null host, we return an arbitrary split\n\tif (host == null) {\n\n\t\tsynchronized (this.unassigned) {\n\t\t\tIterator<LocatableInputSplit> iter = this.unassigned.iterator();\n\t\t\tif (iter.hasNext()) {\n\t\t\t\tLocatableInputSplit next = iter.next();\n\t\t\t\titer.remove();\n\n\t\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\t\tLOG.info(\"Assigning split to null host (random assignment).\");\n\t\t\t\t}\n\n\t\t\t\tremoteAssignments++;\n\t\t\t\treturn next;\n\t\t\t} else {\n\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\tLOG.debug(\"No more unassigned input splits remaining.\");\n\t\t\t\t}\n\t\t\t\treturn null;\n\t\t\t}\n\t\t}\n\t}\n\n\thost = host.toLowerCase(Locale.US);\n\n\t// for any non-null host, we take the list of non-null splits\n\tList<LocatableInputSplit> localSplits = this.localPerHost.get(host);\n\n\t// if we have no list for this host yet, create one\n\tif (localSplits == null) {\n\t\tlocalSplits = new ArrayList<LocatableInputSplit>(16);\n\n\t\t// lock the list, to be sure that others have to wait for that host's local list\n\t\tsynchronized (localSplits) {\n\t\t\tList<LocatableInputSplit> prior = this.localPerHost.putIfAbsent(host, localSplits);\n\n\t\t\t// if someone else beat us in the case to create this list, then we do not populate this one, but\n\t\t\t// simply work with that other list\n\t\t\tif (prior == null) {\n\t\t\t\t// we are the first, we populate\n\n\t\t\t\t// first, copy the remaining splits to release the lock on the set early\n\t\t\t\t// because that is shared among threads\n\t\t\t\tLocatableInputSplit[] remaining;\n\t\t\t\tsynchronized (this.unassigned) {\n\t\t\t\t\tremaining = (LocatableInputSplit[]) this.unassigned.toArray(new LocatableInputSplit[this.unassigned.size()]);\n\t\t\t\t}\n\n\t\t\t\tfor (LocatableInputSplit is : remaining) {\n\t\t\t\t\tif (isLocal(host, is.getHostnames())) {\n\t\t\t\t\t\tlocalSplits.add(is);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\telse {\n\t\t\t\t// someone else was faster\n\t\t\t\tlocalSplits = prior;\n\t\t\t}\n\t\t}\n\t}\n\n\n\t// at this point, we have a list of local splits (possibly empty)\n\t// we need to make sure no one else operates in the current list (that protects against\n\t// list creation races) and that the unassigned set is consistent\n\t// NOTE: we need to obtain the locks in this order, strictly!!!\n\tsynchronized (localSplits) {\n\t\tint size = localSplits.size();\n\t\tif (size > 0) {\n\t\t\tsynchronized (this.unassigned) {\n\t\t\t\tdo {\n\t\t\t\t\t--size;\n\t\t\t\t\tLocatableInputSplit split = localSplits.remove(size);\n\t\t\t\t\tif (this.unassigned.remove(split)) {\n\n\t\t\t\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\t\t\t\tLOG.info(\"Assigning local split to host \" + host);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tlocalAssignments++;\n\t\t\t\t\t\treturn split;\n\t\t\t\t\t}\n\t\t\t\t} while (size > 0);\n\t\t\t}\n\t\t}\n\t}\n\n\t// we did not find a local split, return any\n\tsynchronized (this.unassigned) {\n\t\tIterator<LocatableInputSplit> iter = this.unassigned.iterator();\n\t\tif (iter.hasNext()) {\n\t\t\tLocatableInputSplit next = iter.next();\n\t\t\titer.remove();\n\n\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\tLOG.info(\"Assigning remote split to host \" + host);\n\t\t\t}\n\n\t\t\tremoteAssignments++;\n\t\t\treturn next;\n\t\t} else {\n\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n\t\t\t}\n\t\t\treturn null;\n\t\t}\n\t}\n}",
        "reject_response": "@Override\npublic LocatableInputSplit getNextInputSplit(String host) {\n\t// for a null host, we return an arbitrary split\n\tif (host == null) {\n\n\t\tsynchronized (this.unassigned) {\n\t\t\tIterator<LocatableInputSplit> iter = this.unassigned.iterator();\n\t\t\tif (iter.hasNext()) {\n\t\t\t\tLocatableInputSplit next = iter.next();\n\t\t\t\titer.remove();\n\n\t\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\tLOG.debug(\"Assigning arbitrary split to null host.\");\n\t\t\t\t}\n\n\t\t\t\tremoteAssignments++;\n\t\t\t\treturn next;\n\t\t\t} else {\n\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\tLOG.debug(\"No more unassigned input splits remaining.\");\n\t\t\t\t}\n\t\t\t\treturn null;\n\t\t\t}\n\t\t}\n\t}\n\n\thost = host.toLowerCase(Locale.US);\n\n\t// for any non-null host, we take the list of non-null splits\n\tList<LocatableInputSplit> localSplits = this.localPerHost.get(host);\n\n\t// if we have no list for this host yet, create one\n\tif (localSplits == null) {\n\t\tlocalSplits = new ArrayList<LocatableInputSplit>(16);\n\n\t\t// lock the list, to be sure that others have to wait for that host's local list\n\t\tsynchronized (localSplits) {\n\t\t\tList<LocatableInputSplit> prior = this.localPerHost.putIfAbsent(host, localSplits);\n\n\t\t\t// if someone else beat us in the case to create this list, then we do not populate this one, but\n\t\t\t// simply work with that other list\n\t\t\tif (prior == null) {\n\t\t\t\t// we are the first, we populate\n\n\t\t\t\t// first, copy the remaining splits to release the lock on the set early\n\t\t\t\t// because that is shared among threads\n\t\t\t\tLocatableInputSplit[] remaining;\n\t\t\t\tsynchronized (this.unassigned) {\n\t\t\t\t\tremaining = (LocatableInputSplit[]) this.unassigned.toArray(new LocatableInputSplit[this.unassigned.size()]);\n\t\t\t\t}\n\n\t\t\t\tfor (LocatableInputSplit is : remaining) {\n\t\t\t\t\tif (isLocal(host, is.getHostnames())) {\n\t\t\t\t\t\tlocalSplits.add(is);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\telse {\n\t\t\t\t// someone else was faster\n\t\t\t\tlocalSplits = prior;\n\t\t\t}\n\t\t}\n\t}\n\n\n\t// at this point, we have a list of local splits (possibly empty)\n\t// we need to make sure no one else operates in the current list (that protects against\n\t// list creation races) and that the unassigned set is consistent\n\t// NOTE: we need to obtain the locks in this order, strictly!!!\n\tsynchronized (localSplits) {\n\t\tint size = localSplits.size();\n\t\tif (size > 0) {\n\t\t\tsynchronized (this.unassigned) {\n\t\t\t\tdo {\n\t\t\t\t\t--size;\n\t\t\t\t\tLocatableInputSplit split = localSplits.remove(size);\n\t\t\t\t\tif (this.unassigned.remove(split)) {\n\n\t\t\t\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\t\t\t\tLOG.info(\"Assigning local split to host \" + host);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tlocalAssignments++;\n\t\t\t\t\t\treturn split;\n\t\t\t\t\t}\n\t\t\t\t} while (size > 0);\n\t\t\t}\n\t\t}\n\t}\n\n\t// we did not find a local split, return any\n\tsynchronized (this.unassigned) {\n\t\tIterator<LocatableInputSplit> iter = this.unassigned.iterator();\n\t\tif (iter.hasNext()) {\n\t\t\tLocatableInputSplit next = iter.next();\n\t\t\titer.remove();\n\n\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\tLOG.info(\"Assigning remote split to host \" + host);\n\t\t\t}\n\n\t\t\tremoteAssignments++;\n\t\t\treturn next;\n\t\t} else {\n\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\tLOG.debug(\"No more input splits remaining.\");\n\t\t\t}\n\t\t\treturn null;\n\t\t}\n\t}\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2977,
        "instruction": "@Override\npublic void flush() {\n\tif (ostream != null) {\n\t\ttry {\n\t\t\tsynchronized (this) {\n\t\t\t\tif (ostream != null)\n\t\t\t\t\t// 1) PrinterWriter does not have bufferring of its own so\n\t\t\t\t\t// we need to flush its underlying stream\n\t\t\t\t\t// 2) HDFS flush() does not really flush all the way to disk.\n\t\t\t\t\tostream.hflush();\n\t\t\t\t\tlogger.info(\"Flush HDFS audit logs completed.....\");\n\t\t\t}\n\t\t} catch (IOException e) {\n\t\t\tlogger.error(\"Error on flushing log writer: \" + e.getMessage() +\n\t\t\t \"\\nException will be ignored. name=\" + getName() + \", fileName=\" + currentFileName);\n\t\t}\n\t}\n}",
        "accept_response": "@Override\npublic void flush() {\n\tlogger.info(\"Flush called. name=\" + getName());\n\tif (ostream != null) {\n\t\ttry {\n\t\t\tsynchronized (this) {\n\t\t\t\tif (ostream != null)\n\t\t\t\t\t// 1) PrinterWriter does not have bufferring of its own so\n\t\t\t\t\t// we need to flush its underlying stream\n\t\t\t\t\t// 2) HDFS flush() does not really flush all the way to disk.\n\t\t\t\t\tostream.hflush();\n\t\t\t\t\tlogger.info(\"Flush HDFS audit logs completed.....\");\n\t\t\t}\n\t\t} catch (IOException e) {\n\t\t\tlogger.error(\"Error on flushing log writer: \" + e.getMessage() +\n\t\t\t \"\\nException will be ignored. name=\" + getName() + \", fileName=\" + currentFileName);\n\t\t}\n\t}\n}",
        "reject_response": "@Override\npublic void flush() {\n\tif ( logWriter != null) {\n\t\tlogWriter.flush();\n\t\tlogger.info(\"Flush HDFS audit logs completed.....\");\n\t }\n\tif (ostream != null) {\n\t\ttry {\n\t\t\tsynchronized (this) {\n\t\t\t\tif (ostream != null)\n\t\t\t\t\t// 1) PrinterWriter does not have bufferring of its own so\n\t\t\t\t\t// we need to flush its underlying stream\n\t\t\t\t\t// 2) HDFS flush() does not really flush all the way to disk.\n\t\t\t\t\tostream.hflush();\n\t\t\t\t\tlogger.info(\"Flush HDFS audit logs completed.....\");\n\t\t\t}\n\t\t} catch (IOException e) {\n\t\t\tlogger.error(\"Error on flushing log writer: \" + e.getMessage() +\n\t\t\t \"\\nException will be ignored. name=\" + getName() + \", fileName=\" + currentFileName);\n\t\t}\n\t}\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2467,
        "instruction": "private void cleanupPools() {\n   OperationContextImpl.clearContext();\n\n   // We shutdown the global pools to give a better isolation between tests\n   try {\n      ServerLocatorImpl.clearThreadPools();\n   } catch (Throwable e) {\n      logger.info(threadDump(e.getMessage()));\n      System.err.println(threadDump(e.getMessage()));\n   }\n\n   try {\n      NettyConnector.clearThreadPools();\n   } catch (Exception e) {\n      System.err.println(threadDump(e.getMessage()));\n   }\n}",
        "accept_response": "private void cleanupPools() {\n   OperationContextImpl.clearContext();\n\n   // We shutdown the global pools to give a better isolation between tests\n   try {\n      ServerLocatorImpl.clearThreadPools();\n   } catch (Throwable e) {\n      logger.info(threadDump(e.getMessage()));\n      System.err.println(threadDump(e.getMessage()));\n   }\n\n   try {\n      NettyConnector.clearThreadPools();\n   } catch (Exception e) {\n      logger.info(threadDump(e.getMessage()));\n      System.err.println(threadDump(e.getMessage()));\n   }\n}",
        "reject_response": "private void cleanupPools() {\n   OperationContextImpl.clearContext();\n\n   // We shutdown the global pools to give a better isolation between tests\n   try {\n      ServerLocatorImpl.clearThreadPools();\n   } catch (Throwable e) {\n      logger.info(threadDump(e.getMessage()));\n      System.err.println(threadDump(e.getMessage()));\n   }\n\n   try {\n      NettyConnector.clearThreadPools();\n   } catch (Exception e) {\n      log.info(threadDump(e.getMessage()));\n      System.err.println(threadDump(e.getMessage()));\n   }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3208,
        "instruction": "@Override\npublic InterpreterResult interpret(String cmd, InterpreterContext contextInterpreter) {\n  long start = System.currentTimeMillis();\n  CommandLine cmdLine = CommandLine.parse(\"bash\");\n  cmdLine.addArgument(\"-c\", false);\n  cmdLine.addArgument(cmd, false);\n  DefaultExecutor executor = new DefaultExecutor();\n  ByteArrayOutputStream outputStream = new ByteArrayOutputStream();\n  executor.setStreamHandler(new PumpStreamHandler(outputStream));\n\n  executor.setWatchdog(new ExecuteWatchdog(commandTimeOut));\n  try {\n    int exitValue = executor.execute(cmdLine);\n    return new InterpreterResult(InterpreterResult.Code.SUCCESS, outputStream.toString());\n  } catch (ExecuteException e) {\n    logger.error(\"Can not run \" + cmd, e);\n    return new InterpreterResult(Code.ERROR, e.getMessage());\n  } catch (IOException e) {\n    logger.error(\"Can not run \" + cmd, e);\n    return new InterpreterResult(Code.ERROR, e.getMessage());\n  }\n}",
        "accept_response": "@Override\npublic InterpreterResult interpret(String cmd, InterpreterContext contextInterpreter) {\n  logger.debug(\"Run shell command '\" + cmd + \"'\");\n  long start = System.currentTimeMillis();\n  CommandLine cmdLine = CommandLine.parse(\"bash\");\n  cmdLine.addArgument(\"-c\", false);\n  cmdLine.addArgument(cmd, false);\n  DefaultExecutor executor = new DefaultExecutor();\n  ByteArrayOutputStream outputStream = new ByteArrayOutputStream();\n  executor.setStreamHandler(new PumpStreamHandler(outputStream));\n\n  executor.setWatchdog(new ExecuteWatchdog(commandTimeOut));\n  try {\n    int exitValue = executor.execute(cmdLine);\n    return new InterpreterResult(InterpreterResult.Code.SUCCESS, outputStream.toString());\n  } catch (ExecuteException e) {\n    logger.error(\"Can not run \" + cmd, e);\n    return new InterpreterResult(Code.ERROR, e.getMessage());\n  } catch (IOException e) {\n    logger.error(\"Can not run \" + cmd, e);\n    return new InterpreterResult(Code.ERROR, e.getMessage());\n  }\n}",
        "reject_response": "@Override\npublic InterpreterResult interpret(String cmd, InterpreterContext contextInterpreter) {\n  logger.info(\"Run shell command '\" + cmd + \"'\");\n  long start = System.currentTimeMillis();\n  CommandLine cmdLine = CommandLine.parse(\"bash\");\n  cmdLine.addArgument(\"-c\", false);\n  cmdLine.addArgument(cmd, false);\n  DefaultExecutor executor = new DefaultExecutor();\n  ByteArrayOutputStream outputStream = new ByteArrayOutputStream();\n  executor.setStreamHandler(new PumpStreamHandler(outputStream));\n\n  executor.setWatchdog(new ExecuteWatchdog(commandTimeOut));\n  try {\n    int exitValue = executor.execute(cmdLine);\n    return new InterpreterResult(InterpreterResult.Code.SUCCESS, outputStream.toString());\n  } catch (ExecuteException e) {\n    logger.error(\"Can not run \" + cmd, e);\n    return new InterpreterResult(Code.ERROR, e.getMessage());\n  } catch (IOException e) {\n    logger.error(\"Can not run \" + cmd, e);\n    return new InterpreterResult(Code.ERROR, e.getMessage());\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2695,
        "instruction": "@GET\n@Path(\"getNodesInfo\")\n@Produces(\"application/json\")\npublic Response read() throws Exception {\n    LOG.debug(\"getNodesInfo started\");\n    StringBuilder jsonOutput = new StringBuilder(\"{\\\"regions\\\":[\");\n    try {\n        /*\n         * 1. Initialize the HADOOP client side API for a distributed file\n         * system\n         */\n        Configuration conf = new Configuration();\n        FileSystem fs = FileSystem.get(conf);\n        DistributedFileSystem dfs = (DistributedFileSystem) fs;\n\n        /*\n         * 2. Query the namenode for the datanodes info. Only live nodes are\n         * returned - in accordance with the results returned by\n         * org.apache.hadoop.hdfs.tools.DFSAdmin#report().\n         */\n        DatanodeInfo[] liveNodes = dfs.getDataNodeStats(DatanodeReportType.LIVE);\n\n        /*\n         * 3. Pack the datanodes info in a JSON text format and write it to\n         * the HTTP output stream.\n         */\n        String prefix = \"\";\n        for (DatanodeInfo node : liveNodes) {\n            verifyNode(node);\n            // write one node to the HTTP stream\n            jsonOutput.append(prefix).append(writeNode(node));\n            prefix = \",\";\n        }\n        jsonOutput.append(\"]}\");\n    } catch (NodeDataException e) {\n        LOG.error(\"Nodes verification failed\", e);\n        throw e;\n    } catch (ClientAbortException e) {\n        LOG.error(\"Remote connection closed by HAWQ\", e);\n        throw e;\n    } catch (java.io.IOException e) {\n        LOG.error(\"Unhandled exception thrown\", e);\n        throw e;\n    }\n\n    return Response.ok(jsonOutput.toString(),\n            MediaType.APPLICATION_JSON_TYPE).build();\n}",
        "accept_response": "@GET\n@Path(\"getNodesInfo\")\n@Produces(\"application/json\")\npublic Response read() throws Exception {\n    LOG.debug(\"getNodesInfo started\");\n    StringBuilder jsonOutput = new StringBuilder(\"{\\\"regions\\\":[\");\n    try {\n        /*\n         * 1. Initialize the HADOOP client side API for a distributed file\n         * system\n         */\n        Configuration conf = new Configuration();\n        FileSystem fs = FileSystem.get(conf);\n        DistributedFileSystem dfs = (DistributedFileSystem) fs;\n\n        /*\n         * 2. Query the namenode for the datanodes info. Only live nodes are\n         * returned - in accordance with the results returned by\n         * org.apache.hadoop.hdfs.tools.DFSAdmin#report().\n         */\n        DatanodeInfo[] liveNodes = dfs.getDataNodeStats(DatanodeReportType.LIVE);\n\n        /*\n         * 3. Pack the datanodes info in a JSON text format and write it to\n         * the HTTP output stream.\n         */\n        String prefix = \"\";\n        for (DatanodeInfo node : liveNodes) {\n            verifyNode(node);\n            // write one node to the HTTP stream\n            jsonOutput.append(prefix).append(writeNode(node));\n            prefix = \",\";\n        }\n        jsonOutput.append(\"]}\");\n        LOG.debug(\"getNodesCluster output: \" + jsonOutput);\n    } catch (NodeDataException e) {\n        LOG.error(\"Nodes verification failed\", e);\n        throw e;\n    } catch (ClientAbortException e) {\n        LOG.error(\"Remote connection closed by HAWQ\", e);\n        throw e;\n    } catch (java.io.IOException e) {\n        LOG.error(\"Unhandled exception thrown\", e);\n        throw e;\n    }\n\n    return Response.ok(jsonOutput.toString(),\n            MediaType.APPLICATION_JSON_TYPE).build();\n}",
        "reject_response": "@GET\n@Path(\"getNodesInfo\")\n@Produces(\"application/json\")\npublic Response read() throws Exception {\n    LOG.debug(\"getNodesInfo started\");\n    StringBuilder jsonOutput = new StringBuilder(\"{\\\"regions\\\":[\");\n    try {\n        /*\n         * 1. Initialize the HADOOP client side API for a distributed file\n         * system\n         */\n        Configuration conf = new Configuration();\n        FileSystem fs = FileSystem.get(conf);\n        DistributedFileSystem dfs = (DistributedFileSystem) fs;\n\n        /*\n         * 2. Query the namenode for the datanodes info. Only live nodes are\n         * returned - in accordance with the results returned by\n         * org.apache.hadoop.hdfs.tools.DFSAdmin#report().\n         */\n        DatanodeInfo[] liveNodes = dfs.getDataNodeStats(DatanodeReportType.LIVE);\n\n        /*\n         * 3. Pack the datanodes info in a JSON text format and write it to\n         * the HTTP output stream.\n         */\n        String prefix = \"\";\n        for (DatanodeInfo node : liveNodes) {\n            verifyNode(node);\n            // write one node to the HTTP stream\n            jsonOutput.append(prefix).append(writeNode(node));\n            prefix = \",\";\n        }\n        jsonOutput.append(\"]}\");\n        Log.debug(\"getNodesCluster output: \" + jsonOutput);\n    } catch (NodeDataException e) {\n        LOG.error(\"Nodes verification failed\", e);\n        throw e;\n    } catch (ClientAbortException e) {\n        LOG.error(\"Remote connection closed by HAWQ\", e);\n        throw e;\n    } catch (java.io.IOException e) {\n        LOG.error(\"Unhandled exception thrown\", e);\n        throw e;\n    }\n\n    return Response.ok(jsonOutput.toString(),\n            MediaType.APPLICATION_JSON_TYPE).build();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3102,
        "instruction": "private void checkRead() throws IOException {\n  if (current == totalCountLoadedSoFar) {\n    if (current != 0) {\n      long timeAssembling = System.currentTimeMillis() - startedAssemblingCurrentBlockAt;\n      totalTimeSpentProcessingRecords += timeAssembling;\n      if (DEBUG) LOG.debug(\"Assembled and processed \" + totalCountLoadedSoFar + \" records from \" + columnCount + \" columns in \" + totalTimeSpentProcessingRecords + \" ms: \" + ((float) totalCountLoadedSoFar / totalTimeSpentProcessingRecords) + \" rec/ms, \" + ((float) totalCountLoadedSoFar * columnCount / totalTimeSpentProcessingRecords) + \" cell/ms\");\n      long totalTime = totalTimeSpentProcessingRecords + totalTimeSpentReadingBytes;\n      long percentReading = 100 * totalTimeSpentReadingBytes / totalTime;\n      long percentProcessing = 100 * totalTimeSpentProcessingRecords / totalTime;\n      if (DEBUG) LOG.debug(\"time spent so far \" + percentReading + \"% reading (\"+totalTimeSpentReadingBytes+\" ms) and \" + percentProcessing + \"% processing (\"+totalTimeSpentProcessingRecords+\" ms)\");\n    }\n\n    if (DEBUG) LOG.debug(\"at row \" + current + \". reading next block\");\n    long t0 = System.currentTimeMillis();\n    PageReadStore pages = reader.readNextRowGroup();\n    if (pages == null) {\n      throw new IOException(\"expecting more rows but reached last block. Read \" + current + \" out of \" + total);\n    }\n    long timeSpentReading = System.currentTimeMillis() - t0;\n    totalTimeSpentReadingBytes += timeSpentReading;\n    BenchmarkCounter.incrementTime(timeSpentReading);\n    if (DEBUG) {\n      LOG.debug(\"initializing Record assembly with requested schema \" + requestedSchema);\n    }\n    MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema);\n    recordReader = columnIO.getRecordReader(pages, recordConverter, recordFilter);\n    startedAssemblingCurrentBlockAt = System.currentTimeMillis();\n    totalCountLoadedSoFar += pages.getRowCount();\n    ++ currentBlock;\n  }\n}",
        "accept_response": "private void checkRead() throws IOException {\n  if (current == totalCountLoadedSoFar) {\n    if (current != 0) {\n      long timeAssembling = System.currentTimeMillis() - startedAssemblingCurrentBlockAt;\n      totalTimeSpentProcessingRecords += timeAssembling;\n      if (DEBUG) LOG.debug(\"Assembled and processed \" + totalCountLoadedSoFar + \" records from \" + columnCount + \" columns in \" + totalTimeSpentProcessingRecords + \" ms: \" + ((float) totalCountLoadedSoFar / totalTimeSpentProcessingRecords) + \" rec/ms, \" + ((float) totalCountLoadedSoFar * columnCount / totalTimeSpentProcessingRecords) + \" cell/ms\");\n      long totalTime = totalTimeSpentProcessingRecords + totalTimeSpentReadingBytes;\n      long percentReading = 100 * totalTimeSpentReadingBytes / totalTime;\n      long percentProcessing = 100 * totalTimeSpentProcessingRecords / totalTime;\n      if (DEBUG) LOG.debug(\"time spent so far \" + percentReading + \"% reading (\"+totalTimeSpentReadingBytes+\" ms) and \" + percentProcessing + \"% processing (\"+totalTimeSpentProcessingRecords+\" ms)\");\n    }\n\n    if (DEBUG) LOG.debug(\"at row \" + current + \". reading next block\");\n    long t0 = System.currentTimeMillis();\n    PageReadStore pages = reader.readNextRowGroup();\n    if (pages == null) {\n      throw new IOException(\"expecting more rows but reached last block. Read \" + current + \" out of \" + total);\n    }\n    long timeSpentReading = System.currentTimeMillis() - t0;\n    totalTimeSpentReadingBytes += timeSpentReading;\n    BenchmarkCounter.incrementTime(timeSpentReading);\n    if (DEBUG) {\n      LOG.debug(\"block read in memory in \" + timeSpentReading + \" ms. row count = \" + pages.getRowCount());\n      LOG.debug(\"initializing Record assembly with requested schema \" + requestedSchema);\n    }\n    MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema);\n    recordReader = columnIO.getRecordReader(pages, recordConverter, recordFilter);\n    startedAssemblingCurrentBlockAt = System.currentTimeMillis();\n    totalCountLoadedSoFar += pages.getRowCount();\n    ++ currentBlock;\n  }\n}",
        "reject_response": "private void checkRead() throws IOException {\n  if (current == totalCountLoadedSoFar) {\n    if (current != 0) {\n      long timeAssembling = System.currentTimeMillis() - startedAssemblingCurrentBlockAt;\n      totalTimeSpentProcessingRecords += timeAssembling;\n      if (DEBUG) LOG.debug(\"Assembled and processed \" + totalCountLoadedSoFar + \" records from \" + columnCount + \" columns in \" + totalTimeSpentProcessingRecords + \" ms: \" + ((float) totalCountLoadedSoFar / totalTimeSpentProcessingRecords) + \" rec/ms, \" + ((float) totalCountLoadedSoFar * columnCount / totalTimeSpentProcessingRecords) + \" cell/ms\");\n      long totalTime = totalTimeSpentProcessingRecords + totalTimeSpentReadingBytes;\n      long percentReading = 100 * totalTimeSpentReadingBytes / totalTime;\n      long percentProcessing = 100 * totalTimeSpentProcessingRecords / totalTime;\n      if (DEBUG) LOG.debug(\"time spent so far \" + percentReading + \"% reading (\"+totalTimeSpentReadingBytes+\" ms) and \" + percentProcessing + \"% processing (\"+totalTimeSpentProcessingRecords+\" ms)\");\n    }\n\n    if (DEBUG) LOG.debug(\"at row \" + current + \". reading next block\");\n    long t0 = System.currentTimeMillis();\n    PageReadStore pages = reader.readNextRowGroup();\n    if (pages == null) {\n      throw new IOException(\"expecting more rows but reached last block. Read \" + current + \" out of \" + total);\n    }\n    long timeSpentReading = System.currentTimeMillis() - t0;\n    totalTimeSpentReadingBytes += timeSpentReading;\n    BenchmarkCounter.incrementTime(timeSpentReading);\n    if (DEBUG) {\n    LOG.info(\"block read in memory in \" + timeSpentReading + \" ms. row count = \" + pages.getRowCount());\n    if (Log.DEBUG) LOG.debug(\"initializing Record assembly with requested schema \" + requestedSchema);\n      LOG.debug(\"initializing Record assembly with requested schema \" + requestedSchema);\n    }\n    MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema);\n    recordReader = columnIO.getRecordReader(pages, recordConverter, recordFilter);\n    startedAssemblingCurrentBlockAt = System.currentTimeMillis();\n    totalCountLoadedSoFar += pages.getRowCount();\n    ++ currentBlock;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2795,
        "instruction": "private String getServicePrincipalName(final HttpRequest request, final HttpClientContext clientContext) {\n    String spn = null;\n    if (this.servicePrincipalName != null) {\n        spn = this.servicePrincipalName;\n    } else if (challengeType == ChallengeType.PROXY) {\n        final RouteInfo route = clientContext.getHttpRoute();\n        if (route != null) {\n            spn = \"HTTP/\" + route.getProxyHost().getHostName();\n        } else {\n            // Should not happen\n            spn = null;\n        }\n    } else {\n        final URIAuthority authority = request.getAuthority();\n        if (authority != null) {\n            spn = \"HTTP/\" + authority.getHostName();\n        } else {\n            final RouteInfo route = clientContext.getHttpRoute();\n            if (route != null) {\n                spn = \"HTTP/\" + route.getTargetHost().getHostName();\n            } else {\n                // Should not happen\n                spn = null;\n            }\n        }\n    }\n    return spn;\n}",
        "accept_response": "private String getServicePrincipalName(final HttpRequest request, final HttpClientContext clientContext) {\n    String spn = null;\n    if (this.servicePrincipalName != null) {\n        spn = this.servicePrincipalName;\n    } else if (challengeType == ChallengeType.PROXY) {\n        final RouteInfo route = clientContext.getHttpRoute();\n        if (route != null) {\n            spn = \"HTTP/\" + route.getProxyHost().getHostName();\n        } else {\n            // Should not happen\n            spn = null;\n        }\n    } else {\n        final URIAuthority authority = request.getAuthority();\n        if (authority != null) {\n            spn = \"HTTP/\" + authority.getHostName();\n        } else {\n            final RouteInfo route = clientContext.getHttpRoute();\n            if (route != null) {\n                spn = \"HTTP/\" + route.getTargetHost().getHostName();\n            } else {\n                // Should not happen\n                spn = null;\n            }\n        }\n    }\n    if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Using SPN: {}\", spn);\n    }\n    return spn;\n}",
        "reject_response": "private String getServicePrincipalName(final HttpRequest request, final HttpClientContext clientContext) {\n    String spn = null;\n    if (this.servicePrincipalName != null) {\n        spn = this.servicePrincipalName;\n    } else if (challengeType == ChallengeType.PROXY) {\n        final RouteInfo route = clientContext.getHttpRoute();\n        if (route != null) {\n            spn = \"HTTP/\" + route.getProxyHost().getHostName();\n        } else {\n            // Should not happen\n            spn = null;\n        }\n    } else {\n        final URIAuthority authority = request.getAuthority();\n        if (authority != null) {\n            spn = \"HTTP/\" + authority.getHostName();\n        } else {\n            final RouteInfo route = clientContext.getHttpRoute();\n            if (route != null) {\n                spn = \"HTTP/\" + route.getTargetHost().getHostName();\n            } else {\n                // Should not happen\n                spn = null;\n            }\n        }\n    }\n    if (LOG.isDebugEnabled()) {\n    if (this.log.isDebugEnabled()) {\n        this.log.debug(\"Using SPN: {}\", spn);\n    }\n    return spn;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3128,
        "instruction": "@Override\npublic void serviceStop() throws Exception {\n  if (isSession) {\n    sessionStopped.set(true);\n  }\n  synchronized (this) {\n    if (this.dagSubmissionTimer != null) {\n      this.dagSubmissionTimer.cancel();\n    }\n    if (this.clientAMHeartBeatTimeoutService != null) {\n      this.clientAMHeartBeatTimeoutService.shutdownNow();\n    }\n    // release all the held containers before stop services TEZ-2687\n    initiateStop();\n    stopServices();\n\n    // Given pre-emption, we should delete tez scratch dir only if unregister is\n    // successful\n    boolean deleteTezScratchData = this.amConf.getBoolean(\n        TezConfiguration.TEZ_AM_STAGING_SCRATCH_DATA_AUTO_DELETE,\n        TezConfiguration.TEZ_AM_STAGING_SCRATCH_DATA_AUTO_DELETE_DEFAULT);\n    if (deleteTezScratchData && this.taskSchedulerManager != null\n        && this.taskSchedulerManager.hasUnregistered()) {\n      // Delete tez scratch data dir\n      if (this.tezSystemStagingDir != null) {\n        try {\n          this.appMasterUgi.doAs(new PrivilegedExceptionAction<Void>() {\n            @Override\n            public Void run() throws Exception {\n              FileSystem fs = tezSystemStagingDir.getFileSystem(amConf);\n              boolean deletedStagingDir = fs.delete(tezSystemStagingDir, true);\n              if (!deletedStagingDir) {\n                LOG.warn(\"Failed to delete tez scratch data dir, path=\"\n                    + tezSystemStagingDir);\n              } else {\n                LOG.info(\"Completed deletion of tez scratch data dir, path=\"\n                    + tezSystemStagingDir);\n              }\n              return null;\n            }\n          });\n        } catch (IOException e) {\n          // Best effort to delete tez scratch data dir\n          LOG.warn(\"Failed to delete tez scratch data dir\", e);\n        }\n      }\n    }\n\n    if (execService != null) {\n      execService.shutdownNow();\n    }\n\n    super.serviceStop();\n  }\n}",
        "accept_response": "@Override\npublic void serviceStop() throws Exception {\n  if (isSession) {\n    sessionStopped.set(true);\n  }\n  synchronized (this) {\n    if (this.dagSubmissionTimer != null) {\n      this.dagSubmissionTimer.cancel();\n    }\n    if (this.clientAMHeartBeatTimeoutService != null) {\n      this.clientAMHeartBeatTimeoutService.shutdownNow();\n    }\n    // release all the held containers before stop services TEZ-2687\n    initiateStop();\n    stopServices();\n\n    // Given pre-emption, we should delete tez scratch dir only if unregister is\n    // successful\n    boolean deleteTezScratchData = this.amConf.getBoolean(\n        TezConfiguration.TEZ_AM_STAGING_SCRATCH_DATA_AUTO_DELETE,\n        TezConfiguration.TEZ_AM_STAGING_SCRATCH_DATA_AUTO_DELETE_DEFAULT);\n    LOG.debug(\"Checking whether tez scratch data dir should be deleted, deleteTezScratchData={}\",\n      deleteTezScratchData);\n    if (deleteTezScratchData && this.taskSchedulerManager != null\n        && this.taskSchedulerManager.hasUnregistered()) {\n      // Delete tez scratch data dir\n      if (this.tezSystemStagingDir != null) {\n        try {\n          this.appMasterUgi.doAs(new PrivilegedExceptionAction<Void>() {\n            @Override\n            public Void run() throws Exception {\n              FileSystem fs = tezSystemStagingDir.getFileSystem(amConf);\n              boolean deletedStagingDir = fs.delete(tezSystemStagingDir, true);\n              if (!deletedStagingDir) {\n                LOG.warn(\"Failed to delete tez scratch data dir, path=\"\n                    + tezSystemStagingDir);\n              } else {\n                LOG.info(\"Completed deletion of tez scratch data dir, path=\"\n                    + tezSystemStagingDir);\n              }\n              return null;\n            }\n          });\n        } catch (IOException e) {\n          // Best effort to delete tez scratch data dir\n          LOG.warn(\"Failed to delete tez scratch data dir\", e);\n        }\n      }\n    }\n\n    if (execService != null) {\n      execService.shutdownNow();\n    }\n\n    super.serviceStop();\n  }\n}",
        "reject_response": "@Override\npublic void serviceStop() throws Exception {\n  if (isSession) {\n    sessionStopped.set(true);\n  }\n  synchronized (this) {\n    if (this.dagSubmissionTimer != null) {\n      this.dagSubmissionTimer.cancel();\n    }\n    if (this.clientAMHeartBeatTimeoutService != null) {\n      this.clientAMHeartBeatTimeoutService.shutdownNow();\n    }\n    // release all the held containers before stop services TEZ-2687\n    initiateStop();\n    stopServices();\n\n    // Given pre-emption, we should delete tez scratch dir only if unregister is\n    // successful\n    boolean deleteTezScratchData = this.amConf.getBoolean(\n        TezConfiguration.TEZ_AM_STAGING_SCRATCH_DATA_AUTO_DELETE,\n        TezConfiguration.TEZ_AM_STAGING_SCRATCH_DATA_AUTO_DELETE_DEFAULT);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Checking whether tez scratch data dir should be deleted, deleteTezScratchData=\"\n          + deleteTezScratchData);\n    }\n    if (deleteTezScratchData && this.taskSchedulerManager != null\n        && this.taskSchedulerManager.hasUnregistered()) {\n      // Delete tez scratch data dir\n      if (this.tezSystemStagingDir != null) {\n        try {\n          this.appMasterUgi.doAs(new PrivilegedExceptionAction<Void>() {\n            @Override\n            public Void run() throws Exception {\n              FileSystem fs = tezSystemStagingDir.getFileSystem(amConf);\n              boolean deletedStagingDir = fs.delete(tezSystemStagingDir, true);\n              if (!deletedStagingDir) {\n                LOG.warn(\"Failed to delete tez scratch data dir, path=\"\n                    + tezSystemStagingDir);\n              } else {\n                LOG.info(\"Completed deletion of tez scratch data dir, path=\"\n                    + tezSystemStagingDir);\n              }\n              return null;\n            }\n          });\n        } catch (IOException e) {\n          // Best effort to delete tez scratch data dir\n          LOG.warn(\"Failed to delete tez scratch data dir\", e);\n        }\n      }\n    }\n\n    if (execService != null) {\n      execService.shutdownNow();\n    }\n\n    super.serviceStop();\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3033,
        "instruction": "protected void notifyDistributed(String pubAgentName, DistributionQueueItem queueItem) {\n    sendEvt(pubAgentName, queueItem);\n    if (sendMsg) {\n        sendMsg(pubAgentName, queueItem);\n    }\n}",
        "accept_response": "protected void notifyDistributed(String pubAgentName, DistributionQueueItem queueItem) {\n    LOG.debug(\"Sending distributed notifications for pub agent {} queue item {}\", pubAgentName, queueItem.getPackageId());\n    sendEvt(pubAgentName, queueItem);\n    if (sendMsg) {\n        sendMsg(pubAgentName, queueItem);\n    }\n}",
        "reject_response": "protected void notifyDistributed(String pubAgentName, DistributionQueueItem queueItem) {\n    LOG.info(\"Sending distributed notifications for pub agent {} queue item {}\", pubAgentName, queueItem.getPackageId());\n    sendEvt(pubAgentName, queueItem);\n    if (sendMsg) {\n        sendMsg(pubAgentName, queueItem);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2761,
        "instruction": "@Override\npublic int read(byte[] array, final int arrayOffset, final int len) throws IOException {\n  long readStartPos = position;\n  DiskRangeList drl = new DiskRangeList(readStartPos, readStartPos + len);\n  DataCache.BooleanRef gotAllData = new DataCache.BooleanRef();\n  drl = cache.getFileData(fileKey, drl, 0, new DataCache.DiskRangeListFactory() {\n    @Override\n    public DiskRangeList createCacheChunk(\n        MemoryBuffer buffer, long startOffset, long endOffset) {\n      return new CacheChunk(buffer, startOffset, endOffset);\n    }\n  }, gotAllData);\n  if (gotAllData.value) {\n    long sizeRead = 0;\n    while (drl != null) {\n      assert drl.hasData();\n      long from = drl.getOffset(), to = drl.getEnd();\n      int offsetFromReadStart = (int)(from - readStartPos), candidateSize = (int)(to - from);\n      ByteBuffer data = drl.getData().duplicate();\n      data.get(array, arrayOffset + offsetFromReadStart, candidateSize);\n      cache.releaseBuffer(((CacheChunk)drl).getBuffer());\n      sizeRead += candidateSize;\n      drl = drl.next;\n    }\n    validateAndUpdatePosition(len, sizeRead);\n    return len;\n  }\n  int maxAlloc = cache.getAllocator().getMaxAllocation();\n  // We have some disk data. Separate it by column chunk and put into cache.\n\n  // We started with a single DRL, so we assume there will be no consecutive missing blocks\n  // after the cache has inserted cache data. We also assume all the missing parts will\n  // represent one or several column chunks, since we always cache on column chunk boundaries.\n  DiskRangeList current = drl;\n  FileSystem fs = path.getFileSystem(conf);\n  FSDataInputStream is = fs.open(path, bufferSize);\n  Allocator allocator = cache.getAllocator();\n  long sizeRead = 0;\n  while (current != null) {\n    DiskRangeList candidate = current;\n    current = current.next;\n    long from = candidate.getOffset(), to = candidate.getEnd();\n    // The offset in the destination array for the beginning of this missing range.\n    int offsetFromReadStart = (int)(from - readStartPos), candidateSize = (int)(to - from);\n    if (candidate.hasData()) {\n      ByteBuffer data = candidate.getData().duplicate();\n      data.get(array, arrayOffset + offsetFromReadStart, candidateSize);\n      cache.releaseBuffer(((CacheChunk)candidate).getBuffer());\n      sizeRead += candidateSize;\n      continue;\n    }\n    // The data is not in cache.\n\n    // Account for potential partial chunks.\n    SortedMap<Long, Long> chunksInThisRead = getAndValidateMissingChunks(maxAlloc, from, to);\n\n    is.seek(from);\n    is.readFully(array, arrayOffset + offsetFromReadStart, candidateSize);\n    sizeRead += candidateSize;\n    // Now copy missing chunks (and parts of chunks) into cache buffers.\n    if (fileKey == null || cache == null) continue;\n    int extraDiskDataOffset = 0;\n    // TODO: should we try to make a giant array for one cache call to avoid overhead?\n    for (Map.Entry<Long, Long> missingChunk : chunksInThisRead.entrySet()) {\n      long chunkFrom = Math.max(from, missingChunk.getKey()),\n          chunkTo = Math.min(to, missingChunk.getValue()),\n          chunkLength = chunkTo - chunkFrom;\n      // TODO: if we allow partial reads (right now we disable this), we'd have to handle it here.\n      //       chunksInThisRead should probably be changed to be a struct array indicating both\n      //       partial and full sizes for each chunk; then the partial ones could be merged\n      //       with the previous partial ones, and any newly-full chunks put in the cache.\n      MemoryBuffer[] largeBuffers = null, smallBuffer = null, newCacheData = null;\n      try {\n        int largeBufCount = (int) (chunkLength / maxAlloc);\n        int smallSize = (int) (chunkLength % maxAlloc);\n        int chunkPartCount = largeBufCount + ((smallSize > 0) ? 1 : 0);\n        DiskRange[] cacheRanges = new DiskRange[chunkPartCount];\n        int extraOffsetInChunk = 0;\n        if (maxAlloc < chunkLength) {\n          largeBuffers = new MemoryBuffer[largeBufCount];\n          // Note: we don't use StoppableAllocator here - this is not on an IO thread.\n          allocator.allocateMultiple(largeBuffers, maxAlloc, cache.getDataBufferFactory());\n          for (int i = 0; i < largeBuffers.length; ++i) {\n            // By definition here we copy up to the limit of the buffer.\n            ByteBuffer bb = largeBuffers[i].getByteBufferRaw();\n            int remaining = bb.remaining();\n            assert remaining == maxAlloc;\n            copyDiskDataToCacheBuffer(array,\n                arrayOffset + offsetFromReadStart + extraDiskDataOffset,\n                remaining, bb, cacheRanges, i, chunkFrom + extraOffsetInChunk);\n            extraDiskDataOffset += remaining;\n            extraOffsetInChunk += remaining;\n          }\n        }\n        newCacheData = largeBuffers;\n        largeBuffers = null;\n        if (smallSize > 0) {\n          smallBuffer = new MemoryBuffer[1];\n          // Note: we don't use StoppableAllocator here - this is not on an IO thread.\n          allocator.allocateMultiple(smallBuffer, smallSize, cache.getDataBufferFactory());\n          ByteBuffer bb = smallBuffer[0].getByteBufferRaw();\n          copyDiskDataToCacheBuffer(array,\n              arrayOffset + offsetFromReadStart + extraDiskDataOffset,\n              smallSize, bb, cacheRanges, largeBufCount, chunkFrom + extraOffsetInChunk);\n          extraDiskDataOffset += smallSize;\n          extraOffsetInChunk += smallSize; // Not strictly necessary, noone will look at it.\n          if (newCacheData == null) {\n            newCacheData = smallBuffer;\n          } else {\n            // TODO: add allocate overload with an offset and length\n            MemoryBuffer[] combinedCacheData = new MemoryBuffer[largeBufCount + 1];\n            System.arraycopy(newCacheData, 0, combinedCacheData, 0, largeBufCount);\n            newCacheData = combinedCacheData;\n            newCacheData[largeBufCount] = smallBuffer[0];\n          }\n          smallBuffer = null;\n        }\n        cache.putFileData(fileKey, cacheRanges, newCacheData, 0, tag);\n      } finally {\n        // We do not use the new cache buffers for the actual read, given the way read() API is.\n        // Therefore, we don't need to handle cache collisions - just decref all the buffers.\n        if (newCacheData != null) {\n          for (MemoryBuffer buffer : newCacheData) {\n            if (buffer == null) continue;\n            cache.releaseBuffer(buffer);\n          }\n        }\n        // If we have failed before building newCacheData, deallocate other the allocated.\n        if (largeBuffers != null) {\n          for (MemoryBuffer buffer : largeBuffers) {\n            if (buffer == null) continue;\n            allocator.deallocate(buffer);\n          }\n        }\n        if (smallBuffer != null && smallBuffer[0] != null) {\n          allocator.deallocate(smallBuffer[0]);\n        }\n      }\n    }\n  }\n  validateAndUpdatePosition(len, sizeRead);\n  return len;\n}",
        "accept_response": "@Override\npublic int read(byte[] array, final int arrayOffset, final int len) throws IOException {\n  long readStartPos = position;\n  DiskRangeList drl = new DiskRangeList(readStartPos, readStartPos + len);\n  DataCache.BooleanRef gotAllData = new DataCache.BooleanRef();\n  drl = cache.getFileData(fileKey, drl, 0, new DataCache.DiskRangeListFactory() {\n    @Override\n    public DiskRangeList createCacheChunk(\n        MemoryBuffer buffer, long startOffset, long endOffset) {\n      return new CacheChunk(buffer, startOffset, endOffset);\n    }\n  }, gotAllData);\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"Buffers after cache \" + RecordReaderUtils.stringifyDiskRanges(drl));\n  }\n  if (gotAllData.value) {\n    long sizeRead = 0;\n    while (drl != null) {\n      assert drl.hasData();\n      long from = drl.getOffset(), to = drl.getEnd();\n      int offsetFromReadStart = (int)(from - readStartPos), candidateSize = (int)(to - from);\n      ByteBuffer data = drl.getData().duplicate();\n      data.get(array, arrayOffset + offsetFromReadStart, candidateSize);\n      cache.releaseBuffer(((CacheChunk)drl).getBuffer());\n      sizeRead += candidateSize;\n      drl = drl.next;\n    }\n    validateAndUpdatePosition(len, sizeRead);\n    return len;\n  }\n  int maxAlloc = cache.getAllocator().getMaxAllocation();\n  // We have some disk data. Separate it by column chunk and put into cache.\n\n  // We started with a single DRL, so we assume there will be no consecutive missing blocks\n  // after the cache has inserted cache data. We also assume all the missing parts will\n  // represent one or several column chunks, since we always cache on column chunk boundaries.\n  DiskRangeList current = drl;\n  FileSystem fs = path.getFileSystem(conf);\n  FSDataInputStream is = fs.open(path, bufferSize);\n  Allocator allocator = cache.getAllocator();\n  long sizeRead = 0;\n  while (current != null) {\n    DiskRangeList candidate = current;\n    current = current.next;\n    long from = candidate.getOffset(), to = candidate.getEnd();\n    // The offset in the destination array for the beginning of this missing range.\n    int offsetFromReadStart = (int)(from - readStartPos), candidateSize = (int)(to - from);\n    if (candidate.hasData()) {\n      ByteBuffer data = candidate.getData().duplicate();\n      data.get(array, arrayOffset + offsetFromReadStart, candidateSize);\n      cache.releaseBuffer(((CacheChunk)candidate).getBuffer());\n      sizeRead += candidateSize;\n      continue;\n    }\n    // The data is not in cache.\n\n    // Account for potential partial chunks.\n    SortedMap<Long, Long> chunksInThisRead = getAndValidateMissingChunks(maxAlloc, from, to);\n\n    is.seek(from);\n    is.readFully(array, arrayOffset + offsetFromReadStart, candidateSize);\n    sizeRead += candidateSize;\n    // Now copy missing chunks (and parts of chunks) into cache buffers.\n    if (fileKey == null || cache == null) continue;\n    int extraDiskDataOffset = 0;\n    // TODO: should we try to make a giant array for one cache call to avoid overhead?\n    for (Map.Entry<Long, Long> missingChunk : chunksInThisRead.entrySet()) {\n      long chunkFrom = Math.max(from, missingChunk.getKey()),\n          chunkTo = Math.min(to, missingChunk.getValue()),\n          chunkLength = chunkTo - chunkFrom;\n      // TODO: if we allow partial reads (right now we disable this), we'd have to handle it here.\n      //       chunksInThisRead should probably be changed to be a struct array indicating both\n      //       partial and full sizes for each chunk; then the partial ones could be merged\n      //       with the previous partial ones, and any newly-full chunks put in the cache.\n      MemoryBuffer[] largeBuffers = null, smallBuffer = null, newCacheData = null;\n      try {\n        int largeBufCount = (int) (chunkLength / maxAlloc);\n        int smallSize = (int) (chunkLength % maxAlloc);\n        int chunkPartCount = largeBufCount + ((smallSize > 0) ? 1 : 0);\n        DiskRange[] cacheRanges = new DiskRange[chunkPartCount];\n        int extraOffsetInChunk = 0;\n        if (maxAlloc < chunkLength) {\n          largeBuffers = new MemoryBuffer[largeBufCount];\n          // Note: we don't use StoppableAllocator here - this is not on an IO thread.\n          allocator.allocateMultiple(largeBuffers, maxAlloc, cache.getDataBufferFactory());\n          for (int i = 0; i < largeBuffers.length; ++i) {\n            // By definition here we copy up to the limit of the buffer.\n            ByteBuffer bb = largeBuffers[i].getByteBufferRaw();\n            int remaining = bb.remaining();\n            assert remaining == maxAlloc;\n            copyDiskDataToCacheBuffer(array,\n                arrayOffset + offsetFromReadStart + extraDiskDataOffset,\n                remaining, bb, cacheRanges, i, chunkFrom + extraOffsetInChunk);\n            extraDiskDataOffset += remaining;\n            extraOffsetInChunk += remaining;\n          }\n        }\n        newCacheData = largeBuffers;\n        largeBuffers = null;\n        if (smallSize > 0) {\n          smallBuffer = new MemoryBuffer[1];\n          // Note: we don't use StoppableAllocator here - this is not on an IO thread.\n          allocator.allocateMultiple(smallBuffer, smallSize, cache.getDataBufferFactory());\n          ByteBuffer bb = smallBuffer[0].getByteBufferRaw();\n          copyDiskDataToCacheBuffer(array,\n              arrayOffset + offsetFromReadStart + extraDiskDataOffset,\n              smallSize, bb, cacheRanges, largeBufCount, chunkFrom + extraOffsetInChunk);\n          extraDiskDataOffset += smallSize;\n          extraOffsetInChunk += smallSize; // Not strictly necessary, noone will look at it.\n          if (newCacheData == null) {\n            newCacheData = smallBuffer;\n          } else {\n            // TODO: add allocate overload with an offset and length\n            MemoryBuffer[] combinedCacheData = new MemoryBuffer[largeBufCount + 1];\n            System.arraycopy(newCacheData, 0, combinedCacheData, 0, largeBufCount);\n            newCacheData = combinedCacheData;\n            newCacheData[largeBufCount] = smallBuffer[0];\n          }\n          smallBuffer = null;\n        }\n        cache.putFileData(fileKey, cacheRanges, newCacheData, 0, tag);\n      } finally {\n        // We do not use the new cache buffers for the actual read, given the way read() API is.\n        // Therefore, we don't need to handle cache collisions - just decref all the buffers.\n        if (newCacheData != null) {\n          for (MemoryBuffer buffer : newCacheData) {\n            if (buffer == null) continue;\n            cache.releaseBuffer(buffer);\n          }\n        }\n        // If we have failed before building newCacheData, deallocate other the allocated.\n        if (largeBuffers != null) {\n          for (MemoryBuffer buffer : largeBuffers) {\n            if (buffer == null) continue;\n            allocator.deallocate(buffer);\n          }\n        }\n        if (smallBuffer != null && smallBuffer[0] != null) {\n          allocator.deallocate(smallBuffer[0]);\n        }\n      }\n    }\n  }\n  validateAndUpdatePosition(len, sizeRead);\n  return len;\n}",
        "reject_response": "@Override\npublic int read(byte[] array, final int arrayOffset, final int len) throws IOException {\n  long readStartPos = position;\n  DiskRangeList drl = new DiskRangeList(readStartPos, readStartPos + len);\n  DataCache.BooleanRef gotAllData = new DataCache.BooleanRef();\n  drl = cache.getFileData(fileKey, drl, 0, new DataCache.DiskRangeListFactory() {\n    @Override\n    public DiskRangeList createCacheChunk(\n        MemoryBuffer buffer, long startOffset, long endOffset) {\n      return new CacheChunk(buffer, startOffset, endOffset);\n    }\n  }, gotAllData);\n  if (LOG.isDebugEnabled()) {\n  if (LOG.isInfoEnabled()) {\n    LOG.info(\"Buffers after cache \" + RecordReaderUtils.stringifyDiskRanges(drl));\n  }\n  if (gotAllData.value) {\n    long sizeRead = 0;\n    while (drl != null) {\n      assert drl.hasData();\n      long from = drl.getOffset(), to = drl.getEnd();\n      int offsetFromReadStart = (int)(from - readStartPos), candidateSize = (int)(to - from);\n      ByteBuffer data = drl.getData().duplicate();\n      data.get(array, arrayOffset + offsetFromReadStart, candidateSize);\n      cache.releaseBuffer(((CacheChunk)drl).getBuffer());\n      sizeRead += candidateSize;\n      drl = drl.next;\n    }\n    validateAndUpdatePosition(len, sizeRead);\n    return len;\n  }\n  int maxAlloc = cache.getAllocator().getMaxAllocation();\n  // We have some disk data. Separate it by column chunk and put into cache.\n\n  // We started with a single DRL, so we assume there will be no consecutive missing blocks\n  // after the cache has inserted cache data. We also assume all the missing parts will\n  // represent one or several column chunks, since we always cache on column chunk boundaries.\n  DiskRangeList current = drl;\n  FileSystem fs = path.getFileSystem(conf);\n  FSDataInputStream is = fs.open(path, bufferSize);\n  Allocator allocator = cache.getAllocator();\n  long sizeRead = 0;\n  while (current != null) {\n    DiskRangeList candidate = current;\n    current = current.next;\n    long from = candidate.getOffset(), to = candidate.getEnd();\n    // The offset in the destination array for the beginning of this missing range.\n    int offsetFromReadStart = (int)(from - readStartPos), candidateSize = (int)(to - from);\n    if (candidate.hasData()) {\n      ByteBuffer data = candidate.getData().duplicate();\n      data.get(array, arrayOffset + offsetFromReadStart, candidateSize);\n      cache.releaseBuffer(((CacheChunk)candidate).getBuffer());\n      sizeRead += candidateSize;\n      continue;\n    }\n    // The data is not in cache.\n\n    // Account for potential partial chunks.\n    SortedMap<Long, Long> chunksInThisRead = getAndValidateMissingChunks(maxAlloc, from, to);\n\n    is.seek(from);\n    is.readFully(array, arrayOffset + offsetFromReadStart, candidateSize);\n    sizeRead += candidateSize;\n    // Now copy missing chunks (and parts of chunks) into cache buffers.\n    if (fileKey == null || cache == null) continue;\n    int extraDiskDataOffset = 0;\n    // TODO: should we try to make a giant array for one cache call to avoid overhead?\n    for (Map.Entry<Long, Long> missingChunk : chunksInThisRead.entrySet()) {\n      long chunkFrom = Math.max(from, missingChunk.getKey()),\n          chunkTo = Math.min(to, missingChunk.getValue()),\n          chunkLength = chunkTo - chunkFrom;\n      // TODO: if we allow partial reads (right now we disable this), we'd have to handle it here.\n      //       chunksInThisRead should probably be changed to be a struct array indicating both\n      //       partial and full sizes for each chunk; then the partial ones could be merged\n      //       with the previous partial ones, and any newly-full chunks put in the cache.\n      MemoryBuffer[] largeBuffers = null, smallBuffer = null, newCacheData = null;\n      try {\n        int largeBufCount = (int) (chunkLength / maxAlloc);\n        int smallSize = (int) (chunkLength % maxAlloc);\n        int chunkPartCount = largeBufCount + ((smallSize > 0) ? 1 : 0);\n        DiskRange[] cacheRanges = new DiskRange[chunkPartCount];\n        int extraOffsetInChunk = 0;\n        if (maxAlloc < chunkLength) {\n          largeBuffers = new MemoryBuffer[largeBufCount];\n          // Note: we don't use StoppableAllocator here - this is not on an IO thread.\n          allocator.allocateMultiple(largeBuffers, maxAlloc, cache.getDataBufferFactory());\n          for (int i = 0; i < largeBuffers.length; ++i) {\n            // By definition here we copy up to the limit of the buffer.\n            ByteBuffer bb = largeBuffers[i].getByteBufferRaw();\n            int remaining = bb.remaining();\n            assert remaining == maxAlloc;\n            copyDiskDataToCacheBuffer(array,\n                arrayOffset + offsetFromReadStart + extraDiskDataOffset,\n                remaining, bb, cacheRanges, i, chunkFrom + extraOffsetInChunk);\n            extraDiskDataOffset += remaining;\n            extraOffsetInChunk += remaining;\n          }\n        }\n        newCacheData = largeBuffers;\n        largeBuffers = null;\n        if (smallSize > 0) {\n          smallBuffer = new MemoryBuffer[1];\n          // Note: we don't use StoppableAllocator here - this is not on an IO thread.\n          allocator.allocateMultiple(smallBuffer, smallSize, cache.getDataBufferFactory());\n          ByteBuffer bb = smallBuffer[0].getByteBufferRaw();\n          copyDiskDataToCacheBuffer(array,\n              arrayOffset + offsetFromReadStart + extraDiskDataOffset,\n              smallSize, bb, cacheRanges, largeBufCount, chunkFrom + extraOffsetInChunk);\n          extraDiskDataOffset += smallSize;\n          extraOffsetInChunk += smallSize; // Not strictly necessary, noone will look at it.\n          if (newCacheData == null) {\n            newCacheData = smallBuffer;\n          } else {\n            // TODO: add allocate overload with an offset and length\n            MemoryBuffer[] combinedCacheData = new MemoryBuffer[largeBufCount + 1];\n            System.arraycopy(newCacheData, 0, combinedCacheData, 0, largeBufCount);\n            newCacheData = combinedCacheData;\n            newCacheData[largeBufCount] = smallBuffer[0];\n          }\n          smallBuffer = null;\n        }\n        cache.putFileData(fileKey, cacheRanges, newCacheData, 0, tag);\n      } finally {\n        // We do not use the new cache buffers for the actual read, given the way read() API is.\n        // Therefore, we don't need to handle cache collisions - just decref all the buffers.\n        if (newCacheData != null) {\n          for (MemoryBuffer buffer : newCacheData) {\n            if (buffer == null) continue;\n            cache.releaseBuffer(buffer);\n          }\n        }\n        // If we have failed before building newCacheData, deallocate other the allocated.\n        if (largeBuffers != null) {\n          for (MemoryBuffer buffer : largeBuffers) {\n            if (buffer == null) continue;\n            allocator.deallocate(buffer);\n          }\n        }\n        if (smallBuffer != null && smallBuffer[0] != null) {\n          allocator.deallocate(smallBuffer[0]);\n        }\n      }\n    }\n  }\n  validateAndUpdatePosition(len, sizeRead);\n  return len;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2992,
        "instruction": "@Override\npublic Map<String, Object> validateConfig() throws Exception {\n  Map<String, Object> ret = new HashMap<String, Object>();\n  String serviceName = getServiceName();\n\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"RangerServicePresto.validateConfig(): Service: \" +\n      serviceName);\n  }\n\n  if (configs != null) {\n    try {\n      if (!configs.containsKey(HadoopConfigHolder.RANGER_LOGIN_PASSWORD)) {\n        configs.put(HadoopConfigHolder.RANGER_LOGIN_PASSWORD, null);\n      }\n      ret = PrestoResourceManager.connectionTest(serviceName, configs);\n    } catch (HadoopException he) {\n      throw he;\n    }\n  }\n\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"RangerServicePresto.validateConfig(): Response: \" +\n      ret);\n  }\n  return ret;\n}",
        "accept_response": "@Override\npublic Map<String, Object> validateConfig() throws Exception {\n  Map<String, Object> ret = new HashMap<String, Object>();\n  String serviceName = getServiceName();\n\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"RangerServicePresto.validateConfig(): Service: \" +\n      serviceName);\n  }\n\n  if (configs != null) {\n    try {\n      if (!configs.containsKey(HadoopConfigHolder.RANGER_LOGIN_PASSWORD)) {\n        configs.put(HadoopConfigHolder.RANGER_LOGIN_PASSWORD, null);\n      }\n      ret = PrestoResourceManager.connectionTest(serviceName, configs);\n    } catch (HadoopException he) {\n      LOG.error(\"<== RangerServicePresto.validateConfig() Error:\" + he);\n      throw he;\n    }\n  }\n\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"RangerServicePresto.validateConfig(): Response: \" +\n      ret);\n  }\n  return ret;\n}",
        "reject_response": "@Override\npublic Map<String, Object> validateConfig() throws Exception {\n  Map<String, Object> ret = new HashMap<String, Object>();\n  String serviceName = getServiceName();\n\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"RangerServicePresto.validateConfig(): Service: \" +\n      serviceName);\n  }\n\n  if (configs != null) {\n    try {\n      if (!configs.containsKey(HadoopConfigHolder.RANGER_LOGIN_PASSWORD)) {\n        configs.put(HadoopConfigHolder.RANGER_LOGIN_PASSWORD, null);\n      }\n      ret = PrestoResourceManager.connectionTest(serviceName, configs);\n    } catch (HadoopException he) {\n      LOG.error(\"<== RangerServicePresto.validateConfig Error:\" + he);\n      throw he;\n    }\n  }\n\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"RangerServicePresto.validateConfig(): Response: \" +\n      ret);\n  }\n  return ret;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2975,
        "instruction": "private void internalCopyAllGeneratedToDistributedCache() {\n    if (pigContext.getExecType().isLocal()) {\n        String codePath = codeDir.getAbsolutePath();\n        LOG.info(\"Distributed cache not supported or needed in local mode. Setting key [\"\n                + LOCAL_CODE_DIR + \"] with code temp directory: \" + codePath);\n            conf.set(LOCAL_CODE_DIR, codePath);\n        return;\n    } else {\n        // This let's us avoid NPE in some of the non-traditional pipelines\n        String codePath = codeDir.getAbsolutePath();\n        conf.set(LOCAL_CODE_DIR, codePath);\n    }\n    DistributedCache.createSymlink(conf); // we will read using symlinks\n    StringBuilder serialized = new StringBuilder();\n    boolean first = true;\n    // We attempt to copy over every file in the generated code temp directory\n    for (File f : codeDir.listFiles()) {\n        if (first) {\n            first = false;\n        } else {\n            serialized.append(\",\");\n        }\n        String symlink = f.getName(); //the class name will also be the symlink\n        serialized.append(symlink);\n        Path src = new Path(f.toURI());\n        Path dst;\n        try {\n            dst = FileLocalizer.getTemporaryPath(pigContext);\n        } catch (IOException e) {\n            throw new RuntimeException(\"Error getting temporary path in HDFS\", e);\n        }\n        FileSystem fs;\n        try {\n            fs = dst.getFileSystem(conf);\n        } catch (IOException e) {\n            throw new RuntimeException(\"Unable to get FileSystem\", e);\n        }\n        try {\n            fs.copyFromLocalFile(src, dst);\n            fs.setReplication(dst, (short)conf.getInt(MRConfiguration.SUMIT_REPLICATION, 3));\n        } catch (IOException e) {\n            throw new RuntimeException(\"Unable to copy from local filesystem to HDFS, src = \"\n                    + src + \", dst = \" + dst, e);\n        }\n\n        String destination = dst.toString() + \"#\" + symlink;\n\n        try {\n            DistributedCache.addCacheFile(new URI(destination), conf);\n        } catch (URISyntaxException e) {\n            throw new RuntimeException(\"Unable to add file to distributed cache: \" + destination, e);\n        }\n        LOG.info(\"File successfully added to the distributed cache: \" + symlink);\n    }\n    String toSer = serialized.toString();\n    LOG.info(\"Setting key [\" + GENERATED_CLASSES_KEY + \"] with classes to deserialize [\" + toSer + \"]\");\n    // we must set a key in the job conf so individual jobs know to resolve the shipped classes\n    conf.set(GENERATED_CLASSES_KEY, toSer);\n}",
        "accept_response": "private void internalCopyAllGeneratedToDistributedCache() {\n    LOG.info(\"Starting process to move generated code to distributed cache\");\n    if (pigContext.getExecType().isLocal()) {\n        String codePath = codeDir.getAbsolutePath();\n        LOG.info(\"Distributed cache not supported or needed in local mode. Setting key [\"\n                + LOCAL_CODE_DIR + \"] with code temp directory: \" + codePath);\n            conf.set(LOCAL_CODE_DIR, codePath);\n        return;\n    } else {\n        // This let's us avoid NPE in some of the non-traditional pipelines\n        String codePath = codeDir.getAbsolutePath();\n        conf.set(LOCAL_CODE_DIR, codePath);\n    }\n    DistributedCache.createSymlink(conf); // we will read using symlinks\n    StringBuilder serialized = new StringBuilder();\n    boolean first = true;\n    // We attempt to copy over every file in the generated code temp directory\n    for (File f : codeDir.listFiles()) {\n        if (first) {\n            first = false;\n        } else {\n            serialized.append(\",\");\n        }\n        String symlink = f.getName(); //the class name will also be the symlink\n        serialized.append(symlink);\n        Path src = new Path(f.toURI());\n        Path dst;\n        try {\n            dst = FileLocalizer.getTemporaryPath(pigContext);\n        } catch (IOException e) {\n            throw new RuntimeException(\"Error getting temporary path in HDFS\", e);\n        }\n        FileSystem fs;\n        try {\n            fs = dst.getFileSystem(conf);\n        } catch (IOException e) {\n            throw new RuntimeException(\"Unable to get FileSystem\", e);\n        }\n        try {\n            fs.copyFromLocalFile(src, dst);\n            fs.setReplication(dst, (short)conf.getInt(MRConfiguration.SUMIT_REPLICATION, 3));\n        } catch (IOException e) {\n            throw new RuntimeException(\"Unable to copy from local filesystem to HDFS, src = \"\n                    + src + \", dst = \" + dst, e);\n        }\n\n        String destination = dst.toString() + \"#\" + symlink;\n\n        try {\n            DistributedCache.addCacheFile(new URI(destination), conf);\n        } catch (URISyntaxException e) {\n            throw new RuntimeException(\"Unable to add file to distributed cache: \" + destination, e);\n        }\n        LOG.info(\"File successfully added to the distributed cache: \" + symlink);\n    }\n    String toSer = serialized.toString();\n    LOG.info(\"Setting key [\" + GENERATED_CLASSES_KEY + \"] with classes to deserialize [\" + toSer + \"]\");\n    // we must set a key in the job conf so individual jobs know to resolve the shipped classes\n    conf.set(GENERATED_CLASSES_KEY, toSer);\n}",
        "reject_response": "private void internalCopyAllGeneratedToDistributedCache() {\n    LOG.info(\"Starting process to move generated code to distributed cacche\");\n    if (pigContext.getExecType().isLocal()) {\n        String codePath = codeDir.getAbsolutePath();\n        LOG.info(\"Distributed cache not supported or needed in local mode. Setting key [\"\n                + LOCAL_CODE_DIR + \"] with code temp directory: \" + codePath);\n            conf.set(LOCAL_CODE_DIR, codePath);\n        return;\n    } else {\n        // This let's us avoid NPE in some of the non-traditional pipelines\n        String codePath = codeDir.getAbsolutePath();\n        conf.set(LOCAL_CODE_DIR, codePath);\n    }\n    DistributedCache.createSymlink(conf); // we will read using symlinks\n    StringBuilder serialized = new StringBuilder();\n    boolean first = true;\n    // We attempt to copy over every file in the generated code temp directory\n    for (File f : codeDir.listFiles()) {\n        if (first) {\n            first = false;\n        } else {\n            serialized.append(\",\");\n        }\n        String symlink = f.getName(); //the class name will also be the symlink\n        serialized.append(symlink);\n        Path src = new Path(f.toURI());\n        Path dst;\n        try {\n            dst = FileLocalizer.getTemporaryPath(pigContext);\n        } catch (IOException e) {\n            throw new RuntimeException(\"Error getting temporary path in HDFS\", e);\n        }\n        FileSystem fs;\n        try {\n            fs = dst.getFileSystem(conf);\n        } catch (IOException e) {\n            throw new RuntimeException(\"Unable to get FileSystem\", e);\n        }\n        try {\n            fs.copyFromLocalFile(src, dst);\n            fs.setReplication(dst, (short)conf.getInt(MRConfiguration.SUMIT_REPLICATION, 3));\n        } catch (IOException e) {\n            throw new RuntimeException(\"Unable to copy from local filesystem to HDFS, src = \"\n                    + src + \", dst = \" + dst, e);\n        }\n\n        String destination = dst.toString() + \"#\" + symlink;\n\n        try {\n            DistributedCache.addCacheFile(new URI(destination), conf);\n        } catch (URISyntaxException e) {\n            throw new RuntimeException(\"Unable to add file to distributed cache: \" + destination, e);\n        }\n        LOG.info(\"File successfully added to the distributed cache: \" + symlink);\n    }\n    String toSer = serialized.toString();\n    LOG.info(\"Setting key [\" + GENERATED_CLASSES_KEY + \"] with classes to deserialize [\" + toSer + \"]\");\n    // we must set a key in the job conf so individual jobs know to resolve the shipped classes\n    conf.set(GENERATED_CLASSES_KEY, toSer);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3223,
        "instruction": "private byte[] createSaslToken(final byte[] saslToken) throws SaslException {\n    if (saslToken == null) {\n        // TODO: introspect about runtime environment (such as jaas.conf)\n        saslState = SaslState.FAILED;\n        throw new SaslException(\"Error in authenticating with a Zookeeper Quorum member: the quorum member's saslToken is null.\");\n    }\n\n    Subject subject = login.getSubject();\n    if (subject != null) {\n        synchronized(login) {\n            try {\n                final byte[] retval =\n                    Subject.doAs(subject, new PrivilegedExceptionAction<byte[]>() {\n                            public byte[] run() throws SaslException {\n                                return saslClient.evaluateChallenge(saslToken);\n                            }\n                        });\n                return retval;\n            }\n            catch (PrivilegedActionException e) {\n                String error = \"An error: (\" + e + \") occurred when evaluating Zookeeper Quorum Member's \" +\n                  \" received SASL token.\";\n                // Try to provide hints to use about what went wrong so they can fix their configuration.\n                // TODO: introspect about e: look for GSS information.\n                final String UNKNOWN_SERVER_ERROR_TEXT =\n                  \"(Mechanism level: Server not found in Kerberos database (7) - UNKNOWN_SERVER)\";\n                if (e.toString().contains(UNKNOWN_SERVER_ERROR_TEXT)) {\n                    error += \" This may be caused by Java's being unable to resolve the Zookeeper Quorum Member's\" +\n                      \" hostname correctly. You may want to try to adding\" +\n                      \" '-Dsun.net.spi.nameservice.provider.1=dns,sun' to your client's JVMFLAGS environment.\";\n                }\n                error += \" Zookeeper Client will go to AUTH_FAILED state.\";\n                LOG.error(error);\n                saslState = SaslState.FAILED;\n                throw new SaslException(error);\n            }\n        }\n    }\n    else {\n        throw new SaslException(\"Cannot make SASL token without subject defined. \" +\n          \"For diagnosis, please look for WARNs and ERRORs in your log related to the Login class.\");\n    }\n}",
        "accept_response": "private byte[] createSaslToken(final byte[] saslToken) throws SaslException {\n    if (saslToken == null) {\n        // TODO: introspect about runtime environment (such as jaas.conf)\n        saslState = SaslState.FAILED;\n        throw new SaslException(\"Error in authenticating with a Zookeeper Quorum member: the quorum member's saslToken is null.\");\n    }\n\n    Subject subject = login.getSubject();\n    if (subject != null) {\n        synchronized(login) {\n            try {\n                final byte[] retval =\n                    Subject.doAs(subject, new PrivilegedExceptionAction<byte[]>() {\n                            public byte[] run() throws SaslException {\n                                LOG.debug(\"saslClient.evaluateChallenge(len={})\", saslToken.length);\n                                return saslClient.evaluateChallenge(saslToken);\n                            }\n                        });\n                return retval;\n            }\n            catch (PrivilegedActionException e) {\n                String error = \"An error: (\" + e + \") occurred when evaluating Zookeeper Quorum Member's \" +\n                  \" received SASL token.\";\n                // Try to provide hints to use about what went wrong so they can fix their configuration.\n                // TODO: introspect about e: look for GSS information.\n                final String UNKNOWN_SERVER_ERROR_TEXT =\n                  \"(Mechanism level: Server not found in Kerberos database (7) - UNKNOWN_SERVER)\";\n                if (e.toString().contains(UNKNOWN_SERVER_ERROR_TEXT)) {\n                    error += \" This may be caused by Java's being unable to resolve the Zookeeper Quorum Member's\" +\n                      \" hostname correctly. You may want to try to adding\" +\n                      \" '-Dsun.net.spi.nameservice.provider.1=dns,sun' to your client's JVMFLAGS environment.\";\n                }\n                error += \" Zookeeper Client will go to AUTH_FAILED state.\";\n                LOG.error(error);\n                saslState = SaslState.FAILED;\n                throw new SaslException(error);\n            }\n        }\n    }\n    else {\n        throw new SaslException(\"Cannot make SASL token without subject defined. \" +\n          \"For diagnosis, please look for WARNs and ERRORs in your log related to the Login class.\");\n    }\n}",
        "reject_response": "private byte[] createSaslToken(final byte[] saslToken) throws SaslException {\n    if (saslToken == null) {\n        // TODO: introspect about runtime environment (such as jaas.conf)\n        saslState = SaslState.FAILED;\n        throw new SaslException(\"Error in authenticating with a Zookeeper Quorum member: the quorum member's saslToken is null.\");\n    }\n\n    Subject subject = login.getSubject();\n    if (subject != null) {\n        synchronized(login) {\n            try {\n                final byte[] retval =\n                    Subject.doAs(subject, new PrivilegedExceptionAction<byte[]>() {\n                            public byte[] run() throws SaslException {\n                                LOG.debug(\"saslClient.evaluateChallenge(len=\"+saslToken.length+\")\");\n                                return saslClient.evaluateChallenge(saslToken);\n                            }\n                        });\n                return retval;\n            }\n            catch (PrivilegedActionException e) {\n                String error = \"An error: (\" + e + \") occurred when evaluating Zookeeper Quorum Member's \" +\n                  \" received SASL token.\";\n                // Try to provide hints to use about what went wrong so they can fix their configuration.\n                // TODO: introspect about e: look for GSS information.\n                final String UNKNOWN_SERVER_ERROR_TEXT =\n                  \"(Mechanism level: Server not found in Kerberos database (7) - UNKNOWN_SERVER)\";\n                if (e.toString().contains(UNKNOWN_SERVER_ERROR_TEXT)) {\n                    error += \" This may be caused by Java's being unable to resolve the Zookeeper Quorum Member's\" +\n                      \" hostname correctly. You may want to try to adding\" +\n                      \" '-Dsun.net.spi.nameservice.provider.1=dns,sun' to your client's JVMFLAGS environment.\";\n                }\n                error += \" Zookeeper Client will go to AUTH_FAILED state.\";\n                LOG.error(error);\n                saslState = SaslState.FAILED;\n                throw new SaslException(error);\n            }\n        }\n    }\n    else {\n        throw new SaslException(\"Cannot make SASL token without subject defined. \" +\n          \"For diagnosis, please look for WARNs and ERRORs in your log related to the Login class.\");\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2570,
        "instruction": "private void handleDataIntegrityIssues(@SuppressWarnings(\"unused\") final JsonCommand command, final DataIntegrityViolationException dve) {\n    final Throwable realCause = dve.getMostSpecificCause();\n\n    if (realCause.getMessage().contains(\"email_address\")) { throw new PlatformDataIntegrityException(\"error.msg.email.no.email.address.exists\",\n            \"The group, client or staff provided has no email address.\", \"id\"); }\n\n    throw new PlatformDataIntegrityException(\"error.msg.email.unknown.data.integrity.issue\",\n            \"Unknown data integrity issue with resource: \" + realCause.getMessage());\n}",
        "accept_response": "private void handleDataIntegrityIssues(@SuppressWarnings(\"unused\") final JsonCommand command, final DataIntegrityViolationException dve) {\n    final Throwable realCause = dve.getMostSpecificCause();\n\n    if (realCause.getMessage().contains(\"email_address\")) { throw new PlatformDataIntegrityException(\"error.msg.email.no.email.address.exists\",\n            \"The group, client or staff provided has no email address.\", \"id\"); }\n\n    logger.error(\"Error occured.\", dve);\n    throw new PlatformDataIntegrityException(\"error.msg.email.unknown.data.integrity.issue\",\n            \"Unknown data integrity issue with resource: \" + realCause.getMessage());\n}",
        "reject_response": "private void handleDataIntegrityIssues(@SuppressWarnings(\"unused\") final JsonCommand command, final DataIntegrityViolationException dve) {\n    final Throwable realCause = dve.getMostSpecificCause();\n\n    if (realCause.getMessage().contains(\"email_address\")) { throw new PlatformDataIntegrityException(\"error.msg.email.no.email.address.exists\",\n            \"The group, client or staff provided has no email address.\", \"id\"); }\n\n    logger.error(dve.getMessage(), dve);\n    throw new PlatformDataIntegrityException(\"error.msg.email.unknown.data.integrity.issue\",\n            \"Unknown data integrity issue with resource: \" + realCause.getMessage());\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2665,
        "instruction": "@Override\npublic void operationComplete(ChannelFuture future) throws Exception {\n  if (future.isDone() && !future.isSuccess()) {\n    checkRequestsAfterChannelFailure(future.channel());\n  }\n}",
        "accept_response": "@Override\npublic void operationComplete(ChannelFuture future) throws Exception {\n  if (future.isDone() && !future.isSuccess()) {\n    LOG.error(\"Channel failed channelId=\" + future.channel().hashCode(),\n        future.cause());\n    checkRequestsAfterChannelFailure(future.channel());\n  }\n}",
        "reject_response": "@Override\npublic void operationComplete(ChannelFuture future) throws Exception {\n  if (future.isDone() && !future.isSuccess()) {\n    LOG.error(\"Request failed\", future.cause());\n    checkRequestsAfterChannelFailure(future.channel());\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3235,
        "instruction": "public Proposal propose(Request request) throws XidRolloverException {\n    /**\n     * Address the rollover issue. All lower 32bits set indicate a new leader\n     * election. Force a re-election instead. See ZOOKEEPER-1277\n     */\n    if ((request.zxid & 0xffffffffL) == 0xffffffffL) {\n        String msg =\n                \"zxid lower 32 bits have rolled over, forcing re-election, and therefore new epoch start\";\n        shutdown(msg);\n        throw new XidRolloverException(msg);\n    }\n\n    byte[] data = SerializeUtils.serializeRequest(request);\n    proposalStats.setLastBufferSize(data.length);\n    QuorumPacket pp = new QuorumPacket(Leader.PROPOSAL, request.zxid, data, null);\n\n    Proposal p = new Proposal();\n    p.packet = pp;\n    p.request = request;\n\n    synchronized(this) {\n       p.addQuorumVerifier(self.getQuorumVerifier());\n\n       if (request.getHdr().getType() == OpCode.reconfig){\n           self.setLastSeenQuorumVerifier(request.qv, true);\n       }\n\n       if (self.getQuorumVerifier().getVersion()<self.getLastSeenQuorumVerifier().getVersion()) {\n           p.addQuorumVerifier(self.getLastSeenQuorumVerifier());\n       }\n\n\n        lastProposed = p.packet.getZxid();\n        outstandingProposals.put(lastProposed, p);\n        sendPacket(pp);\n    }\n    ServerMetrics.getMetrics().PROPOSAL_COUNT.add(1);\n    return p;\n}",
        "accept_response": "public Proposal propose(Request request) throws XidRolloverException {\n    /**\n     * Address the rollover issue. All lower 32bits set indicate a new leader\n     * election. Force a re-election instead. See ZOOKEEPER-1277\n     */\n    if ((request.zxid & 0xffffffffL) == 0xffffffffL) {\n        String msg =\n                \"zxid lower 32 bits have rolled over, forcing re-election, and therefore new epoch start\";\n        shutdown(msg);\n        throw new XidRolloverException(msg);\n    }\n\n    byte[] data = SerializeUtils.serializeRequest(request);\n    proposalStats.setLastBufferSize(data.length);\n    QuorumPacket pp = new QuorumPacket(Leader.PROPOSAL, request.zxid, data, null);\n\n    Proposal p = new Proposal();\n    p.packet = pp;\n    p.request = request;\n\n    synchronized(this) {\n       p.addQuorumVerifier(self.getQuorumVerifier());\n\n       if (request.getHdr().getType() == OpCode.reconfig){\n           self.setLastSeenQuorumVerifier(request.qv, true);\n       }\n\n       if (self.getQuorumVerifier().getVersion()<self.getLastSeenQuorumVerifier().getVersion()) {\n           p.addQuorumVerifier(self.getLastSeenQuorumVerifier());\n       }\n\n        LOG.debug(\"Proposing:: {}\", request);\n\n        lastProposed = p.packet.getZxid();\n        outstandingProposals.put(lastProposed, p);\n        sendPacket(pp);\n    }\n    ServerMetrics.getMetrics().PROPOSAL_COUNT.add(1);\n    return p;\n}",
        "reject_response": "public Proposal propose(Request request) throws XidRolloverException {\n    /**\n     * Address the rollover issue. All lower 32bits set indicate a new leader\n     * election. Force a re-election instead. See ZOOKEEPER-1277\n     */\n    if ((request.zxid & 0xffffffffL) == 0xffffffffL) {\n        String msg =\n                \"zxid lower 32 bits have rolled over, forcing re-election, and therefore new epoch start\";\n        shutdown(msg);\n        throw new XidRolloverException(msg);\n    }\n\n    byte[] data = SerializeUtils.serializeRequest(request);\n    proposalStats.setLastBufferSize(data.length);\n    QuorumPacket pp = new QuorumPacket(Leader.PROPOSAL, request.zxid, data, null);\n\n    Proposal p = new Proposal();\n    p.packet = pp;\n    p.request = request;\n\n    synchronized(this) {\n       p.addQuorumVerifier(self.getQuorumVerifier());\n\n       if (request.getHdr().getType() == OpCode.reconfig){\n           self.setLastSeenQuorumVerifier(request.qv, true);\n       }\n\n       if (self.getQuorumVerifier().getVersion()<self.getLastSeenQuorumVerifier().getVersion()) {\n           p.addQuorumVerifier(self.getLastSeenQuorumVerifier());\n       }\n\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Proposing:: \" + request);\n        }\n\n        lastProposed = p.packet.getZxid();\n        outstandingProposals.put(lastProposed, p);\n        sendPacket(pp);\n    }\n    ServerMetrics.getMetrics().PROPOSAL_COUNT.add(1);\n    return p;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2850,
        "instruction": "@Override\npublic TSExecuteBatchStatementResp insertBatch(TSBatchInsertionReq req) {\n  long t1 = System.currentTimeMillis();\n  try {\n    if (!checkLogin()) {\n      logger.info(INFO_NOT_LOGIN, IoTDBConstant.GLOBAL_DB_NAME);\n      return getTSBatchExecuteStatementResp(getStatus(TSStatusCode.NOT_LOGIN_ERROR), null);\n    }\n\n    BatchInsertPlan batchInsertPlan = new BatchInsertPlan(req.deviceId, req.measurements);\n    batchInsertPlan.setTimes(QueryDataSetUtils.readTimesFromBuffer(req.timestamps, req.size));\n    batchInsertPlan.setColumns(QueryDataSetUtils\n        .readValuesFromBuffer(req.values, req.types, req.measurements.size(), req.size));\n    batchInsertPlan.setRowCount(req.size);\n    batchInsertPlan.setTimeBuffer(req.timestamps);\n    batchInsertPlan.setValueBuffer(req.values);\n    batchInsertPlan.setDataTypes(req.types);\n\n    boolean isAllSuccessful = true;\n    TSStatus status = checkAuthority(batchInsertPlan);\n    if (status != null) {\n      return new TSExecuteBatchStatementResp(status);\n    }\n    Integer[] results = processor.getExecutor().insertBatch(batchInsertPlan);\n\n    for (Integer result : results) {\n      if (result != TSStatusCode.SUCCESS_STATUS.getStatusCode()) {\n        isAllSuccessful = false;\n        break;\n      }\n    }\n\n    if (isAllSuccessful) {\n      logger.debug(\"Insert one RowBatch successfully\");\n      return getTSBatchExecuteStatementResp(getStatus(TSStatusCode.SUCCESS_STATUS),\n          Arrays.asList(results));\n    } else {\n      logger.debug(\"Insert one RowBatch failed!\");\n      return getTSBatchExecuteStatementResp(getStatus(TSStatusCode.INTERNAL_SERVER_ERROR),\n          Arrays.asList(results));\n    }\n  } catch (Exception e) {\n    return getTSBatchExecuteStatementResp(\n        getStatus(TSStatusCode.EXECUTE_STATEMENT_ERROR, e.getMessage()), null);\n  } finally {\n    Measurement.INSTANCE.addOperationLatency(Operation.EXECUTE_RPC_BATCH_INSERT, t1);\n  }\n}",
        "accept_response": "@Override\npublic TSExecuteBatchStatementResp insertBatch(TSBatchInsertionReq req) {\n  long t1 = System.currentTimeMillis();\n  try {\n    if (!checkLogin()) {\n      logger.info(INFO_NOT_LOGIN, IoTDBConstant.GLOBAL_DB_NAME);\n      return getTSBatchExecuteStatementResp(getStatus(TSStatusCode.NOT_LOGIN_ERROR), null);\n    }\n\n    BatchInsertPlan batchInsertPlan = new BatchInsertPlan(req.deviceId, req.measurements);\n    batchInsertPlan.setTimes(QueryDataSetUtils.readTimesFromBuffer(req.timestamps, req.size));\n    batchInsertPlan.setColumns(QueryDataSetUtils\n        .readValuesFromBuffer(req.values, req.types, req.measurements.size(), req.size));\n    batchInsertPlan.setRowCount(req.size);\n    batchInsertPlan.setTimeBuffer(req.timestamps);\n    batchInsertPlan.setValueBuffer(req.values);\n    batchInsertPlan.setDataTypes(req.types);\n\n    boolean isAllSuccessful = true;\n    TSStatus status = checkAuthority(batchInsertPlan);\n    if (status != null) {\n      return new TSExecuteBatchStatementResp(status);\n    }\n    Integer[] results = processor.getExecutor().insertBatch(batchInsertPlan);\n\n    for (Integer result : results) {\n      if (result != TSStatusCode.SUCCESS_STATUS.getStatusCode()) {\n        isAllSuccessful = false;\n        break;\n      }\n    }\n\n    if (isAllSuccessful) {\n      logger.debug(\"Insert one RowBatch successfully\");\n      return getTSBatchExecuteStatementResp(getStatus(TSStatusCode.SUCCESS_STATUS),\n          Arrays.asList(results));\n    } else {\n      logger.debug(\"Insert one RowBatch failed!\");\n      return getTSBatchExecuteStatementResp(getStatus(TSStatusCode.INTERNAL_SERVER_ERROR),\n          Arrays.asList(results));\n    }\n  } catch (Exception e) {\n    logger.info(\"{}: error occurs when executing statements\", IoTDBConstant.GLOBAL_DB_NAME, e);\n    return getTSBatchExecuteStatementResp(\n        getStatus(TSStatusCode.EXECUTE_STATEMENT_ERROR, e.getMessage()), null);\n  } finally {\n    Measurement.INSTANCE.addOperationLatency(Operation.EXECUTE_RPC_BATCH_INSERT, t1);\n  }\n}",
        "reject_response": "@Override\npublic TSExecuteBatchStatementResp insertBatch(TSBatchInsertionReq req) {\n  long t1 = System.currentTimeMillis();\n  try {\n    if (!checkLogin()) {\n      logger.info(INFO_NOT_LOGIN, IoTDBConstant.GLOBAL_DB_NAME);\n      return getTSBatchExecuteStatementResp(getStatus(TSStatusCode.NOT_LOGIN_ERROR), null);\n    }\n\n    BatchInsertPlan batchInsertPlan = new BatchInsertPlan(req.deviceId, req.measurements);\n    batchInsertPlan.setTimes(QueryDataSetUtils.readTimesFromBuffer(req.timestamps, req.size));\n    batchInsertPlan.setColumns(QueryDataSetUtils\n        .readValuesFromBuffer(req.values, req.types, req.measurements.size(), req.size));\n    batchInsertPlan.setRowCount(req.size);\n    batchInsertPlan.setTimeBuffer(req.timestamps);\n    batchInsertPlan.setValueBuffer(req.values);\n    batchInsertPlan.setDataTypes(req.types);\n\n    boolean isAllSuccessful = true;\n    TSStatus status = checkAuthority(batchInsertPlan);\n    if (status != null) {\n      return new TSExecuteBatchStatementResp(status);\n    }\n    Integer[] results = processor.getExecutor().insertBatch(batchInsertPlan);\n\n    for (Integer result : results) {\n      if (result != TSStatusCode.SUCCESS_STATUS.getStatusCode()) {\n        isAllSuccessful = false;\n        break;\n      }\n    }\n\n    if (isAllSuccessful) {\n      logger.debug(\"Insert one RowBatch successfully\");\n      return getTSBatchExecuteStatementResp(getStatus(TSStatusCode.SUCCESS_STATUS),\n          Arrays.asList(results));\n    } else {\n      logger.debug(\"Insert one RowBatch failed!\");\n      return getTSBatchExecuteStatementResp(getStatus(TSStatusCode.INTERNAL_SERVER_ERROR),\n          Arrays.asList(results));\n    }\n  } catch (Exception e) {\n    logger.error(\"{}: error occurs when executing statements\", IoTDBConstant.GLOBAL_DB_NAME, e);\n    return getTSBatchExecuteStatementResp(\n        getStatus(TSStatusCode.EXECUTE_STATEMENT_ERROR, e.getMessage()), null);\n  } finally {\n    Measurement.INSTANCE.addOperationLatency(Operation.EXECUTE_RPC_BATCH_INSERT, t1);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3179,
        "instruction": "@Override\npublic void rollbackLastReInitializationAsync(ContainerId containerId){\n  if (!(callbackHandler instanceof AbstractCallbackHandler)) {\n    LOG.error(\"Callback handler does not implement container rollback \"\n        + \"callback methods\");\n    return;\n  }\n  AbstractCallbackHandler handler = (AbstractCallbackHandler) callbackHandler;\n  if (containers.get(containerId) == null) {\n    handler.onRollbackLastReInitializationError(\n        containerId, RPCUtil.getRemoteException(\n            \"Container \" + containerId + \" is not started\"));\n  }\n  try {\n    events.put(new ContainerEvent(containerId,\n        client.getNodeIdOfStartedContainer(containerId),\n        null, ContainerEventType.ROLLBACK_LAST_REINIT));\n  } catch (InterruptedException e) {\n    handler.onRollbackLastReInitializationError(containerId, e);\n  }\n}",
        "accept_response": "@Override\npublic void rollbackLastReInitializationAsync(ContainerId containerId){\n  if (!(callbackHandler instanceof AbstractCallbackHandler)) {\n    LOG.error(\"Callback handler does not implement container rollback \"\n        + \"callback methods\");\n    return;\n  }\n  AbstractCallbackHandler handler = (AbstractCallbackHandler) callbackHandler;\n  if (containers.get(containerId) == null) {\n    handler.onRollbackLastReInitializationError(\n        containerId, RPCUtil.getRemoteException(\n            \"Container \" + containerId + \" is not started\"));\n  }\n  try {\n    events.put(new ContainerEvent(containerId,\n        client.getNodeIdOfStartedContainer(containerId),\n        null, ContainerEventType.ROLLBACK_LAST_REINIT));\n  } catch (InterruptedException e) {\n    LOG.warn(\"Exception when scheduling the event Rollback \" +\n            \"re-initialization of Container {}\", containerId);\n    handler.onRollbackLastReInitializationError(containerId, e);\n  }\n}",
        "reject_response": "@Override\npublic void rollbackLastReInitializationAsync(ContainerId containerId){\n  if (!(callbackHandler instanceof AbstractCallbackHandler)) {\n    LOG.error(\"Callback handler does not implement container rollback \"\n        + \"callback methods\");\n    return;\n  }\n  AbstractCallbackHandler handler = (AbstractCallbackHandler) callbackHandler;\n  if (containers.get(containerId) == null) {\n    handler.onRollbackLastReInitializationError(\n        containerId, RPCUtil.getRemoteException(\n            \"Container \" + containerId + \" is not started\"));\n  }\n  try {\n    events.put(new ContainerEvent(containerId,\n        client.getNodeIdOfStartedContainer(containerId),\n        null, ContainerEventType.ROLLBACK_LAST_REINIT));\n  } catch (InterruptedException e) {\n    LOG.warn(\"Exception when scheduling the event Rollback re-initialization\"\n        + \" of Container \" + containerId);\n    handler.onRollbackLastReInitializationError(containerId, e);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2604,
        "instruction": "public synchronized void stop() {\n\trunning = false;\n\tif (handlers != null) {\n\t\tfor (int i = 0; i < handlerCount; i++) {\n\t\t\tif (handlers[i] != null) {\n\t\t\t\thandlers[i].interrupt();\n\t\t\t}\n\t\t}\n\t}\n\tlistener.interrupt();\n\tlistener.doStop();\n\tresponder.interrupt();\n\tnotifyAll();\n\n\t// Wait until shut down of handlers is complete\n\tif (this.handlers != null) {\n\n\t\twhile (true) {\n\n\t\t\tint i = 0;\n\t\t\tfor (; i < this.handlerCount; i++) {\n\t\t\t\tif (this.handlers[i] != null) {\n\t\t\t\t\tif (!this.handlers[i].isShutDown()) {\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (i < this.handlerCount) {\n\t\t\t\ttry {\n\t\t\t\t\twait(100);\n\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// exit while loop\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\t// Wait until shut down of responder is complete\n\twhile (!this.responder.isShutDown()) {\n\t\ttry {\n\t\t\twait(100);\n\t\t} catch (InterruptedException e) {\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t// Wait until shut down of listener is complete\n\twhile (!this.listener.isShutDown()) {\n\t\ttry {\n\t\t\twait(100);\n\t\t} catch (InterruptedException e) {\n\t\t\tbreak;\n\t\t}\n\t}\n}",
        "accept_response": "public synchronized void stop() {\n\tLOG.debug(\"Stopping server on \" + port);\n\trunning = false;\n\tif (handlers != null) {\n\t\tfor (int i = 0; i < handlerCount; i++) {\n\t\t\tif (handlers[i] != null) {\n\t\t\t\thandlers[i].interrupt();\n\t\t\t}\n\t\t}\n\t}\n\tlistener.interrupt();\n\tlistener.doStop();\n\tresponder.interrupt();\n\tnotifyAll();\n\n\t// Wait until shut down of handlers is complete\n\tif (this.handlers != null) {\n\n\t\twhile (true) {\n\n\t\t\tint i = 0;\n\t\t\tfor (; i < this.handlerCount; i++) {\n\t\t\t\tif (this.handlers[i] != null) {\n\t\t\t\t\tif (!this.handlers[i].isShutDown()) {\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (i < this.handlerCount) {\n\t\t\t\ttry {\n\t\t\t\t\twait(100);\n\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// exit while loop\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\t// Wait until shut down of responder is complete\n\twhile (!this.responder.isShutDown()) {\n\t\ttry {\n\t\t\twait(100);\n\t\t} catch (InterruptedException e) {\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t// Wait until shut down of listener is complete\n\twhile (!this.listener.isShutDown()) {\n\t\ttry {\n\t\t\twait(100);\n\t\t} catch (InterruptedException e) {\n\t\t\tbreak;\n\t\t}\n\t}\n}",
        "reject_response": "public synchronized void stop() {\n\tLOG.info(\"Stopping server on \" + port);\n\trunning = false;\n\tif (handlers != null) {\n\t\tfor (int i = 0; i < handlerCount; i++) {\n\t\t\tif (handlers[i] != null) {\n\t\t\t\thandlers[i].interrupt();\n\t\t\t}\n\t\t}\n\t}\n\tlistener.interrupt();\n\tlistener.doStop();\n\tresponder.interrupt();\n\tnotifyAll();\n\n\t// Wait until shut down of handlers is complete\n\tif (this.handlers != null) {\n\n\t\twhile (true) {\n\n\t\t\tint i = 0;\n\t\t\tfor (; i < this.handlerCount; i++) {\n\t\t\t\tif (this.handlers[i] != null) {\n\t\t\t\t\tif (!this.handlers[i].isShutDown()) {\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (i < this.handlerCount) {\n\t\t\t\ttry {\n\t\t\t\t\twait(100);\n\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// exit while loop\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\t// Wait until shut down of responder is complete\n\twhile (!this.responder.isShutDown()) {\n\t\ttry {\n\t\t\twait(100);\n\t\t} catch (InterruptedException e) {\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t// Wait until shut down of listener is complete\n\twhile (!this.listener.isShutDown()) {\n\t\ttry {\n\t\t\twait(100);\n\t\t} catch (InterruptedException e) {\n\t\t\tbreak;\n\t\t}\n\t}\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2738,
        "instruction": "Connection register(SocketChannel channel, int ingressPort,\n    boolean isOnAuxiliaryPort) {\n  if (isFull()) {\n    return null;\n  }\n  Connection connection = new Connection(channel, Time.now(),\n      ingressPort, isOnAuxiliaryPort);\n  add(connection);\n  return connection;\n}",
        "accept_response": "Connection register(SocketChannel channel, int ingressPort,\n    boolean isOnAuxiliaryPort) {\n  if (isFull()) {\n    return null;\n  }\n  Connection connection = new Connection(channel, Time.now(),\n      ingressPort, isOnAuxiliaryPort);\n  add(connection);\n  LOG.debug(\"Server connection from {}; # active connections: {}; # queued calls: {}.\",\n      connection, size(), callQueue.size());\n  return connection;\n}",
        "reject_response": "Connection register(SocketChannel channel, int ingressPort,\n    boolean isOnAuxiliaryPort) {\n  if (isFull()) {\n    return null;\n  }\n  Connection connection = new Connection(channel, Time.now(),\n      ingressPort, isOnAuxiliaryPort);\n  add(connection);\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"Server connection from \" + connection +\n        \"; # active connections: \" + size() +\n        \"; # queued calls: \" + callQueue.size());\n  }\n  return connection;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2906,
        "instruction": "public DNSLookupContinuation onDNSResponse(DNSResponse lookup, SPFSession spfData) throws PermErrorException, TempErrorException, NeutralException, NoneException {\n    try {\n        List<String> records = lookup.getResponse();\n\n        if (records == null) {\n            return null;\n        }\n\n        // See SPF-Spec 6.2\n        //\n        // If domain-spec is empty, or there are any DNS processing errors (any RCODE other than 0),\n        // or if no records are returned, or if more than one record is returned, or if there are syntax\n        // errors in the explanation string, then proceed as if no exp modifier was given.\n        if (records.size() > 1) {\n\n            // Only catch the error and return null\n\n        } else {\n\n            String exp = records.get(0);\n            if (exp.length()>=2 && exp.charAt(0) == '\"' && exp.charAt(exp.length() -1 ) == '\"') {\n                exp = exp.substring(1, exp.length() - 1);\n            }\n\n            spfData.setAttribute(ATTRIBUTE_EXPAND_EXPLANATION, exp);\n\n            if ((exp != null) && (!exp.equals(\"\"))) {\n\n                try {\n                    spfData.pushChecker(expandedExplanationChecker);\n                    return macroExpand.checkExpand(exp, spfData, MacroExpand.EXPLANATION);\n                } catch (PermErrorException e) {\n                    // ignore syntax error on explanation expansion\n                }\n            }\n\n        }\n\n\n    } catch (TimeoutException e) {\n        // Nothing todo here.. just return null\n    }\n    return null;\n}",
        "accept_response": "public DNSLookupContinuation onDNSResponse(DNSResponse lookup, SPFSession spfData) throws PermErrorException, TempErrorException, NeutralException, NoneException {\n    try {\n        List<String> records = lookup.getResponse();\n\n        if (records == null) {\n            return null;\n        }\n\n        // See SPF-Spec 6.2\n        //\n        // If domain-spec is empty, or there are any DNS processing errors (any RCODE other than 0),\n        // or if no records are returned, or if more than one record is returned, or if there are syntax\n        // errors in the explanation string, then proceed as if no exp modifier was given.\n        if (records.size() > 1) {\n\n            LOGGER.debug(\"More then one TXT-Record found for explanation\");\n            // Only catch the error and return null\n\n        } else {\n\n            String exp = records.get(0);\n            if (exp.length()>=2 && exp.charAt(0) == '\"' && exp.charAt(exp.length() -1 ) == '\"') {\n                exp = exp.substring(1, exp.length() - 1);\n            }\n\n            spfData.setAttribute(ATTRIBUTE_EXPAND_EXPLANATION, exp);\n\n            if ((exp != null) && (!exp.equals(\"\"))) {\n\n                try {\n                    spfData.pushChecker(expandedExplanationChecker);\n                    return macroExpand.checkExpand(exp, spfData, MacroExpand.EXPLANATION);\n                } catch (PermErrorException e) {\n                    // ignore syntax error on explanation expansion\n                }\n            }\n\n        }\n\n\n    } catch (TimeoutException e) {\n        // Nothing todo here.. just return null\n    }\n    return null;\n}",
        "reject_response": "public DNSLookupContinuation onDNSResponse(DNSResponse lookup, SPFSession spfData) throws PermErrorException, TempErrorException, NeutralException, NoneException {\n    try {\n        List<String> records = lookup.getResponse();\n\n        if (records == null) {\n            return null;\n        }\n\n        // See SPF-Spec 6.2\n        //\n        // If domain-spec is empty, or there are any DNS processing errors (any RCODE other than 0),\n        // or if no records are returned, or if more than one record is returned, or if there are syntax\n        // errors in the explanation string, then proceed as if no exp modifier was given.\n        if (records.size() > 1) {\n\n            log.debug(\"More then one TXT-Record found for explanation\");\n            // Only catch the error and return null\n\n        } else {\n\n            String exp = records.get(0);\n            if (exp.length()>=2 && exp.charAt(0) == '\"' && exp.charAt(exp.length() -1 ) == '\"') {\n                exp = exp.substring(1, exp.length() - 1);\n            }\n\n            spfData.setAttribute(ATTRIBUTE_EXPAND_EXPLANATION, exp);\n\n            if ((exp != null) && (!exp.equals(\"\"))) {\n\n                try {\n                    spfData.pushChecker(expandedExplanationChecker);\n                    return macroExpand.checkExpand(exp, spfData, MacroExpand.EXPLANATION);\n                } catch (PermErrorException e) {\n                    // ignore syntax error on explanation expansion\n                }\n            }\n\n        }\n\n\n    } catch (TimeoutException e) {\n        // Nothing todo here.. just return null\n    }\n    return null;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2885,
        "instruction": "public void reset() {\n  LOG.info(\"Resetting the JoshuaConfiguration to its defaults ...\");\n  LOG.info(\"\\n\\tResetting the StatefullFF global state index ...\");\n  LOG.info(\"\\n\\t...done\");\n  StatefulFF.resetGlobalStateIndex();\n  tms = new ArrayList<String>();\n  weights_file = \"\";\n  default_non_terminal = \"[X]\";\n  oovList = new ArrayList<OOVItem>();\n  oovList.add(new OOVItem(default_non_terminal, 1.0f));\n  goal_symbol = \"[GOAL]\";\n  amortized_sorting = true;\n  constrain_parse = false;\n  use_pos_labels = false;\n  true_oovs_only = false;\n  filter_grammar = false;\n  pop_limit = 100;\n  maxlen = 200;\n  use_unique_nbest = false;\n  include_align_index = false;\n  topN = 1;\n  outputFormat = \"%i ||| %s ||| %f ||| %c\";\n  num_parallel_decoders = 1;\n  hypergraphFilePattern = \"\";\n  mark_oovs = false;\n  // oracleFile = null;\n  parse = false; // perform synchronous parsing\n  features = new ArrayList<String>();\n  weights = new ArrayList<String>();\n  server_port = 0;\n\n  reordering_limit = 8;\n  num_translation_options = 20;\n}",
        "accept_response": "public void reset() {\n  LOG.info(\"Resetting the JoshuaConfiguration to its defaults ...\");\n  LOG.info(\"\\n\\tResetting the StatefullFF global state index ...\");\n  LOG.info(\"\\n\\t...done\");\n  StatefulFF.resetGlobalStateIndex();\n  tms = new ArrayList<String>();\n  weights_file = \"\";\n  default_non_terminal = \"[X]\";\n  oovList = new ArrayList<OOVItem>();\n  oovList.add(new OOVItem(default_non_terminal, 1.0f));\n  goal_symbol = \"[GOAL]\";\n  amortized_sorting = true;\n  constrain_parse = false;\n  use_pos_labels = false;\n  true_oovs_only = false;\n  filter_grammar = false;\n  pop_limit = 100;\n  maxlen = 200;\n  use_unique_nbest = false;\n  include_align_index = false;\n  topN = 1;\n  outputFormat = \"%i ||| %s ||| %f ||| %c\";\n  num_parallel_decoders = 1;\n  hypergraphFilePattern = \"\";\n  mark_oovs = false;\n  // oracleFile = null;\n  parse = false; // perform synchronous parsing\n  features = new ArrayList<String>();\n  weights = new ArrayList<String>();\n  server_port = 0;\n\n  reordering_limit = 8;\n  num_translation_options = 20;\n  LOG.info(\"...done\");\n}",
        "reject_response": "public void reset() {\n  LOG.info(\"Resetting the JoshuaConfiguration to its defaults ...\");\n  LOG.info(\"\\n\\tResetting the StatefullFF global state index ...\");\n  LOG.info(\"\\n\\t...done\");\n  StatefulFF.resetGlobalStateIndex();\n  tms = new ArrayList<String>();\n  weights_file = \"\";\n  default_non_terminal = \"[X]\";\n  oovList = new ArrayList<OOVItem>();\n  oovList.add(new OOVItem(default_non_terminal, 1.0f));\n  goal_symbol = \"[GOAL]\";\n  amortized_sorting = true;\n  constrain_parse = false;\n  use_pos_labels = false;\n  true_oovs_only = false;\n  filter_grammar = false;\n  pop_limit = 100;\n  maxlen = 200;\n  use_unique_nbest = false;\n  include_align_index = false;\n  topN = 1;\n  outputFormat = \"%i ||| %s ||| %f ||| %c\";\n  num_parallel_decoders = 1;\n  hypergraphFilePattern = \"\";\n  mark_oovs = false;\n  // oracleFile = null;\n  parse = false; // perform synchronous parsing\n  features = new ArrayList<String>();\n  weights = new ArrayList<String>();\n  server_port = 0;\n\n  reordering_limit = 8;\n  num_translation_options = 20;\n  logger.info(\"...done\");\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3081,
        "instruction": "private static Set<String> getUnixGroups(final String user) throws IOException {\n    String result = \"\";\n    try {\n        result = ShellUtils.execCommand(ShellUtils.getGroupsForUserCommand(user));\n    } catch (ExitCodeException e) {\n        // if we didn't get the group - just return empty list;\n        return new HashSet<String>();\n    }\n\n    StringTokenizer tokenizer =\n        new StringTokenizer(result, ShellUtils.TOKEN_SEPARATOR_REGEX);\n    Set<String> groups = new HashSet<String>();\n    while (tokenizer.hasMoreTokens()) {\n        groups.add(tokenizer.nextToken());\n    }\n    return groups;\n}",
        "accept_response": "private static Set<String> getUnixGroups(final String user) throws IOException {\n    String result = \"\";\n    try {\n        result = ShellUtils.execCommand(ShellUtils.getGroupsForUserCommand(user));\n    } catch (ExitCodeException e) {\n        // if we didn't get the group - just return empty list;\n        LOG.debug(\"unable to get groups for user \" + user + \".ShellUtils command failed with exit code \"+ e.getExitCode());\n        return new HashSet<String>();\n    }\n\n    StringTokenizer tokenizer =\n        new StringTokenizer(result, ShellUtils.TOKEN_SEPARATOR_REGEX);\n    Set<String> groups = new HashSet<String>();\n    while (tokenizer.hasMoreTokens()) {\n        groups.add(tokenizer.nextToken());\n    }\n    return groups;\n}",
        "reject_response": "private static Set<String> getUnixGroups(final String user) throws IOException {\n    String result = \"\";\n    try {\n        result = ShellUtils.execCommand(ShellUtils.getGroupsForUserCommand(user));\n    } catch (ExitCodeException e) {\n        // if we didn't get the group - just return empty list;\n        LOG.warn(\"got exception trying to get groups for user \" + user, e);\n        return new HashSet<String>();\n    }\n\n    StringTokenizer tokenizer =\n        new StringTokenizer(result, ShellUtils.TOKEN_SEPARATOR_REGEX);\n    Set<String> groups = new HashSet<String>();\n    while (tokenizer.hasMoreTokens()) {\n        groups.add(tokenizer.nextToken());\n    }\n    return groups;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2660,
        "instruction": "@Override\nprotected boolean operateOnPartitionedRegion(DistributionManager dm, PartitionedRegion r,\n    long startTime) throws CacheException, ForceReattemptException {\n  Map<Integer, SizeEntry> sizes;\n  if (r != null) {\n    PartitionedRegionDataStore ds = r.getDataStore();\n    if (ds != null) { // datastore exists\n      if (this.bucketIds != null) {\n        if (estimate) {\n          sizes = ds.getSizeEstimateLocallyForBuckets(this.bucketIds);\n        } else {\n          sizes = ds.getSizeLocallyForBuckets(this.bucketIds);\n        }\n      } else {\n        if (estimate) {\n          sizes = ds.getSizeEstimateForLocalPrimaryBuckets();\n        } else {\n          sizes = ds.getSizeForLocalBuckets();\n        }\n      }\n      r.getPrStats().endPartitionMessagesProcessing(startTime);\n      SizeReplyMessage.send(getSender(), getProcessorId(), dm, sizes);\n    } // datastore exists\n    else {\n      logger.warn(LocalizedMessage.create(\n          LocalizedStrings.SizeMessage_SIZEMESSAGE_DATA_STORE_NOT_CONFIGURED_FOR_THIS_MEMBER));\n      ReplyMessage.send(getSender(), getProcessorId(),\n          new ReplyException(new ForceReattemptException(\n              LocalizedStrings.SizeMessage_0_1_NO_DATASTORE_HERE_2.toLocalizedString())),\n          dm, r.isInternalRegion());\n    }\n  } else {\n    if (logger.isDebugEnabled()) {\n      // Note that this is more likely to happen with this message\n      // because of it returning false from failIfRegionMissing.\n    }\n    ReplyMessage.send(getSender(), getProcessorId(),\n        new ReplyException(new ForceReattemptException(\n            LocalizedStrings.SizeMessage_0_COULD_NOT_FIND_PARTITIONED_REGION_WITH_ID_1\n                .toLocalizedString(\n                    new Object[] {dm.getDistributionManagerId(), Integer.valueOf(regionId)}))),\n        dm, r != null && r.isInternalRegion());\n  }\n  // Unless there was an exception thrown, this message handles sending the\n  // response\n  return false;\n}",
        "accept_response": "@Override\nprotected boolean operateOnPartitionedRegion(DistributionManager dm, PartitionedRegion r,\n    long startTime) throws CacheException, ForceReattemptException {\n  Map<Integer, SizeEntry> sizes;\n  if (r != null) {\n    PartitionedRegionDataStore ds = r.getDataStore();\n    if (ds != null) { // datastore exists\n      if (this.bucketIds != null) {\n        if (estimate) {\n          sizes = ds.getSizeEstimateLocallyForBuckets(this.bucketIds);\n        } else {\n          sizes = ds.getSizeLocallyForBuckets(this.bucketIds);\n        }\n      } else {\n        if (estimate) {\n          sizes = ds.getSizeEstimateForLocalPrimaryBuckets();\n        } else {\n          sizes = ds.getSizeForLocalBuckets();\n        }\n      }\n      r.getPrStats().endPartitionMessagesProcessing(startTime);\n      SizeReplyMessage.send(getSender(), getProcessorId(), dm, sizes);\n    } // datastore exists\n    else {\n      logger.warn(LocalizedMessage.create(\n          LocalizedStrings.SizeMessage_SIZEMESSAGE_DATA_STORE_NOT_CONFIGURED_FOR_THIS_MEMBER));\n      ReplyMessage.send(getSender(), getProcessorId(),\n          new ReplyException(new ForceReattemptException(\n              LocalizedStrings.SizeMessage_0_1_NO_DATASTORE_HERE_2.toLocalizedString())),\n          dm, r.isInternalRegion());\n    }\n  } else {\n    if (logger.isDebugEnabled()) {\n      // Note that this is more likely to happen with this message\n      // because of it returning false from failIfRegionMissing.\n      logger.debug(LocalizedMessage.create(\n          LocalizedStrings.SizeMessage_SIZEMESSAGE_REGION_NOT_FOUND_FOR_THIS_MEMBER, regionId));\n    }\n    ReplyMessage.send(getSender(), getProcessorId(),\n        new ReplyException(new ForceReattemptException(\n            LocalizedStrings.SizeMessage_0_COULD_NOT_FIND_PARTITIONED_REGION_WITH_ID_1\n                .toLocalizedString(\n                    new Object[] {dm.getDistributionManagerId(), Integer.valueOf(regionId)}))),\n        dm, r != null && r.isInternalRegion());\n  }\n  // Unless there was an exception thrown, this message handles sending the\n  // response\n  return false;\n}",
        "reject_response": "@Override\nprotected boolean operateOnPartitionedRegion(DistributionManager dm, PartitionedRegion r,\n    long startTime) throws CacheException, ForceReattemptException {\n  Map<Integer, SizeEntry> sizes;\n  if (r != null) {\n    PartitionedRegionDataStore ds = r.getDataStore();\n    if (ds != null) { // datastore exists\n      if (this.bucketIds != null) {\n        if (estimate) {\n          sizes = ds.getSizeEstimateLocallyForBuckets(this.bucketIds);\n        } else {\n          sizes = ds.getSizeLocallyForBuckets(this.bucketIds);\n        }\n      } else {\n        if (estimate) {\n          sizes = ds.getSizeEstimateForLocalPrimaryBuckets();\n        } else {\n          sizes = ds.getSizeForLocalBuckets();\n        }\n      }\n      r.getPrStats().endPartitionMessagesProcessing(startTime);\n      SizeReplyMessage.send(getSender(), getProcessorId(), dm, sizes);\n    } // datastore exists\n    else {\n      logger.warn(LocalizedMessage.create(\n          LocalizedStrings.SizeMessage_SIZEMESSAGE_DATA_STORE_NOT_CONFIGURED_FOR_THIS_MEMBER));\n      ReplyMessage.send(getSender(), getProcessorId(),\n          new ReplyException(new ForceReattemptException(\n              LocalizedStrings.SizeMessage_0_1_NO_DATASTORE_HERE_2.toLocalizedString())),\n          dm, r.isInternalRegion());\n    }\n  } else {\n    if (logger.isDebugEnabled()) {\n      // Note that this is more likely to happen with this message\n      // because of it returning false from failIfRegionMissing.\n    logger.warn(LocalizedMessage.create(\n        LocalizedStrings.SizeMessage_SIZEMESSAGE_REGION_NOT_FOUND_FOR_THIS_MEMBER, regionId));\n    }\n    ReplyMessage.send(getSender(), getProcessorId(),\n        new ReplyException(new ForceReattemptException(\n            LocalizedStrings.SizeMessage_0_COULD_NOT_FIND_PARTITIONED_REGION_WITH_ID_1\n                .toLocalizedString(\n                    new Object[] {dm.getDistributionManagerId(), Integer.valueOf(regionId)}))),\n        dm, r != null && r.isInternalRegion());\n  }\n  // Unless there was an exception thrown, this message handles sending the\n  // response\n  return false;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3030,
        "instruction": "@Override\npublic boolean canInit() {\n  setListenAddressWithoutSchema(TransportConfig.getAddress());\n\n  URIEndpointObject ep = (URIEndpointObject) getEndpoint().getAddress();\n  if (ep == null) {\n    return true;\n  }\n\n  if (!NetUtils.canTcpListen(ep.getSocketAddress().getAddress(), ep.getPort())) {\n    return false;\n  }\n\n  return true;\n}",
        "accept_response": "@Override\npublic boolean canInit() {\n  setListenAddressWithoutSchema(TransportConfig.getAddress());\n\n  URIEndpointObject ep = (URIEndpointObject) getEndpoint().getAddress();\n  if (ep == null) {\n    return true;\n  }\n\n  if (!NetUtils.canTcpListen(ep.getSocketAddress().getAddress(), ep.getPort())) {\n    LOGGER.warn(\n        \"Can not start VertxRestTransport, the port:{} may have been occupied. You can ignore this message if you are using a web container like tomcat.\",\n        ep.getPort());\n    return false;\n  }\n\n  return true;\n}",
        "reject_response": "@Override\npublic boolean canInit() {\n  setListenAddressWithoutSchema(TransportConfig.getAddress());\n\n  URIEndpointObject ep = (URIEndpointObject) getEndpoint().getAddress();\n  if (ep == null) {\n    return true;\n  }\n\n  if (!NetUtils.canTcpListen(ep.getSocketAddress().getAddress(), ep.getPort())) {\n    log.info(\"can not listen {}, skip {}.\", ep.getSocketAddress(), this.getClass().getName());\n    return false;\n  }\n\n  return true;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2786,
        "instruction": "ClassicHttpResponse cacheAndReturnResponse(\n        final HttpHost target,\n        final HttpRequest request,\n        final ClassicHttpResponse backendResponse,\n        final ExecChain.Scope scope,\n        final Date requestSent,\n        final Date responseReceived) throws IOException {\n    LOG.debug(\"Caching backend response\");\n    final ByteArrayBuffer buf;\n    final HttpEntity entity = backendResponse.getEntity();\n    if (entity != null) {\n        buf = new ByteArrayBuffer(1024);\n        final InputStream inStream = entity.getContent();\n        final byte[] tmp = new byte[2048];\n        long total = 0;\n        int l;\n        while ((l = inStream.read(tmp)) != -1) {\n            buf.append(tmp, 0, l);\n            total += l;\n            if (total > cacheConfig.getMaxObjectSize()) {\n                LOG.debug(\"Backend response content length exceeds maximum\");\n                backendResponse.setEntity(new CombinedEntity(entity, buf));\n                return backendResponse;\n            }\n        }\n    } else {\n        buf = null;\n    }\n    backendResponse.close();\n\n    final HttpCacheEntry cacheEntry;\n    if (cacheConfig.isFreshnessCheckEnabled()) {\n        final HttpCacheEntry existingEntry = responseCache.getCacheEntry(target, request);\n        if (DateUtils.isAfter(existingEntry, backendResponse, HttpHeaders.DATE)) {\n            LOG.debug(\"Backend already contains fresher cache entry\");\n            cacheEntry = existingEntry;\n        } else {\n            cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n        }\n    } else {\n        cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n        LOG.debug(\"Backend response successfully cached (freshness check skipped)\");\n    }\n    return convert(responseGenerator.generateResponse(request, cacheEntry), scope);\n}",
        "accept_response": "ClassicHttpResponse cacheAndReturnResponse(\n        final HttpHost target,\n        final HttpRequest request,\n        final ClassicHttpResponse backendResponse,\n        final ExecChain.Scope scope,\n        final Date requestSent,\n        final Date responseReceived) throws IOException {\n    LOG.debug(\"Caching backend response\");\n    final ByteArrayBuffer buf;\n    final HttpEntity entity = backendResponse.getEntity();\n    if (entity != null) {\n        buf = new ByteArrayBuffer(1024);\n        final InputStream inStream = entity.getContent();\n        final byte[] tmp = new byte[2048];\n        long total = 0;\n        int l;\n        while ((l = inStream.read(tmp)) != -1) {\n            buf.append(tmp, 0, l);\n            total += l;\n            if (total > cacheConfig.getMaxObjectSize()) {\n                LOG.debug(\"Backend response content length exceeds maximum\");\n                backendResponse.setEntity(new CombinedEntity(entity, buf));\n                return backendResponse;\n            }\n        }\n    } else {\n        buf = null;\n    }\n    backendResponse.close();\n\n    final HttpCacheEntry cacheEntry;\n    if (cacheConfig.isFreshnessCheckEnabled()) {\n        final HttpCacheEntry existingEntry = responseCache.getCacheEntry(target, request);\n        if (DateUtils.isAfter(existingEntry, backendResponse, HttpHeaders.DATE)) {\n            LOG.debug(\"Backend already contains fresher cache entry\");\n            cacheEntry = existingEntry;\n        } else {\n            cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n            LOG.debug(\"Backend response successfully cached\");\n        }\n    } else {\n        cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n        LOG.debug(\"Backend response successfully cached (freshness check skipped)\");\n    }\n    return convert(responseGenerator.generateResponse(request, cacheEntry), scope);\n}",
        "reject_response": "ClassicHttpResponse cacheAndReturnResponse(\n        final HttpHost target,\n        final HttpRequest request,\n        final ClassicHttpResponse backendResponse,\n        final ExecChain.Scope scope,\n        final Date requestSent,\n        final Date responseReceived) throws IOException {\n    LOG.debug(\"Caching backend response\");\n    final ByteArrayBuffer buf;\n    final HttpEntity entity = backendResponse.getEntity();\n    if (entity != null) {\n        buf = new ByteArrayBuffer(1024);\n        final InputStream inStream = entity.getContent();\n        final byte[] tmp = new byte[2048];\n        long total = 0;\n        int l;\n        while ((l = inStream.read(tmp)) != -1) {\n            buf.append(tmp, 0, l);\n            total += l;\n            if (total > cacheConfig.getMaxObjectSize()) {\n                LOG.debug(\"Backend response content length exceeds maximum\");\n                backendResponse.setEntity(new CombinedEntity(entity, buf));\n                return backendResponse;\n            }\n        }\n    } else {\n        buf = null;\n    }\n    backendResponse.close();\n\n    final HttpCacheEntry cacheEntry;\n    if (cacheConfig.isFreshnessCheckEnabled()) {\n        final HttpCacheEntry existingEntry = responseCache.getCacheEntry(target, request);\n        if (DateUtils.isAfter(existingEntry, backendResponse, HttpHeaders.DATE)) {\n            LOG.debug(\"Backend already contains fresher cache entry\");\n            cacheEntry = existingEntry;\n        } else {\n            cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n            log.debug(\"Backend response successfully cached\");\n        }\n    } else {\n        cacheEntry = responseCache.createCacheEntry(target, request, backendResponse, buf, requestSent, responseReceived);\n        LOG.debug(\"Backend response successfully cached (freshness check skipped)\");\n    }\n    return convert(responseGenerator.generateResponse(request, cacheEntry), scope);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3177,
        "instruction": "@Override\npublic void updateContainerResourceAsync(Container container) {\n  if (!(callbackHandler instanceof AbstractCallbackHandler)) {\n    LOG.error(\"Callback handler does not implement container resource \"\n        + \"increase callback methods\");\n    return;\n  }\n  AbstractCallbackHandler handler = (AbstractCallbackHandler) callbackHandler;\n  if (containers.get(container.getId()) == null) {\n    handler.onUpdateContainerResourceError(\n        container.getId(),\n        RPCUtil.getRemoteException(\n            \"Container \" + container.getId() +\n                \" is neither started nor scheduled to start\"));\n  }\n  try {\n    events.put(new UpdateContainerResourceEvent(container, false));\n  } catch (InterruptedException e) {\n    handler.onUpdateContainerResourceError(container.getId(), e);\n  }\n}",
        "accept_response": "@Override\npublic void updateContainerResourceAsync(Container container) {\n  if (!(callbackHandler instanceof AbstractCallbackHandler)) {\n    LOG.error(\"Callback handler does not implement container resource \"\n        + \"increase callback methods\");\n    return;\n  }\n  AbstractCallbackHandler handler = (AbstractCallbackHandler) callbackHandler;\n  if (containers.get(container.getId()) == null) {\n    handler.onUpdateContainerResourceError(\n        container.getId(),\n        RPCUtil.getRemoteException(\n            \"Container \" + container.getId() +\n                \" is neither started nor scheduled to start\"));\n  }\n  try {\n    events.put(new UpdateContainerResourceEvent(container, false));\n  } catch (InterruptedException e) {\n    LOG.warn(\"Exception when scheduling the event of \" +\n            \"increasing resource of Container {}.\", container.getId());\n    handler.onUpdateContainerResourceError(container.getId(), e);\n  }\n}",
        "reject_response": "@Override\npublic void updateContainerResourceAsync(Container container) {\n  if (!(callbackHandler instanceof AbstractCallbackHandler)) {\n    LOG.error(\"Callback handler does not implement container resource \"\n        + \"increase callback methods\");\n    return;\n  }\n  AbstractCallbackHandler handler = (AbstractCallbackHandler) callbackHandler;\n  if (containers.get(container.getId()) == null) {\n    handler.onUpdateContainerResourceError(\n        container.getId(),\n        RPCUtil.getRemoteException(\n            \"Container \" + container.getId() +\n                \" is neither started nor scheduled to start\"));\n  }\n  try {\n    events.put(new UpdateContainerResourceEvent(container, false));\n  } catch (InterruptedException e) {\n    LOG.warn(\"Exception when scheduling the event of increasing resource of \"\n        + \"Container \" + container.getId());\n    handler.onUpdateContainerResourceError(container.getId(), e);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2832,
        "instruction": "private SegmentArchiveResult archiveSegment(long absIdx) throws StorageException {\n    long segIdx = absIdx % dsCfg.getWalSegments();\n\n    File origFile = new File(walWorkDir, FileDescriptor.fileName(segIdx));\n\n    String name = FileDescriptor.fileName(absIdx);\n\n    File dstTmpFile = new File(walArchiveDir, name + \".tmp\");\n\n    File dstFile = new File(walArchiveDir, name);\n\n    if (log.isInfoEnabled())\n\n    try {\n        Files.deleteIfExists(dstTmpFile.toPath());\n\n        Files.copy(origFile.toPath(), dstTmpFile.toPath());\n\n        Files.move(dstTmpFile.toPath(), dstFile.toPath());\n\n        if (mode != WALMode.NONE) {\n            try (FileIO f0 = ioFactory.create(dstFile, CREATE, READ, WRITE)) {\n                f0.force();\n            }\n        }\n    }\n    catch (IOException e) {\n        throw new StorageException(\"Failed to archive WAL segment [\" +\n            \"srcFile=\" + origFile.getAbsolutePath() +\n            \", dstFile=\" + dstTmpFile.getAbsolutePath() + ']', e);\n    }\n\n    if (log.isInfoEnabled())\n        log.info(\"Copied file [src=\" + origFile.getAbsolutePath() +\n            \", dst=\" + dstFile.getAbsolutePath() + ']');\n\n    return new SegmentArchiveResult(absIdx, origFile, dstFile);\n}",
        "accept_response": "private SegmentArchiveResult archiveSegment(long absIdx) throws StorageException {\n    long segIdx = absIdx % dsCfg.getWalSegments();\n\n    File origFile = new File(walWorkDir, FileDescriptor.fileName(segIdx));\n\n    String name = FileDescriptor.fileName(absIdx);\n\n    File dstTmpFile = new File(walArchiveDir, name + \".tmp\");\n\n    File dstFile = new File(walArchiveDir, name);\n\n    if (log.isInfoEnabled())\n        log.info(\"Starting to copy WAL segment [absIdx=\" + absIdx + \", segIdx=\" + segIdx +\n            \", origFile=\" + origFile.getAbsolutePath() + \", dstFile=\" + dstFile.getAbsolutePath() + ']');\n\n    try {\n        Files.deleteIfExists(dstTmpFile.toPath());\n\n        Files.copy(origFile.toPath(), dstTmpFile.toPath());\n\n        Files.move(dstTmpFile.toPath(), dstFile.toPath());\n\n        if (mode != WALMode.NONE) {\n            try (FileIO f0 = ioFactory.create(dstFile, CREATE, READ, WRITE)) {\n                f0.force();\n            }\n        }\n    }\n    catch (IOException e) {\n        throw new StorageException(\"Failed to archive WAL segment [\" +\n            \"srcFile=\" + origFile.getAbsolutePath() +\n            \", dstFile=\" + dstTmpFile.getAbsolutePath() + ']', e);\n    }\n\n    if (log.isInfoEnabled())\n        log.info(\"Copied file [src=\" + origFile.getAbsolutePath() +\n            \", dst=\" + dstFile.getAbsolutePath() + ']');\n\n    return new SegmentArchiveResult(absIdx, origFile, dstFile);\n}",
        "reject_response": "private SegmentArchiveResult archiveSegment(long absIdx) throws StorageException {\n    long segIdx = absIdx % dsCfg.getWalSegments();\n\n    File origFile = new File(walWorkDir, FileDescriptor.fileName(segIdx));\n\n    String name = FileDescriptor.fileName(absIdx);\n\n    File dstTmpFile = new File(walArchiveDir, name + \".tmp\");\n\n    File dstFile = new File(walArchiveDir, name);\n\n    if (log.isInfoEnabled())\n    if (log.isDebugEnabled())\n        log.debug(\"Starting to copy WAL segment [absIdx=\" + absIdx + \", segIdx=\" + segIdx +\n\n    try {\n        Files.deleteIfExists(dstTmpFile.toPath());\n\n        Files.copy(origFile.toPath(), dstTmpFile.toPath());\n\n        Files.move(dstTmpFile.toPath(), dstFile.toPath());\n\n        if (mode != WALMode.NONE) {\n            try (FileIO f0 = ioFactory.create(dstFile, CREATE, READ, WRITE)) {\n                f0.force();\n            }\n        }\n    }\n    catch (IOException e) {\n        throw new StorageException(\"Failed to archive WAL segment [\" +\n            \"srcFile=\" + origFile.getAbsolutePath() +\n            \", dstFile=\" + dstTmpFile.getAbsolutePath() + ']', e);\n    }\n\n    if (log.isInfoEnabled())\n        log.info(\"Copied file [src=\" + origFile.getAbsolutePath() +\n            \", dst=\" + dstFile.getAbsolutePath() + ']');\n\n    return new SegmentArchiveResult(absIdx, origFile, dstFile);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3027,
        "instruction": "private String getPrettyFormat(OutgoingMessageEnvelope envelope) {\n  lastErrorMsg = \"\";\n  String value = new String((byte[]) envelope.getMessage());\n  ObjectMapper mapper = new ObjectMapper();\n  String formattedValue;\n  try {\n    Object json = mapper.readValue(value, Object.class);\n    formattedValue = mapper.writerWithDefaultPrettyPrinter().writeValueAsString(json);\n  } catch (IOException ex) {\n    formattedValue = value;\n    String msg = \"getPrettyFormat failed with exception while formatting json \";\n    lastErrorMsg = msg + ex.toString();\n  }\n  return formattedValue;\n}",
        "accept_response": "private String getPrettyFormat(OutgoingMessageEnvelope envelope) {\n  lastErrorMsg = \"\";\n  String value = new String((byte[]) envelope.getMessage());\n  ObjectMapper mapper = new ObjectMapper();\n  String formattedValue;\n  try {\n    Object json = mapper.readValue(value, Object.class);\n    formattedValue = mapper.writerWithDefaultPrettyPrinter().writeValueAsString(json);\n  } catch (IOException ex) {\n    formattedValue = value;\n    String msg = \"getPrettyFormat failed with exception while formatting json \";\n    lastErrorMsg = msg + ex.toString();\n    LOG.error(msg, ex);\n  }\n  return formattedValue;\n}",
        "reject_response": "private String getPrettyFormat(OutgoingMessageEnvelope envelope) {\n  lastErrorMsg = \"\";\n  String value = new String((byte[]) envelope.getMessage());\n  ObjectMapper mapper = new ObjectMapper();\n  String formattedValue;\n  try {\n    Object json = mapper.readValue(value, Object.class);\n    formattedValue = mapper.writerWithDefaultPrettyPrinter().writeValueAsString(json);\n  } catch (IOException ex) {\n    formattedValue = value;\n    String msg = \"getPrettyFormat failed with exception while formatting json \";\n    lastErrorMsg = msg + ex.toString();\n    LOG.error(\"Error while formatting json\", e);\n  }\n  return formattedValue;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2655,
        "instruction": "private ObjectNode getSelectedRegionJson(Cluster cluster, String selectedRegionFullPath) {\n  Long totalHeapSize = cluster.getTotalHeapSize();\n  Long totalDiskUsage = cluster.getTotalBytesOnDisk();\n\n  Cluster.Region reg = cluster.getClusterRegion(selectedRegionFullPath);\n  if (reg != null) {\n    ObjectNode regionJSON = mapper.createObjectNode();\n\n    regionJSON.put(\"name\", reg.getName());\n    regionJSON.put(\"path\", reg.getFullPath());\n    regionJSON.put(\"totalMemory\", totalHeapSize);\n    regionJSON.put(\"systemRegionEntryCount\", reg.getSystemRegionEntryCount());\n    regionJSON.put(\"memberCount\", reg.getMemberCount());\n\n    final String regionType = reg.getRegionType();\n    regionJSON.put(\"type\", regionType);\n    regionJSON.put(\"getsRate\", reg.getGetsRate());\n    regionJSON.put(\"putsRate\", reg.getPutsRate());\n    regionJSON.put(\"lruEvictionRate\", reg.getLruEvictionRate());\n\n    DecimalFormat df2 = new DecimalFormat(PulseConstants.DECIMAL_FORMAT_PATTERN);\n    Cluster.Member[] clusterMembersList = cluster.getMembers();\n\n    // collect members of this region\n    List<Cluster.Member> clusterMembersL = new ArrayList<Cluster.Member>();\n    for (String memberName : reg.getMemberName()) {\n      for (Cluster.Member member : clusterMembersList) {\n        String name = member.getName();\n        name = name.replace(\":\", \"-\");\n        String id = member.getId();\n        id = id.replace(\":\", \"-\");\n\n        if ((memberName.equals(id)) || (memberName.equals(name))) {\n          clusterMembersL.add(member);\n        }\n      }\n    }\n\n    // sort members of this region\n    Collections.sort(clusterMembersL, memberCurrentHeapUsageComparator);\n\n    // return sorted member list by heap usage\n    ArrayNode memberArray = mapper.createArrayNode();\n    for (Cluster.Member member : clusterMembersL) {\n      ObjectNode regionMember = mapper.createObjectNode();\n      regionMember.put(\"memberId\", member.getId());\n      regionMember.put(\"name\", member.getName());\n      regionMember.put(\"host\", member.getHost());\n\n      long usedHeapSize = cluster.getUsedHeapSize();\n      long currentHeap = member.getCurrentHeapSize();\n      if (usedHeapSize > 0) {\n        double heapUsage = ((double) currentHeap / (double) usedHeapSize) * 100;\n        regionMember.put(\"heapUsage\", Double.valueOf(df2.format(heapUsage)));\n      } else {\n        regionMember.put(\"heapUsage\", 0);\n      }\n      double currentCPUUsage = member.getCpuUsage();\n      double loadAvg = member.getLoadAverage();\n\n      regionMember.put(\"cpuUsage\", Double.valueOf(df2.format(currentCPUUsage)));\n      regionMember.put(\"currentHeapUsage\", member.getCurrentHeapSize());\n      regionMember.put(\"isManager\", member.isManager());\n      regionMember.put(\"uptime\", TimeUtils.convertTimeSecondsToHMS(member.getUptime()));\n\n      regionMember.put(\"loadAvg\", Double.valueOf(df2.format(loadAvg)));\n      regionMember.put(\"sockets\", member.getTotalFileDescriptorOpen());\n      regionMember.put(\"threads\", member.getNumThreads());\n\n      regionMember.put(\"clients\", member.getMemberClientsHMap().size());\n\n      regionMember.put(\"queues\", member.getQueueBacklog());\n      memberArray.add(regionMember);\n    }\n\n    regionJSON.put(\"members\", memberArray);\n    regionJSON.put(\"entryCount\", reg.getSystemRegionEntryCount());\n\n    regionJSON.put(\"persistence\",\n        reg.getPersistentEnabled() ? PulseService.VALUE_ON : PulseService.VALUE_OFF);\n\n    regionJSON.put(\"isEnableOffHeapMemory\",\n        reg.isEnableOffHeapMemory() ? PulseService.VALUE_ON : PulseService.VALUE_OFF);\n\n    String regCompCodec = reg.getCompressionCodec();\n    if (StringUtils.isNotBlank(regCompCodec)) {\n      regionJSON.put(\"compressionCodec\", reg.getCompressionCodec());\n    } else {\n      regionJSON.put(\"compressionCodec\", PulseService.VALUE_NA);\n    }\n\n\n    regionJSON.put(\"regionPath\", reg.getFullPath());\n\n\n    regionJSON.put(\"memoryReadsTrend\", mapper.<JsonNode>valueToTree(\n        reg.getRegionStatisticTrend(Cluster.Region.REGION_STAT_GETS_PER_SEC_TREND)));\n    regionJSON.put(\"memoryWritesTrend\", mapper.<JsonNode>valueToTree(\n        reg.getRegionStatisticTrend(Cluster.Region.REGION_STAT_PUTS_PER_SEC_TREND)));\n    regionJSON.put(\"diskReadsTrend\", mapper.<JsonNode>valueToTree(\n        reg.getRegionStatisticTrend(Cluster.Region.REGION_STAT_DISK_READS_PER_SEC_TREND)));\n    regionJSON.put(\"diskWritesTrend\", mapper.<JsonNode>valueToTree(\n        reg.getRegionStatisticTrend(Cluster.Region.REGION_STAT_DISK_WRITES_PER_SEC_TREND)));\n\n    regionJSON.put(\"emptyNodes\", reg.getEmptyNode());\n    Long entrySize = reg.getEntrySize();\n    DecimalFormat form = new DecimalFormat(PulseConstants.DECIMAL_FORMAT_PATTERN_2);\n    String entrySizeInMB = form.format(entrySize / (1024f * 1024f));\n    if (entrySize < 0) {\n      regionJSON.put(this.ENTRY_SIZE, PulseService.VALUE_NA);\n    } else {\n      regionJSON.put(this.ENTRY_SIZE, entrySizeInMB);\n    }\n    regionJSON.put(\"dataUsage\", reg.getDiskUsage());\n    regionJSON.put(\"wanEnabled\", reg.getWanEnabled());\n    regionJSON.put(\"totalDataUsage\", totalDiskUsage);\n    regionJSON.put(\"memoryUsage\", entrySizeInMB);\n\n    return regionJSON;\n  } else {\n    ObjectNode responseJSON = mapper.createObjectNode();\n    responseJSON.put(\"errorOnRegion\", \"Region [\" + selectedRegionFullPath + \"] is not available\");\n    return responseJSON;\n  }\n}",
        "accept_response": "private ObjectNode getSelectedRegionJson(Cluster cluster, String selectedRegionFullPath) {\n  Long totalHeapSize = cluster.getTotalHeapSize();\n  Long totalDiskUsage = cluster.getTotalBytesOnDisk();\n\n  Cluster.Region reg = cluster.getClusterRegion(selectedRegionFullPath);\n  if (reg != null) {\n    ObjectNode regionJSON = mapper.createObjectNode();\n\n    regionJSON.put(\"name\", reg.getName());\n    regionJSON.put(\"path\", reg.getFullPath());\n    regionJSON.put(\"totalMemory\", totalHeapSize);\n    regionJSON.put(\"systemRegionEntryCount\", reg.getSystemRegionEntryCount());\n    regionJSON.put(\"memberCount\", reg.getMemberCount());\n\n    final String regionType = reg.getRegionType();\n    regionJSON.put(\"type\", regionType);\n    regionJSON.put(\"getsRate\", reg.getGetsRate());\n    regionJSON.put(\"putsRate\", reg.getPutsRate());\n    regionJSON.put(\"lruEvictionRate\", reg.getLruEvictionRate());\n\n    DecimalFormat df2 = new DecimalFormat(PulseConstants.DECIMAL_FORMAT_PATTERN);\n    Cluster.Member[] clusterMembersList = cluster.getMembers();\n\n    // collect members of this region\n    List<Cluster.Member> clusterMembersL = new ArrayList<Cluster.Member>();\n    for (String memberName : reg.getMemberName()) {\n      for (Cluster.Member member : clusterMembersList) {\n        String name = member.getName();\n        name = name.replace(\":\", \"-\");\n        String id = member.getId();\n        id = id.replace(\":\", \"-\");\n\n        if ((memberName.equals(id)) || (memberName.equals(name))) {\n          clusterMembersL.add(member);\n        }\n      }\n    }\n\n    // sort members of this region\n    Collections.sort(clusterMembersL, memberCurrentHeapUsageComparator);\n\n    // return sorted member list by heap usage\n    ArrayNode memberArray = mapper.createArrayNode();\n    for (Cluster.Member member : clusterMembersL) {\n      ObjectNode regionMember = mapper.createObjectNode();\n      regionMember.put(\"memberId\", member.getId());\n      regionMember.put(\"name\", member.getName());\n      regionMember.put(\"host\", member.getHost());\n\n      long usedHeapSize = cluster.getUsedHeapSize();\n      long currentHeap = member.getCurrentHeapSize();\n      if (usedHeapSize > 0) {\n        double heapUsage = ((double) currentHeap / (double) usedHeapSize) * 100;\n        regionMember.put(\"heapUsage\", Double.valueOf(df2.format(heapUsage)));\n      } else {\n        regionMember.put(\"heapUsage\", 0);\n      }\n      double currentCPUUsage = member.getCpuUsage();\n      double loadAvg = member.getLoadAverage();\n\n      regionMember.put(\"cpuUsage\", Double.valueOf(df2.format(currentCPUUsage)));\n      regionMember.put(\"currentHeapUsage\", member.getCurrentHeapSize());\n      regionMember.put(\"isManager\", member.isManager());\n      regionMember.put(\"uptime\", TimeUtils.convertTimeSecondsToHMS(member.getUptime()));\n\n      regionMember.put(\"loadAvg\", Double.valueOf(df2.format(loadAvg)));\n      regionMember.put(\"sockets\", member.getTotalFileDescriptorOpen());\n      regionMember.put(\"threads\", member.getNumThreads());\n\n      regionMember.put(\"clients\", member.getMemberClientsHMap().size());\n\n      regionMember.put(\"queues\", member.getQueueBacklog());\n      memberArray.add(regionMember);\n    }\n\n    regionJSON.put(\"members\", memberArray);\n    regionJSON.put(\"entryCount\", reg.getSystemRegionEntryCount());\n\n    regionJSON.put(\"persistence\",\n        reg.getPersistentEnabled() ? PulseService.VALUE_ON : PulseService.VALUE_OFF);\n\n    regionJSON.put(\"isEnableOffHeapMemory\",\n        reg.isEnableOffHeapMemory() ? PulseService.VALUE_ON : PulseService.VALUE_OFF);\n\n    String regCompCodec = reg.getCompressionCodec();\n    if (StringUtils.isNotBlank(regCompCodec)) {\n      regionJSON.put(\"compressionCodec\", reg.getCompressionCodec());\n    } else {\n      regionJSON.put(\"compressionCodec\", PulseService.VALUE_NA);\n    }\n\n\n    regionJSON.put(\"regionPath\", reg.getFullPath());\n\n\n    regionJSON.put(\"memoryReadsTrend\", mapper.<JsonNode>valueToTree(\n        reg.getRegionStatisticTrend(Cluster.Region.REGION_STAT_GETS_PER_SEC_TREND)));\n    regionJSON.put(\"memoryWritesTrend\", mapper.<JsonNode>valueToTree(\n        reg.getRegionStatisticTrend(Cluster.Region.REGION_STAT_PUTS_PER_SEC_TREND)));\n    regionJSON.put(\"diskReadsTrend\", mapper.<JsonNode>valueToTree(\n        reg.getRegionStatisticTrend(Cluster.Region.REGION_STAT_DISK_READS_PER_SEC_TREND)));\n    regionJSON.put(\"diskWritesTrend\", mapper.<JsonNode>valueToTree(\n        reg.getRegionStatisticTrend(Cluster.Region.REGION_STAT_DISK_WRITES_PER_SEC_TREND)));\n\n    regionJSON.put(\"emptyNodes\", reg.getEmptyNode());\n    Long entrySize = reg.getEntrySize();\n    DecimalFormat form = new DecimalFormat(PulseConstants.DECIMAL_FORMAT_PATTERN_2);\n    String entrySizeInMB = form.format(entrySize / (1024f * 1024f));\n    if (entrySize < 0) {\n      regionJSON.put(this.ENTRY_SIZE, PulseService.VALUE_NA);\n    } else {\n      regionJSON.put(this.ENTRY_SIZE, entrySizeInMB);\n    }\n    regionJSON.put(\"dataUsage\", reg.getDiskUsage());\n    regionJSON.put(\"wanEnabled\", reg.getWanEnabled());\n    regionJSON.put(\"totalDataUsage\", totalDiskUsage);\n    regionJSON.put(\"memoryUsage\", entrySizeInMB);\n\n    logger.debug(\"calling getSelectedRegionJson :: regionJSON = {}\", regionJSON);\n    return regionJSON;\n  } else {\n    ObjectNode responseJSON = mapper.createObjectNode();\n    responseJSON.put(\"errorOnRegion\", \"Region [\" + selectedRegionFullPath + \"] is not available\");\n    return responseJSON;\n  }\n}",
        "reject_response": "private ObjectNode getSelectedRegionJson(Cluster cluster, String selectedRegionFullPath) {\n  Long totalHeapSize = cluster.getTotalHeapSize();\n  Long totalDiskUsage = cluster.getTotalBytesOnDisk();\n\n  Cluster.Region reg = cluster.getClusterRegion(selectedRegionFullPath);\n  if (reg != null) {\n    ObjectNode regionJSON = mapper.createObjectNode();\n\n    regionJSON.put(\"name\", reg.getName());\n    regionJSON.put(\"path\", reg.getFullPath());\n    regionJSON.put(\"totalMemory\", totalHeapSize);\n    regionJSON.put(\"systemRegionEntryCount\", reg.getSystemRegionEntryCount());\n    regionJSON.put(\"memberCount\", reg.getMemberCount());\n\n    final String regionType = reg.getRegionType();\n    regionJSON.put(\"type\", regionType);\n    regionJSON.put(\"getsRate\", reg.getGetsRate());\n    regionJSON.put(\"putsRate\", reg.getPutsRate());\n    regionJSON.put(\"lruEvictionRate\", reg.getLruEvictionRate());\n\n    DecimalFormat df2 = new DecimalFormat(PulseConstants.DECIMAL_FORMAT_PATTERN);\n    Cluster.Member[] clusterMembersList = cluster.getMembers();\n\n    // collect members of this region\n    List<Cluster.Member> clusterMembersL = new ArrayList<Cluster.Member>();\n    for (String memberName : reg.getMemberName()) {\n      for (Cluster.Member member : clusterMembersList) {\n        String name = member.getName();\n        name = name.replace(\":\", \"-\");\n        String id = member.getId();\n        id = id.replace(\":\", \"-\");\n\n        if ((memberName.equals(id)) || (memberName.equals(name))) {\n          clusterMembersL.add(member);\n        }\n      }\n    }\n\n    // sort members of this region\n    Collections.sort(clusterMembersL, memberCurrentHeapUsageComparator);\n\n    // return sorted member list by heap usage\n    ArrayNode memberArray = mapper.createArrayNode();\n    for (Cluster.Member member : clusterMembersL) {\n      ObjectNode regionMember = mapper.createObjectNode();\n      regionMember.put(\"memberId\", member.getId());\n      regionMember.put(\"name\", member.getName());\n      regionMember.put(\"host\", member.getHost());\n\n      long usedHeapSize = cluster.getUsedHeapSize();\n      long currentHeap = member.getCurrentHeapSize();\n      if (usedHeapSize > 0) {\n        double heapUsage = ((double) currentHeap / (double) usedHeapSize) * 100;\n        regionMember.put(\"heapUsage\", Double.valueOf(df2.format(heapUsage)));\n      } else {\n        regionMember.put(\"heapUsage\", 0);\n      }\n      double currentCPUUsage = member.getCpuUsage();\n      double loadAvg = member.getLoadAverage();\n\n      regionMember.put(\"cpuUsage\", Double.valueOf(df2.format(currentCPUUsage)));\n      regionMember.put(\"currentHeapUsage\", member.getCurrentHeapSize());\n      regionMember.put(\"isManager\", member.isManager());\n      regionMember.put(\"uptime\", TimeUtils.convertTimeSecondsToHMS(member.getUptime()));\n\n      regionMember.put(\"loadAvg\", Double.valueOf(df2.format(loadAvg)));\n      regionMember.put(\"sockets\", member.getTotalFileDescriptorOpen());\n      regionMember.put(\"threads\", member.getNumThreads());\n\n      regionMember.put(\"clients\", member.getMemberClientsHMap().size());\n\n      regionMember.put(\"queues\", member.getQueueBacklog());\n      memberArray.add(regionMember);\n    }\n\n    regionJSON.put(\"members\", memberArray);\n    regionJSON.put(\"entryCount\", reg.getSystemRegionEntryCount());\n\n    regionJSON.put(\"persistence\",\n        reg.getPersistentEnabled() ? PulseService.VALUE_ON : PulseService.VALUE_OFF);\n\n    regionJSON.put(\"isEnableOffHeapMemory\",\n        reg.isEnableOffHeapMemory() ? PulseService.VALUE_ON : PulseService.VALUE_OFF);\n\n    String regCompCodec = reg.getCompressionCodec();\n    if (StringUtils.isNotBlank(regCompCodec)) {\n      regionJSON.put(\"compressionCodec\", reg.getCompressionCodec());\n    } else {\n      regionJSON.put(\"compressionCodec\", PulseService.VALUE_NA);\n    }\n\n\n    regionJSON.put(\"regionPath\", reg.getFullPath());\n\n\n    regionJSON.put(\"memoryReadsTrend\", mapper.<JsonNode>valueToTree(\n        reg.getRegionStatisticTrend(Cluster.Region.REGION_STAT_GETS_PER_SEC_TREND)));\n    regionJSON.put(\"memoryWritesTrend\", mapper.<JsonNode>valueToTree(\n        reg.getRegionStatisticTrend(Cluster.Region.REGION_STAT_PUTS_PER_SEC_TREND)));\n    regionJSON.put(\"diskReadsTrend\", mapper.<JsonNode>valueToTree(\n        reg.getRegionStatisticTrend(Cluster.Region.REGION_STAT_DISK_READS_PER_SEC_TREND)));\n    regionJSON.put(\"diskWritesTrend\", mapper.<JsonNode>valueToTree(\n        reg.getRegionStatisticTrend(Cluster.Region.REGION_STAT_DISK_WRITES_PER_SEC_TREND)));\n\n    regionJSON.put(\"emptyNodes\", reg.getEmptyNode());\n    Long entrySize = reg.getEntrySize();\n    DecimalFormat form = new DecimalFormat(PulseConstants.DECIMAL_FORMAT_PATTERN_2);\n    String entrySizeInMB = form.format(entrySize / (1024f * 1024f));\n    if (entrySize < 0) {\n      regionJSON.put(this.ENTRY_SIZE, PulseService.VALUE_NA);\n    } else {\n      regionJSON.put(this.ENTRY_SIZE, entrySizeInMB);\n    }\n    regionJSON.put(\"dataUsage\", reg.getDiskUsage());\n    regionJSON.put(\"wanEnabled\", reg.getWanEnabled());\n    regionJSON.put(\"totalDataUsage\", totalDiskUsage);\n    regionJSON.put(\"memoryUsage\", entrySizeInMB);\n\n    LOGGER.fine(\"calling getSelectedRegionJson :: regionJSON = \" + regionJSON);\n    return regionJSON;\n  } else {\n    ObjectNode responseJSON = mapper.createObjectNode();\n    responseJSON.put(\"errorOnRegion\", \"Region [\" + selectedRegionFullPath + \"] is not available\");\n    return responseJSON;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2670,
        "instruction": "@InterfaceAudience.Public\n@InterfaceStability.Evolving\npublic void logoutUserFromKeytab() throws IOException {\n  if (!hasKerberosCredentials()) {\n    return;\n  }\n\n  // Shutdown the background task performing login renewal.\n  if (getKerberosLoginRenewalExecutor().isPresent()) {\n    getKerberosLoginRenewalExecutor().get().shutdownNow();\n  }\n\n  HadoopLoginContext login = getLogin();\n  String keytabFile = getKeytab();\n  if (login == null || keytabFile == null) {\n    throw new KerberosAuthException(MUST_FIRST_LOGIN_FROM_KEYTAB);\n  }\n\n  try {\n    // hadoop login context internally locks credentials.\n    login.logout();\n  } catch (LoginException le) {\n    KerberosAuthException kae = new KerberosAuthException(LOGOUT_FAILURE, le);\n    kae.setUser(user.toString());\n    kae.setKeytabFile(keytabFile);\n    throw kae;\n  }\n\n  LOG.info(\"Logout successful for user \" + getUserName()\n      + \" using keytab file \" + keytabFile);\n}",
        "accept_response": "@InterfaceAudience.Public\n@InterfaceStability.Evolving\npublic void logoutUserFromKeytab() throws IOException {\n  if (!hasKerberosCredentials()) {\n    return;\n  }\n\n  // Shutdown the background task performing login renewal.\n  if (getKerberosLoginRenewalExecutor().isPresent()) {\n    getKerberosLoginRenewalExecutor().get().shutdownNow();\n  }\n\n  HadoopLoginContext login = getLogin();\n  String keytabFile = getKeytab();\n  if (login == null || keytabFile == null) {\n    throw new KerberosAuthException(MUST_FIRST_LOGIN_FROM_KEYTAB);\n  }\n\n  try {\n    LOG.debug(\"Initiating logout for {}\", getUserName());\n    // hadoop login context internally locks credentials.\n    login.logout();\n  } catch (LoginException le) {\n    KerberosAuthException kae = new KerberosAuthException(LOGOUT_FAILURE, le);\n    kae.setUser(user.toString());\n    kae.setKeytabFile(keytabFile);\n    throw kae;\n  }\n\n  LOG.info(\"Logout successful for user \" + getUserName()\n      + \" using keytab file \" + keytabFile);\n}",
        "reject_response": "@InterfaceAudience.Public\n@InterfaceStability.Evolving\npublic void logoutUserFromKeytab() throws IOException {\n  if (!hasKerberosCredentials()) {\n    return;\n  }\n\n  // Shutdown the background task performing login renewal.\n  if (getKerberosLoginRenewalExecutor().isPresent()) {\n    getKerberosLoginRenewalExecutor().get().shutdownNow();\n  }\n\n  HadoopLoginContext login = getLogin();\n  String keytabFile = getKeytab();\n  if (login == null || keytabFile == null) {\n    throw new KerberosAuthException(MUST_FIRST_LOGIN_FROM_KEYTAB);\n  }\n\n  try {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Initiating logout for \" + getUserName());\n    }\n    // hadoop login context internally locks credentials.\n    login.logout();\n  } catch (LoginException le) {\n    KerberosAuthException kae = new KerberosAuthException(LOGOUT_FAILURE, le);\n    kae.setUser(user.toString());\n    kae.setKeytabFile(keytabFile);\n    throw kae;\n  }\n\n  LOG.info(\"Logout successful for user \" + getUserName()\n      + \" using keytab file \" + keytabFile);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2937,
        "instruction": "private void exportHBaseConfiguration(String hbaseTableName) throws IOException {\n\n    Configuration hbaseConf = HBaseConnection.getCurrentHBaseConfiguration();\n    HadoopUtil.healSickConfig(hbaseConf);\n    Job job = Job.getInstance(hbaseConf, hbaseTableName);\n    HTable table = new HTable(hbaseConf, hbaseTableName);\n    HFileOutputFormat2.configureIncrementalLoadMap(job, table);\n\n    FileSystem fs = HadoopUtil.getWorkingFileSystem();\n    FSDataOutputStream out = null;\n    try {\n        out = fs.create(new Path(hbaseConfPath));\n        job.getConfiguration().writeXml(out);\n    } finally {\n        IOUtils.closeQuietly(out);\n    }\n}",
        "accept_response": "private void exportHBaseConfiguration(String hbaseTableName) throws IOException {\n\n    Configuration hbaseConf = HBaseConnection.getCurrentHBaseConfiguration();\n    HadoopUtil.healSickConfig(hbaseConf);\n    Job job = Job.getInstance(hbaseConf, hbaseTableName);\n    HTable table = new HTable(hbaseConf, hbaseTableName);\n    HFileOutputFormat2.configureIncrementalLoadMap(job, table);\n\n    logger.info(\"Saving HBase configuration to {}\", hbaseConfPath);\n    FileSystem fs = HadoopUtil.getWorkingFileSystem();\n    FSDataOutputStream out = null;\n    try {\n        out = fs.create(new Path(hbaseConfPath));\n        job.getConfiguration().writeXml(out);\n    } finally {\n        IOUtils.closeQuietly(out);\n    }\n}",
        "reject_response": "private void exportHBaseConfiguration(String hbaseTableName) throws IOException {\n\n    Configuration hbaseConf = HBaseConnection.getCurrentHBaseConfiguration();\n    HadoopUtil.healSickConfig(hbaseConf);\n    Job job = Job.getInstance(hbaseConf, hbaseTableName);\n    HTable table = new HTable(hbaseConf, hbaseTableName);\n    HFileOutputFormat2.configureIncrementalLoadMap(job, table);\n\n    logger.info(\"Saving HBase configuration to {0}\", hbaseConfPath);\n    FileSystem fs = HadoopUtil.getWorkingFileSystem();\n    FSDataOutputStream out = null;\n    try {\n        out = fs.create(new Path(hbaseConfPath));\n        job.getConfiguration().writeXml(out);\n    } finally {\n        IOUtils.closeQuietly(out);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2625,
        "instruction": "@RequestMapping(value = \"/pulseUpdate\", method = RequestMethod.POST)\npublic void getPulseUpdate(HttpServletRequest request, HttpServletResponse response)\n    throws IOException {\n  String pulseData = request.getParameter(\"pulseData\");\n\n  ObjectNode responseMap = mapper.createObjectNode();\n\n  JsonNode requestMap = null;\n\n\n  try {\n    requestMap = mapper.readTree(pulseData);\n    Iterator<?> keys = requestMap.fieldNames();\n\n    // Execute Services\n    while (keys.hasNext()) {\n      String serviceName = keys.next().toString();\n      try {\n        PulseService pulseService = pulseServiceFactory.getPulseServiceInstance(serviceName);\n        responseMap.put(serviceName, pulseService.execute(request));\n      } catch (Exception serviceException) {\n        responseMap.put(serviceName, EMPTY_JSON);\n      }\n    }\n  } catch (Exception e) {\n    logger.debug(\"Exception Occurred : \", e);\n  }\n\n  // Create Response\n  response.getOutputStream().write(responseMap.toString().getBytes());\n}",
        "accept_response": "@RequestMapping(value = \"/pulseUpdate\", method = RequestMethod.POST)\npublic void getPulseUpdate(HttpServletRequest request, HttpServletResponse response)\n    throws IOException {\n  String pulseData = request.getParameter(\"pulseData\");\n\n  ObjectNode responseMap = mapper.createObjectNode();\n\n  JsonNode requestMap = null;\n\n\n  try {\n    requestMap = mapper.readTree(pulseData);\n    Iterator<?> keys = requestMap.fieldNames();\n\n    // Execute Services\n    while (keys.hasNext()) {\n      String serviceName = keys.next().toString();\n      try {\n        PulseService pulseService = pulseServiceFactory.getPulseServiceInstance(serviceName);\n        responseMap.put(serviceName, pulseService.execute(request));\n      } catch (Exception serviceException) {\n        logger.warn(\"serviceException [for service {}] = {}\", serviceName,\n            serviceException.getMessage());\n        responseMap.put(serviceName, EMPTY_JSON);\n      }\n    }\n  } catch (Exception e) {\n    logger.debug(\"Exception Occurred : \", e);\n  }\n\n  // Create Response\n  response.getOutputStream().write(responseMap.toString().getBytes());\n}",
        "reject_response": "@RequestMapping(value = \"/pulseUpdate\", method = RequestMethod.POST)\npublic void getPulseUpdate(HttpServletRequest request, HttpServletResponse response)\n    throws IOException {\n  String pulseData = request.getParameter(\"pulseData\");\n\n  ObjectNode responseMap = mapper.createObjectNode();\n\n  JsonNode requestMap = null;\n\n\n  try {\n    requestMap = mapper.readTree(pulseData);\n    Iterator<?> keys = requestMap.fieldNames();\n\n    // Execute Services\n    while (keys.hasNext()) {\n      String serviceName = keys.next().toString();\n      try {\n        PulseService pulseService = pulseServiceFactory.getPulseServiceInstance(serviceName);\n        responseMap.put(serviceName, pulseService.execute(request));\n      } catch (Exception serviceException) {\n        LOGGER.warning(\"serviceException [for service \" + serviceName + \"] = \"\n            + serviceException.getMessage());\n        responseMap.put(serviceName, EMPTY_JSON);\n      }\n    }\n  } catch (Exception e) {\n    logger.debug(\"Exception Occurred : \", e);\n  }\n\n  // Create Response\n  response.getOutputStream().write(responseMap.toString().getBytes());\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2548,
        "instruction": "@Override\npublic ScanBatch getBatch(FragmentContext context, HiveDrillNativeParquetSubScan config, List<RecordBatch> children)\n    throws ExecutionSetupException {\n  final Table table = config.getTable();\n  final List<InputSplit> splits = config.getInputSplits();\n  final List<Partition> partitions = config.getPartitions();\n  final List<SchemaPath> columns = config.getColumns();\n  final String partitionDesignator = context.getOptions()\n      .getOption(ExecConstants.FILESYSTEM_PARTITION_COLUMN_LABEL).string_val;\n  List<Map<String, String>> implicitColumns = Lists.newLinkedList();\n  boolean selectAllQuery = AbstractRecordReader.isStarQuery(columns);\n\n  final boolean hasPartitions = (partitions != null && partitions.size() > 0);\n\n  final List<String[]> partitionColumns = Lists.newArrayList();\n  final List<Integer> selectedPartitionColumns = Lists.newArrayList();\n  List<SchemaPath> newColumns = columns;\n  if (!selectAllQuery) {\n    // Separate out the partition and non-partition columns. Non-partition columns are passed directly to the\n    // ParquetRecordReader. Partition columns are passed to ScanBatch.\n    newColumns = Lists.newArrayList();\n    Pattern pattern = Pattern.compile(String.format(\"%s[0-9]+\", partitionDesignator));\n    for (SchemaPath column : columns) {\n      Matcher m = pattern.matcher(column.getAsUnescapedPath());\n      if (m.matches()) {\n        selectedPartitionColumns.add(\n            Integer.parseInt(column.getAsUnescapedPath().substring(partitionDesignator.length())));\n      } else {\n        newColumns.add(column);\n      }\n    }\n  }\n\n  final OperatorContext oContext = context.newOperatorContext(config);\n\n  int currentPartitionIndex = 0;\n  final List<RecordReader> readers = Lists.newArrayList();\n\n  final HiveConf conf = config.getHiveConf();\n\n  // TODO: In future we can get this cache from Metadata cached on filesystem.\n  final Map<String, ParquetMetadata> footerCache = Maps.newHashMap();\n\n  Map<String, String> mapWithMaxColumns = Maps.newLinkedHashMap();\n  try {\n    for (InputSplit split : splits) {\n      final FileSplit fileSplit = (FileSplit) split;\n      final Path finalPath = fileSplit.getPath();\n      final JobConf cloneJob =\n          new ProjectionPusher().pushProjectionsAndFilters(new JobConf(conf), finalPath.getParent());\n      final FileSystem fs = finalPath.getFileSystem(cloneJob);\n\n      ParquetMetadata parquetMetadata = footerCache.get(finalPath.toString());\n      if (parquetMetadata == null){\n        parquetMetadata = ParquetFileReader.readFooter(cloneJob, finalPath);\n        footerCache.put(finalPath.toString(), parquetMetadata);\n      }\n      final List<Integer> rowGroupNums = getRowGroupNumbersFromFileSplit(fileSplit, parquetMetadata);\n\n      for(int rowGroupNum : rowGroupNums) {\n        //DRILL-5009 : Skip the row group if the row count is zero\n        if (parquetMetadata.getBlocks().get(rowGroupNum).getRowCount() == 0) {\n          continue;\n        }\n        // Drill has only ever written a single row group per file, only detect corruption\n        // in the first row group\n        ParquetReaderUtility.DateCorruptionStatus containsCorruptDates =\n            ParquetReaderUtility.detectCorruptDates(parquetMetadata, config.getColumns(), true);\n        readers.add(new ParquetRecordReader(\n                context,\n                Path.getPathWithoutSchemeAndAuthority(finalPath).toString(),\n                rowGroupNum, fs,\n                CodecFactory.createDirectCodecFactory(fs.getConf(),\n                    new ParquetDirectByteBufferAllocator(oContext.getAllocator()), 0),\n                parquetMetadata,\n                newColumns,\n                containsCorruptDates)\n        );\n        Map<String, String> implicitValues = Maps.newLinkedHashMap();\n\n        if (hasPartitions) {\n          List<String> values = partitions.get(currentPartitionIndex).getValues();\n          for (int i = 0; i < values.size(); i++) {\n            if (selectAllQuery || selectedPartitionColumns.contains(i)) {\n              implicitValues.put(partitionDesignator + i, values.get(i));\n            }\n          }\n        }\n        implicitColumns.add(implicitValues);\n        if (implicitValues.size() > mapWithMaxColumns.size()) {\n          mapWithMaxColumns = implicitValues;\n        }\n      }\n      currentPartitionIndex++;\n    }\n  } catch (final IOException|RuntimeException e) {\n    AutoCloseables.close(e, readers);\n    throw new ExecutionSetupException(\"Failed to create RecordReaders. \" + e.getMessage(), e);\n  }\n\n  // all readers should have the same number of implicit columns, add missing ones with value null\n  mapWithMaxColumns = Maps.transformValues(mapWithMaxColumns, Functions.constant((String) null));\n  for (Map<String, String> map : implicitColumns) {\n    map.putAll(Maps.difference(map, mapWithMaxColumns).entriesOnlyOnRight());\n  }\n\n  // If there are no readers created (which is possible when the table is empty or no row groups are matched),\n  // create an empty RecordReader to output the schema\n  if (readers.size() == 0) {\n    readers.add(new HiveDefaultReader(table, null, null, columns, context, conf,\n      ImpersonationUtil.createProxyUgi(config.getUserName(), context.getQueryUserName())));\n  }\n\n  return new ScanBatch(config, context, oContext, readers.iterator(), implicitColumns);\n}",
        "accept_response": "@Override\npublic ScanBatch getBatch(FragmentContext context, HiveDrillNativeParquetSubScan config, List<RecordBatch> children)\n    throws ExecutionSetupException {\n  final Table table = config.getTable();\n  final List<InputSplit> splits = config.getInputSplits();\n  final List<Partition> partitions = config.getPartitions();\n  final List<SchemaPath> columns = config.getColumns();\n  final String partitionDesignator = context.getOptions()\n      .getOption(ExecConstants.FILESYSTEM_PARTITION_COLUMN_LABEL).string_val;\n  List<Map<String, String>> implicitColumns = Lists.newLinkedList();\n  boolean selectAllQuery = AbstractRecordReader.isStarQuery(columns);\n\n  final boolean hasPartitions = (partitions != null && partitions.size() > 0);\n\n  final List<String[]> partitionColumns = Lists.newArrayList();\n  final List<Integer> selectedPartitionColumns = Lists.newArrayList();\n  List<SchemaPath> newColumns = columns;\n  if (!selectAllQuery) {\n    // Separate out the partition and non-partition columns. Non-partition columns are passed directly to the\n    // ParquetRecordReader. Partition columns are passed to ScanBatch.\n    newColumns = Lists.newArrayList();\n    Pattern pattern = Pattern.compile(String.format(\"%s[0-9]+\", partitionDesignator));\n    for (SchemaPath column : columns) {\n      Matcher m = pattern.matcher(column.getAsUnescapedPath());\n      if (m.matches()) {\n        selectedPartitionColumns.add(\n            Integer.parseInt(column.getAsUnescapedPath().substring(partitionDesignator.length())));\n      } else {\n        newColumns.add(column);\n      }\n    }\n  }\n\n  final OperatorContext oContext = context.newOperatorContext(config);\n\n  int currentPartitionIndex = 0;\n  final List<RecordReader> readers = Lists.newArrayList();\n\n  final HiveConf conf = config.getHiveConf();\n\n  // TODO: In future we can get this cache from Metadata cached on filesystem.\n  final Map<String, ParquetMetadata> footerCache = Maps.newHashMap();\n\n  Map<String, String> mapWithMaxColumns = Maps.newLinkedHashMap();\n  try {\n    for (InputSplit split : splits) {\n      final FileSplit fileSplit = (FileSplit) split;\n      final Path finalPath = fileSplit.getPath();\n      final JobConf cloneJob =\n          new ProjectionPusher().pushProjectionsAndFilters(new JobConf(conf), finalPath.getParent());\n      final FileSystem fs = finalPath.getFileSystem(cloneJob);\n\n      ParquetMetadata parquetMetadata = footerCache.get(finalPath.toString());\n      if (parquetMetadata == null){\n        parquetMetadata = ParquetFileReader.readFooter(cloneJob, finalPath);\n        footerCache.put(finalPath.toString(), parquetMetadata);\n      }\n      final List<Integer> rowGroupNums = getRowGroupNumbersFromFileSplit(fileSplit, parquetMetadata);\n\n      for(int rowGroupNum : rowGroupNums) {\n        //DRILL-5009 : Skip the row group if the row count is zero\n        if (parquetMetadata.getBlocks().get(rowGroupNum).getRowCount() == 0) {\n          continue;\n        }\n        // Drill has only ever written a single row group per file, only detect corruption\n        // in the first row group\n        ParquetReaderUtility.DateCorruptionStatus containsCorruptDates =\n            ParquetReaderUtility.detectCorruptDates(parquetMetadata, config.getColumns(), true);\n        if (logger.isDebugEnabled()) {\n          logger.debug(containsCorruptDates.toString());\n        }\n        readers.add(new ParquetRecordReader(\n                context,\n                Path.getPathWithoutSchemeAndAuthority(finalPath).toString(),\n                rowGroupNum, fs,\n                CodecFactory.createDirectCodecFactory(fs.getConf(),\n                    new ParquetDirectByteBufferAllocator(oContext.getAllocator()), 0),\n                parquetMetadata,\n                newColumns,\n                containsCorruptDates)\n        );\n        Map<String, String> implicitValues = Maps.newLinkedHashMap();\n\n        if (hasPartitions) {\n          List<String> values = partitions.get(currentPartitionIndex).getValues();\n          for (int i = 0; i < values.size(); i++) {\n            if (selectAllQuery || selectedPartitionColumns.contains(i)) {\n              implicitValues.put(partitionDesignator + i, values.get(i));\n            }\n          }\n        }\n        implicitColumns.add(implicitValues);\n        if (implicitValues.size() > mapWithMaxColumns.size()) {\n          mapWithMaxColumns = implicitValues;\n        }\n      }\n      currentPartitionIndex++;\n    }\n  } catch (final IOException|RuntimeException e) {\n    AutoCloseables.close(e, readers);\n    throw new ExecutionSetupException(\"Failed to create RecordReaders. \" + e.getMessage(), e);\n  }\n\n  // all readers should have the same number of implicit columns, add missing ones with value null\n  mapWithMaxColumns = Maps.transformValues(mapWithMaxColumns, Functions.constant((String) null));\n  for (Map<String, String> map : implicitColumns) {\n    map.putAll(Maps.difference(map, mapWithMaxColumns).entriesOnlyOnRight());\n  }\n\n  // If there are no readers created (which is possible when the table is empty or no row groups are matched),\n  // create an empty RecordReader to output the schema\n  if (readers.size() == 0) {\n    readers.add(new HiveDefaultReader(table, null, null, columns, context, conf,\n      ImpersonationUtil.createProxyUgi(config.getUserName(), context.getQueryUserName())));\n  }\n\n  return new ScanBatch(config, context, oContext, readers.iterator(), implicitColumns);\n}",
        "reject_response": "@Override\npublic ScanBatch getBatch(FragmentContext context, HiveDrillNativeParquetSubScan config, List<RecordBatch> children)\n    throws ExecutionSetupException {\n  final Table table = config.getTable();\n  final List<InputSplit> splits = config.getInputSplits();\n  final List<Partition> partitions = config.getPartitions();\n  final List<SchemaPath> columns = config.getColumns();\n  final String partitionDesignator = context.getOptions()\n      .getOption(ExecConstants.FILESYSTEM_PARTITION_COLUMN_LABEL).string_val;\n  List<Map<String, String>> implicitColumns = Lists.newLinkedList();\n  boolean selectAllQuery = AbstractRecordReader.isStarQuery(columns);\n\n  final boolean hasPartitions = (partitions != null && partitions.size() > 0);\n\n  final List<String[]> partitionColumns = Lists.newArrayList();\n  final List<Integer> selectedPartitionColumns = Lists.newArrayList();\n  List<SchemaPath> newColumns = columns;\n  if (!selectAllQuery) {\n    // Separate out the partition and non-partition columns. Non-partition columns are passed directly to the\n    // ParquetRecordReader. Partition columns are passed to ScanBatch.\n    newColumns = Lists.newArrayList();\n    Pattern pattern = Pattern.compile(String.format(\"%s[0-9]+\", partitionDesignator));\n    for (SchemaPath column : columns) {\n      Matcher m = pattern.matcher(column.getAsUnescapedPath());\n      if (m.matches()) {\n        selectedPartitionColumns.add(\n            Integer.parseInt(column.getAsUnescapedPath().substring(partitionDesignator.length())));\n      } else {\n        newColumns.add(column);\n      }\n    }\n  }\n\n  final OperatorContext oContext = context.newOperatorContext(config);\n\n  int currentPartitionIndex = 0;\n  final List<RecordReader> readers = Lists.newArrayList();\n\n  final HiveConf conf = config.getHiveConf();\n\n  // TODO: In future we can get this cache from Metadata cached on filesystem.\n  final Map<String, ParquetMetadata> footerCache = Maps.newHashMap();\n\n  Map<String, String> mapWithMaxColumns = Maps.newLinkedHashMap();\n  try {\n    for (InputSplit split : splits) {\n      final FileSplit fileSplit = (FileSplit) split;\n      final Path finalPath = fileSplit.getPath();\n      final JobConf cloneJob =\n          new ProjectionPusher().pushProjectionsAndFilters(new JobConf(conf), finalPath.getParent());\n      final FileSystem fs = finalPath.getFileSystem(cloneJob);\n\n      ParquetMetadata parquetMetadata = footerCache.get(finalPath.toString());\n      if (parquetMetadata == null){\n        parquetMetadata = ParquetFileReader.readFooter(cloneJob, finalPath);\n        footerCache.put(finalPath.toString(), parquetMetadata);\n      }\n      final List<Integer> rowGroupNums = getRowGroupNumbersFromFileSplit(fileSplit, parquetMetadata);\n\n      for(int rowGroupNum : rowGroupNums) {\n        //DRILL-5009 : Skip the row group if the row count is zero\n        if (parquetMetadata.getBlocks().get(rowGroupNum).getRowCount() == 0) {\n          continue;\n        }\n        // Drill has only ever written a single row group per file, only detect corruption\n        // in the first row group\n        ParquetReaderUtility.DateCorruptionStatus containsCorruptDates =\n            ParquetReaderUtility.detectCorruptDates(parquetMetadata, config.getColumns(), true);\n        if (logger.isDebugEnabled()) {\n        logger.info(containsCorruptDates.toString());\n        }\n        readers.add(new ParquetRecordReader(\n                context,\n                Path.getPathWithoutSchemeAndAuthority(finalPath).toString(),\n                rowGroupNum, fs,\n                CodecFactory.createDirectCodecFactory(fs.getConf(),\n                    new ParquetDirectByteBufferAllocator(oContext.getAllocator()), 0),\n                parquetMetadata,\n                newColumns,\n                containsCorruptDates)\n        );\n        Map<String, String> implicitValues = Maps.newLinkedHashMap();\n\n        if (hasPartitions) {\n          List<String> values = partitions.get(currentPartitionIndex).getValues();\n          for (int i = 0; i < values.size(); i++) {\n            if (selectAllQuery || selectedPartitionColumns.contains(i)) {\n              implicitValues.put(partitionDesignator + i, values.get(i));\n            }\n          }\n        }\n        implicitColumns.add(implicitValues);\n        if (implicitValues.size() > mapWithMaxColumns.size()) {\n          mapWithMaxColumns = implicitValues;\n        }\n      }\n      currentPartitionIndex++;\n    }\n  } catch (final IOException|RuntimeException e) {\n    AutoCloseables.close(e, readers);\n    throw new ExecutionSetupException(\"Failed to create RecordReaders. \" + e.getMessage(), e);\n  }\n\n  // all readers should have the same number of implicit columns, add missing ones with value null\n  mapWithMaxColumns = Maps.transformValues(mapWithMaxColumns, Functions.constant((String) null));\n  for (Map<String, String> map : implicitColumns) {\n    map.putAll(Maps.difference(map, mapWithMaxColumns).entriesOnlyOnRight());\n  }\n\n  // If there are no readers created (which is possible when the table is empty or no row groups are matched),\n  // create an empty RecordReader to output the schema\n  if (readers.size() == 0) {\n    readers.add(new HiveDefaultReader(table, null, null, columns, context, conf,\n      ImpersonationUtil.createProxyUgi(config.getUserName(), context.getQueryUserName())));\n  }\n\n  return new ScanBatch(config, context, oContext, readers.iterator(), implicitColumns);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2828,
        "instruction": "private SegmentArchiveResult archiveSegment(long absIdx) throws IgniteCheckedException {\n    long segIdx = absIdx % dsCfg.getWalSegments();\n\n    File origFile = new File(walWorkDir, FileDescriptor.fileName(segIdx));\n\n    String name = FileDescriptor.fileName(absIdx);\n\n    File dstTmpFile = new File(walArchiveDir, name + \".tmp\");\n\n    File dstFile = new File(walArchiveDir, name);\n\n    if (log.isInfoEnabled())\n        log.info(\"Starting to copy WAL segment [absIdx=\" + absIdx + \", segIdx=\" + segIdx +\n            \", origFile=\" + origFile.getAbsolutePath() + \", dstFile=\" + dstFile.getAbsolutePath() + ']');\n\n    try {\n        Files.deleteIfExists(dstTmpFile.toPath());\n\n        Files.copy(origFile.toPath(), dstTmpFile.toPath());\n\n        Files.move(dstTmpFile.toPath(), dstFile.toPath());\n\n        if (mode != WALMode.NONE) {\n            try (FileIO f0 = ioFactory.create(dstFile, CREATE, READ, WRITE)) {\n                f0.force();\n            }\n        }\n    }\n    catch (IOException e) {\n        throw new IgniteCheckedException(\"Failed to archive WAL segment [\" +\n            \"srcFile=\" + origFile.getAbsolutePath() +\n            \", dstFile=\" + dstTmpFile.getAbsolutePath() + ']', e);\n    }\n\n    if (log.isInfoEnabled())\n\n    return new SegmentArchiveResult(absIdx, origFile, dstFile);\n}",
        "accept_response": "private SegmentArchiveResult archiveSegment(long absIdx) throws IgniteCheckedException {\n    long segIdx = absIdx % dsCfg.getWalSegments();\n\n    File origFile = new File(walWorkDir, FileDescriptor.fileName(segIdx));\n\n    String name = FileDescriptor.fileName(absIdx);\n\n    File dstTmpFile = new File(walArchiveDir, name + \".tmp\");\n\n    File dstFile = new File(walArchiveDir, name);\n\n    if (log.isInfoEnabled())\n        log.info(\"Starting to copy WAL segment [absIdx=\" + absIdx + \", segIdx=\" + segIdx +\n            \", origFile=\" + origFile.getAbsolutePath() + \", dstFile=\" + dstFile.getAbsolutePath() + ']');\n\n    try {\n        Files.deleteIfExists(dstTmpFile.toPath());\n\n        Files.copy(origFile.toPath(), dstTmpFile.toPath());\n\n        Files.move(dstTmpFile.toPath(), dstFile.toPath());\n\n        if (mode != WALMode.NONE) {\n            try (FileIO f0 = ioFactory.create(dstFile, CREATE, READ, WRITE)) {\n                f0.force();\n            }\n        }\n    }\n    catch (IOException e) {\n        throw new IgniteCheckedException(\"Failed to archive WAL segment [\" +\n            \"srcFile=\" + origFile.getAbsolutePath() +\n            \", dstFile=\" + dstTmpFile.getAbsolutePath() + ']', e);\n    }\n\n    if (log.isInfoEnabled())\n        log.info(\"Copied file [src=\" + origFile.getAbsolutePath() +\n            \", dst=\" + dstFile.getAbsolutePath() + ']');\n\n    return new SegmentArchiveResult(absIdx, origFile, dstFile);\n}",
        "reject_response": "private SegmentArchiveResult archiveSegment(long absIdx) throws IgniteCheckedException {\n    long segIdx = absIdx % dsCfg.getWalSegments();\n\n    File origFile = new File(walWorkDir, FileDescriptor.fileName(segIdx));\n\n    String name = FileDescriptor.fileName(absIdx);\n\n    File dstTmpFile = new File(walArchiveDir, name + \".tmp\");\n\n    File dstFile = new File(walArchiveDir, name);\n\n    if (log.isInfoEnabled())\n        log.info(\"Starting to copy WAL segment [absIdx=\" + absIdx + \", segIdx=\" + segIdx +\n            \", origFile=\" + origFile.getAbsolutePath() + \", dstFile=\" + dstFile.getAbsolutePath() + ']');\n\n    try {\n        Files.deleteIfExists(dstTmpFile.toPath());\n\n        Files.copy(origFile.toPath(), dstTmpFile.toPath());\n\n        Files.move(dstTmpFile.toPath(), dstFile.toPath());\n\n        if (mode != WALMode.NONE) {\n            try (FileIO f0 = ioFactory.create(dstFile, CREATE, READ, WRITE)) {\n                f0.force();\n            }\n        }\n    }\n    catch (IOException e) {\n        throw new IgniteCheckedException(\"Failed to archive WAL segment [\" +\n            \"srcFile=\" + origFile.getAbsolutePath() +\n            \", dstFile=\" + dstTmpFile.getAbsolutePath() + ']', e);\n    }\n\n    if (log.isInfoEnabled())\n    if (log.isDebugEnabled())\n        log.debug(\"Copied file [src=\" + origFile.getAbsolutePath() +\n\n    return new SegmentArchiveResult(absIdx, origFile, dstFile);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3193,
        "instruction": "private void reportNewCollectorInfoToNM(ApplicationId appId,\n    org.apache.hadoop.yarn.api.records.Token token)\n    throws YarnException, IOException {\n  ReportNewCollectorInfoRequest request =\n      ReportNewCollectorInfoRequest.newInstance(appId,\n          this.timelineRestServerBindAddress, token);\n  getNMCollectorService().reportNewCollectorInfo(request);\n}",
        "accept_response": "private void reportNewCollectorInfoToNM(ApplicationId appId,\n    org.apache.hadoop.yarn.api.records.Token token)\n    throws YarnException, IOException {\n  ReportNewCollectorInfoRequest request =\n      ReportNewCollectorInfoRequest.newInstance(appId,\n          this.timelineRestServerBindAddress, token);\n  LOG.info(\"Report a new collector for application: {}\" +\n      \" to the NM Collector Service.\", appId);\n  getNMCollectorService().reportNewCollectorInfo(request);\n}",
        "reject_response": "private void reportNewCollectorInfoToNM(ApplicationId appId,\n    org.apache.hadoop.yarn.api.records.Token token)\n    throws YarnException, IOException {\n  ReportNewCollectorInfoRequest request =\n      ReportNewCollectorInfoRequest.newInstance(appId,\n          this.timelineRestServerBindAddress, token);\n  LOG.info(\"Report a new collector for application: \" + appId +\n      \" to the NM Collector Service.\");\n  getNMCollectorService().reportNewCollectorInfo(request);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2920,
        "instruction": "protected void maybeCommit() {\n    long now = time.milliseconds();\n\n    if (commitTimeMs >= 0 && lastCommitMs + commitTimeMs < now) {\n\n        commitAll();\n        lastCommitMs = now;\n\n        processStandbyRecords = true;\n    }\n}",
        "accept_response": "protected void maybeCommit() {\n    long now = time.milliseconds();\n\n    if (commitTimeMs >= 0 && lastCommitMs + commitTimeMs < now) {\n        log.trace(\"stream-thread [{}] Committing processor instances because the commit interval has elapsed\", this.getName());\n\n        commitAll();\n        lastCommitMs = now;\n\n        processStandbyRecords = true;\n    }\n}",
        "reject_response": "protected void maybeCommit() {\n    long now = time.milliseconds();\n\n    if (commitTimeMs >= 0 && lastCommitMs + commitTimeMs < now) {\n        log.trace(\"Committing processor instances because the commit interval has elapsed.\");\n\n        commitAll();\n        lastCommitMs = now;\n\n        processStandbyRecords = true;\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3156,
        "instruction": "@Override\npublic ServiceResults postCollection( ServiceContext context ) throws Exception {\n\n    checkPermissionsForCollection( context );\n\n    if ( context.getPayload().isBatch() ) {\n        List<Entity> entities = new ArrayList<Entity>();\n        List<Map<String, Object>> batch = context.getPayload().getBatchProperties();\n        logger.debug( \"Attempting to batch create \" + batch.size() + \" entities in collection \" + context\n                .getCollectionName() );\n        int i = 1;\n        for ( Map<String, Object> p : batch ) {\n            logger.debug( \"Creating entity \" + i + \" in collection \" + context.getCollectionName() );\n\n            Entity item = null;\n\n            try {\n                item = em.createItemInCollection( context.getOwner(), context.getCollectionName(), getEntityType(),\n                        p );\n            }\n            catch ( Exception e ) {\n\n                i++;\n                continue;\n            }\n\n            logger.debug(\n                    \"Entity \" + i + \" created in collection \" + context.getCollectionName() + \" with UUID \" + item\n                            .getUuid() );\n\n            item = importEntity( context, item );\n            entities.add( item );\n            i++;\n        }\n        return new ServiceResults( this, context, Type.COLLECTION, Results.fromEntities( entities ), null, null );\n    }\n\n    Entity item = em.createItemInCollection( context.getOwner(), context.getCollectionName(), getEntityType(),\n            context.getProperties() );\n\n    item = importEntity( context, item );\n\n    return new ServiceResults( this, context, Type.COLLECTION, Results.fromEntity( item ), null, null );\n}",
        "accept_response": "@Override\npublic ServiceResults postCollection( ServiceContext context ) throws Exception {\n\n    checkPermissionsForCollection( context );\n\n    if ( context.getPayload().isBatch() ) {\n        List<Entity> entities = new ArrayList<Entity>();\n        List<Map<String, Object>> batch = context.getPayload().getBatchProperties();\n        logger.debug( \"Attempting to batch create \" + batch.size() + \" entities in collection \" + context\n                .getCollectionName() );\n        int i = 1;\n        for ( Map<String, Object> p : batch ) {\n            logger.debug( \"Creating entity \" + i + \" in collection \" + context.getCollectionName() );\n\n            Entity item = null;\n\n            try {\n                item = em.createItemInCollection( context.getOwner(), context.getCollectionName(), getEntityType(),\n                        p );\n            }\n            catch ( Exception e ) {\n                logger.debug( \"Entity \" + i + \" unable to be created in collection \" + context.getCollectionName(),\n                        e );\n\n                i++;\n                continue;\n            }\n\n            logger.debug(\n                    \"Entity \" + i + \" created in collection \" + context.getCollectionName() + \" with UUID \" + item\n                            .getUuid() );\n\n            item = importEntity( context, item );\n            entities.add( item );\n            i++;\n        }\n        return new ServiceResults( this, context, Type.COLLECTION, Results.fromEntities( entities ), null, null );\n    }\n\n    Entity item = em.createItemInCollection( context.getOwner(), context.getCollectionName(), getEntityType(),\n            context.getProperties() );\n\n    item = importEntity( context, item );\n\n    return new ServiceResults( this, context, Type.COLLECTION, Results.fromEntity( item ), null, null );\n}",
        "reject_response": "@Override\npublic ServiceResults postCollection( ServiceContext context ) throws Exception {\n\n    checkPermissionsForCollection( context );\n\n    if ( context.getPayload().isBatch() ) {\n        List<Entity> entities = new ArrayList<Entity>();\n        List<Map<String, Object>> batch = context.getPayload().getBatchProperties();\n        logger.debug( \"Attempting to batch create \" + batch.size() + \" entities in collection \" + context\n                .getCollectionName() );\n        int i = 1;\n        for ( Map<String, Object> p : batch ) {\n            logger.debug( \"Creating entity \" + i + \" in collection \" + context.getCollectionName() );\n\n            Entity item = null;\n\n            try {\n                item = em.createItemInCollection( context.getOwner(), context.getCollectionName(), getEntityType(),\n                        p );\n            }\n            catch ( Exception e ) {\n                logger.error( \"Entity \" + i + \" unable to be created in collection \" + context.getCollectionName(),\n\n                i++;\n                continue;\n            }\n\n            logger.debug(\n                    \"Entity \" + i + \" created in collection \" + context.getCollectionName() + \" with UUID \" + item\n                            .getUuid() );\n\n            item = importEntity( context, item );\n            entities.add( item );\n            i++;\n        }\n        return new ServiceResults( this, context, Type.COLLECTION, Results.fromEntities( entities ), null, null );\n    }\n\n    Entity item = em.createItemInCollection( context.getOwner(), context.getCollectionName(), getEntityType(),\n            context.getProperties() );\n\n    item = importEntity( context, item );\n\n    return new ServiceResults( this, context, Type.COLLECTION, Results.fromEntity( item ), null, null );\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2575,
        "instruction": "private void handleCodeValueDataIntegrityIssues(final JsonCommand command, final DataIntegrityViolationException dve) {\n    final Throwable realCause = dve.getMostSpecificCause();\n    if (realCause.getMessage().contains(\"code_value\")) {\n        final String name = command.stringValueOfParameterNamed(\"name\");\n        throw new PlatformDataIntegrityException(\"error.msg.code.value.duplicate.label\", \"A code value with lable '\" + name\n                + \"' already exists\", \"name\", name);\n    }\n\n    throw new PlatformDataIntegrityException(\"error.msg.code.value.unknown.data.integrity.issue\",\n            \"Unknown data integrity issue with resource: \" + realCause.getMessage());\n}",
        "accept_response": "private void handleCodeValueDataIntegrityIssues(final JsonCommand command, final DataIntegrityViolationException dve) {\n    final Throwable realCause = dve.getMostSpecificCause();\n    if (realCause.getMessage().contains(\"code_value\")) {\n        final String name = command.stringValueOfParameterNamed(\"name\");\n        throw new PlatformDataIntegrityException(\"error.msg.code.value.duplicate.label\", \"A code value with lable '\" + name\n                + \"' already exists\", \"name\", name);\n    }\n\n    logger.error(\"Error occured.\", dve);\n    throw new PlatformDataIntegrityException(\"error.msg.code.value.unknown.data.integrity.issue\",\n            \"Unknown data integrity issue with resource: \" + realCause.getMessage());\n}",
        "reject_response": "private void handleCodeValueDataIntegrityIssues(final JsonCommand command, final DataIntegrityViolationException dve) {\n    final Throwable realCause = dve.getMostSpecificCause();\n    if (realCause.getMessage().contains(\"code_value\")) {\n        final String name = command.stringValueOfParameterNamed(\"name\");\n        throw new PlatformDataIntegrityException(\"error.msg.code.value.duplicate.label\", \"A code value with lable '\" + name\n                + \"' already exists\", \"name\", name);\n    }\n\n    logger.error(dve.getMessage(), dve);\n    throw new PlatformDataIntegrityException(\"error.msg.code.value.unknown.data.integrity.issue\",\n            \"Unknown data integrity issue with resource: \" + realCause.getMessage());\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3210,
        "instruction": "public void run() {\n  JobProgressPoller progressUpdator = null;\n  try {\n    progressUpdator = new JobProgressPoller(this, progressUpdateIntervalMs);\n    progressUpdator.start();\n    dateStarted = new Date();\n    result = jobRun();\n    this.exception = null;\n    errorMessage = null;\n    dateFinished = new Date();\n    progressUpdator.terminate();\n  } catch (NullPointerException e) {\n    LOGGER.error(\"Job failed\", e);\n    progressUpdator.terminate();\n    this.exception = e;\n    result = e.getMessage();\n    errorMessage = getStack(e);\n    dateFinished = new Date();\n  } catch (Throwable e) {\n    progressUpdator.terminate();\n    this.exception = e;\n    result = e.getMessage();\n    errorMessage = getStack(e);\n    dateFinished = new Date();\n  } finally {\n    //aborted = false;\n  }\n}",
        "accept_response": "public void run() {\n  JobProgressPoller progressUpdator = null;\n  try {\n    progressUpdator = new JobProgressPoller(this, progressUpdateIntervalMs);\n    progressUpdator.start();\n    dateStarted = new Date();\n    result = jobRun();\n    this.exception = null;\n    errorMessage = null;\n    dateFinished = new Date();\n    progressUpdator.terminate();\n  } catch (NullPointerException e) {\n    LOGGER.error(\"Job failed\", e);\n    progressUpdator.terminate();\n    this.exception = e;\n    result = e.getMessage();\n    errorMessage = getStack(e);\n    dateFinished = new Date();\n  } catch (Throwable e) {\n    LOGGER.error(\"Job failed\", e);\n    progressUpdator.terminate();\n    this.exception = e;\n    result = e.getMessage();\n    errorMessage = getStack(e);\n    dateFinished = new Date();\n  } finally {\n    //aborted = false;\n  }\n}",
        "reject_response": "public void run() {\n  JobProgressPoller progressUpdator = null;\n  try {\n    progressUpdator = new JobProgressPoller(this, progressUpdateIntervalMs);\n    progressUpdator.start();\n    dateStarted = new Date();\n    result = jobRun();\n    this.exception = null;\n    errorMessage = null;\n    dateFinished = new Date();\n    progressUpdator.terminate();\n  } catch (NullPointerException e) {\n    LOGGER.error(\"Job failed\", e);\n    progressUpdator.terminate();\n    this.exception = e;\n    result = e.getMessage();\n    errorMessage = getStack(e);\n    dateFinished = new Date();\n  } catch (Throwable e) {\n    logger().error(\"Job failed\", e);\n    progressUpdator.terminate();\n    this.exception = e;\n    result = e.getMessage();\n    errorMessage = getStack(e);\n    dateFinished = new Date();\n  } finally {\n    //aborted = false;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2898,
        "instruction": "private Collection<TermDefinition> createTermDefinitionCollection(Class<?>[] classes) {\n    Collection<TermDefinition> l = new ArrayList<TermDefinition>();\n    for (int j = 0; j < classes.length; j++) {\n        try {\n            l.add(new DefaultTermDefinition(classes[j]));\n        } catch (Exception e) {\n            throw new IllegalStateException(\n                    \"Unable to create the term collection\");\n        }\n    }\n    return Collections.synchronizedCollection(l);\n}",
        "accept_response": "private Collection<TermDefinition> createTermDefinitionCollection(Class<?>[] classes) {\n    Collection<TermDefinition> l = new ArrayList<TermDefinition>();\n    for (int j = 0; j < classes.length; j++) {\n        try {\n            l.add(new DefaultTermDefinition(classes[j]));\n        } catch (Exception e) {\n            LOGGER.debug(\"Unable to create the term collection\", e);\n            throw new IllegalStateException(\n                    \"Unable to create the term collection\");\n        }\n    }\n    return Collections.synchronizedCollection(l);\n}",
        "reject_response": "private Collection<TermDefinition> createTermDefinitionCollection(Class<?>[] classes) {\n    Collection<TermDefinition> l = new ArrayList<TermDefinition>();\n    for (int j = 0; j < classes.length; j++) {\n        try {\n            l.add(new DefaultTermDefinition(classes[j]));\n        } catch (Exception e) {\n            log.debug(\"Unable to create the term collection\", e);\n            throw new IllegalStateException(\n                    \"Unable to create the term collection\");\n        }\n    }\n    return Collections.synchronizedCollection(l);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3063,
        "instruction": "@Override\npublic void handleRequestBody(SolrQueryRequest req, SolrQueryResponse rsp) throws Exception {\n  // Make sure the cores is enabled\n  CoreContainer cores = checkErrors();\n\n  // Pick the action\n  SolrParams params = req.getParams();\n  String a = params.get(CoreAdminParams.ACTION);\n  if (a != null) {\n    CollectionAction action = CollectionAction.get(a);\n    if (action == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown action: \" + a);\n    }\n    CollectionOperation operation = CollectionOperation.get(action);\n    MDCLoggingContext.setCollection(req.getParams().get(COLLECTION));\n    invokeAction(req, rsp, cores, action, operation);\n  } else {\n    throw new SolrException(ErrorCode.BAD_REQUEST, \"action is a required param\");\n  }\n  rsp.setHttpCaching(false);\n}",
        "accept_response": "@Override\npublic void handleRequestBody(SolrQueryRequest req, SolrQueryResponse rsp) throws Exception {\n  // Make sure the cores is enabled\n  CoreContainer cores = checkErrors();\n\n  // Pick the action\n  SolrParams params = req.getParams();\n  String a = params.get(CoreAdminParams.ACTION);\n  if (a != null) {\n    CollectionAction action = CollectionAction.get(a);\n    if (action == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown action: \" + a);\n    }\n    CollectionOperation operation = CollectionOperation.get(action);\n    if (log.isDebugEnabled()) {\n      log.debug(\"Invoked Collection Action :{} with params {} and sendToOCPQueue={}\"\n          , action.toLower(), req.getParamString(), operation.sendToOCPQueue);\n    }\n    MDCLoggingContext.setCollection(req.getParams().get(COLLECTION));\n    invokeAction(req, rsp, cores, action, operation);\n  } else {\n    throw new SolrException(ErrorCode.BAD_REQUEST, \"action is a required param\");\n  }\n  rsp.setHttpCaching(false);\n}",
        "reject_response": "@Override\npublic void handleRequestBody(SolrQueryRequest req, SolrQueryResponse rsp) throws Exception {\n  // Make sure the cores is enabled\n  CoreContainer cores = checkErrors();\n\n  // Pick the action\n  SolrParams params = req.getParams();\n  String a = params.get(CoreAdminParams.ACTION);\n  if (a != null) {\n    CollectionAction action = CollectionAction.get(a);\n    if (action == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown action: \" + a);\n    }\n    CollectionOperation operation = CollectionOperation.get(action);\n    if (log.isDebugEnabled()) {\n    if (log.isInfoEnabled()) {\n      log.info(\"Invoked Collection Action :{} with params {} and sendToOCPQueue={}\"\n    }\n    MDCLoggingContext.setCollection(req.getParams().get(COLLECTION));\n    invokeAction(req, rsp, cores, action, operation);\n  } else {\n    throw new SolrException(ErrorCode.BAD_REQUEST, \"action is a required param\");\n  }\n  rsp.setHttpCaching(false);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2944,
        "instruction": "private void emitSensorData(String sensorType, String message, int delay) {\n    try {\n        Thread.sleep(delay);\n    } catch (InterruptedException e) {\n        e.printStackTrace();\n    }\n    emitToKafka(sensorType, message);\n}",
        "accept_response": "private void emitSensorData(String sensorType, String message, int delay) {\n    try {\n        Thread.sleep(delay);\n    } catch (InterruptedException e) {\n        e.printStackTrace();\n    }\n    LOG.info(\"Emitting {} message {}\", sensorType, message);\n    emitToKafka(sensorType, message);\n}",
        "reject_response": "private void emitSensorData(String sensorType, String message, int delay) {\n    try {\n        Thread.sleep(delay);\n    } catch (InterruptedException e) {\n        e.printStackTrace();\n    }\n    LOG.info(\"Emitting \" + sensorType + \" message \" + message);\n    emitToKafka(sensorType, message);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2644,
        "instruction": "public Cluster getCluster(String username, String password) {\n  synchronized (this.clusterMap) {\n    String key = username;\n    Cluster data = this.clusterMap.get(key);\n\n    if (data == null) {\n      try {\n        data = new Cluster(this.jmxHost, this.jmxPort, username, password);\n        // Assign name to thread created\n        data.setName(\n            PulseConstants.APP_NAME + \"-\" + this.jmxHost + \":\" + this.jmxPort + \":\" + username);\n        // Start Thread\n        data.start();\n        data.waitForInitialization(15, TimeUnit.SECONDS);\n        this.clusterMap.put(key, data);\n      } catch (ConnectException | InterruptedException e) {\n        data = null;\n        logger.debug(e);\n      }\n    } else {\n      data.setJmxUserPassword(password);\n\n    }\n    return data;\n  }\n}",
        "accept_response": "public Cluster getCluster(String username, String password) {\n  synchronized (this.clusterMap) {\n    String key = username;\n    Cluster data = this.clusterMap.get(key);\n\n    if (data == null) {\n      try {\n        logger.info(\"{} : {}\", resourceBundle.getString(\"LOG_MSG_CREATE_NEW_THREAD\"), key);\n        data = new Cluster(this.jmxHost, this.jmxPort, username, password);\n        // Assign name to thread created\n        data.setName(\n            PulseConstants.APP_NAME + \"-\" + this.jmxHost + \":\" + this.jmxPort + \":\" + username);\n        // Start Thread\n        data.start();\n        data.waitForInitialization(15, TimeUnit.SECONDS);\n        this.clusterMap.put(key, data);\n      } catch (ConnectException | InterruptedException e) {\n        data = null;\n        logger.debug(e);\n      }\n    } else {\n      data.setJmxUserPassword(password);\n\n    }\n    return data;\n  }\n}",
        "reject_response": "public Cluster getCluster(String username, String password) {\n  synchronized (this.clusterMap) {\n    String key = username;\n    Cluster data = this.clusterMap.get(key);\n\n    if (data == null) {\n      try {\n        if (LOGGER.infoEnabled()) {\n          LOGGER.info(resourceBundle.getString(\"LOG_MSG_CREATE_NEW_THREAD\") + \" : \" + key);\n        }\n        data = new Cluster(this.jmxHost, this.jmxPort, username, password);\n        // Assign name to thread created\n        data.setName(\n            PulseConstants.APP_NAME + \"-\" + this.jmxHost + \":\" + this.jmxPort + \":\" + username);\n        // Start Thread\n        data.start();\n        data.waitForInitialization(15, TimeUnit.SECONDS);\n        this.clusterMap.put(key, data);\n      } catch (ConnectException | InterruptedException e) {\n        data = null;\n        logger.debug(e);\n      }\n    } else {\n      data.setJmxUserPassword(password);\n\n    }\n    return data;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2477,
        "instruction": "private Channel getChannel() throws IOException {\n  if (!isChannelReady(channel)) {\n    // Need to reconnect\n    // Upgrade to write lock\n    stateLock.readLock().unlock();\n    stateLock.writeLock().lock();\n    try {\n      if (!isChannelReady(channel)) {\n        synchronized (channelFutureLock) {\n          if (!stopping) {\n            channelFuture = bootstrap.connect(remoteAddr);\n          }\n        }\n        if (channelFuture != null) {\n          try {\n            channelFuture.await(connectTimeoutMillis);\n          } catch (InterruptedException e) {\n            Thread.currentThread().interrupt(); // Reset interrupt flag\n            throw new IOException(\"Interrupted while connecting to \" + remoteAddr);\n          }\n\n          synchronized (channelFutureLock) {\n            if (!channelFuture.isSuccess()) {\n              throw new IOException(\"Error connecting to \" + remoteAddr, channelFuture.getCause());\n            }\n            channel = channelFuture.getChannel();\n            channelFuture = null;\n          }\n        }\n      }\n    } finally {\n      // Downgrade to read lock:\n      stateLock.readLock().lock();\n      stateLock.writeLock().unlock();\n    }\n  }\n  return channel;\n}",
        "accept_response": "private Channel getChannel() throws IOException {\n  if (!isChannelReady(channel)) {\n    // Need to reconnect\n    // Upgrade to write lock\n    stateLock.readLock().unlock();\n    stateLock.writeLock().lock();\n    try {\n      if (!isChannelReady(channel)) {\n        synchronized (channelFutureLock) {\n          if (!stopping) {\n            LOG.debug(\"Connecting to {}\", remoteAddr);\n            channelFuture = bootstrap.connect(remoteAddr);\n          }\n        }\n        if (channelFuture != null) {\n          try {\n            channelFuture.await(connectTimeoutMillis);\n          } catch (InterruptedException e) {\n            Thread.currentThread().interrupt(); // Reset interrupt flag\n            throw new IOException(\"Interrupted while connecting to \" + remoteAddr);\n          }\n\n          synchronized (channelFutureLock) {\n            if (!channelFuture.isSuccess()) {\n              throw new IOException(\"Error connecting to \" + remoteAddr, channelFuture.getCause());\n            }\n            channel = channelFuture.getChannel();\n            channelFuture = null;\n          }\n        }\n      }\n    } finally {\n      // Downgrade to read lock:\n      stateLock.readLock().lock();\n      stateLock.writeLock().unlock();\n    }\n  }\n  return channel;\n}",
        "reject_response": "private Channel getChannel() throws IOException {\n  if (!isChannelReady(channel)) {\n    // Need to reconnect\n    // Upgrade to write lock\n    stateLock.readLock().unlock();\n    stateLock.writeLock().lock();\n    try {\n      if (!isChannelReady(channel)) {\n        synchronized (channelFutureLock) {\n          if (!stopping) {\n            LOG.debug(\"Connecting to \" + remoteAddr);\n            channelFuture = bootstrap.connect(remoteAddr);\n          }\n        }\n        if (channelFuture != null) {\n          try {\n            channelFuture.await(connectTimeoutMillis);\n          } catch (InterruptedException e) {\n            Thread.currentThread().interrupt(); // Reset interrupt flag\n            throw new IOException(\"Interrupted while connecting to \" + remoteAddr);\n          }\n\n          synchronized (channelFutureLock) {\n            if (!channelFuture.isSuccess()) {\n              throw new IOException(\"Error connecting to \" + remoteAddr, channelFuture.getCause());\n            }\n            channel = channelFuture.getChannel();\n            channelFuture = null;\n          }\n        }\n      }\n    } finally {\n      // Downgrade to read lock:\n      stateLock.readLock().lock();\n      stateLock.writeLock().unlock();\n    }\n  }\n  return channel;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2999,
        "instruction": "public static Map<String, Object> connectionTest(String serviceName, Map<String, String> configs) throws Exception {\n  Map<String, Object> ret = null;\n\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"==> PrestoResourceManager.connectionTest() ServiceName: \" + serviceName + \" Configs: \" + configs);\n  }\n\n  try {\n    ret = PrestoClient.connectionTest(serviceName, configs);\n  } catch (Exception e) {\n    LOG.error(\"<== PrestoResourceManager.connectionTest() Error: \" + e);\n    throw e;\n  }\n\n  return ret;\n}",
        "accept_response": "public static Map<String, Object> connectionTest(String serviceName, Map<String, String> configs) throws Exception {\n  Map<String, Object> ret = null;\n\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"==> PrestoResourceManager.connectionTest() ServiceName: \" + serviceName + \" Configs: \" + configs);\n  }\n\n  try {\n    ret = PrestoClient.connectionTest(serviceName, configs);\n  } catch (Exception e) {\n    LOG.error(\"<== PrestoResourceManager.connectionTest() Error: \" + e);\n    throw e;\n  }\n\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"<== PrestoResourceManager.connectionTest() Result : \" + ret);\n  }\n\n  return ret;\n}",
        "reject_response": "public static Map<String, Object> connectionTest(String serviceName, Map<String, String> configs) throws Exception {\n  Map<String, Object> ret = null;\n\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"==> PrestoResourceManager.connectionTest() ServiceName: \" + serviceName + \" Configs: \" + configs);\n  }\n\n  try {\n    ret = PrestoClient.connectionTest(serviceName, configs);\n  } catch (Exception e) {\n    LOG.error(\"<== PrestoResourceManager.connectionTest() Error: \" + e);\n    throw e;\n  }\n\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"<== PrestoResourceManager.connectionTest Result : \" + ret);\n  }\n\n  return ret;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3178,
        "instruction": "@Override\npublic void reInitializeContainerAsync(ContainerId containerId,\n    ContainerLaunchContext containerLaunchContex, boolean autoCommit){\n  if (!(callbackHandler instanceof AbstractCallbackHandler)) {\n    LOG.error(\"Callback handler does not implement container re-initialize \"\n        + \"callback methods\");\n    return;\n  }\n  AbstractCallbackHandler handler = (AbstractCallbackHandler) callbackHandler;\n  if (containers.get(containerId) == null) {\n    handler.onContainerReInitializeError(\n        containerId, RPCUtil.getRemoteException(\n            \"Container \" + containerId + \" is not started\"));\n  }\n  try {\n    events.put(new ReInitializeContainerEvevnt(containerId,\n        client.getNodeIdOfStartedContainer(containerId),\n        containerLaunchContex, autoCommit));\n  } catch (InterruptedException e) {\n    handler.onContainerReInitializeError(containerId, e);\n  }\n}",
        "accept_response": "@Override\npublic void reInitializeContainerAsync(ContainerId containerId,\n    ContainerLaunchContext containerLaunchContex, boolean autoCommit){\n  if (!(callbackHandler instanceof AbstractCallbackHandler)) {\n    LOG.error(\"Callback handler does not implement container re-initialize \"\n        + \"callback methods\");\n    return;\n  }\n  AbstractCallbackHandler handler = (AbstractCallbackHandler) callbackHandler;\n  if (containers.get(containerId) == null) {\n    handler.onContainerReInitializeError(\n        containerId, RPCUtil.getRemoteException(\n            \"Container \" + containerId + \" is not started\"));\n  }\n  try {\n    events.put(new ReInitializeContainerEvevnt(containerId,\n        client.getNodeIdOfStartedContainer(containerId),\n        containerLaunchContex, autoCommit));\n  } catch (InterruptedException e) {\n    LOG.warn(\"Exception when scheduling the event of \" +\n            \"re-initializing of Container {}\", containerId);\n    handler.onContainerReInitializeError(containerId, e);\n  }\n}",
        "reject_response": "@Override\npublic void reInitializeContainerAsync(ContainerId containerId,\n    ContainerLaunchContext containerLaunchContex, boolean autoCommit){\n  if (!(callbackHandler instanceof AbstractCallbackHandler)) {\n    LOG.error(\"Callback handler does not implement container re-initialize \"\n        + \"callback methods\");\n    return;\n  }\n  AbstractCallbackHandler handler = (AbstractCallbackHandler) callbackHandler;\n  if (containers.get(containerId) == null) {\n    handler.onContainerReInitializeError(\n        containerId, RPCUtil.getRemoteException(\n            \"Container \" + containerId + \" is not started\"));\n  }\n  try {\n    events.put(new ReInitializeContainerEvevnt(containerId,\n        client.getNodeIdOfStartedContainer(containerId),\n        containerLaunchContex, autoCommit));\n  } catch (InterruptedException e) {\n    LOG.warn(\"Exception when scheduling the event of re-initializing of \"\n        + \"Container \" + containerId);\n    handler.onContainerReInitializeError(containerId, e);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3181,
        "instruction": "@Override\npublic void commitLastReInitializationAsync(ContainerId containerId){\n  if (!(callbackHandler instanceof AbstractCallbackHandler)) {\n    LOG.error(\"Callback handler does not implement container commit last \" +\n        \"re-initialization callback methods\");\n    return;\n  }\n  AbstractCallbackHandler handler = (AbstractCallbackHandler) callbackHandler;\n  if (containers.get(containerId) == null) {\n    handler.onCommitLastReInitializationError(\n        containerId, RPCUtil.getRemoteException(\n            \"Container \" + containerId + \" is not started\"));\n  }\n  try {\n    events.put(new ContainerEvent(containerId,\n        client.getNodeIdOfStartedContainer(containerId),\n        null, ContainerEventType.COMMIT_LAST_REINT));\n  } catch (InterruptedException e) {\n    handler.onCommitLastReInitializationError(containerId, e);\n  }\n}",
        "accept_response": "@Override\npublic void commitLastReInitializationAsync(ContainerId containerId){\n  if (!(callbackHandler instanceof AbstractCallbackHandler)) {\n    LOG.error(\"Callback handler does not implement container commit last \" +\n        \"re-initialization callback methods\");\n    return;\n  }\n  AbstractCallbackHandler handler = (AbstractCallbackHandler) callbackHandler;\n  if (containers.get(containerId) == null) {\n    handler.onCommitLastReInitializationError(\n        containerId, RPCUtil.getRemoteException(\n            \"Container \" + containerId + \" is not started\"));\n  }\n  try {\n    events.put(new ContainerEvent(containerId,\n        client.getNodeIdOfStartedContainer(containerId),\n        null, ContainerEventType.COMMIT_LAST_REINT));\n  } catch (InterruptedException e) {\n    LOG.warn(\"Exception when scheduling the event \" +\n            \"Commit re-initialization of Container {}\", containerId);\n    handler.onCommitLastReInitializationError(containerId, e);\n  }\n}",
        "reject_response": "@Override\npublic void commitLastReInitializationAsync(ContainerId containerId){\n  if (!(callbackHandler instanceof AbstractCallbackHandler)) {\n    LOG.error(\"Callback handler does not implement container commit last \" +\n        \"re-initialization callback methods\");\n    return;\n  }\n  AbstractCallbackHandler handler = (AbstractCallbackHandler) callbackHandler;\n  if (containers.get(containerId) == null) {\n    handler.onCommitLastReInitializationError(\n        containerId, RPCUtil.getRemoteException(\n            \"Container \" + containerId + \" is not started\"));\n  }\n  try {\n    events.put(new ContainerEvent(containerId,\n        client.getNodeIdOfStartedContainer(containerId),\n        null, ContainerEventType.COMMIT_LAST_REINT));\n  } catch (InterruptedException e) {\n    LOG.warn(\"Exception when scheduling the event Commit re-initialization\"\n        + \" of Container \" + containerId);\n    handler.onCommitLastReInitializationError(containerId, e);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3218,
        "instruction": "@SuppressFBWarnings(value = \"NP_NULL_PARAM_DEREF_NONVIRTUAL\",\n        justification = \"findPrefixInChildren will assign a value to this.id\")\npublic boolean execute() throws KeeperException, InterruptedException {\n    do {\n        if (id == null) {\n            long sessionId = zookeeper.getSessionId();\n            String prefix = \"x-\" + sessionId + \"-\";\n            // lets try look up the current ID if we failed\n            // in the middle of creating the znode\n            findPrefixInChildren(prefix, zookeeper, dir);\n            idName = new ZNodeName(id);\n        }\n        List<String> names = zookeeper.getChildren(dir, false);\n        if (names.isEmpty()) {\n            LOG.warn(\"No children in: \" + dir + \" when we've just \" +\n            \"created one! Lets recreate it...\");\n            // lets force the recreation of the id\n            id = null;\n        } else {\n            // lets sort them explicitly (though they do seem to come back in order ususally :)\n            SortedSet<ZNodeName> sortedNames = new TreeSet<ZNodeName>();\n            for (String name : names) {\n                sortedNames.add(new ZNodeName(dir + \"/\" + name));\n            }\n            ownerId = sortedNames.first().getName();\n            SortedSet<ZNodeName> lessThanMe = sortedNames.headSet(idName);\n            if (!lessThanMe.isEmpty()) {\n                ZNodeName lastChildName = lessThanMe.last();\n                lastChildId = lastChildName.getName();\n                Stat stat = zookeeper.exists(lastChildId, new LockWatcher());\n                if (stat != null) {\n                    return Boolean.FALSE;\n                } else {\n                    LOG.warn(\"Could not find the\" +\n                                    \" stats for less than me: \" + lastChildName.getName());\n                }\n            } else {\n                if (isOwner()) {\n                    LockListener lockListener = getLockListener();\n                    if (lockListener != null) {\n                        lockListener.lockAcquired();\n                    }\n                    return Boolean.TRUE;\n                }\n            }\n        }\n    }\n    while (id == null);\n    return Boolean.FALSE;\n}",
        "accept_response": "@SuppressFBWarnings(value = \"NP_NULL_PARAM_DEREF_NONVIRTUAL\",\n        justification = \"findPrefixInChildren will assign a value to this.id\")\npublic boolean execute() throws KeeperException, InterruptedException {\n    do {\n        if (id == null) {\n            long sessionId = zookeeper.getSessionId();\n            String prefix = \"x-\" + sessionId + \"-\";\n            // lets try look up the current ID if we failed\n            // in the middle of creating the znode\n            findPrefixInChildren(prefix, zookeeper, dir);\n            idName = new ZNodeName(id);\n        }\n        List<String> names = zookeeper.getChildren(dir, false);\n        if (names.isEmpty()) {\n            LOG.warn(\"No children in: \" + dir + \" when we've just \" +\n            \"created one! Lets recreate it...\");\n            // lets force the recreation of the id\n            id = null;\n        } else {\n            // lets sort them explicitly (though they do seem to come back in order ususally :)\n            SortedSet<ZNodeName> sortedNames = new TreeSet<ZNodeName>();\n            for (String name : names) {\n                sortedNames.add(new ZNodeName(dir + \"/\" + name));\n            }\n            ownerId = sortedNames.first().getName();\n            SortedSet<ZNodeName> lessThanMe = sortedNames.headSet(idName);\n            if (!lessThanMe.isEmpty()) {\n                ZNodeName lastChildName = lessThanMe.last();\n                lastChildId = lastChildName.getName();\n                LOG.debug(\"watching less than me node: {}\", lastChildId);\n                Stat stat = zookeeper.exists(lastChildId, new LockWatcher());\n                if (stat != null) {\n                    return Boolean.FALSE;\n                } else {\n                    LOG.warn(\"Could not find the\" +\n                                    \" stats for less than me: \" + lastChildName.getName());\n                }\n            } else {\n                if (isOwner()) {\n                    LockListener lockListener = getLockListener();\n                    if (lockListener != null) {\n                        lockListener.lockAcquired();\n                    }\n                    return Boolean.TRUE;\n                }\n            }\n        }\n    }\n    while (id == null);\n    return Boolean.FALSE;\n}",
        "reject_response": "@SuppressFBWarnings(value = \"NP_NULL_PARAM_DEREF_NONVIRTUAL\",\n        justification = \"findPrefixInChildren will assign a value to this.id\")\npublic boolean execute() throws KeeperException, InterruptedException {\n    do {\n        if (id == null) {\n            long sessionId = zookeeper.getSessionId();\n            String prefix = \"x-\" + sessionId + \"-\";\n            // lets try look up the current ID if we failed\n            // in the middle of creating the znode\n            findPrefixInChildren(prefix, zookeeper, dir);\n            idName = new ZNodeName(id);\n        }\n        List<String> names = zookeeper.getChildren(dir, false);\n        if (names.isEmpty()) {\n            LOG.warn(\"No children in: \" + dir + \" when we've just \" +\n            \"created one! Lets recreate it...\");\n            // lets force the recreation of the id\n            id = null;\n        } else {\n            // lets sort them explicitly (though they do seem to come back in order ususally :)\n            SortedSet<ZNodeName> sortedNames = new TreeSet<ZNodeName>();\n            for (String name : names) {\n                sortedNames.add(new ZNodeName(dir + \"/\" + name));\n            }\n            ownerId = sortedNames.first().getName();\n            SortedSet<ZNodeName> lessThanMe = sortedNames.headSet(idName);\n            if (!lessThanMe.isEmpty()) {\n                ZNodeName lastChildName = lessThanMe.last();\n                lastChildId = lastChildName.getName();\n                if (LOG.isDebugEnabled()) {\n                    LOG.debug(\"watching less than me node: \" + lastChildId);\n                }\n                Stat stat = zookeeper.exists(lastChildId, new LockWatcher());\n                if (stat != null) {\n                    return Boolean.FALSE;\n                } else {\n                    LOG.warn(\"Could not find the\" +\n                                    \" stats for less than me: \" + lastChildName.getName());\n                }\n            } else {\n                if (isOwner()) {\n                    LockListener lockListener = getLockListener();\n                    if (lockListener != null) {\n                        lockListener.lockAcquired();\n                    }\n                    return Boolean.TRUE;\n                }\n            }\n        }\n    }\n    while (id == null);\n    return Boolean.FALSE;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2512,
        "instruction": "@VisibleForTesting\npublic static boolean handleCommitError(String message, Throwable t)\n{\n    JVMStabilityInspector.inspectCommitLogThrowable(t);\n    switch (DatabaseDescriptor.getCommitFailurePolicy())\n    {\n        // Needed here for unit tests to not fail on default assertion\n        case die:\n        case stop:\n            StorageService.instance.stopTransports();\n            //$FALL-THROUGH$\n        case stop_commit:\n            String errorMsg = String.format(\"%s. Commit disk failure policy is %s; terminating thread.\", message, DatabaseDescriptor.getCommitFailurePolicy());\n            logger.error(addAdditionalInformationIfPossible(errorMsg), t);\n            return false;\n        case ignore:\n            return true;\n        default:\n            throw new AssertionError(DatabaseDescriptor.getCommitFailurePolicy());\n    }\n}",
        "accept_response": "@VisibleForTesting\npublic static boolean handleCommitError(String message, Throwable t)\n{\n    JVMStabilityInspector.inspectCommitLogThrowable(t);\n    switch (DatabaseDescriptor.getCommitFailurePolicy())\n    {\n        // Needed here for unit tests to not fail on default assertion\n        case die:\n        case stop:\n            StorageService.instance.stopTransports();\n            //$FALL-THROUGH$\n        case stop_commit:\n            String errorMsg = String.format(\"%s. Commit disk failure policy is %s; terminating thread.\", message, DatabaseDescriptor.getCommitFailurePolicy());\n            logger.error(addAdditionalInformationIfPossible(errorMsg), t);\n            return false;\n        case ignore:\n            logger.error(addAdditionalInformationIfPossible(message), t);\n            return true;\n        default:\n            throw new AssertionError(DatabaseDescriptor.getCommitFailurePolicy());\n    }\n}",
        "reject_response": "@VisibleForTesting\npublic static boolean handleCommitError(String message, Throwable t)\n{\n    JVMStabilityInspector.inspectCommitLogThrowable(t);\n    switch (DatabaseDescriptor.getCommitFailurePolicy())\n    {\n        // Needed here for unit tests to not fail on default assertion\n        case die:\n        case stop:\n            StorageService.instance.stopTransports();\n            //$FALL-THROUGH$\n        case stop_commit:\n            String errorMsg = String.format(\"%s. Commit disk failure policy is %s; terminating thread.\", message, DatabaseDescriptor.getCommitFailurePolicy());\n            logger.error(addAdditionalInformationIfPossible(errorMsg), t);\n            return false;\n        case ignore:\n            logger.error(message, t);\n            return true;\n        default:\n            throw new AssertionError(DatabaseDescriptor.getCommitFailurePolicy());\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2952,
        "instruction": "@POST\n@Consumes(MediaType.WILDCARD)\n@Produces(MediaType.TEXT_PLAIN)\n@Path(\"oidc/exchange\")\n@ApiOperation(\n        value = \"Retrieves a JWT following a successful login sequence using the configured OpenId Connect provider.\",\n        response = String.class,\n        notes = NON_GUARANTEED_ENDPOINT\n)\npublic Response oidcExchange(@Context HttpServletRequest httpServletRequest, @Context HttpServletResponse httpServletResponse) {\n    // only consider user specific access over https\n    if (!httpServletRequest.isSecure()) {\n        throw new AuthenticationNotSupportedException(AUTHENTICATION_NOT_ENABLED_MSG);\n    }\n\n    // ensure oidc is enabled\n    if (!oidcService.isOidcEnabled()) {\n        return Response.status(Response.Status.CONFLICT).entity(OPEN_ID_CONNECT_SUPPORT_IS_NOT_CONFIGURED_MSG).build();\n    }\n\n    final String oidcRequestIdentifier = getCookieValue(httpServletRequest.getCookies(), OIDC_REQUEST_IDENTIFIER);\n    if (oidcRequestIdentifier == null) {\n        final String message = \"The login request identifier was not found in the request. Unable to continue.\";\n        logger.warn(message);\n        return Response.status(Response.Status.BAD_REQUEST).entity(message).build();\n    }\n\n    // remove the oidc request cookie\n    removeOidcRequestCookie(httpServletResponse);\n\n    // get the jwt\n    final String jwt = oidcService.getJwt(oidcRequestIdentifier);\n    if (jwt == null) {\n        throw new IllegalArgumentException(\"A JWT for this login request identifier could not be found. Unable to continue.\");\n    }\n\n    // generate the response\n    return generateOkResponse(jwt).build();\n}",
        "accept_response": "@POST\n@Consumes(MediaType.WILDCARD)\n@Produces(MediaType.TEXT_PLAIN)\n@Path(\"oidc/exchange\")\n@ApiOperation(\n        value = \"Retrieves a JWT following a successful login sequence using the configured OpenId Connect provider.\",\n        response = String.class,\n        notes = NON_GUARANTEED_ENDPOINT\n)\npublic Response oidcExchange(@Context HttpServletRequest httpServletRequest, @Context HttpServletResponse httpServletResponse) {\n    // only consider user specific access over https\n    if (!httpServletRequest.isSecure()) {\n        throw new AuthenticationNotSupportedException(AUTHENTICATION_NOT_ENABLED_MSG);\n    }\n\n    // ensure oidc is enabled\n    if (!oidcService.isOidcEnabled()) {\n        logger.debug(OPEN_ID_CONNECT_SUPPORT_IS_NOT_CONFIGURED_MSG);\n        return Response.status(Response.Status.CONFLICT).entity(OPEN_ID_CONNECT_SUPPORT_IS_NOT_CONFIGURED_MSG).build();\n    }\n\n    final String oidcRequestIdentifier = getCookieValue(httpServletRequest.getCookies(), OIDC_REQUEST_IDENTIFIER);\n    if (oidcRequestIdentifier == null) {\n        final String message = \"The login request identifier was not found in the request. Unable to continue.\";\n        logger.warn(message);\n        return Response.status(Response.Status.BAD_REQUEST).entity(message).build();\n    }\n\n    // remove the oidc request cookie\n    removeOidcRequestCookie(httpServletResponse);\n\n    // get the jwt\n    final String jwt = oidcService.getJwt(oidcRequestIdentifier);\n    if (jwt == null) {\n        throw new IllegalArgumentException(\"A JWT for this login request identifier could not be found. Unable to continue.\");\n    }\n\n    // generate the response\n    return generateOkResponse(jwt).build();\n}",
        "reject_response": "@POST\n@Consumes(MediaType.WILDCARD)\n@Produces(MediaType.TEXT_PLAIN)\n@Path(\"oidc/exchange\")\n@ApiOperation(\n        value = \"Retrieves a JWT following a successful login sequence using the configured OpenId Connect provider.\",\n        response = String.class,\n        notes = NON_GUARANTEED_ENDPOINT\n)\npublic Response oidcExchange(@Context HttpServletRequest httpServletRequest, @Context HttpServletResponse httpServletResponse) {\n    // only consider user specific access over https\n    if (!httpServletRequest.isSecure()) {\n        throw new AuthenticationNotSupportedException(AUTHENTICATION_NOT_ENABLED_MSG);\n    }\n\n    // ensure oidc is enabled\n    if (!oidcService.isOidcEnabled()) {\n        logger.warn(OPEN_ID_CONNECT_SUPPORT_IS_NOT_CONFIGURED_MSG);\n        return Response.status(Response.Status.CONFLICT).entity(OPEN_ID_CONNECT_SUPPORT_IS_NOT_CONFIGURED_MSG).build();\n    }\n\n    final String oidcRequestIdentifier = getCookieValue(httpServletRequest.getCookies(), OIDC_REQUEST_IDENTIFIER);\n    if (oidcRequestIdentifier == null) {\n        final String message = \"The login request identifier was not found in the request. Unable to continue.\";\n        logger.warn(message);\n        return Response.status(Response.Status.BAD_REQUEST).entity(message).build();\n    }\n\n    // remove the oidc request cookie\n    removeOidcRequestCookie(httpServletResponse);\n\n    // get the jwt\n    final String jwt = oidcService.getJwt(oidcRequestIdentifier);\n    if (jwt == null) {\n        throw new IllegalArgumentException(\"A JWT for this login request identifier could not be found. Unable to continue.\");\n    }\n\n    // generate the response\n    return generateOkResponse(jwt).build();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3234,
        "instruction": "synchronized public boolean tryToCommit(Proposal p, long zxid, SocketAddress followerAddr) {\n   // make sure that ops are committed in order. With reconfigurations it is now possible\n   // that different operations wait for different sets of acks, and we still want to enforce\n   // that they are committed in order. Currently we only permit one outstanding reconfiguration\n   // such that the reconfiguration and subsequent outstanding ops proposed while the reconfig is\n   // pending all wait for a quorum of old and new config, so it's not possible to get enough acks\n   // for an operation without getting enough acks for preceding ops. But in the future if multiple\n   // concurrent reconfigs are allowed, this can happen.\n   if (outstandingProposals.containsKey(zxid - 1)) return false;\n\n   // in order to be committed, a proposal must be accepted by a quorum.\n   //\n   // getting a quorum from all necessary configurations.\n    if (!p.hasAllQuorums()) {\n       return false;\n    }\n\n    // commit proposals in order\n    if (zxid != lastCommitted+1) {\n       LOG.warn(\"Commiting zxid 0x\" + Long.toHexString(zxid)\n                + \" from \" + followerAddr + \" not first!\");\n        LOG.warn(\"First is \"\n                + (lastCommitted+1));\n    }\n\n    outstandingProposals.remove(zxid);\n\n    if (p.request != null) {\n         toBeApplied.add(p);\n    }\n\n    if (p.request == null) {\n        LOG.warn(\"Going to commmit null: \" + p);\n    } else if (p.request.getHdr().getType() == OpCode.reconfig) {\n\n        //if this server is voter in new config with the same quorum address,\n        //then it will remain the leader\n        //otherwise an up-to-date follower will be designated as leader. This saves\n        //leader election time, unless the designated leader fails\n        Long designatedLeader = getDesignatedLeader(p, zxid);\n        //LOG.warn(\"designated leader is: \" + designatedLeader);\n\n        QuorumVerifier newQV = p.qvAcksetPairs.get(p.qvAcksetPairs.size()-1).getQuorumVerifier();\n\n        self.processReconfig(newQV, designatedLeader, zk.getZxid(), true);\n\n        if (designatedLeader != self.getId()) {\n            allowedToCommit = false;\n        }\n\n        // we're sending the designated leader, and if the leader is changing the followers are\n        // responsible for closing the connection - this way we are sure that at least a majority of them\n        // receive the commit message.\n        commitAndActivate(zxid, designatedLeader);\n        informAndActivate(p, designatedLeader);\n        //turnOffFollowers();\n    } else {\n        p.request.logLatency(ServerMetrics.getMetrics().QUORUM_ACK_LATENCY);\n        commit(zxid);\n        inform(p);\n    }\n    zk.commitProcessor.commit(p.request);\n    if(pendingSyncs.containsKey(zxid)){\n        for(LearnerSyncRequest r: pendingSyncs.remove(zxid)) {\n            sendSync(r);\n        }\n    }\n\n    return  true;\n}",
        "accept_response": "synchronized public boolean tryToCommit(Proposal p, long zxid, SocketAddress followerAddr) {\n   // make sure that ops are committed in order. With reconfigurations it is now possible\n   // that different operations wait for different sets of acks, and we still want to enforce\n   // that they are committed in order. Currently we only permit one outstanding reconfiguration\n   // such that the reconfiguration and subsequent outstanding ops proposed while the reconfig is\n   // pending all wait for a quorum of old and new config, so it's not possible to get enough acks\n   // for an operation without getting enough acks for preceding ops. But in the future if multiple\n   // concurrent reconfigs are allowed, this can happen.\n   if (outstandingProposals.containsKey(zxid - 1)) return false;\n\n   // in order to be committed, a proposal must be accepted by a quorum.\n   //\n   // getting a quorum from all necessary configurations.\n    if (!p.hasAllQuorums()) {\n       return false;\n    }\n\n    // commit proposals in order\n    if (zxid != lastCommitted+1) {\n       LOG.warn(\"Commiting zxid 0x\" + Long.toHexString(zxid)\n                + \" from \" + followerAddr + \" not first!\");\n        LOG.warn(\"First is \"\n                + (lastCommitted+1));\n    }\n\n    outstandingProposals.remove(zxid);\n\n    if (p.request != null) {\n         toBeApplied.add(p);\n    }\n\n    if (p.request == null) {\n        LOG.warn(\"Going to commmit null: \" + p);\n    } else if (p.request.getHdr().getType() == OpCode.reconfig) {\n        LOG.debug(\"Committing a reconfiguration! {}\", outstandingProposals.size());\n\n        //if this server is voter in new config with the same quorum address,\n        //then it will remain the leader\n        //otherwise an up-to-date follower will be designated as leader. This saves\n        //leader election time, unless the designated leader fails\n        Long designatedLeader = getDesignatedLeader(p, zxid);\n        //LOG.warn(\"designated leader is: \" + designatedLeader);\n\n        QuorumVerifier newQV = p.qvAcksetPairs.get(p.qvAcksetPairs.size()-1).getQuorumVerifier();\n\n        self.processReconfig(newQV, designatedLeader, zk.getZxid(), true);\n\n        if (designatedLeader != self.getId()) {\n            allowedToCommit = false;\n        }\n\n        // we're sending the designated leader, and if the leader is changing the followers are\n        // responsible for closing the connection - this way we are sure that at least a majority of them\n        // receive the commit message.\n        commitAndActivate(zxid, designatedLeader);\n        informAndActivate(p, designatedLeader);\n        //turnOffFollowers();\n    } else {\n        p.request.logLatency(ServerMetrics.getMetrics().QUORUM_ACK_LATENCY);\n        commit(zxid);\n        inform(p);\n    }\n    zk.commitProcessor.commit(p.request);\n    if(pendingSyncs.containsKey(zxid)){\n        for(LearnerSyncRequest r: pendingSyncs.remove(zxid)) {\n            sendSync(r);\n        }\n    }\n\n    return  true;\n}",
        "reject_response": "synchronized public boolean tryToCommit(Proposal p, long zxid, SocketAddress followerAddr) {\n   // make sure that ops are committed in order. With reconfigurations it is now possible\n   // that different operations wait for different sets of acks, and we still want to enforce\n   // that they are committed in order. Currently we only permit one outstanding reconfiguration\n   // such that the reconfiguration and subsequent outstanding ops proposed while the reconfig is\n   // pending all wait for a quorum of old and new config, so it's not possible to get enough acks\n   // for an operation without getting enough acks for preceding ops. But in the future if multiple\n   // concurrent reconfigs are allowed, this can happen.\n   if (outstandingProposals.containsKey(zxid - 1)) return false;\n\n   // in order to be committed, a proposal must be accepted by a quorum.\n   //\n   // getting a quorum from all necessary configurations.\n    if (!p.hasAllQuorums()) {\n       return false;\n    }\n\n    // commit proposals in order\n    if (zxid != lastCommitted+1) {\n       LOG.warn(\"Commiting zxid 0x\" + Long.toHexString(zxid)\n                + \" from \" + followerAddr + \" not first!\");\n        LOG.warn(\"First is \"\n                + (lastCommitted+1));\n    }\n\n    outstandingProposals.remove(zxid);\n\n    if (p.request != null) {\n         toBeApplied.add(p);\n    }\n\n    if (p.request == null) {\n        LOG.warn(\"Going to commmit null: \" + p);\n    } else if (p.request.getHdr().getType() == OpCode.reconfig) {\n        LOG.debug(\"Committing a reconfiguration! \" + outstandingProposals.size());\n\n        //if this server is voter in new config with the same quorum address,\n        //then it will remain the leader\n        //otherwise an up-to-date follower will be designated as leader. This saves\n        //leader election time, unless the designated leader fails\n        Long designatedLeader = getDesignatedLeader(p, zxid);\n        //LOG.warn(\"designated leader is: \" + designatedLeader);\n\n        QuorumVerifier newQV = p.qvAcksetPairs.get(p.qvAcksetPairs.size()-1).getQuorumVerifier();\n\n        self.processReconfig(newQV, designatedLeader, zk.getZxid(), true);\n\n        if (designatedLeader != self.getId()) {\n            allowedToCommit = false;\n        }\n\n        // we're sending the designated leader, and if the leader is changing the followers are\n        // responsible for closing the connection - this way we are sure that at least a majority of them\n        // receive the commit message.\n        commitAndActivate(zxid, designatedLeader);\n        informAndActivate(p, designatedLeader);\n        //turnOffFollowers();\n    } else {\n        p.request.logLatency(ServerMetrics.getMetrics().QUORUM_ACK_LATENCY);\n        commit(zxid);\n        inform(p);\n    }\n    zk.commitProcessor.commit(p.request);\n    if(pendingSyncs.containsKey(zxid)){\n        for(LearnerSyncRequest r: pendingSyncs.remove(zxid)) {\n            sendSync(r);\n        }\n    }\n\n    return  true;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2697,
        "instruction": "@GET\n@Path(\"getNodesInfo\")\n@Produces(\"application/json\")\npublic Response read() throws Exception {\n    StringBuilder jsonOutput = new StringBuilder(\"{\\\"regions\\\":[\");\n    try {\n        /*\n         * 1. Initialize the HADOOP client side API for a distributed file\n         * system\n         */\n        Configuration conf = new Configuration();\n        FileSystem fs = FileSystem.get(conf);\n        DistributedFileSystem dfs = (DistributedFileSystem) fs;\n\n        /*\n         * 2. Query the namenode for the datanodes info. Only live nodes are\n         * returned - in accordance with the results returned by\n         * org.apache.hadoop.hdfs.tools.DFSAdmin#report().\n         */\n        DatanodeInfo[] liveNodes = dfs.getDataNodeStats(DatanodeReportType.LIVE);\n\n        /*\n         * 3. Pack the datanodes info in a JSON text format and write it to\n         * the HTTP output stream.\n         */\n        String prefix = \"\";\n        for (DatanodeInfo node : liveNodes) {\n            verifyNode(node);\n            // write one node to the HTTP stream\n            jsonOutput.append(prefix).append(writeNode(node));\n            prefix = \",\";\n        }\n        jsonOutput.append(\"]}\");\n        LOG.debug(\"getNodesCluster output: \" + jsonOutput);\n    } catch (NodeDataException e) {\n        LOG.error(\"Nodes verification failed\", e);\n        throw e;\n    } catch (ClientAbortException e) {\n        LOG.error(\"Remote connection closed by HAWQ\", e);\n        throw e;\n    } catch (java.io.IOException e) {\n        LOG.error(\"Unhandled exception thrown\", e);\n        throw e;\n    }\n\n    return Response.ok(jsonOutput.toString(),\n            MediaType.APPLICATION_JSON_TYPE).build();\n}",
        "accept_response": "@GET\n@Path(\"getNodesInfo\")\n@Produces(\"application/json\")\npublic Response read() throws Exception {\n    LOG.debug(\"getNodesInfo started\");\n    StringBuilder jsonOutput = new StringBuilder(\"{\\\"regions\\\":[\");\n    try {\n        /*\n         * 1. Initialize the HADOOP client side API for a distributed file\n         * system\n         */\n        Configuration conf = new Configuration();\n        FileSystem fs = FileSystem.get(conf);\n        DistributedFileSystem dfs = (DistributedFileSystem) fs;\n\n        /*\n         * 2. Query the namenode for the datanodes info. Only live nodes are\n         * returned - in accordance with the results returned by\n         * org.apache.hadoop.hdfs.tools.DFSAdmin#report().\n         */\n        DatanodeInfo[] liveNodes = dfs.getDataNodeStats(DatanodeReportType.LIVE);\n\n        /*\n         * 3. Pack the datanodes info in a JSON text format and write it to\n         * the HTTP output stream.\n         */\n        String prefix = \"\";\n        for (DatanodeInfo node : liveNodes) {\n            verifyNode(node);\n            // write one node to the HTTP stream\n            jsonOutput.append(prefix).append(writeNode(node));\n            prefix = \",\";\n        }\n        jsonOutput.append(\"]}\");\n        LOG.debug(\"getNodesCluster output: \" + jsonOutput);\n    } catch (NodeDataException e) {\n        LOG.error(\"Nodes verification failed\", e);\n        throw e;\n    } catch (ClientAbortException e) {\n        LOG.error(\"Remote connection closed by HAWQ\", e);\n        throw e;\n    } catch (java.io.IOException e) {\n        LOG.error(\"Unhandled exception thrown\", e);\n        throw e;\n    }\n\n    return Response.ok(jsonOutput.toString(),\n            MediaType.APPLICATION_JSON_TYPE).build();\n}",
        "reject_response": "@GET\n@Path(\"getNodesInfo\")\n@Produces(\"application/json\")\npublic Response read() throws Exception {\n    Log.debug(\"getNodesInfo started\");\n    StringBuilder jsonOutput = new StringBuilder(\"{\\\"regions\\\":[\");\n    try {\n        /*\n         * 1. Initialize the HADOOP client side API for a distributed file\n         * system\n         */\n        Configuration conf = new Configuration();\n        FileSystem fs = FileSystem.get(conf);\n        DistributedFileSystem dfs = (DistributedFileSystem) fs;\n\n        /*\n         * 2. Query the namenode for the datanodes info. Only live nodes are\n         * returned - in accordance with the results returned by\n         * org.apache.hadoop.hdfs.tools.DFSAdmin#report().\n         */\n        DatanodeInfo[] liveNodes = dfs.getDataNodeStats(DatanodeReportType.LIVE);\n\n        /*\n         * 3. Pack the datanodes info in a JSON text format and write it to\n         * the HTTP output stream.\n         */\n        String prefix = \"\";\n        for (DatanodeInfo node : liveNodes) {\n            verifyNode(node);\n            // write one node to the HTTP stream\n            jsonOutput.append(prefix).append(writeNode(node));\n            prefix = \",\";\n        }\n        jsonOutput.append(\"]}\");\n        LOG.debug(\"getNodesCluster output: \" + jsonOutput);\n    } catch (NodeDataException e) {\n        LOG.error(\"Nodes verification failed\", e);\n        throw e;\n    } catch (ClientAbortException e) {\n        LOG.error(\"Remote connection closed by HAWQ\", e);\n        throw e;\n    } catch (java.io.IOException e) {\n        LOG.error(\"Unhandled exception thrown\", e);\n        throw e;\n    }\n\n    return Response.ok(jsonOutput.toString(),\n            MediaType.APPLICATION_JSON_TYPE).build();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2492,
        "instruction": "public void commit(IndexWriter writer) throws CorruptIndexException, IOException {\n  synchronized (_running) {\n    long s = System.nanoTime();\n    writer.commit();\n    long m = System.nanoTime();\n    LOG.info(\"Commit took [{0} ms] for [{1}/{2}]\", (m - s) / 1000000.0, _table, _shard);\n    rollLog();\n    long e = System.nanoTime();\n  }\n}",
        "accept_response": "public void commit(IndexWriter writer) throws CorruptIndexException, IOException {\n  synchronized (_running) {\n    long s = System.nanoTime();\n    writer.commit();\n    long m = System.nanoTime();\n    LOG.info(\"Commit took [{0} ms] for [{1}/{2}]\", (m - s) / 1000000.0, _table, _shard);\n    rollLog();\n    long e = System.nanoTime();\n    LOG.info(\"Log roller took [{0} ms] for [{1}/{2}]\", (e - m) / 1000000.0, _table, _shard);\n  }\n}",
        "reject_response": "public void commit(IndexWriter writer) throws CorruptIndexException, IOException {\n  synchronized (_running) {\n    long s = System.nanoTime();\n    writer.commit();\n    long m = System.nanoTime();\n    LOG.info(\"Commit took [{0} ms] for [{1}/{2}]\", (m - s) / 1000000.0, _table, _shard);\n    rollLog();\n    long e = System.nanoTime();\n    LOG.info(\"Log roller took [{0}] for [{1}]\", (e - m) / 1000000.0, writer);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3172,
        "instruction": "@Override\nprotected void validate(Set<NodeAttribute> nodeAttributes)\n    throws IOException {\n  try {\n    NodeLabelUtil.validateNodeAttributes(nodeAttributes);\n  } catch (IOException e) {\n    throw e;\n  }\n}",
        "accept_response": "@Override\nprotected void validate(Set<NodeAttribute> nodeAttributes)\n    throws IOException {\n  try {\n    NodeLabelUtil.validateNodeAttributes(nodeAttributes);\n  } catch (IOException e) {\n    LOG.error(\"Invalid node attribute(s) from Provider : {}.\", e.getMessage());\n    throw e;\n  }\n}",
        "reject_response": "@Override\nprotected void validate(Set<NodeAttribute> nodeAttributes)\n    throws IOException {\n  try {\n    NodeLabelUtil.validateNodeAttributes(nodeAttributes);\n  } catch (IOException e) {\n    LOG.error(\n        \"Invalid node attribute(s) from Provider : \" + e.getMessage());\n    throw e;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2398,
        "instruction": "public static List<KeyValue> scan(ClientContext context, ScanState scanState, int timeOut) throws ScanTimedOutException, AccumuloException,\n    AccumuloSecurityException, TableNotFoundException {\n  TabletLocation loc = null;\n  Instance instance = context.getInstance();\n  long startTime = System.currentTimeMillis();\n  String lastError = null;\n  String error = null;\n  int tooManyFilesCount = 0;\n  long sleepMillis = 100;\n\n  List<KeyValue> results = null;\n\n  Span span = Trace.start(\"scan\");\n  try {\n    while (results == null && !scanState.finished) {\n      if (Thread.currentThread().isInterrupted()) {\n        throw new AccumuloException(\"Thread interrupted\");\n      }\n\n      if ((System.currentTimeMillis() - startTime) / 1000.0 > timeOut)\n        throw new ScanTimedOutException();\n\n      while (loc == null) {\n        long currentTime = System.currentTimeMillis();\n        if ((currentTime - startTime) / 1000.0 > timeOut)\n          throw new ScanTimedOutException();\n\n        Span locateSpan = Trace.start(\"scan:locateTablet\");\n        try {\n          loc = TabletLocator.getLocator(context, scanState.tableId).locateTablet(context, scanState.startRow, scanState.skipStartRow, false);\n\n          if (loc == null) {\n            if (!Tables.exists(instance, scanState.tableId.toString()))\n              throw new TableDeletedException(scanState.tableId.toString());\n            else if (Tables.getTableState(instance, scanState.tableId.toString()) == TableState.OFFLINE)\n              throw new TableOfflineException(instance, scanState.tableId.toString());\n\n            error = \"Failed to locate tablet for table : \" + scanState.tableId + \" row : \" + scanState.startRow;\n            if (!error.equals(lastError))\n              log.debug(error);\n            else if (log.isTraceEnabled())\n              log.trace(error);\n            lastError = error;\n            sleepMillis = pause(sleepMillis);\n          } else {\n            // when a tablet splits we do want to continue scanning the low child\n            // of the split if we are already passed it\n            Range dataRange = loc.tablet_extent.toDataRange();\n\n            if (scanState.range.getStartKey() != null && dataRange.afterEndKey(scanState.range.getStartKey())) {\n              // go to the next tablet\n              scanState.startRow = loc.tablet_extent.getEndRow();\n              scanState.skipStartRow = true;\n              loc = null;\n            } else if (scanState.range.getEndKey() != null && dataRange.beforeStartKey(scanState.range.getEndKey())) {\n              // should not happen\n              throw new RuntimeException(\"Unexpected tablet, extent : \" + loc.tablet_extent + \"  range : \" + scanState.range + \" startRow : \"\n                  + scanState.startRow);\n            }\n          }\n        } catch (AccumuloServerException e) {\n          throw e;\n        } catch (AccumuloException e) {\n          error = \"exception from tablet loc \" + e.getMessage();\n          if (!error.equals(lastError))\n            log.debug(error);\n          else if (log.isTraceEnabled())\n            log.trace(error);\n\n          lastError = error;\n          sleepMillis = pause(sleepMillis);\n        } finally {\n          locateSpan.stop();\n        }\n      }\n\n      Span scanLocation = Trace.start(\"scan:location\");\n      scanLocation.data(\"tserver\", loc.tablet_location);\n      try {\n        results = scan(loc, scanState, context);\n      } catch (AccumuloSecurityException e) {\n        Tables.clearCache(instance);\n        if (!Tables.exists(instance, scanState.tableId.toString()))\n          throw new TableDeletedException(scanState.tableId.toString());\n        e.setTableInfo(Tables.getPrintableTableInfoFromId(instance, scanState.tableId.toString()));\n        throw e;\n      } catch (TApplicationException tae) {\n        throw new AccumuloServerException(loc.tablet_location, tae);\n      } catch (NotServingTabletException e) {\n        error = \"Scan failed, not serving tablet \" + loc;\n        if (!error.equals(lastError))\n          log.debug(error);\n        else if (log.isTraceEnabled())\n          log.trace(error);\n        lastError = error;\n\n        TabletLocator.getLocator(context, scanState.tableId).invalidateCache(loc.tablet_extent);\n        loc = null;\n\n        // no need to try the current scan id somewhere else\n        scanState.scanID = null;\n\n        if (scanState.isolated)\n          throw new IsolationException();\n\n        sleepMillis = pause(sleepMillis);\n      } catch (NoSuchScanIDException e) {\n        error = \"Scan failed, no such scan id \" + scanState.scanID + \" \" + loc;\n        if (!error.equals(lastError))\n          log.debug(error);\n        else if (log.isTraceEnabled())\n          log.trace(error);\n        lastError = error;\n\n        if (scanState.isolated)\n          throw new IsolationException();\n\n        scanState.scanID = null;\n      } catch (TooManyFilesException e) {\n        error = \"Tablet has too many files \" + loc + \" retrying...\";\n        if (!error.equals(lastError)) {\n          log.debug(error);\n          tooManyFilesCount = 0;\n        } else {\n          tooManyFilesCount++;\n          if (tooManyFilesCount == 300)\n            log.warn(error);\n          else if (log.isTraceEnabled())\n            log.trace(error);\n        }\n        lastError = error;\n\n        // not sure what state the scan session on the server side is\n        // in after this occurs, so lets be cautious and start a new\n        // scan session\n        scanState.scanID = null;\n\n        if (scanState.isolated)\n          throw new IsolationException();\n\n        sleepMillis = pause(sleepMillis);\n      } catch (TException e) {\n        TabletLocator.getLocator(context, scanState.tableId).invalidateCache(context.getInstance(), loc.tablet_location);\n        error = \"Scan failed, thrift error \" + e.getClass().getName() + \"  \" + e.getMessage() + \" \" + loc;\n        if (!error.equals(lastError))\n          log.debug(error);\n        else if (log.isTraceEnabled())\n          log.trace(error);\n        lastError = error;\n        loc = null;\n\n        // do not want to continue using the same scan id, if a timeout occurred could cause a batch to be skipped\n        // because a thread on the server side may still be processing the timed out continue scan\n        scanState.scanID = null;\n\n        if (scanState.isolated)\n          throw new IsolationException();\n\n        sleepMillis = pause(sleepMillis);\n      } finally {\n        scanLocation.stop();\n      }\n    }\n\n    if (results != null && results.size() == 0 && scanState.finished) {\n      results = null;\n    }\n\n    return results;\n  } catch (InterruptedException ex) {\n    throw new AccumuloException(ex);\n  } finally {\n    span.stop();\n  }\n}",
        "accept_response": "public static List<KeyValue> scan(ClientContext context, ScanState scanState, int timeOut) throws ScanTimedOutException, AccumuloException,\n    AccumuloSecurityException, TableNotFoundException {\n  TabletLocation loc = null;\n  Instance instance = context.getInstance();\n  long startTime = System.currentTimeMillis();\n  String lastError = null;\n  String error = null;\n  int tooManyFilesCount = 0;\n  long sleepMillis = 100;\n\n  List<KeyValue> results = null;\n\n  Span span = Trace.start(\"scan\");\n  try {\n    while (results == null && !scanState.finished) {\n      if (Thread.currentThread().isInterrupted()) {\n        throw new AccumuloException(\"Thread interrupted\");\n      }\n\n      if ((System.currentTimeMillis() - startTime) / 1000.0 > timeOut)\n        throw new ScanTimedOutException();\n\n      while (loc == null) {\n        long currentTime = System.currentTimeMillis();\n        if ((currentTime - startTime) / 1000.0 > timeOut)\n          throw new ScanTimedOutException();\n\n        Span locateSpan = Trace.start(\"scan:locateTablet\");\n        try {\n          loc = TabletLocator.getLocator(context, scanState.tableId).locateTablet(context, scanState.startRow, scanState.skipStartRow, false);\n\n          if (loc == null) {\n            if (!Tables.exists(instance, scanState.tableId.toString()))\n              throw new TableDeletedException(scanState.tableId.toString());\n            else if (Tables.getTableState(instance, scanState.tableId.toString()) == TableState.OFFLINE)\n              throw new TableOfflineException(instance, scanState.tableId.toString());\n\n            error = \"Failed to locate tablet for table : \" + scanState.tableId + \" row : \" + scanState.startRow;\n            if (!error.equals(lastError))\n              log.debug(error);\n            else if (log.isTraceEnabled())\n              log.trace(error);\n            lastError = error;\n            sleepMillis = pause(sleepMillis);\n          } else {\n            // when a tablet splits we do want to continue scanning the low child\n            // of the split if we are already passed it\n            Range dataRange = loc.tablet_extent.toDataRange();\n\n            if (scanState.range.getStartKey() != null && dataRange.afterEndKey(scanState.range.getStartKey())) {\n              // go to the next tablet\n              scanState.startRow = loc.tablet_extent.getEndRow();\n              scanState.skipStartRow = true;\n              loc = null;\n            } else if (scanState.range.getEndKey() != null && dataRange.beforeStartKey(scanState.range.getEndKey())) {\n              // should not happen\n              throw new RuntimeException(\"Unexpected tablet, extent : \" + loc.tablet_extent + \"  range : \" + scanState.range + \" startRow : \"\n                  + scanState.startRow);\n            }\n          }\n        } catch (AccumuloServerException e) {\n          log.debug(\"Scan failed, server side exception : {}\", e.getMessage());\n          throw e;\n        } catch (AccumuloException e) {\n          error = \"exception from tablet loc \" + e.getMessage();\n          if (!error.equals(lastError))\n            log.debug(error);\n          else if (log.isTraceEnabled())\n            log.trace(error);\n\n          lastError = error;\n          sleepMillis = pause(sleepMillis);\n        } finally {\n          locateSpan.stop();\n        }\n      }\n\n      Span scanLocation = Trace.start(\"scan:location\");\n      scanLocation.data(\"tserver\", loc.tablet_location);\n      try {\n        results = scan(loc, scanState, context);\n      } catch (AccumuloSecurityException e) {\n        Tables.clearCache(instance);\n        if (!Tables.exists(instance, scanState.tableId.toString()))\n          throw new TableDeletedException(scanState.tableId.toString());\n        e.setTableInfo(Tables.getPrintableTableInfoFromId(instance, scanState.tableId.toString()));\n        throw e;\n      } catch (TApplicationException tae) {\n        throw new AccumuloServerException(loc.tablet_location, tae);\n      } catch (NotServingTabletException e) {\n        error = \"Scan failed, not serving tablet \" + loc;\n        if (!error.equals(lastError))\n          log.debug(error);\n        else if (log.isTraceEnabled())\n          log.trace(error);\n        lastError = error;\n\n        TabletLocator.getLocator(context, scanState.tableId).invalidateCache(loc.tablet_extent);\n        loc = null;\n\n        // no need to try the current scan id somewhere else\n        scanState.scanID = null;\n\n        if (scanState.isolated)\n          throw new IsolationException();\n\n        sleepMillis = pause(sleepMillis);\n      } catch (NoSuchScanIDException e) {\n        error = \"Scan failed, no such scan id \" + scanState.scanID + \" \" + loc;\n        if (!error.equals(lastError))\n          log.debug(error);\n        else if (log.isTraceEnabled())\n          log.trace(error);\n        lastError = error;\n\n        if (scanState.isolated)\n          throw new IsolationException();\n\n        scanState.scanID = null;\n      } catch (TooManyFilesException e) {\n        error = \"Tablet has too many files \" + loc + \" retrying...\";\n        if (!error.equals(lastError)) {\n          log.debug(error);\n          tooManyFilesCount = 0;\n        } else {\n          tooManyFilesCount++;\n          if (tooManyFilesCount == 300)\n            log.warn(error);\n          else if (log.isTraceEnabled())\n            log.trace(error);\n        }\n        lastError = error;\n\n        // not sure what state the scan session on the server side is\n        // in after this occurs, so lets be cautious and start a new\n        // scan session\n        scanState.scanID = null;\n\n        if (scanState.isolated)\n          throw new IsolationException();\n\n        sleepMillis = pause(sleepMillis);\n      } catch (TException e) {\n        TabletLocator.getLocator(context, scanState.tableId).invalidateCache(context.getInstance(), loc.tablet_location);\n        error = \"Scan failed, thrift error \" + e.getClass().getName() + \"  \" + e.getMessage() + \" \" + loc;\n        if (!error.equals(lastError))\n          log.debug(error);\n        else if (log.isTraceEnabled())\n          log.trace(error);\n        lastError = error;\n        loc = null;\n\n        // do not want to continue using the same scan id, if a timeout occurred could cause a batch to be skipped\n        // because a thread on the server side may still be processing the timed out continue scan\n        scanState.scanID = null;\n\n        if (scanState.isolated)\n          throw new IsolationException();\n\n        sleepMillis = pause(sleepMillis);\n      } finally {\n        scanLocation.stop();\n      }\n    }\n\n    if (results != null && results.size() == 0 && scanState.finished) {\n      results = null;\n    }\n\n    return results;\n  } catch (InterruptedException ex) {\n    throw new AccumuloException(ex);\n  } finally {\n    span.stop();\n  }\n}",
        "reject_response": "public static List<KeyValue> scan(ClientContext context, ScanState scanState, int timeOut) throws ScanTimedOutException, AccumuloException,\n    AccumuloSecurityException, TableNotFoundException {\n  TabletLocation loc = null;\n  Instance instance = context.getInstance();\n  long startTime = System.currentTimeMillis();\n  String lastError = null;\n  String error = null;\n  int tooManyFilesCount = 0;\n  long sleepMillis = 100;\n\n  List<KeyValue> results = null;\n\n  Span span = Trace.start(\"scan\");\n  try {\n    while (results == null && !scanState.finished) {\n      if (Thread.currentThread().isInterrupted()) {\n        throw new AccumuloException(\"Thread interrupted\");\n      }\n\n      if ((System.currentTimeMillis() - startTime) / 1000.0 > timeOut)\n        throw new ScanTimedOutException();\n\n      while (loc == null) {\n        long currentTime = System.currentTimeMillis();\n        if ((currentTime - startTime) / 1000.0 > timeOut)\n          throw new ScanTimedOutException();\n\n        Span locateSpan = Trace.start(\"scan:locateTablet\");\n        try {\n          loc = TabletLocator.getLocator(context, scanState.tableId).locateTablet(context, scanState.startRow, scanState.skipStartRow, false);\n\n          if (loc == null) {\n            if (!Tables.exists(instance, scanState.tableId.toString()))\n              throw new TableDeletedException(scanState.tableId.toString());\n            else if (Tables.getTableState(instance, scanState.tableId.toString()) == TableState.OFFLINE)\n              throw new TableOfflineException(instance, scanState.tableId.toString());\n\n            error = \"Failed to locate tablet for table : \" + scanState.tableId + \" row : \" + scanState.startRow;\n            if (!error.equals(lastError))\n              log.debug(error);\n            else if (log.isTraceEnabled())\n              log.trace(error);\n            lastError = error;\n            sleepMillis = pause(sleepMillis);\n          } else {\n            // when a tablet splits we do want to continue scanning the low child\n            // of the split if we are already passed it\n            Range dataRange = loc.tablet_extent.toDataRange();\n\n            if (scanState.range.getStartKey() != null && dataRange.afterEndKey(scanState.range.getStartKey())) {\n              // go to the next tablet\n              scanState.startRow = loc.tablet_extent.getEndRow();\n              scanState.skipStartRow = true;\n              loc = null;\n            } else if (scanState.range.getEndKey() != null && dataRange.beforeStartKey(scanState.range.getEndKey())) {\n              // should not happen\n              throw new RuntimeException(\"Unexpected tablet, extent : \" + loc.tablet_extent + \"  range : \" + scanState.range + \" startRow : \"\n                  + scanState.startRow);\n            }\n          }\n        } catch (AccumuloServerException e) {\n          log.debug(\"Scan failed, server side exception : \" + e.getMessage());\n          throw e;\n        } catch (AccumuloException e) {\n          error = \"exception from tablet loc \" + e.getMessage();\n          if (!error.equals(lastError))\n            log.debug(error);\n          else if (log.isTraceEnabled())\n            log.trace(error);\n\n          lastError = error;\n          sleepMillis = pause(sleepMillis);\n        } finally {\n          locateSpan.stop();\n        }\n      }\n\n      Span scanLocation = Trace.start(\"scan:location\");\n      scanLocation.data(\"tserver\", loc.tablet_location);\n      try {\n        results = scan(loc, scanState, context);\n      } catch (AccumuloSecurityException e) {\n        Tables.clearCache(instance);\n        if (!Tables.exists(instance, scanState.tableId.toString()))\n          throw new TableDeletedException(scanState.tableId.toString());\n        e.setTableInfo(Tables.getPrintableTableInfoFromId(instance, scanState.tableId.toString()));\n        throw e;\n      } catch (TApplicationException tae) {\n        throw new AccumuloServerException(loc.tablet_location, tae);\n      } catch (NotServingTabletException e) {\n        error = \"Scan failed, not serving tablet \" + loc;\n        if (!error.equals(lastError))\n          log.debug(error);\n        else if (log.isTraceEnabled())\n          log.trace(error);\n        lastError = error;\n\n        TabletLocator.getLocator(context, scanState.tableId).invalidateCache(loc.tablet_extent);\n        loc = null;\n\n        // no need to try the current scan id somewhere else\n        scanState.scanID = null;\n\n        if (scanState.isolated)\n          throw new IsolationException();\n\n        sleepMillis = pause(sleepMillis);\n      } catch (NoSuchScanIDException e) {\n        error = \"Scan failed, no such scan id \" + scanState.scanID + \" \" + loc;\n        if (!error.equals(lastError))\n          log.debug(error);\n        else if (log.isTraceEnabled())\n          log.trace(error);\n        lastError = error;\n\n        if (scanState.isolated)\n          throw new IsolationException();\n\n        scanState.scanID = null;\n      } catch (TooManyFilesException e) {\n        error = \"Tablet has too many files \" + loc + \" retrying...\";\n        if (!error.equals(lastError)) {\n          log.debug(error);\n          tooManyFilesCount = 0;\n        } else {\n          tooManyFilesCount++;\n          if (tooManyFilesCount == 300)\n            log.warn(error);\n          else if (log.isTraceEnabled())\n            log.trace(error);\n        }\n        lastError = error;\n\n        // not sure what state the scan session on the server side is\n        // in after this occurs, so lets be cautious and start a new\n        // scan session\n        scanState.scanID = null;\n\n        if (scanState.isolated)\n          throw new IsolationException();\n\n        sleepMillis = pause(sleepMillis);\n      } catch (TException e) {\n        TabletLocator.getLocator(context, scanState.tableId).invalidateCache(context.getInstance(), loc.tablet_location);\n        error = \"Scan failed, thrift error \" + e.getClass().getName() + \"  \" + e.getMessage() + \" \" + loc;\n        if (!error.equals(lastError))\n          log.debug(error);\n        else if (log.isTraceEnabled())\n          log.trace(error);\n        lastError = error;\n        loc = null;\n\n        // do not want to continue using the same scan id, if a timeout occurred could cause a batch to be skipped\n        // because a thread on the server side may still be processing the timed out continue scan\n        scanState.scanID = null;\n\n        if (scanState.isolated)\n          throw new IsolationException();\n\n        sleepMillis = pause(sleepMillis);\n      } finally {\n        scanLocation.stop();\n      }\n    }\n\n    if (results != null && results.size() == 0 && scanState.finished) {\n      results = null;\n    }\n\n    return results;\n  } catch (InterruptedException ex) {\n    throw new AccumuloException(ex);\n  } finally {\n    span.stop();\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3125,
        "instruction": "void startServices() {\n  try {\n    Throwable firstError = null;\n    List<ServiceThread> threads = new ArrayList<ServiceThread>();\n    LOG.debug(\"Begin parallel start\");\n\n    for(ServiceWithDependency sd : services.values()) {\n      // start the service. If this fails that service\n      // will be stopped and an exception raised\n      ServiceThread st = new ServiceThread(sd, services);\n      threads.add(st);\n    }\n\n    for(ServiceThread st : threads) {\n      st.start();\n    }\n    for(ServiceThread st : threads) {\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"Waiting for service thread to join for \" + st.getName());\n      }\n      st.join();\n      if(st.error != null && firstError == null) {\n        firstError = st.error;\n      }\n    }\n\n    if(firstError != null) {\n      throw ServiceStateException.convert(firstError);\n    }\n  } catch (InterruptedException e) {\n    e.printStackTrace();\n  }\n}",
        "accept_response": "void startServices() {\n  try {\n    Throwable firstError = null;\n    List<ServiceThread> threads = new ArrayList<ServiceThread>();\n    LOG.debug(\"Begin parallel start\");\n\n    for(ServiceWithDependency sd : services.values()) {\n      // start the service. If this fails that service\n      // will be stopped and an exception raised\n      ServiceThread st = new ServiceThread(sd, services);\n      threads.add(st);\n    }\n\n    for(ServiceThread st : threads) {\n      st.start();\n    }\n    for(ServiceThread st : threads) {\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"Waiting for service thread to join for \" + st.getName());\n      }\n      st.join();\n      if(st.error != null && firstError == null) {\n        firstError = st.error;\n      }\n    }\n\n    if(firstError != null) {\n      throw ServiceStateException.convert(firstError);\n    }\n    LOG.debug(\"End parallel start\");\n  } catch (InterruptedException e) {\n    e.printStackTrace();\n  }\n}",
        "reject_response": "void startServices() {\n  try {\n    Throwable firstError = null;\n    List<ServiceThread> threads = new ArrayList<ServiceThread>();\n    LOG.debug(\"Begin parallel start\");\n\n    for(ServiceWithDependency sd : services.values()) {\n      // start the service. If this fails that service\n      // will be stopped and an exception raised\n      ServiceThread st = new ServiceThread(sd, services);\n      threads.add(st);\n    }\n\n    for(ServiceThread st : threads) {\n      st.start();\n    }\n    for(ServiceThread st : threads) {\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"Waiting for service thread to join for \" + st.getName());\n      }\n      st.join();\n      if(st.error != null && firstError == null) {\n        firstError = st.error;\n      }\n    }\n\n    if(firstError != null) {\n      throw ServiceStateException.convert(firstError);\n    }\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"End parallel start\");\n    }\n  } catch (InterruptedException e) {\n    e.printStackTrace();\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2427,
        "instruction": "@Override\npublic void start() throws Exception {\n    if (stopped.get() || !started.compareAndSet(false, true)) {\n        // lets just ignore redundant start() calls\n        // as its way too easy to not be completely sure if start() has been\n        // called or not with the gazillion of different configuration\n        // mechanisms\n        // throw new IllegalStateException(\"Already started.\");\n        return;\n    }\n\n    setStartException(null);\n    stopping.set(false);\n    startDate = new Date();\n    MDC.put(\"activemq.broker\", brokerName);\n\n    try {\n        checkMemorySystemUsageLimits();\n        if (systemExitOnShutdown && useShutdownHook) {\n            throw new ConfigurationException(\"'useShutdownHook' property cannot be be used with 'systemExitOnShutdown', please turn it off (useShutdownHook=false)\");\n        }\n        processHelperProperties();\n        if (isUseJmx()) {\n            // need to remove MDC during starting JMX, as that would otherwise causes leaks, as spawned threads inheirt the MDC and\n            // we cannot cleanup clear that during shutdown of the broker.\n            MDC.remove(\"activemq.broker\");\n            try {\n                startManagementContext();\n                for (NetworkConnector connector : getNetworkConnectors()) {\n                    registerNetworkConnectorMBean(connector);\n                }\n            } finally {\n                MDC.put(\"activemq.broker\", brokerName);\n            }\n        }\n\n        // in jvm master slave, lets not publish over existing broker till we get the lock\n        final BrokerRegistry brokerRegistry = BrokerRegistry.getInstance();\n        if (brokerRegistry.lookup(getBrokerName()) == null) {\n            brokerRegistry.bind(getBrokerName(), BrokerService.this);\n        }\n        startPersistenceAdapter(startAsync);\n        startBroker(startAsync);\n        brokerRegistry.bind(getBrokerName(), BrokerService.this);\n    } catch (Exception e) {\n        try {\n            if (!stopped.get()) {\n                stop();\n            }\n        } catch (Exception ex) {\n            LOG.warn(\"Failed to stop broker after failure in start. This exception will be ignored.\", ex);\n        }\n        throw e;\n    } finally {\n        MDC.remove(\"activemq.broker\");\n    }\n}",
        "accept_response": "@Override\npublic void start() throws Exception {\n    if (stopped.get() || !started.compareAndSet(false, true)) {\n        // lets just ignore redundant start() calls\n        // as its way too easy to not be completely sure if start() has been\n        // called or not with the gazillion of different configuration\n        // mechanisms\n        // throw new IllegalStateException(\"Already started.\");\n        return;\n    }\n\n    setStartException(null);\n    stopping.set(false);\n    startDate = new Date();\n    MDC.put(\"activemq.broker\", brokerName);\n\n    try {\n        checkMemorySystemUsageLimits();\n        if (systemExitOnShutdown && useShutdownHook) {\n            throw new ConfigurationException(\"'useShutdownHook' property cannot be be used with 'systemExitOnShutdown', please turn it off (useShutdownHook=false)\");\n        }\n        processHelperProperties();\n        if (isUseJmx()) {\n            // need to remove MDC during starting JMX, as that would otherwise causes leaks, as spawned threads inheirt the MDC and\n            // we cannot cleanup clear that during shutdown of the broker.\n            MDC.remove(\"activemq.broker\");\n            try {\n                startManagementContext();\n                for (NetworkConnector connector : getNetworkConnectors()) {\n                    registerNetworkConnectorMBean(connector);\n                }\n            } finally {\n                MDC.put(\"activemq.broker\", brokerName);\n            }\n        }\n\n        // in jvm master slave, lets not publish over existing broker till we get the lock\n        final BrokerRegistry brokerRegistry = BrokerRegistry.getInstance();\n        if (brokerRegistry.lookup(getBrokerName()) == null) {\n            brokerRegistry.bind(getBrokerName(), BrokerService.this);\n        }\n        startPersistenceAdapter(startAsync);\n        startBroker(startAsync);\n        brokerRegistry.bind(getBrokerName(), BrokerService.this);\n    } catch (Exception e) {\n        LOG.error(\"Failed to start Apache ActiveMQ ({}, {})\", getBrokerName(), brokerId, e);\n        try {\n            if (!stopped.get()) {\n                stop();\n            }\n        } catch (Exception ex) {\n            LOG.warn(\"Failed to stop broker after failure in start. This exception will be ignored.\", ex);\n        }\n        throw e;\n    } finally {\n        MDC.remove(\"activemq.broker\");\n    }\n}",
        "reject_response": "@Override\npublic void start() throws Exception {\n    if (stopped.get() || !started.compareAndSet(false, true)) {\n        // lets just ignore redundant start() calls\n        // as its way too easy to not be completely sure if start() has been\n        // called or not with the gazillion of different configuration\n        // mechanisms\n        // throw new IllegalStateException(\"Already started.\");\n        return;\n    }\n\n    setStartException(null);\n    stopping.set(false);\n    startDate = new Date();\n    MDC.put(\"activemq.broker\", brokerName);\n\n    try {\n        checkMemorySystemUsageLimits();\n        if (systemExitOnShutdown && useShutdownHook) {\n            throw new ConfigurationException(\"'useShutdownHook' property cannot be be used with 'systemExitOnShutdown', please turn it off (useShutdownHook=false)\");\n        }\n        processHelperProperties();\n        if (isUseJmx()) {\n            // need to remove MDC during starting JMX, as that would otherwise causes leaks, as spawned threads inheirt the MDC and\n            // we cannot cleanup clear that during shutdown of the broker.\n            MDC.remove(\"activemq.broker\");\n            try {\n                startManagementContext();\n                for (NetworkConnector connector : getNetworkConnectors()) {\n                    registerNetworkConnectorMBean(connector);\n                }\n            } finally {\n                MDC.put(\"activemq.broker\", brokerName);\n            }\n        }\n\n        // in jvm master slave, lets not publish over existing broker till we get the lock\n        final BrokerRegistry brokerRegistry = BrokerRegistry.getInstance();\n        if (brokerRegistry.lookup(getBrokerName()) == null) {\n            brokerRegistry.bind(getBrokerName(), BrokerService.this);\n        }\n        startPersistenceAdapter(startAsync);\n        startBroker(startAsync);\n        brokerRegistry.bind(getBrokerName(), BrokerService.this);\n    } catch (Exception e) {\n        LOG.error(\"Failed to start Apache ActiveMQ ({}, {})\", new Object[]{ getBrokerName(), brokerId }, e);\n        try {\n            if (!stopped.get()) {\n                stop();\n            }\n        } catch (Exception ex) {\n            LOG.warn(\"Failed to stop broker after failure in start. This exception will be ignored.\", ex);\n        }\n        throw e;\n    } finally {\n        MDC.remove(\"activemq.broker\");\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2469,
        "instruction": "@SuppressWarnings(\"squid:S2445\")\n@Override\nprotected void appendToLogTail(ILogRecord logRecord) {\n    syncAppendToLogTail(logRecord);\n\n    if (logRecord.isReplicate()) {\n        try {\n            replicationManager.replicate(logRecord);\n        } catch (InterruptedException e) {\n            Thread.currentThread().interrupt();\n            throw new ACIDException(e);\n        }\n    }\n\n    if (logRecord.getLogSource() == LogSource.LOCAL && waitForFlush(logRecord) && !logRecord.isFlushed()) {\n        InvokeUtil.doUninterruptibly(() -> {\n            synchronized (logRecord) {\n                while (!logRecord.isFlushed()) {\n                    logRecord.wait();\n                }\n                //wait for job Commit/Abort ACK from replicas\n                if (logRecord.isReplicate() && (logRecord.getLogType() == LogType.JOB_COMMIT\n                        || logRecord.getLogType() == LogType.ABORT)) {\n                    long replicationTimeOut = replicationTimeoutMillis;\n                    while (!logRecord.isReplicated()) {\n                        if (replicationTimeOut <= 0) {\n                            logRecord.setReplicated(true);\n                            continue;\n                        }\n                        final long startTime = System.nanoTime();\n                        logRecord.wait(replicationTimeOut);\n                        replicationTimeOut -= TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - startTime);\n                    }\n                }\n            }\n        });\n    }\n}",
        "accept_response": "@SuppressWarnings(\"squid:S2445\")\n@Override\nprotected void appendToLogTail(ILogRecord logRecord) {\n    syncAppendToLogTail(logRecord);\n\n    if (logRecord.isReplicate()) {\n        try {\n            replicationManager.replicate(logRecord);\n        } catch (InterruptedException e) {\n            Thread.currentThread().interrupt();\n            throw new ACIDException(e);\n        }\n    }\n\n    if (logRecord.getLogSource() == LogSource.LOCAL && waitForFlush(logRecord) && !logRecord.isFlushed()) {\n        InvokeUtil.doUninterruptibly(() -> {\n            synchronized (logRecord) {\n                while (!logRecord.isFlushed()) {\n                    logRecord.wait();\n                }\n                //wait for job Commit/Abort ACK from replicas\n                if (logRecord.isReplicate() && (logRecord.getLogType() == LogType.JOB_COMMIT\n                        || logRecord.getLogType() == LogType.ABORT)) {\n                    long replicationTimeOut = replicationTimeoutMillis;\n                    while (!logRecord.isReplicated()) {\n                        if (replicationTimeOut <= 0) {\n                            LOGGER.warn(\n                                    \"{} ms passed without receiving acks for log {}; setting log as replicated due to timeout\",\n                                    replicationTimeoutMillis, logRecord.getLogRecordForDisplay());\n                            logRecord.setReplicated(true);\n                            continue;\n                        }\n                        final long startTime = System.nanoTime();\n                        logRecord.wait(replicationTimeOut);\n                        replicationTimeOut -= TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - startTime);\n                    }\n                }\n            }\n        });\n    }\n}",
        "reject_response": "@SuppressWarnings(\"squid:S2445\")\n@Override\nprotected void appendToLogTail(ILogRecord logRecord) {\n    syncAppendToLogTail(logRecord);\n\n    if (logRecord.isReplicate()) {\n        try {\n            replicationManager.replicate(logRecord);\n        } catch (InterruptedException e) {\n            Thread.currentThread().interrupt();\n            throw new ACIDException(e);\n        }\n    }\n\n    if (logRecord.getLogSource() == LogSource.LOCAL && waitForFlush(logRecord) && !logRecord.isFlushed()) {\n        InvokeUtil.doUninterruptibly(() -> {\n            synchronized (logRecord) {\n                while (!logRecord.isFlushed()) {\n                    logRecord.wait();\n                }\n                //wait for job Commit/Abort ACK from replicas\n                if (logRecord.isReplicate() && (logRecord.getLogType() == LogType.JOB_COMMIT\n                        || logRecord.getLogType() == LogType.ABORT)) {\n                    long replicationTimeOut = replicationTimeoutMillis;\n                    while (!logRecord.isReplicated()) {\n                        if (replicationTimeOut <= 0) {\n                        logRecord.wait();\n                            logRecord.setReplicated(true);\n                            continue;\n                        }\n                        final long startTime = System.nanoTime();\n                        logRecord.wait(replicationTimeOut);\n                        replicationTimeOut -= TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - startTime);\n                    }\n                }\n            }\n        });\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3201,
        "instruction": "@PUT\n@Path(\"/entities\")\n@Consumes({ MediaType.APPLICATION_JSON /* , MediaType.APPLICATION_XML */})\npublic Response putEntities(\n    @Context HttpServletRequest req,\n    @Context HttpServletResponse res,\n    @QueryParam(\"async\") String async,\n    @QueryParam(\"subappwrite\") String isSubAppEntities,\n    @QueryParam(\"appid\") String appId,\n    TimelineEntities entities) {\n  init(res);\n  UserGroupInformation callerUgi = getUser(req);\n  boolean isAsync = async != null && async.trim().equalsIgnoreCase(\"true\");\n  if (callerUgi == null) {\n    String msg = \"The owner of the posted timeline entities is not set\";\n    LOG.error(msg);\n    throw new ForbiddenException(msg);\n  }\n\n  long startTime = Time.monotonicNow();\n  boolean succeeded = false;\n  try {\n    ApplicationId appID = parseApplicationId(appId);\n    if (appID == null) {\n      return Response.status(Response.Status.BAD_REQUEST).build();\n    }\n    NodeTimelineCollectorManager collectorManager =\n        (NodeTimelineCollectorManager) context.getAttribute(\n            NodeTimelineCollectorManager.COLLECTOR_MANAGER_ATTR_KEY);\n    TimelineCollector collector = collectorManager.get(appID);\n    if (collector == null) {\n      throw new NotFoundException(\"Application: \"+ appId + \" is not found\");\n    }\n\n    if (isAsync) {\n      collector.putEntitiesAsync(processTimelineEntities(entities, appId,\n          Boolean.valueOf(isSubAppEntities)), callerUgi);\n    } else {\n      collector.putEntities(processTimelineEntities(entities, appId,\n          Boolean.valueOf(isSubAppEntities)), callerUgi);\n    }\n\n    succeeded = true;\n    return Response.ok().build();\n  } catch (NotFoundException | ForbiddenException e) {\n    throw new WebApplicationException(e,\n        Response.Status.INTERNAL_SERVER_ERROR);\n  } catch (IOException e) {\n    LOG.error(\"Error putting entities\", e);\n    throw new WebApplicationException(e,\n        Response.Status.INTERNAL_SERVER_ERROR);\n  } catch (Exception e) {\n    LOG.error(\"Unexpected error while putting entities\", e);\n    throw new WebApplicationException(e,\n        Response.Status.INTERNAL_SERVER_ERROR);\n  } finally {\n    long latency = Time.monotonicNow() - startTime;\n    if (isAsync) {\n      METRICS.addAsyncPutEntitiesLatency(latency, succeeded);\n    } else {\n      METRICS.addPutEntitiesLatency(latency, succeeded);\n    }\n  }\n}",
        "accept_response": "@PUT\n@Path(\"/entities\")\n@Consumes({ MediaType.APPLICATION_JSON /* , MediaType.APPLICATION_XML */})\npublic Response putEntities(\n    @Context HttpServletRequest req,\n    @Context HttpServletResponse res,\n    @QueryParam(\"async\") String async,\n    @QueryParam(\"subappwrite\") String isSubAppEntities,\n    @QueryParam(\"appid\") String appId,\n    TimelineEntities entities) {\n  init(res);\n  UserGroupInformation callerUgi = getUser(req);\n  boolean isAsync = async != null && async.trim().equalsIgnoreCase(\"true\");\n  if (callerUgi == null) {\n    String msg = \"The owner of the posted timeline entities is not set\";\n    LOG.error(msg);\n    throw new ForbiddenException(msg);\n  }\n\n  long startTime = Time.monotonicNow();\n  boolean succeeded = false;\n  try {\n    ApplicationId appID = parseApplicationId(appId);\n    if (appID == null) {\n      return Response.status(Response.Status.BAD_REQUEST).build();\n    }\n    NodeTimelineCollectorManager collectorManager =\n        (NodeTimelineCollectorManager) context.getAttribute(\n            NodeTimelineCollectorManager.COLLECTOR_MANAGER_ATTR_KEY);\n    TimelineCollector collector = collectorManager.get(appID);\n    if (collector == null) {\n      LOG.error(\"Application: {} is not found\", appId);\n      throw new NotFoundException(\"Application: \"+ appId + \" is not found\");\n    }\n\n    if (isAsync) {\n      collector.putEntitiesAsync(processTimelineEntities(entities, appId,\n          Boolean.valueOf(isSubAppEntities)), callerUgi);\n    } else {\n      collector.putEntities(processTimelineEntities(entities, appId,\n          Boolean.valueOf(isSubAppEntities)), callerUgi);\n    }\n\n    succeeded = true;\n    return Response.ok().build();\n  } catch (NotFoundException | ForbiddenException e) {\n    throw new WebApplicationException(e,\n        Response.Status.INTERNAL_SERVER_ERROR);\n  } catch (IOException e) {\n    LOG.error(\"Error putting entities\", e);\n    throw new WebApplicationException(e,\n        Response.Status.INTERNAL_SERVER_ERROR);\n  } catch (Exception e) {\n    LOG.error(\"Unexpected error while putting entities\", e);\n    throw new WebApplicationException(e,\n        Response.Status.INTERNAL_SERVER_ERROR);\n  } finally {\n    long latency = Time.monotonicNow() - startTime;\n    if (isAsync) {\n      METRICS.addAsyncPutEntitiesLatency(latency, succeeded);\n    } else {\n      METRICS.addPutEntitiesLatency(latency, succeeded);\n    }\n  }\n}",
        "reject_response": "@PUT\n@Path(\"/entities\")\n@Consumes({ MediaType.APPLICATION_JSON /* , MediaType.APPLICATION_XML */})\npublic Response putEntities(\n    @Context HttpServletRequest req,\n    @Context HttpServletResponse res,\n    @QueryParam(\"async\") String async,\n    @QueryParam(\"subappwrite\") String isSubAppEntities,\n    @QueryParam(\"appid\") String appId,\n    TimelineEntities entities) {\n  init(res);\n  UserGroupInformation callerUgi = getUser(req);\n  boolean isAsync = async != null && async.trim().equalsIgnoreCase(\"true\");\n  if (callerUgi == null) {\n    String msg = \"The owner of the posted timeline entities is not set\";\n    LOG.error(msg);\n    throw new ForbiddenException(msg);\n  }\n\n  long startTime = Time.monotonicNow();\n  boolean succeeded = false;\n  try {\n    ApplicationId appID = parseApplicationId(appId);\n    if (appID == null) {\n      return Response.status(Response.Status.BAD_REQUEST).build();\n    }\n    NodeTimelineCollectorManager collectorManager =\n        (NodeTimelineCollectorManager) context.getAttribute(\n            NodeTimelineCollectorManager.COLLECTOR_MANAGER_ATTR_KEY);\n    TimelineCollector collector = collectorManager.get(appID);\n    if (collector == null) {\n      LOG.error(\"Application: \"+ appId + \" is not found\");\n      throw new NotFoundException(\"Application: \"+ appId + \" is not found\");\n    }\n\n    if (isAsync) {\n      collector.putEntitiesAsync(processTimelineEntities(entities, appId,\n          Boolean.valueOf(isSubAppEntities)), callerUgi);\n    } else {\n      collector.putEntities(processTimelineEntities(entities, appId,\n          Boolean.valueOf(isSubAppEntities)), callerUgi);\n    }\n\n    succeeded = true;\n    return Response.ok().build();\n  } catch (NotFoundException | ForbiddenException e) {\n    throw new WebApplicationException(e,\n        Response.Status.INTERNAL_SERVER_ERROR);\n  } catch (IOException e) {\n    LOG.error(\"Error putting entities\", e);\n    throw new WebApplicationException(e,\n        Response.Status.INTERNAL_SERVER_ERROR);\n  } catch (Exception e) {\n    LOG.error(\"Unexpected error while putting entities\", e);\n    throw new WebApplicationException(e,\n        Response.Status.INTERNAL_SERVER_ERROR);\n  } finally {\n    long latency = Time.monotonicNow() - startTime;\n    if (isAsync) {\n      METRICS.addAsyncPutEntitiesLatency(latency, succeeded);\n    } else {\n      METRICS.addPutEntitiesLatency(latency, succeeded);\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3099,
        "instruction": "private void checkRead() throws IOException {\n  if (current == totalCountLoadedSoFar) {\n    if (current != 0) {\n      long timeAssembling = System.currentTimeMillis() - startedAssemblingCurrentBlockAt;\n      totalTimeSpentProcessingRecords += timeAssembling;\n      long totalTime = totalTimeSpentProcessingRecords + totalTimeSpentReadingBytes;\n      long percentReading = 100 * totalTimeSpentReadingBytes / totalTime;\n      long percentProcessing = 100 * totalTimeSpentProcessingRecords / totalTime;\n      if (DEBUG) LOG.debug(\"time spent so far \" + percentReading + \"% reading (\"+totalTimeSpentReadingBytes+\" ms) and \" + percentProcessing + \"% processing (\"+totalTimeSpentProcessingRecords+\" ms)\");\n    }\n\n    if (DEBUG) LOG.debug(\"at row \" + current + \". reading next block\");\n    long t0 = System.currentTimeMillis();\n    PageReadStore pages = reader.readNextRowGroup();\n    if (pages == null) {\n      throw new IOException(\"expecting more rows but reached last block. Read \" + current + \" out of \" + total);\n    }\n    long timeSpentReading = System.currentTimeMillis() - t0;\n    totalTimeSpentReadingBytes += timeSpentReading;\n    BenchmarkCounter.incrementTime(timeSpentReading);\n    if (DEBUG) {\n      LOG.debug(\"block read in memory in \" + timeSpentReading + \" ms. row count = \" + pages.getRowCount());\n      LOG.debug(\"initializing Record assembly with requested schema \" + requestedSchema);\n    }\n    MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema);\n    recordReader = columnIO.getRecordReader(pages, recordConverter, recordFilter);\n    startedAssemblingCurrentBlockAt = System.currentTimeMillis();\n    totalCountLoadedSoFar += pages.getRowCount();\n    ++ currentBlock;\n  }\n}",
        "accept_response": "private void checkRead() throws IOException {\n  if (current == totalCountLoadedSoFar) {\n    if (current != 0) {\n      long timeAssembling = System.currentTimeMillis() - startedAssemblingCurrentBlockAt;\n      totalTimeSpentProcessingRecords += timeAssembling;\n      if (DEBUG) LOG.debug(\"Assembled and processed \" + totalCountLoadedSoFar + \" records from \" + columnCount + \" columns in \" + totalTimeSpentProcessingRecords + \" ms: \" + ((float) totalCountLoadedSoFar / totalTimeSpentProcessingRecords) + \" rec/ms, \" + ((float) totalCountLoadedSoFar * columnCount / totalTimeSpentProcessingRecords) + \" cell/ms\");\n      long totalTime = totalTimeSpentProcessingRecords + totalTimeSpentReadingBytes;\n      long percentReading = 100 * totalTimeSpentReadingBytes / totalTime;\n      long percentProcessing = 100 * totalTimeSpentProcessingRecords / totalTime;\n      if (DEBUG) LOG.debug(\"time spent so far \" + percentReading + \"% reading (\"+totalTimeSpentReadingBytes+\" ms) and \" + percentProcessing + \"% processing (\"+totalTimeSpentProcessingRecords+\" ms)\");\n    }\n\n    if (DEBUG) LOG.debug(\"at row \" + current + \". reading next block\");\n    long t0 = System.currentTimeMillis();\n    PageReadStore pages = reader.readNextRowGroup();\n    if (pages == null) {\n      throw new IOException(\"expecting more rows but reached last block. Read \" + current + \" out of \" + total);\n    }\n    long timeSpentReading = System.currentTimeMillis() - t0;\n    totalTimeSpentReadingBytes += timeSpentReading;\n    BenchmarkCounter.incrementTime(timeSpentReading);\n    if (DEBUG) {\n      LOG.debug(\"block read in memory in \" + timeSpentReading + \" ms. row count = \" + pages.getRowCount());\n      LOG.debug(\"initializing Record assembly with requested schema \" + requestedSchema);\n    }\n    MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema);\n    recordReader = columnIO.getRecordReader(pages, recordConverter, recordFilter);\n    startedAssemblingCurrentBlockAt = System.currentTimeMillis();\n    totalCountLoadedSoFar += pages.getRowCount();\n    ++ currentBlock;\n  }\n}",
        "reject_response": "private void checkRead() throws IOException {\n  if (current == totalCountLoadedSoFar) {\n    if (current != 0) {\n      long timeAssembling = System.currentTimeMillis() - startedAssemblingCurrentBlockAt;\n      totalTimeSpentProcessingRecords += timeAssembling;\n      LOG.info(\"Assembled and processed \" + totalCountLoadedSoFar + \" records from \" + columnCount + \" columns in \" + totalTimeSpentProcessingRecords + \" ms: \"+((float)totalCountLoadedSoFar / totalTimeSpentProcessingRecords) + \" rec/ms, \" + ((float)totalCountLoadedSoFar * columnCount / totalTimeSpentProcessingRecords) + \" cell/ms\");\n      long totalTime = totalTimeSpentProcessingRecords + totalTimeSpentReadingBytes;\n      long percentReading = 100 * totalTimeSpentReadingBytes / totalTime;\n      long percentProcessing = 100 * totalTimeSpentProcessingRecords / totalTime;\n      if (DEBUG) LOG.debug(\"time spent so far \" + percentReading + \"% reading (\"+totalTimeSpentReadingBytes+\" ms) and \" + percentProcessing + \"% processing (\"+totalTimeSpentProcessingRecords+\" ms)\");\n    }\n\n    if (DEBUG) LOG.debug(\"at row \" + current + \". reading next block\");\n    long t0 = System.currentTimeMillis();\n    PageReadStore pages = reader.readNextRowGroup();\n    if (pages == null) {\n      throw new IOException(\"expecting more rows but reached last block. Read \" + current + \" out of \" + total);\n    }\n    long timeSpentReading = System.currentTimeMillis() - t0;\n    totalTimeSpentReadingBytes += timeSpentReading;\n    BenchmarkCounter.incrementTime(timeSpentReading);\n    if (DEBUG) {\n      LOG.debug(\"block read in memory in \" + timeSpentReading + \" ms. row count = \" + pages.getRowCount());\n      LOG.debug(\"initializing Record assembly with requested schema \" + requestedSchema);\n    }\n    MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema);\n    recordReader = columnIO.getRecordReader(pages, recordConverter, recordFilter);\n    startedAssemblingCurrentBlockAt = System.currentTimeMillis();\n    totalCountLoadedSoFar += pages.getRowCount();\n    ++ currentBlock;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2807,
        "instruction": "@SuppressWarnings({\"unchecked\", \"NonPrivateFieldAccessedInSynchronizedContext\"})\npublic void onPage(@Nullable UUID nodeId, @Nullable Collection<?> data, @Nullable Throwable err, boolean finished) {\n    if (isCancelled())\n        return;\n\n    if (log.isDebugEnabled())\n\n    try {\n        if (err != null)\n            synchronized (this) {\n                enqueue(Collections.emptyList());\n\n                onDone(new IgniteCheckedException(nodeId != null ?\n                    S.toString(\"Failed to execute query on node\",\n                        \"query\", qry, true,\n                        \"nodeId\", nodeId, false) :\n                    S.toString(\"Failed to execute query locally\",\n                        \"query\", qry, true),\n                    err));\n\n                onPage(nodeId, true);\n\n                notifyAll();\n            }\n        else {\n            if (data == null)\n                data = Collections.emptyList();\n\n            data = dedupIfRequired((Collection<Object>)data);\n\n            data = cctx.unwrapBinariesIfNeeded((Collection<Object>)data, qry.query().keepBinary());\n\n            synchronized (this) {\n                enqueue(data);\n\n                if (qry.query().keepAll())\n                    allCol.addAll(maskNulls((Collection<Object>)data));\n\n                if (onPage(nodeId, finished)) {\n                    onDone((Collection<R>)(qry.query().keepAll() ? unmaskNulls(allCol) : data));\n\n                    clear();\n                }\n\n                notifyAll();\n            }\n        }\n    }\n    catch (Throwable e) {\n        onPageError(nodeId, e);\n\n        if (e instanceof Error)\n            throw (Error)e;\n    }\n}",
        "accept_response": "@SuppressWarnings({\"unchecked\", \"NonPrivateFieldAccessedInSynchronizedContext\"})\npublic void onPage(@Nullable UUID nodeId, @Nullable Collection<?> data, @Nullable Throwable err, boolean finished) {\n    if (isCancelled())\n        return;\n\n    if (log.isDebugEnabled())\n        log.debug(S.toString(\"Received query result page\",\n            \"nodeId\", nodeId, false,\n            \"data\", data, true,\n            \"err\", err, false,\n            \"finished\", finished, false));\n\n    try {\n        if (err != null)\n            synchronized (this) {\n                enqueue(Collections.emptyList());\n\n                onDone(new IgniteCheckedException(nodeId != null ?\n                    S.toString(\"Failed to execute query on node\",\n                        \"query\", qry, true,\n                        \"nodeId\", nodeId, false) :\n                    S.toString(\"Failed to execute query locally\",\n                        \"query\", qry, true),\n                    err));\n\n                onPage(nodeId, true);\n\n                notifyAll();\n            }\n        else {\n            if (data == null)\n                data = Collections.emptyList();\n\n            data = dedupIfRequired((Collection<Object>)data);\n\n            data = cctx.unwrapBinariesIfNeeded((Collection<Object>)data, qry.query().keepBinary());\n\n            synchronized (this) {\n                enqueue(data);\n\n                if (qry.query().keepAll())\n                    allCol.addAll(maskNulls((Collection<Object>)data));\n\n                if (onPage(nodeId, finished)) {\n                    onDone((Collection<R>)(qry.query().keepAll() ? unmaskNulls(allCol) : data));\n\n                    clear();\n                }\n\n                notifyAll();\n            }\n        }\n    }\n    catch (Throwable e) {\n        onPageError(nodeId, e);\n\n        if (e instanceof Error)\n            throw (Error)e;\n    }\n}",
        "reject_response": "@SuppressWarnings({\"unchecked\", \"NonPrivateFieldAccessedInSynchronizedContext\"})\npublic void onPage(@Nullable UUID nodeId, @Nullable Collection<?> data, @Nullable Throwable err, boolean finished) {\n    if (isCancelled())\n        return;\n\n    if (log.isDebugEnabled())\n        log.debug(\"Received query result page [nodeId=\" + nodeId + \", data=\" + data +\n            \", err=\" + err + \", finished=\" + finished + \"]\");\n\n    try {\n        if (err != null)\n            synchronized (this) {\n                enqueue(Collections.emptyList());\n\n                onDone(new IgniteCheckedException(nodeId != null ?\n                    S.toString(\"Failed to execute query on node\",\n                        \"query\", qry, true,\n                        \"nodeId\", nodeId, false) :\n                    S.toString(\"Failed to execute query locally\",\n                        \"query\", qry, true),\n                    err));\n\n                onPage(nodeId, true);\n\n                notifyAll();\n            }\n        else {\n            if (data == null)\n                data = Collections.emptyList();\n\n            data = dedupIfRequired((Collection<Object>)data);\n\n            data = cctx.unwrapBinariesIfNeeded((Collection<Object>)data, qry.query().keepBinary());\n\n            synchronized (this) {\n                enqueue(data);\n\n                if (qry.query().keepAll())\n                    allCol.addAll(maskNulls((Collection<Object>)data));\n\n                if (onPage(nodeId, finished)) {\n                    onDone((Collection<R>)(qry.query().keepAll() ? unmaskNulls(allCol) : data));\n\n                    clear();\n                }\n\n                notifyAll();\n            }\n        }\n    }\n    catch (Throwable e) {\n        onPageError(nodeId, e);\n\n        if (e instanceof Error)\n            throw (Error)e;\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2744,
        "instruction": "private void removeDatanode(DatanodeDescriptor nodeInfo,\n    boolean removeBlocksFromBlocksMap) {\n  assert namesystem.hasWriteLock();\n  heartbeatManager.removeDatanode(nodeInfo);\n  if (removeBlocksFromBlocksMap) {\n    blockManager.removeBlocksAssociatedTo(nodeInfo);\n  }\n  networktopology.remove(nodeInfo);\n  decrementVersionCount(nodeInfo.getSoftwareVersion());\n  blockManager.getBlockReportLeaseManager().unregister(nodeInfo);\n\n  blockManager.checkSafeMode();\n}",
        "accept_response": "private void removeDatanode(DatanodeDescriptor nodeInfo,\n    boolean removeBlocksFromBlocksMap) {\n  assert namesystem.hasWriteLock();\n  heartbeatManager.removeDatanode(nodeInfo);\n  if (removeBlocksFromBlocksMap) {\n    blockManager.removeBlocksAssociatedTo(nodeInfo);\n  }\n  networktopology.remove(nodeInfo);\n  decrementVersionCount(nodeInfo.getSoftwareVersion());\n  blockManager.getBlockReportLeaseManager().unregister(nodeInfo);\n\n  LOG.debug(\"remove datanode {}.\", nodeInfo);\n  blockManager.checkSafeMode();\n}",
        "reject_response": "private void removeDatanode(DatanodeDescriptor nodeInfo,\n    boolean removeBlocksFromBlocksMap) {\n  assert namesystem.hasWriteLock();\n  heartbeatManager.removeDatanode(nodeInfo);\n  if (removeBlocksFromBlocksMap) {\n    blockManager.removeBlocksAssociatedTo(nodeInfo);\n  }\n  networktopology.remove(nodeInfo);\n  decrementVersionCount(nodeInfo.getSoftwareVersion());\n  blockManager.getBlockReportLeaseManager().unregister(nodeInfo);\n\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"remove datanode \" + nodeInfo);\n  }\n  blockManager.checkSafeMode();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3024,
        "instruction": "private void printLoadDataWhenFirstBoot(final TopicConfigSerializeWrapper tcs) {\n    Iterator<Entry<String, TopicConfig>> it = tcs.getTopicConfigTable().entrySet().iterator();\n    while (it.hasNext()) {\n        Entry<String, TopicConfig> next = it.next();\n    }\n}",
        "accept_response": "private void printLoadDataWhenFirstBoot(final TopicConfigSerializeWrapper tcs) {\n    Iterator<Entry<String, TopicConfig>> it = tcs.getTopicConfigTable().entrySet().iterator();\n    while (it.hasNext()) {\n        Entry<String, TopicConfig> next = it.next();\n        log.info(\"load exist local topic, {}\", next.getValue().toString());\n    }\n}",
        "reject_response": "private void printLoadDataWhenFirstBoot(final TopicConfigSerializeWrapper tcs) {\n    Iterator<Entry<String, TopicConfig>> it = tcs.getTopicConfigTable().entrySet().iterator();\n    while (it.hasNext()) {\n        Entry<String, TopicConfig> next = it.next();\n        LOG.info(\"load exist local topic, {}\", next.getValue().toString());\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2496,
        "instruction": "@Override\npublic final T get() {\n    if (log.isTraceEnabled())\n\n    ExecutionContext exec = BrooklynTaskTags.getCurrentExecutionContext();\n    if (exec == null) {\n        throw new IllegalStateException(\"No execution context available to resolve \" + dsl);\n    }\n\n    Task<T> task = newTask();\n    T result;\n    try {\n        result = exec.submit(task).get();\n    } catch (InterruptedException | ExecutionException e) {\n        Task<?> currentTask = Tasks.current();\n        if (currentTask != null && currentTask.isCancelled()) {\n            task.cancel(true);\n        }\n        throw Exceptions.propagate(e);\n    }\n\n    if (log.isDebugEnabled()) {\n        // https://issues.apache.org/jira/browse/BROOKLYN-269\n        // We must not log sensitve data, such as from $brooklyn:external, or if the value\n        // is to be used as a password etc. Unfortunately we don't know the context, so can't\n        // use Sanitizer.sanitize.\n        log.debug(\"Resolved \"+dsl);\n    }\n    return result;\n}",
        "accept_response": "@Override\npublic final T get() {\n    if (log.isTraceEnabled())\n        log.trace(\"Queuing task to resolve {}, called by {}\", dsl, Tasks.current());\n\n    ExecutionContext exec = BrooklynTaskTags.getCurrentExecutionContext();\n    if (exec == null) {\n        throw new IllegalStateException(\"No execution context available to resolve \" + dsl);\n    }\n\n    Task<T> task = newTask();\n    T result;\n    try {\n        result = exec.submit(task).get();\n    } catch (InterruptedException | ExecutionException e) {\n        Task<?> currentTask = Tasks.current();\n        if (currentTask != null && currentTask.isCancelled()) {\n            task.cancel(true);\n        }\n        throw Exceptions.propagate(e);\n    }\n\n    if (log.isDebugEnabled()) {\n        // https://issues.apache.org/jira/browse/BROOKLYN-269\n        // We must not log sensitve data, such as from $brooklyn:external, or if the value\n        // is to be used as a password etc. Unfortunately we don't know the context, so can't\n        // use Sanitizer.sanitize.\n        log.debug(\"Resolved \"+dsl);\n    }\n    return result;\n}",
        "reject_response": "@Override\npublic final T get() {\n    if (log.isTraceEnabled())\n    if (log.isDebugEnabled())\n        log.debug(\"Queuing task to resolve \"+dsl+\", called by \"+Tasks.current());\n\n    ExecutionContext exec = BrooklynTaskTags.getCurrentExecutionContext();\n    if (exec == null) {\n        throw new IllegalStateException(\"No execution context available to resolve \" + dsl);\n    }\n\n    Task<T> task = newTask();\n    T result;\n    try {\n        result = exec.submit(task).get();\n    } catch (InterruptedException | ExecutionException e) {\n        Task<?> currentTask = Tasks.current();\n        if (currentTask != null && currentTask.isCancelled()) {\n            task.cancel(true);\n        }\n        throw Exceptions.propagate(e);\n    }\n\n    if (log.isDebugEnabled()) {\n        // https://issues.apache.org/jira/browse/BROOKLYN-269\n        // We must not log sensitve data, such as from $brooklyn:external, or if the value\n        // is to be used as a password etc. Unfortunately we don't know the context, so can't\n        // use Sanitizer.sanitize.\n        log.debug(\"Resolved \"+dsl);\n    }\n    return result;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3071,
        "instruction": "protected void handleMessage(\n    BlockTransferMessage msgObj,\n    TransportClient client,\n    RpcResponseCallback callback) {\n  if (msgObj instanceof OpenBlocks) {\n    final Timer.Context responseDelayContext = metrics.openBlockRequestLatencyMillis.time();\n    try {\n      OpenBlocks msg = (OpenBlocks) msgObj;\n      checkAuth(client, msg.appId);\n\n      List<ManagedBuffer> blocks = Lists.newArrayList();\n      long totalBlockSize = 0;\n      for (String blockId : msg.blockIds) {\n        final ManagedBuffer block = blockManager.getBlockData(msg.appId, msg.execId, blockId);\n        totalBlockSize += block != null ? block.size() : 0;\n        blocks.add(block);\n      }\n      long streamId = streamManager.registerStream(client.getClientId(), blocks.iterator());\n      callback.onSuccess(new StreamHandle(streamId, msg.blockIds.length).toByteBuffer());\n      metrics.blockTransferRateBytes.mark(totalBlockSize);\n    } finally {\n      responseDelayContext.stop();\n    }\n\n  } else if (msgObj instanceof RegisterExecutor) {\n    final Timer.Context responseDelayContext =\n      metrics.registerExecutorRequestLatencyMillis.time();\n    try {\n      RegisterExecutor msg = (RegisterExecutor) msgObj;\n      checkAuth(client, msg.appId);\n      blockManager.registerExecutor(msg.appId, msg.execId, msg.executorInfo);\n      callback.onSuccess(ByteBuffer.wrap(new byte[0]));\n    } finally {\n      responseDelayContext.stop();\n    }\n\n  } else {\n    throw new UnsupportedOperationException(\"Unexpected message: \" + msgObj);\n  }\n}",
        "accept_response": "protected void handleMessage(\n    BlockTransferMessage msgObj,\n    TransportClient client,\n    RpcResponseCallback callback) {\n  if (msgObj instanceof OpenBlocks) {\n    final Timer.Context responseDelayContext = metrics.openBlockRequestLatencyMillis.time();\n    try {\n      OpenBlocks msg = (OpenBlocks) msgObj;\n      checkAuth(client, msg.appId);\n\n      List<ManagedBuffer> blocks = Lists.newArrayList();\n      long totalBlockSize = 0;\n      for (String blockId : msg.blockIds) {\n        final ManagedBuffer block = blockManager.getBlockData(msg.appId, msg.execId, blockId);\n        totalBlockSize += block != null ? block.size() : 0;\n        blocks.add(block);\n      }\n      long streamId = streamManager.registerStream(client.getClientId(), blocks.iterator());\n      if (logger.isTraceEnabled()) {\n        logger.trace(\"Registered streamId {} with {} buffers for client {} from host {}\",\n                     streamId,\n                     msg.blockIds.length,\n                     client.getClientId(),\n                     getRemoteAddress(client.getChannel()));\n      }\n      callback.onSuccess(new StreamHandle(streamId, msg.blockIds.length).toByteBuffer());\n      metrics.blockTransferRateBytes.mark(totalBlockSize);\n    } finally {\n      responseDelayContext.stop();\n    }\n\n  } else if (msgObj instanceof RegisterExecutor) {\n    final Timer.Context responseDelayContext =\n      metrics.registerExecutorRequestLatencyMillis.time();\n    try {\n      RegisterExecutor msg = (RegisterExecutor) msgObj;\n      checkAuth(client, msg.appId);\n      blockManager.registerExecutor(msg.appId, msg.execId, msg.executorInfo);\n      callback.onSuccess(ByteBuffer.wrap(new byte[0]));\n    } finally {\n      responseDelayContext.stop();\n    }\n\n  } else {\n    throw new UnsupportedOperationException(\"Unexpected message: \" + msgObj);\n  }\n}",
        "reject_response": "protected void handleMessage(\n    BlockTransferMessage msgObj,\n    TransportClient client,\n    RpcResponseCallback callback) {\n  if (msgObj instanceof OpenBlocks) {\n    final Timer.Context responseDelayContext = metrics.openBlockRequestLatencyMillis.time();\n    try {\n      OpenBlocks msg = (OpenBlocks) msgObj;\n      checkAuth(client, msg.appId);\n\n      List<ManagedBuffer> blocks = Lists.newArrayList();\n      long totalBlockSize = 0;\n      for (String blockId : msg.blockIds) {\n        final ManagedBuffer block = blockManager.getBlockData(msg.appId, msg.execId, blockId);\n        totalBlockSize += block != null ? block.size() : 0;\n        blocks.add(block);\n      }\n      long streamId = streamManager.registerStream(client.getClientId(), blocks.iterator());\n      if (logger.isTraceEnabled()) {\n      logger.trace(\"Registered streamId {} with {} buffers for client {} from host {}\",\n                   streamId,\n                   msg.blockIds.length,\n                   client.getClientId(),\n                   NettyUtils.getRemoteAddress(client.getChannel()));\n      }\n      callback.onSuccess(new StreamHandle(streamId, msg.blockIds.length).toByteBuffer());\n      metrics.blockTransferRateBytes.mark(totalBlockSize);\n    } finally {\n      responseDelayContext.stop();\n    }\n\n  } else if (msgObj instanceof RegisterExecutor) {\n    final Timer.Context responseDelayContext =\n      metrics.registerExecutorRequestLatencyMillis.time();\n    try {\n      RegisterExecutor msg = (RegisterExecutor) msgObj;\n      checkAuth(client, msg.appId);\n      blockManager.registerExecutor(msg.appId, msg.execId, msg.executorInfo);\n      callback.onSuccess(ByteBuffer.wrap(new byte[0]));\n    } finally {\n      responseDelayContext.stop();\n    }\n\n  } else {\n    throw new UnsupportedOperationException(\"Unexpected message: \" + msgObj);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2535,
        "instruction": "public void onIRStateChange(LocalSession session)\n{\n    // we should only be registered as listeners for PreviewKind.REPAIRED, but double check here\n    if (previewKind == PreviewKind.REPAIRED &&\n        session.getState() == ConsistentSession.State.FINALIZED &&\n        includesTables(session.tableIds))\n    {\n        for (Range<Token> range : session.ranges)\n        {\n            if (range.intersects(ranges()))\n            {\n                forceShutdown(RepairException.warn(\"An incremental repair with session id \"+session.sessionID+\" finished during this preview repair runtime\"));\n                return;\n            }\n        }\n    }\n}",
        "accept_response": "public void onIRStateChange(LocalSession session)\n{\n    // we should only be registered as listeners for PreviewKind.REPAIRED, but double check here\n    if (previewKind == PreviewKind.REPAIRED &&\n        session.getState() == ConsistentSession.State.FINALIZED &&\n        includesTables(session.tableIds))\n    {\n        for (Range<Token> range : session.ranges)\n        {\n            if (range.intersects(ranges()))\n            {\n                logger.warn(\"{} An intersecting incremental repair with session id = {} finished, preview repair might not be accurate\", previewKind.logPrefix(getId()), session.sessionID);\n                forceShutdown(RepairException.warn(\"An incremental repair with session id \"+session.sessionID+\" finished during this preview repair runtime\"));\n                return;\n            }\n        }\n    }\n}",
        "reject_response": "public void onIRStateChange(LocalSession session)\n{\n    // we should only be registered as listeners for PreviewKind.REPAIRED, but double check here\n    if (previewKind == PreviewKind.REPAIRED &&\n        session.getState() == ConsistentSession.State.FINALIZED &&\n        includesTables(session.tableIds))\n    {\n        for (Range<Token> range : session.ranges)\n        {\n            if (range.intersects(ranges()))\n            {\n                logger.error(\"{} An intersecting incremental repair with session id = {} finished, preview repair might not be accurate\", previewKind.logPrefix(getId()), session.sessionID);\n                forceShutdown(new Exception(\"An incremental repair with session id \"+session.sessionID+\" finished during this preview repair runtime\"));\n                forceShutdown(RepairException.warn(\"An incremental repair with session id \"+session.sessionID+\" finished during this preview repair runtime\"));\n                return;\n            }\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3205,
        "instruction": "private synchronized void writeInternal(String clusterId, String userId,\n                                        String flowName, String flowVersion,\n                                        long flowRun, String appId,\n                                        TimelineEntity entity,\n                                        TimelineWriteResponse response)\n                                        throws IOException {\n  String entityTypePathStr = clusterId + File.separator + userId +\n      File.separator + escape(flowName) + File.separator +\n      escape(flowVersion) + File.separator + flowRun + File.separator + appId\n      + File.separator + entity.getType();\n  Path entityTypePath = new Path(entitiesPath, entityTypePathStr);\n  try {\n    mkdirs(entityTypePath);\n    Path filePath =\n            new Path(entityTypePath,\n                    entity.getId() + TIMELINE_SERVICE_STORAGE_EXTENSION);\n    createFileWithRetries(filePath);\n\n    byte[] record =  new StringBuilder()\n            .append(TimelineUtils.dumpTimelineRecordtoJSON(entity))\n            .append(\"\\n\").toString().getBytes(\"UTF-8\");\n    writeFileWithRetries(filePath, record);\n  } catch (Exception ioe) {\n    TimelineWriteError error = createTimelineWriteError(entity);\n    /*\n     * TODO: set an appropriate error code after PoC could possibly be:\n     * error.setErrorCode(TimelineWriteError.IO_EXCEPTION);\n     */\n    response.addError(error);\n  }\n}",
        "accept_response": "private synchronized void writeInternal(String clusterId, String userId,\n                                        String flowName, String flowVersion,\n                                        long flowRun, String appId,\n                                        TimelineEntity entity,\n                                        TimelineWriteResponse response)\n                                        throws IOException {\n  String entityTypePathStr = clusterId + File.separator + userId +\n      File.separator + escape(flowName) + File.separator +\n      escape(flowVersion) + File.separator + flowRun + File.separator + appId\n      + File.separator + entity.getType();\n  Path entityTypePath = new Path(entitiesPath, entityTypePathStr);\n  try {\n    mkdirs(entityTypePath);\n    Path filePath =\n            new Path(entityTypePath,\n                    entity.getId() + TIMELINE_SERVICE_STORAGE_EXTENSION);\n    createFileWithRetries(filePath);\n\n    byte[] record =  new StringBuilder()\n            .append(TimelineUtils.dumpTimelineRecordtoJSON(entity))\n            .append(\"\\n\").toString().getBytes(\"UTF-8\");\n    writeFileWithRetries(filePath, record);\n  } catch (Exception ioe) {\n    LOG.warn(\"Interrupted operation:{}\", ioe.getMessage());\n    TimelineWriteError error = createTimelineWriteError(entity);\n    /*\n     * TODO: set an appropriate error code after PoC could possibly be:\n     * error.setErrorCode(TimelineWriteError.IO_EXCEPTION);\n     */\n    response.addError(error);\n  }\n}",
        "reject_response": "private synchronized void writeInternal(String clusterId, String userId,\n                                        String flowName, String flowVersion,\n                                        long flowRun, String appId,\n                                        TimelineEntity entity,\n                                        TimelineWriteResponse response)\n                                        throws IOException {\n  String entityTypePathStr = clusterId + File.separator + userId +\n      File.separator + escape(flowName) + File.separator +\n      escape(flowVersion) + File.separator + flowRun + File.separator + appId\n      + File.separator + entity.getType();\n  Path entityTypePath = new Path(entitiesPath, entityTypePathStr);\n  try {\n    mkdirs(entityTypePath);\n    Path filePath =\n            new Path(entityTypePath,\n                    entity.getId() + TIMELINE_SERVICE_STORAGE_EXTENSION);\n    createFileWithRetries(filePath);\n\n    byte[] record =  new StringBuilder()\n            .append(TimelineUtils.dumpTimelineRecordtoJSON(entity))\n            .append(\"\\n\").toString().getBytes(\"UTF-8\");\n    writeFileWithRetries(filePath, record);\n  } catch (Exception ioe) {\n    LOG.warn(\"Interrupted operation:\" + ioe.getMessage());\n    TimelineWriteError error = createTimelineWriteError(entity);\n    /*\n     * TODO: set an appropriate error code after PoC could possibly be:\n     * error.setErrorCode(TimelineWriteError.IO_EXCEPTION);\n     */\n    response.addError(error);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2875,
        "instruction": "private void _parse(UpdateSink sink, Reader r)\n{\n    ARQParser parser = null ;\n    try {\n        parser = new ARQParser(r) ;\n        parser.setUpdateSink(sink) ;\n        parser.UpdateUnit() ;\n    }\n    catch (org.apache.jena.sparql.lang.arq.ParseException ex)\n    {\n        throw new QueryParseException(ex.getMessage(),\n                                      ex.currentToken.beginLine,\n                                      ex.currentToken.beginColumn\n        ) ; }\n    catch (org.apache.jena.sparql.lang.arq.TokenMgrError tErr)\n    {\n        // Last valid token : not the same as token error message - but this should not happen\n        int col = parser.token.endColumn ;\n        int line = parser.token.endLine ;\n        throw new QueryParseException(tErr.getMessage(), line, col) ; }\n\n    catch (QueryException ex) { throw ex ; }\n    catch (JenaException ex)  { throw new QueryException(ex.getMessage(), ex) ; }\n    catch (Error err)\n    {\n        // The token stream can throw errors.\n        throw new QueryParseException(err.getMessage(), err, -1, -1) ;\n    }\n    catch (Throwable th)\n    {\n        throw new QueryException(th.getMessage(), th) ;\n    }\n}",
        "accept_response": "private void _parse(UpdateSink sink, Reader r)\n{\n    ARQParser parser = null ;\n    try {\n        parser = new ARQParser(r) ;\n        parser.setUpdateSink(sink) ;\n        parser.UpdateUnit() ;\n    }\n    catch (org.apache.jena.sparql.lang.arq.ParseException ex)\n    {\n        throw new QueryParseException(ex.getMessage(),\n                                      ex.currentToken.beginLine,\n                                      ex.currentToken.beginColumn\n        ) ; }\n    catch (org.apache.jena.sparql.lang.arq.TokenMgrError tErr)\n    {\n        // Last valid token : not the same as token error message - but this should not happen\n        int col = parser.token.endColumn ;\n        int line = parser.token.endLine ;\n        throw new QueryParseException(tErr.getMessage(), line, col) ; }\n\n    catch (QueryException ex) { throw ex ; }\n    catch (JenaException ex)  { throw new QueryException(ex.getMessage(), ex) ; }\n    catch (Error err)\n    {\n        // The token stream can throw errors.\n        throw new QueryParseException(err.getMessage(), err, -1, -1) ;\n    }\n    catch (Throwable th)\n    {\n        Log.error(this, \"Unexpected throwable: \",th) ;\n        throw new QueryException(th.getMessage(), th) ;\n    }\n}",
        "reject_response": "private void _parse(UpdateSink sink, Reader r)\n{\n    ARQParser parser = null ;\n    try {\n        parser = new ARQParser(r) ;\n        parser.setUpdateSink(sink) ;\n        parser.UpdateUnit() ;\n    }\n    catch (org.apache.jena.sparql.lang.arq.ParseException ex)\n    {\n        throw new QueryParseException(ex.getMessage(),\n                                      ex.currentToken.beginLine,\n                                      ex.currentToken.beginColumn\n        ) ; }\n    catch (org.apache.jena.sparql.lang.arq.TokenMgrError tErr)\n    {\n        // Last valid token : not the same as token error message - but this should not happen\n        int col = parser.token.endColumn ;\n        int line = parser.token.endLine ;\n        throw new QueryParseException(tErr.getMessage(), line, col) ; }\n\n    catch (QueryException ex) { throw ex ; }\n    catch (JenaException ex)  { throw new QueryException(ex.getMessage(), ex) ; }\n    catch (Error err)\n    {\n        // The token stream can throw errors.\n        throw new QueryParseException(err.getMessage(), err, -1, -1) ;\n    }\n    catch (Throwable th)\n    {\n        Log.fatal(this, \"Unexpected throwable: \",th) ;\n        throw new QueryException(th.getMessage(), th) ;\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2816,
        "instruction": "public void inject(GridDeployment dep, ComputeTask<?, ?> task, GridTaskSessionImpl ses,\n    ComputeLoadBalancer balancer, ComputeTaskContinuousMapper mapper) throws IgniteCheckedException {\n    if (log.isDebugEnabled())\n\n    // Unwrap Proxy object.\n    Object obj = unwrapTarget(task);\n\n    inject(obj, GridResourceIoc.AnnotationSet.TASK, dep, null, ses, balancer, mapper);\n}",
        "accept_response": "public void inject(GridDeployment dep, ComputeTask<?, ?> task, GridTaskSessionImpl ses,\n    ComputeLoadBalancer balancer, ComputeTaskContinuousMapper mapper) throws IgniteCheckedException {\n    if (log.isDebugEnabled())\n        log.debug(S.toString(\"Injecting resources\", \"task\", task, true));\n\n    // Unwrap Proxy object.\n    Object obj = unwrapTarget(task);\n\n    inject(obj, GridResourceIoc.AnnotationSet.TASK, dep, null, ses, balancer, mapper);\n}",
        "reject_response": "public void inject(GridDeployment dep, ComputeTask<?, ?> task, GridTaskSessionImpl ses,\n    ComputeLoadBalancer balancer, ComputeTaskContinuousMapper mapper) throws IgniteCheckedException {\n    if (log.isDebugEnabled())\n        log.debug(\"Injecting resources: \" + task);\n\n    // Unwrap Proxy object.\n    Object obj = unwrapTarget(task);\n\n    inject(obj, GridResourceIoc.AnnotationSet.TASK, dep, null, ses, balancer, mapper);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3211,
        "instruction": "private void unregister(String path,ZKMBeanInfo bean) throws JMException  {\n    if(path==null)\n        return;\n    if (!bean.isHidden()) {\n        final ObjectName objName = makeObjectName(path, bean);\n        synchronized (LOCK) {\n           mBeanServer.unregisterMBean(objName);\n        }\n    }\n}",
        "accept_response": "private void unregister(String path,ZKMBeanInfo bean) throws JMException  {\n    if(path==null)\n        return;\n    if (!bean.isHidden()) {\n        final ObjectName objName = makeObjectName(path, bean);\n        LOG.debug(\"Unregister MBean [{}]\", objName);\n        synchronized (LOCK) {\n           mBeanServer.unregisterMBean(objName);\n        }\n    }\n}",
        "reject_response": "private void unregister(String path,ZKMBeanInfo bean) throws JMException  {\n    if(path==null)\n        return;\n    if (!bean.isHidden()) {\n        final ObjectName objName = makeObjectName(path, bean);\n        if (LOG.isInfoEnabled()) {\n            LOG.info(\"Unregister MBean [{}]\", objName);\n        }\n        synchronized (LOCK) {\n           mBeanServer.unregisterMBean(objName);\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2743,
        "instruction": "private void selectRpcInputStreams(Collection<EditLogInputStream> streams,\n    long fromTxnId, boolean onlyDurableTxns) throws IOException {\n  QuorumCall<AsyncLogger, GetJournaledEditsResponseProto> q =\n      loggers.getJournaledEdits(fromTxnId, maxTxnsPerRpc);\n  Map<AsyncLogger, GetJournaledEditsResponseProto> responseMap =\n      loggers.waitForWriteQuorum(q, selectInputStreamsTimeoutMs,\n          \"selectRpcInputStreams\");\n  assert responseMap.size() >= loggers.getMajoritySize() :\n      \"Quorum call returned without a majority\";\n\n  List<Integer> responseCounts = new ArrayList<>();\n  for (GetJournaledEditsResponseProto resp : responseMap.values()) {\n    responseCounts.add(resp.getTxnCount());\n  }\n  Collections.sort(responseCounts);\n  int highestTxnCount = responseCounts.get(responseCounts.size() - 1);\n  if (LOG.isDebugEnabled() || highestTxnCount < 0) {\n    StringBuilder msg = new StringBuilder(\"Requested edits starting from \");\n    msg.append(fromTxnId).append(\"; got \").append(responseMap.size())\n        .append(\" responses: <\");\n    for (Map.Entry<AsyncLogger, GetJournaledEditsResponseProto> ent :\n        responseMap.entrySet()) {\n      msg.append(\"[\").append(ent.getKey()).append(\", \")\n          .append(ent.getValue().getTxnCount()).append(\"],\");\n    }\n    msg.append(\">\");\n    if (highestTxnCount < 0) {\n      throw new IOException(\"Did not get any valid JournaledEdits \" +\n          \"responses: \" + msg);\n    } else {\n      LOG.debug(msg.toString());\n    }\n  }\n  // Cancel any outstanding calls to JN's.\n  q.cancelCalls();\n\n  int maxAllowedTxns = !onlyDurableTxns ? highestTxnCount :\n      responseCounts.get(responseCounts.size() - loggers.getMajoritySize());\n  if (maxAllowedTxns == 0) {\n    return;\n  }\n  LogAction logAction = selectInputStreamLogHelper.record(fromTxnId);\n  if (logAction.shouldLog()) {\n    LOG.info(\"Selected loggers with >= \" + maxAllowedTxns + \" transactions \" +\n        \"starting from lowest txn ID \" + logAction.getStats(0).getMin() +\n        LogThrottlingHelper.getLogSupressionMessage(logAction));\n  }\n  PriorityQueue<EditLogInputStream> allStreams = new PriorityQueue<>(\n      JournalSet.EDIT_LOG_INPUT_STREAM_COMPARATOR);\n  for (GetJournaledEditsResponseProto resp : responseMap.values()) {\n    long endTxnId = fromTxnId - 1 +\n        Math.min(maxAllowedTxns, resp.getTxnCount());\n    allStreams.add(EditLogFileInputStream.fromByteString(\n        resp.getEditLog(), fromTxnId, endTxnId, true));\n  }\n  JournalSet.chainAndMakeRedundantStreams(streams, allStreams, fromTxnId);\n}",
        "accept_response": "private void selectRpcInputStreams(Collection<EditLogInputStream> streams,\n    long fromTxnId, boolean onlyDurableTxns) throws IOException {\n  QuorumCall<AsyncLogger, GetJournaledEditsResponseProto> q =\n      loggers.getJournaledEdits(fromTxnId, maxTxnsPerRpc);\n  Map<AsyncLogger, GetJournaledEditsResponseProto> responseMap =\n      loggers.waitForWriteQuorum(q, selectInputStreamsTimeoutMs,\n          \"selectRpcInputStreams\");\n  assert responseMap.size() >= loggers.getMajoritySize() :\n      \"Quorum call returned without a majority\";\n\n  List<Integer> responseCounts = new ArrayList<>();\n  for (GetJournaledEditsResponseProto resp : responseMap.values()) {\n    responseCounts.add(resp.getTxnCount());\n  }\n  Collections.sort(responseCounts);\n  int highestTxnCount = responseCounts.get(responseCounts.size() - 1);\n  if (LOG.isDebugEnabled() || highestTxnCount < 0) {\n    StringBuilder msg = new StringBuilder(\"Requested edits starting from \");\n    msg.append(fromTxnId).append(\"; got \").append(responseMap.size())\n        .append(\" responses: <\");\n    for (Map.Entry<AsyncLogger, GetJournaledEditsResponseProto> ent :\n        responseMap.entrySet()) {\n      msg.append(\"[\").append(ent.getKey()).append(\", \")\n          .append(ent.getValue().getTxnCount()).append(\"],\");\n    }\n    msg.append(\">\");\n    if (highestTxnCount < 0) {\n      throw new IOException(\"Did not get any valid JournaledEdits \" +\n          \"responses: \" + msg);\n    } else {\n      LOG.debug(msg.toString());\n    }\n  }\n  // Cancel any outstanding calls to JN's.\n  q.cancelCalls();\n\n  int maxAllowedTxns = !onlyDurableTxns ? highestTxnCount :\n      responseCounts.get(responseCounts.size() - loggers.getMajoritySize());\n  if (maxAllowedTxns == 0) {\n    LOG.debug(\"No new edits available in logs; requested starting from ID {}\",\n        fromTxnId);\n    return;\n  }\n  LogAction logAction = selectInputStreamLogHelper.record(fromTxnId);\n  if (logAction.shouldLog()) {\n    LOG.info(\"Selected loggers with >= \" + maxAllowedTxns + \" transactions \" +\n        \"starting from lowest txn ID \" + logAction.getStats(0).getMin() +\n        LogThrottlingHelper.getLogSupressionMessage(logAction));\n  }\n  PriorityQueue<EditLogInputStream> allStreams = new PriorityQueue<>(\n      JournalSet.EDIT_LOG_INPUT_STREAM_COMPARATOR);\n  for (GetJournaledEditsResponseProto resp : responseMap.values()) {\n    long endTxnId = fromTxnId - 1 +\n        Math.min(maxAllowedTxns, resp.getTxnCount());\n    allStreams.add(EditLogFileInputStream.fromByteString(\n        resp.getEditLog(), fromTxnId, endTxnId, true));\n  }\n  JournalSet.chainAndMakeRedundantStreams(streams, allStreams, fromTxnId);\n}",
        "reject_response": "private void selectRpcInputStreams(Collection<EditLogInputStream> streams,\n    long fromTxnId, boolean onlyDurableTxns) throws IOException {\n  QuorumCall<AsyncLogger, GetJournaledEditsResponseProto> q =\n      loggers.getJournaledEdits(fromTxnId, maxTxnsPerRpc);\n  Map<AsyncLogger, GetJournaledEditsResponseProto> responseMap =\n      loggers.waitForWriteQuorum(q, selectInputStreamsTimeoutMs,\n          \"selectRpcInputStreams\");\n  assert responseMap.size() >= loggers.getMajoritySize() :\n      \"Quorum call returned without a majority\";\n\n  List<Integer> responseCounts = new ArrayList<>();\n  for (GetJournaledEditsResponseProto resp : responseMap.values()) {\n    responseCounts.add(resp.getTxnCount());\n  }\n  Collections.sort(responseCounts);\n  int highestTxnCount = responseCounts.get(responseCounts.size() - 1);\n  if (LOG.isDebugEnabled() || highestTxnCount < 0) {\n    StringBuilder msg = new StringBuilder(\"Requested edits starting from \");\n    msg.append(fromTxnId).append(\"; got \").append(responseMap.size())\n        .append(\" responses: <\");\n    for (Map.Entry<AsyncLogger, GetJournaledEditsResponseProto> ent :\n        responseMap.entrySet()) {\n      msg.append(\"[\").append(ent.getKey()).append(\", \")\n          .append(ent.getValue().getTxnCount()).append(\"],\");\n    }\n    msg.append(\">\");\n    if (highestTxnCount < 0) {\n      throw new IOException(\"Did not get any valid JournaledEdits \" +\n          \"responses: \" + msg);\n    } else {\n      LOG.debug(msg.toString());\n    }\n  }\n  // Cancel any outstanding calls to JN's.\n  q.cancelCalls();\n\n  int maxAllowedTxns = !onlyDurableTxns ? highestTxnCount :\n      responseCounts.get(responseCounts.size() - loggers.getMajoritySize());\n  if (maxAllowedTxns == 0) {\n    LOG.debug(\"No new edits available in logs; requested starting from \" +\n        \"ID {}\", fromTxnId);\n    return;\n  }\n  LogAction logAction = selectInputStreamLogHelper.record(fromTxnId);\n  if (logAction.shouldLog()) {\n    LOG.info(\"Selected loggers with >= \" + maxAllowedTxns + \" transactions \" +\n        \"starting from lowest txn ID \" + logAction.getStats(0).getMin() +\n        LogThrottlingHelper.getLogSupressionMessage(logAction));\n  }\n  PriorityQueue<EditLogInputStream> allStreams = new PriorityQueue<>(\n      JournalSet.EDIT_LOG_INPUT_STREAM_COMPARATOR);\n  for (GetJournaledEditsResponseProto resp : responseMap.values()) {\n    long endTxnId = fromTxnId - 1 +\n        Math.min(maxAllowedTxns, resp.getTxnCount());\n    allStreams.add(EditLogFileInputStream.fromByteString(\n        resp.getEditLog(), fromTxnId, endTxnId, true));\n  }\n  JournalSet.chainAndMakeRedundantStreams(streams, allStreams, fromTxnId);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3079,
        "instruction": "@Override\npublic void start() {\n  // Start embedded server\n  try {\n    port = NetworkUtils.findAvailablePort();\n    LOG.info(\"Will bind to port \" + port);\n\n    server = new NetworkServerControl(InetAddress.getByName(\"localhost\"), port);\n    server.start(new LoggerWriter(LOG, Level.INFO));\n\n    // Start won't thrown an exception in case that it fails to start, one\n    // have to explicitly call ping() in order to verify if the server is\n    // up. Check DERBY-1465 for more details.\n    //\n    // In addition we've observed that in some scenarios ping() can get into\n    // deadlock waiting on remote server forever and hence we're having\n    // our own timeout handling around it.\n    ExecutorService executorService = Executors.newSingleThreadExecutor();\n    Future future = executorService.submit(new Callable<Object>() {\n      @Override\n      public Object call() throws Exception {\n        while (true) {\n          try {\n            server.ping();\n            break;\n          } catch (Exception e) {\n            LOG.warn(\"Could not ping derby server on port \" + port, e);\n          }\n\n          Thread.sleep(1000);\n        }\n\n        return null;\n      }\n    });\n    future.get(10, TimeUnit.SECONDS);\n\n    // Server successfully started at this point\n    started = true;\n  } catch (Exception e) {\n    String message = \"Can't start embedded Derby server\";\n    throw new RuntimeException(message, e);\n  }\n\n  super.start();\n}",
        "accept_response": "@Override\npublic void start() {\n  // Start embedded server\n  try {\n    port = NetworkUtils.findAvailablePort();\n    LOG.info(\"Will bind to port \" + port);\n\n    server = new NetworkServerControl(InetAddress.getByName(\"localhost\"), port);\n    server.start(new LoggerWriter(LOG, Level.INFO));\n\n    // Start won't thrown an exception in case that it fails to start, one\n    // have to explicitly call ping() in order to verify if the server is\n    // up. Check DERBY-1465 for more details.\n    //\n    // In addition we've observed that in some scenarios ping() can get into\n    // deadlock waiting on remote server forever and hence we're having\n    // our own timeout handling around it.\n    ExecutorService executorService = Executors.newSingleThreadExecutor();\n    Future future = executorService.submit(new Callable<Object>() {\n      @Override\n      public Object call() throws Exception {\n        while (true) {\n          try {\n            server.ping();\n            break;\n          } catch (Exception e) {\n            LOG.warn(\"Could not ping derby server on port \" + port, e);\n          }\n\n          Thread.sleep(1000);\n        }\n\n        return null;\n      }\n    });\n    future.get(10, TimeUnit.SECONDS);\n\n    // Server successfully started at this point\n    started = true;\n  } catch (Exception e) {\n    String message = \"Can't start embedded Derby server\";\n    LOG.fatal(message, e);\n    throw new RuntimeException(message, e);\n  }\n\n  super.start();\n}",
        "reject_response": "@Override\npublic void start() {\n  // Start embedded server\n  try {\n    port = NetworkUtils.findAvailablePort();\n    LOG.info(\"Will bind to port \" + port);\n\n    server = new NetworkServerControl(InetAddress.getByName(\"localhost\"), port);\n    server.start(new LoggerWriter(LOG, Level.INFO));\n\n    // Start won't thrown an exception in case that it fails to start, one\n    // have to explicitly call ping() in order to verify if the server is\n    // up. Check DERBY-1465 for more details.\n    //\n    // In addition we've observed that in some scenarios ping() can get into\n    // deadlock waiting on remote server forever and hence we're having\n    // our own timeout handling around it.\n    ExecutorService executorService = Executors.newSingleThreadExecutor();\n    Future future = executorService.submit(new Callable<Object>() {\n      @Override\n      public Object call() throws Exception {\n        while (true) {\n          try {\n            server.ping();\n            break;\n          } catch (Exception e) {\n            LOG.warn(\"Could not ping derby server on port \" + port, e);\n          }\n\n          Thread.sleep(1000);\n        }\n\n        return null;\n      }\n    });\n    future.get(10, TimeUnit.SECONDS);\n\n    // Server successfully started at this point\n    started = true;\n  } catch (Exception e) {\n    String message = \"Can't start embedded Derby server\";\n    LOG.error(\"Can't start Derby network server\", e);\n    throw new RuntimeException(\"Can't derby server\", e);\n    throw new RuntimeException(message, e);\n  }\n\n  super.start();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2878,
        "instruction": "public static BPlusTreeParams readMeta(MetaFile mf)\n{\n    try {\n        int pOrder = mf.getPropertyAsInteger(ParamOrder) ;\n        int pKeyLen = mf.getPropertyAsInteger(ParamKeyLength) ;\n        int pRecLen = mf.getPropertyAsInteger(ParamValueLength) ;\n        return new BPlusTreeParams(pOrder, pKeyLen, pRecLen) ;\n    } catch (NumberFormatException ex)\n    {\n        throw new TDBException(\"Failed to read metadata\") ;\n    }\n}",
        "accept_response": "public static BPlusTreeParams readMeta(MetaFile mf)\n{\n    try {\n        int pOrder = mf.getPropertyAsInteger(ParamOrder) ;\n        int pKeyLen = mf.getPropertyAsInteger(ParamKeyLength) ;\n        int pRecLen = mf.getPropertyAsInteger(ParamValueLength) ;\n        return new BPlusTreeParams(pOrder, pKeyLen, pRecLen) ;\n    } catch (NumberFormatException ex)\n    {\n        Log.error(BPlusTreeParams.class, \"Badly formed metadata for B+Tree\") ;\n        throw new TDBException(\"Failed to read metadata\") ;\n    }\n}",
        "reject_response": "public static BPlusTreeParams readMeta(MetaFile mf)\n{\n    try {\n        int pOrder = mf.getPropertyAsInteger(ParamOrder) ;\n        int pKeyLen = mf.getPropertyAsInteger(ParamKeyLength) ;\n        int pRecLen = mf.getPropertyAsInteger(ParamValueLength) ;\n        return new BPlusTreeParams(pOrder, pKeyLen, pRecLen) ;\n    } catch (NumberFormatException ex)\n    {\n        Log.fatal(BPlusTreeParams.class, \"Badly formed metadata for B+Tree\") ;\n        throw new TDBException(\"Failed to read metadata\") ;\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2630,
        "instruction": "private boolean updateData() {\n  // This will eventually call JMX. Currently we will update this with\n  // some dummy data.\n  // Connect if required or hold a connection. If unable to connect,\n  // return false\n  return this.updater.updateData();\n}",
        "accept_response": "private boolean updateData() {\n  // This will eventually call JMX. Currently we will update this with\n  // some dummy data.\n  // Connect if required or hold a connection. If unable to connect,\n  // return false\n  logger.debug(\"{} :: {}:{}\", resourceBundle.getString(\"LOG_MSG_CLUSTER_DATA_IS_UPDATING\"),\n      this.serverName, this.port);\n  return this.updater.updateData();\n}",
        "reject_response": "private boolean updateData() {\n  // This will eventually call JMX. Currently we will update this with\n  // some dummy data.\n  // Connect if required or hold a connection. If unable to connect,\n  // return false\n  if (LOGGER.finerEnabled()) {\n    LOGGER.finer(resourceBundle.getString(\"LOG_MSG_CLUSTER_DATA_IS_UPDATING\") + \"::\"\n        + this.serverName + \":\" + this.port);\n  }\n  return this.updater.updateData();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2718,
        "instruction": "protected Path closeRecoveredEditsWriter(RecoveredEditsWriter editsWriter,\n    List<IOException> thrown) throws IOException {\n  try {\n    editsWriter.writer.close();\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not close recovered edits at \" + editsWriter.path;\n    LOG.error(errorMsg, ioe);\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  final String msg = \"Closed recovered edits writer path=\" + editsWriter.path + \" (wrote \"\n    + editsWriter.editsWritten + \" edits, skipped \" + editsWriter.editsSkipped + \" edits in \" + (\n    editsWriter.nanosSpent / 1000 / 1000) + \" ms)\";\n  updateStatusWithMsg(msg);\n  if (editsWriter.editsWritten == 0) {\n    // just remove the empty recovered.edits file\n    if (walSplitter.walFS.exists(editsWriter.path)\n        && !walSplitter.walFS.delete(editsWriter.path, false)) {\n      final String errorMsg = \"Failed deleting empty \" + editsWriter.path;\n      LOG.warn(errorMsg);\n      updateStatusWithMsg(errorMsg);\n      throw new IOException(\"Failed deleting empty  \" + editsWriter.path);\n    }\n    return null;\n  }\n\n  Path dst = getCompletedRecoveredEditsFilePath(editsWriter.path,\n    regionMaximumEditLogSeqNum.get(Bytes.toString(editsWriter.encodedRegionName)));\n  try {\n    if (!dst.equals(editsWriter.path) && walSplitter.walFS.exists(dst)) {\n      deleteOneWithFewerEntries(editsWriter, dst);\n    }\n    // Skip the unit tests which create a splitter that reads and\n    // writes the data without touching disk.\n    // TestHLogSplit#testThreading is an example.\n    if (walSplitter.walFS.exists(editsWriter.path)) {\n      if (!walSplitter.walFS.rename(editsWriter.path, dst)) {\n        final String errorMsg =\n          \"Failed renaming recovered edits \" + editsWriter.path + \" to \" + dst;\n        updateStatusWithMsg(errorMsg);\n        throw new IOException(errorMsg);\n      }\n      final String renameEditMsg = \"Rename recovered edits \" + editsWriter.path + \" to \" + dst;\n      LOG.info(renameEditMsg);\n      updateStatusWithMsg(renameEditMsg);\n    }\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not rename recovered edits \" + editsWriter.path\n      + \" to \" + dst;\n    LOG.error(errorMsg, ioe);\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  return dst;\n}",
        "accept_response": "protected Path closeRecoveredEditsWriter(RecoveredEditsWriter editsWriter,\n    List<IOException> thrown) throws IOException {\n  try {\n    editsWriter.writer.close();\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not close recovered edits at \" + editsWriter.path;\n    LOG.error(errorMsg, ioe);\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  final String msg = \"Closed recovered edits writer path=\" + editsWriter.path + \" (wrote \"\n    + editsWriter.editsWritten + \" edits, skipped \" + editsWriter.editsSkipped + \" edits in \" + (\n    editsWriter.nanosSpent / 1000 / 1000) + \" ms)\";\n  LOG.info(msg);\n  updateStatusWithMsg(msg);\n  if (editsWriter.editsWritten == 0) {\n    // just remove the empty recovered.edits file\n    if (walSplitter.walFS.exists(editsWriter.path)\n        && !walSplitter.walFS.delete(editsWriter.path, false)) {\n      final String errorMsg = \"Failed deleting empty \" + editsWriter.path;\n      LOG.warn(errorMsg);\n      updateStatusWithMsg(errorMsg);\n      throw new IOException(\"Failed deleting empty  \" + editsWriter.path);\n    }\n    return null;\n  }\n\n  Path dst = getCompletedRecoveredEditsFilePath(editsWriter.path,\n    regionMaximumEditLogSeqNum.get(Bytes.toString(editsWriter.encodedRegionName)));\n  try {\n    if (!dst.equals(editsWriter.path) && walSplitter.walFS.exists(dst)) {\n      deleteOneWithFewerEntries(editsWriter, dst);\n    }\n    // Skip the unit tests which create a splitter that reads and\n    // writes the data without touching disk.\n    // TestHLogSplit#testThreading is an example.\n    if (walSplitter.walFS.exists(editsWriter.path)) {\n      if (!walSplitter.walFS.rename(editsWriter.path, dst)) {\n        final String errorMsg =\n          \"Failed renaming recovered edits \" + editsWriter.path + \" to \" + dst;\n        updateStatusWithMsg(errorMsg);\n        throw new IOException(errorMsg);\n      }\n      final String renameEditMsg = \"Rename recovered edits \" + editsWriter.path + \" to \" + dst;\n      LOG.info(renameEditMsg);\n      updateStatusWithMsg(renameEditMsg);\n    }\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not rename recovered edits \" + editsWriter.path\n      + \" to \" + dst;\n    LOG.error(errorMsg, ioe);\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  return dst;\n}",
        "reject_response": "protected Path closeRecoveredEditsWriter(RecoveredEditsWriter editsWriter,\n    List<IOException> thrown) throws IOException {\n  try {\n    editsWriter.writer.close();\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not close recovered edits at \" + editsWriter.path;\n    LOG.error(errorMsg, ioe);\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  final String msg = \"Closed recovered edits writer path=\" + editsWriter.path + \" (wrote \"\n    + editsWriter.editsWritten + \" edits, skipped \" + editsWriter.editsSkipped + \" edits in \" + (\n    editsWriter.nanosSpent / 1000 / 1000) + \" ms)\";\n  LOG.info(\"Closed recovered edits writer path={} (wrote {} edits, skipped {} edits in {} ms\",\n    editsWriter.path, editsWriter.editsWritten, editsWriter.editsSkipped,\n    editsWriter.nanosSpent / 1000 / 1000);\n  updateStatusWithMsg(msg);\n  if (editsWriter.editsWritten == 0) {\n    // just remove the empty recovered.edits file\n    if (walSplitter.walFS.exists(editsWriter.path)\n        && !walSplitter.walFS.delete(editsWriter.path, false)) {\n      final String errorMsg = \"Failed deleting empty \" + editsWriter.path;\n      LOG.warn(errorMsg);\n      updateStatusWithMsg(errorMsg);\n      throw new IOException(\"Failed deleting empty  \" + editsWriter.path);\n    }\n    return null;\n  }\n\n  Path dst = getCompletedRecoveredEditsFilePath(editsWriter.path,\n    regionMaximumEditLogSeqNum.get(Bytes.toString(editsWriter.encodedRegionName)));\n  try {\n    if (!dst.equals(editsWriter.path) && walSplitter.walFS.exists(dst)) {\n      deleteOneWithFewerEntries(editsWriter, dst);\n    }\n    // Skip the unit tests which create a splitter that reads and\n    // writes the data without touching disk.\n    // TestHLogSplit#testThreading is an example.\n    if (walSplitter.walFS.exists(editsWriter.path)) {\n      if (!walSplitter.walFS.rename(editsWriter.path, dst)) {\n        final String errorMsg =\n          \"Failed renaming recovered edits \" + editsWriter.path + \" to \" + dst;\n        updateStatusWithMsg(errorMsg);\n        throw new IOException(errorMsg);\n      }\n      final String renameEditMsg = \"Rename recovered edits \" + editsWriter.path + \" to \" + dst;\n      LOG.info(renameEditMsg);\n      updateStatusWithMsg(renameEditMsg);\n    }\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not rename recovered edits \" + editsWriter.path\n      + \" to \" + dst;\n    LOG.error(errorMsg, ioe);\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  return dst;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2397,
        "instruction": "@Transactional\nServiceConfigVersionResponse applyConfigs(Set<Config> configs, String user, String serviceConfigVersionNote) {\n\n  String serviceName = null;\n  for (Config config : configs) {\n    for (Entry<String, String> entry : serviceConfigTypes.entries()) {\n      if (StringUtils.equals(entry.getValue(), config.getType())) {\n        if (serviceName == null) {\n          serviceName = entry.getKey();\n          break;\n        } else if (!serviceName.equals(entry.getKey())) {\n          String error = \"Updating configs for multiple services by a \" +\n              \"single API request isn't supported\";\n          IllegalArgumentException exception = new IllegalArgumentException(error);\n          LOG.error(error + \", config version not created\");\n          throw exception;\n        } else {\n          break;\n        }\n      }\n    }\n  }\n\n  for (Config config: configs) {\n    selectConfig(config.getType(), config.getTag(), user);\n  }\n\n  if (serviceName == null) {\n    ArrayList<String> configTypes = new ArrayList<>();\n    for (Config config: configs) {\n      configTypes.add(config.getType());\n    }\n    return null;\n  } else {\n    return createServiceConfigVersion(serviceName, user, serviceConfigVersionNote);\n  }\n\n}",
        "accept_response": "@Transactional\nServiceConfigVersionResponse applyConfigs(Set<Config> configs, String user, String serviceConfigVersionNote) {\n\n  String serviceName = null;\n  for (Config config : configs) {\n    for (Entry<String, String> entry : serviceConfigTypes.entries()) {\n      if (StringUtils.equals(entry.getValue(), config.getType())) {\n        if (serviceName == null) {\n          serviceName = entry.getKey();\n          break;\n        } else if (!serviceName.equals(entry.getKey())) {\n          String error = \"Updating configs for multiple services by a \" +\n              \"single API request isn't supported\";\n          IllegalArgumentException exception = new IllegalArgumentException(error);\n          LOG.error(error + \", config version not created\");\n          throw exception;\n        } else {\n          break;\n        }\n      }\n    }\n  }\n\n  for (Config config: configs) {\n    selectConfig(config.getType(), config.getTag(), user);\n  }\n\n  if (serviceName == null) {\n    ArrayList<String> configTypes = new ArrayList<>();\n    for (Config config: configs) {\n      configTypes.add(config.getType());\n    }\n    LOG.error(\"No service found for config types '{}', service config version not created\", configTypes);\n    return null;\n  } else {\n    return createServiceConfigVersion(serviceName, user, serviceConfigVersionNote);\n  }\n\n}",
        "reject_response": "@Transactional\nServiceConfigVersionResponse applyConfigs(Set<Config> configs, String user, String serviceConfigVersionNote) {\n\n  String serviceName = null;\n  for (Config config : configs) {\n    for (Entry<String, String> entry : serviceConfigTypes.entries()) {\n      if (StringUtils.equals(entry.getValue(), config.getType())) {\n        if (serviceName == null) {\n          serviceName = entry.getKey();\n          break;\n        } else if (!serviceName.equals(entry.getKey())) {\n          String error = \"Updating configs for multiple services by a \" +\n              \"single API request isn't supported\";\n          IllegalArgumentException exception = new IllegalArgumentException(error);\n          LOG.error(error + \", config version not created\");\n          throw exception;\n        } else {\n          break;\n        }\n      }\n    }\n  }\n\n  for (Config config: configs) {\n    selectConfig(config.getType(), config.getTag(), user);\n  }\n\n  if (serviceName == null) {\n    ArrayList<String> configTypes = new ArrayList<>();\n    for (Config config: configs) {\n      configTypes.add(config.getType());\n    }\n    LOG.error(\"No service found for config type '{}', service config version not created\");\n    return null;\n  } else {\n    return createServiceConfigVersion(serviceName, user, serviceConfigVersionNote);\n  }\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2551,
        "instruction": "  private ParquetFileMetadata_v3 getParquetFileMetadata_v3(ParquetTableMetadata_v3 parquetTableMetadata,\n      FileStatus file) throws IOException {\n    ParquetMetadata metadata = ParquetFileReader.readFooter(fs.getConf(), file);\n    MessageType schema = metadata.getFileMetaData().getSchema();\n\n//    Map<SchemaPath, OriginalType> originalTypeMap = Maps.newHashMap();\n    Map<SchemaPath, ColTypeInfo> colTypeInfoMap = Maps.newHashMap();\n    schema.getPaths();\n    for (String[] path : schema.getPaths()) {\n      colTypeInfoMap.put(SchemaPath.getCompoundPath(path), getColTypeInfo(schema, schema, path, 0));\n    }\n\n    List<RowGroupMetadata_v3> rowGroupMetadataList = Lists.newArrayList();\n\n    ArrayList<SchemaPath> ALL_COLS = new ArrayList<>();\n    ALL_COLS.add(AbstractRecordReader.STAR_COLUMN);\n    boolean autoCorrectCorruptDates = formatConfig.autoCorrectCorruptDates;\n    ParquetReaderUtility.DateCorruptionStatus containsCorruptDates = ParquetReaderUtility.detectCorruptDates(metadata, ALL_COLS, autoCorrectCorruptDates);\n    for (BlockMetaData rowGroup : metadata.getBlocks()) {\n      List<ColumnMetadata_v3> columnMetadataList = Lists.newArrayList();\n      long length = 0;\n      for (ColumnChunkMetaData col : rowGroup.getColumns()) {\n        ColumnMetadata_v3 columnMetadata;\n\n        boolean statsAvailable = (col.getStatistics() != null && !col.getStatistics().isEmpty());\n\n        Statistics<?> stats = col.getStatistics();\n        String[] columnName = col.getPath().toArray();\n        SchemaPath columnSchemaName = SchemaPath.getCompoundPath(columnName);\n        ColTypeInfo colTypeInfo = colTypeInfoMap.get(columnSchemaName);\n\n        ColumnTypeMetadata_v3 columnTypeMetadata =\n            new ColumnTypeMetadata_v3(columnName, col.getType(), colTypeInfo.originalType,\n                colTypeInfo.precision, colTypeInfo.scale, colTypeInfo.repetitionLevel, colTypeInfo.definitionLevel);\n\n        if (parquetTableMetadata.columnTypeInfo == null) {\n          parquetTableMetadata.columnTypeInfo = new ConcurrentHashMap<>();\n        }\n        // Save the column schema info. We'll merge it into one list\n        parquetTableMetadata.columnTypeInfo\n            .put(new ColumnTypeMetadata_v3.Key(columnTypeMetadata.name), columnTypeMetadata);\n        if (statsAvailable) {\n          // Write stats when they are not null\n          Object minValue = null;\n          Object maxValue = null;\n          if (stats.genericGetMax() != null && stats.genericGetMin() != null ) {\n            minValue = stats.genericGetMin();\n            maxValue = stats.genericGetMax();\n            if (containsCorruptDates == ParquetReaderUtility.DateCorruptionStatus.META_SHOWS_CORRUPTION\n                && columnTypeMetadata.originalType == OriginalType.DATE) {\n              minValue = ParquetReaderUtility.autoCorrectCorruptedDate((Integer) minValue);\n              maxValue = ParquetReaderUtility.autoCorrectCorruptedDate((Integer) maxValue);\n            }\n\n          }\n          columnMetadata =\n              new ColumnMetadata_v3(columnTypeMetadata.name, col.getType(), minValue, maxValue, stats.getNumNulls());\n        } else {\n          columnMetadata = new ColumnMetadata_v3(columnTypeMetadata.name, col.getType(), null, null, null);\n        }\n        columnMetadataList.add(columnMetadata);\n        length += col.getTotalSize();\n      }\n\n      // DRILL-5009: Skip the RowGroup if it is empty\n      // Note we still read the schema even if there are no values in the RowGroup\n      if (rowGroup.getRowCount() == 0) {\n        continue;\n      }\n      RowGroupMetadata_v3 rowGroupMeta =\n          new RowGroupMetadata_v3(rowGroup.getStartingPos(), length, rowGroup.getRowCount(),\n              getHostAffinity(file, rowGroup.getStartingPos(), length), columnMetadataList);\n\n      rowGroupMetadataList.add(rowGroupMeta);\n    }\n    String path = Path.getPathWithoutSchemeAndAuthority(file.getPath()).toString();\n\n    return new ParquetFileMetadata_v3(path, file.getLen(), rowGroupMetadataList);\n  }",
        "accept_response": "  private ParquetFileMetadata_v3 getParquetFileMetadata_v3(ParquetTableMetadata_v3 parquetTableMetadata,\n      FileStatus file) throws IOException {\n    ParquetMetadata metadata = ParquetFileReader.readFooter(fs.getConf(), file);\n    MessageType schema = metadata.getFileMetaData().getSchema();\n\n//    Map<SchemaPath, OriginalType> originalTypeMap = Maps.newHashMap();\n    Map<SchemaPath, ColTypeInfo> colTypeInfoMap = Maps.newHashMap();\n    schema.getPaths();\n    for (String[] path : schema.getPaths()) {\n      colTypeInfoMap.put(SchemaPath.getCompoundPath(path), getColTypeInfo(schema, schema, path, 0));\n    }\n\n    List<RowGroupMetadata_v3> rowGroupMetadataList = Lists.newArrayList();\n\n    ArrayList<SchemaPath> ALL_COLS = new ArrayList<>();\n    ALL_COLS.add(AbstractRecordReader.STAR_COLUMN);\n    boolean autoCorrectCorruptDates = formatConfig.autoCorrectCorruptDates;\n    ParquetReaderUtility.DateCorruptionStatus containsCorruptDates = ParquetReaderUtility.detectCorruptDates(metadata, ALL_COLS, autoCorrectCorruptDates);\n    if (logger.isDebugEnabled()) {\n      logger.debug(containsCorruptDates.toString());\n    }\n    for (BlockMetaData rowGroup : metadata.getBlocks()) {\n      List<ColumnMetadata_v3> columnMetadataList = Lists.newArrayList();\n      long length = 0;\n      for (ColumnChunkMetaData col : rowGroup.getColumns()) {\n        ColumnMetadata_v3 columnMetadata;\n\n        boolean statsAvailable = (col.getStatistics() != null && !col.getStatistics().isEmpty());\n\n        Statistics<?> stats = col.getStatistics();\n        String[] columnName = col.getPath().toArray();\n        SchemaPath columnSchemaName = SchemaPath.getCompoundPath(columnName);\n        ColTypeInfo colTypeInfo = colTypeInfoMap.get(columnSchemaName);\n\n        ColumnTypeMetadata_v3 columnTypeMetadata =\n            new ColumnTypeMetadata_v3(columnName, col.getType(), colTypeInfo.originalType,\n                colTypeInfo.precision, colTypeInfo.scale, colTypeInfo.repetitionLevel, colTypeInfo.definitionLevel);\n\n        if (parquetTableMetadata.columnTypeInfo == null) {\n          parquetTableMetadata.columnTypeInfo = new ConcurrentHashMap<>();\n        }\n        // Save the column schema info. We'll merge it into one list\n        parquetTableMetadata.columnTypeInfo\n            .put(new ColumnTypeMetadata_v3.Key(columnTypeMetadata.name), columnTypeMetadata);\n        if (statsAvailable) {\n          // Write stats when they are not null\n          Object minValue = null;\n          Object maxValue = null;\n          if (stats.genericGetMax() != null && stats.genericGetMin() != null ) {\n            minValue = stats.genericGetMin();\n            maxValue = stats.genericGetMax();\n            if (containsCorruptDates == ParquetReaderUtility.DateCorruptionStatus.META_SHOWS_CORRUPTION\n                && columnTypeMetadata.originalType == OriginalType.DATE) {\n              minValue = ParquetReaderUtility.autoCorrectCorruptedDate((Integer) minValue);\n              maxValue = ParquetReaderUtility.autoCorrectCorruptedDate((Integer) maxValue);\n            }\n\n          }\n          columnMetadata =\n              new ColumnMetadata_v3(columnTypeMetadata.name, col.getType(), minValue, maxValue, stats.getNumNulls());\n        } else {\n          columnMetadata = new ColumnMetadata_v3(columnTypeMetadata.name, col.getType(), null, null, null);\n        }\n        columnMetadataList.add(columnMetadata);\n        length += col.getTotalSize();\n      }\n\n      // DRILL-5009: Skip the RowGroup if it is empty\n      // Note we still read the schema even if there are no values in the RowGroup\n      if (rowGroup.getRowCount() == 0) {\n        continue;\n      }\n      RowGroupMetadata_v3 rowGroupMeta =\n          new RowGroupMetadata_v3(rowGroup.getStartingPos(), length, rowGroup.getRowCount(),\n              getHostAffinity(file, rowGroup.getStartingPos(), length), columnMetadataList);\n\n      rowGroupMetadataList.add(rowGroupMeta);\n    }\n    String path = Path.getPathWithoutSchemeAndAuthority(file.getPath()).toString();\n\n    return new ParquetFileMetadata_v3(path, file.getLen(), rowGroupMetadataList);\n  }",
        "reject_response": "  private ParquetFileMetadata_v3 getParquetFileMetadata_v3(ParquetTableMetadata_v3 parquetTableMetadata,\n      FileStatus file) throws IOException {\n    ParquetMetadata metadata = ParquetFileReader.readFooter(fs.getConf(), file);\n    MessageType schema = metadata.getFileMetaData().getSchema();\n\n//    Map<SchemaPath, OriginalType> originalTypeMap = Maps.newHashMap();\n    Map<SchemaPath, ColTypeInfo> colTypeInfoMap = Maps.newHashMap();\n    schema.getPaths();\n    for (String[] path : schema.getPaths()) {\n      colTypeInfoMap.put(SchemaPath.getCompoundPath(path), getColTypeInfo(schema, schema, path, 0));\n    }\n\n    List<RowGroupMetadata_v3> rowGroupMetadataList = Lists.newArrayList();\n\n    ArrayList<SchemaPath> ALL_COLS = new ArrayList<>();\n    ALL_COLS.add(AbstractRecordReader.STAR_COLUMN);\n    boolean autoCorrectCorruptDates = formatConfig.autoCorrectCorruptDates;\n    ParquetReaderUtility.DateCorruptionStatus containsCorruptDates = ParquetReaderUtility.detectCorruptDates(metadata, ALL_COLS, autoCorrectCorruptDates);\n    if (logger.isDebugEnabled()) {\n    logger.info(containsCorruptDates.toString());\n    }\n    for (BlockMetaData rowGroup : metadata.getBlocks()) {\n      List<ColumnMetadata_v3> columnMetadataList = Lists.newArrayList();\n      long length = 0;\n      for (ColumnChunkMetaData col : rowGroup.getColumns()) {\n        ColumnMetadata_v3 columnMetadata;\n\n        boolean statsAvailable = (col.getStatistics() != null && !col.getStatistics().isEmpty());\n\n        Statistics<?> stats = col.getStatistics();\n        String[] columnName = col.getPath().toArray();\n        SchemaPath columnSchemaName = SchemaPath.getCompoundPath(columnName);\n        ColTypeInfo colTypeInfo = colTypeInfoMap.get(columnSchemaName);\n\n        ColumnTypeMetadata_v3 columnTypeMetadata =\n            new ColumnTypeMetadata_v3(columnName, col.getType(), colTypeInfo.originalType,\n                colTypeInfo.precision, colTypeInfo.scale, colTypeInfo.repetitionLevel, colTypeInfo.definitionLevel);\n\n        if (parquetTableMetadata.columnTypeInfo == null) {\n          parquetTableMetadata.columnTypeInfo = new ConcurrentHashMap<>();\n        }\n        // Save the column schema info. We'll merge it into one list\n        parquetTableMetadata.columnTypeInfo\n            .put(new ColumnTypeMetadata_v3.Key(columnTypeMetadata.name), columnTypeMetadata);\n        if (statsAvailable) {\n          // Write stats when they are not null\n          Object minValue = null;\n          Object maxValue = null;\n          if (stats.genericGetMax() != null && stats.genericGetMin() != null ) {\n            minValue = stats.genericGetMin();\n            maxValue = stats.genericGetMax();\n            if (containsCorruptDates == ParquetReaderUtility.DateCorruptionStatus.META_SHOWS_CORRUPTION\n                && columnTypeMetadata.originalType == OriginalType.DATE) {\n              minValue = ParquetReaderUtility.autoCorrectCorruptedDate((Integer) minValue);\n              maxValue = ParquetReaderUtility.autoCorrectCorruptedDate((Integer) maxValue);\n            }\n\n          }\n          columnMetadata =\n              new ColumnMetadata_v3(columnTypeMetadata.name, col.getType(), minValue, maxValue, stats.getNumNulls());\n        } else {\n          columnMetadata = new ColumnMetadata_v3(columnTypeMetadata.name, col.getType(), null, null, null);\n        }\n        columnMetadataList.add(columnMetadata);\n        length += col.getTotalSize();\n      }\n\n      // DRILL-5009: Skip the RowGroup if it is empty\n      // Note we still read the schema even if there are no values in the RowGroup\n      if (rowGroup.getRowCount() == 0) {\n        continue;\n      }\n      RowGroupMetadata_v3 rowGroupMeta =\n          new RowGroupMetadata_v3(rowGroup.getStartingPos(), length, rowGroup.getRowCount(),\n              getHostAffinity(file, rowGroup.getStartingPos(), length), columnMetadataList);\n\n      rowGroupMetadataList.add(rowGroupMeta);\n    }\n    String path = Path.getPathWithoutSchemeAndAuthority(file.getPath()).toString();\n\n    return new ParquetFileMetadata_v3(path, file.getLen(), rowGroupMetadataList);\n  }",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3034,
        "instruction": "private void registerExporter(Bundle bundle, Class<?> annotatedClass, String resourceType, Exporter exporterAnnotation,\n                              List<ServiceRegistration> regs, ExportServlet.ExportedObjectAccessor accessor) {\n    if (accessor != null) {\n        Map<String, String> baseOptions = getOptions(exporterAnnotation);\n        ExportServlet servlet = new ExportServlet(bundle.getBundleContext(), factory, bindingsValuesProvidersByContext,\n                scriptEngineFactory, annotatedClass, exporterAnnotation.selector(), exporterAnnotation.name(), accessor, baseOptions);\n        Dictionary<String, Object> registrationProps = new Hashtable<>();\n        registrationProps.put(\"sling.servlet.resourceTypes\", resourceType);\n        registrationProps.put(\"sling.servlet.selectors\", exporterAnnotation.selector());\n        registrationProps.put(\"sling.servlet.extensions\", exporterAnnotation.extensions());\n        registrationProps.put(PROP_EXPORTER_SERVLET_CLASS, annotatedClass.getName());\n        registrationProps.put(PROP_EXPORTER_SERVLET_NAME, exporterAnnotation.name());\n\n\n        ServiceRegistration reg = bundleContext.registerService(Servlet.class.getName(), servlet, registrationProps);\n        regs.add(reg);\n    }\n}",
        "accept_response": "private void registerExporter(Bundle bundle, Class<?> annotatedClass, String resourceType, Exporter exporterAnnotation,\n                              List<ServiceRegistration> regs, ExportServlet.ExportedObjectAccessor accessor) {\n    if (accessor != null) {\n        Map<String, String> baseOptions = getOptions(exporterAnnotation);\n        ExportServlet servlet = new ExportServlet(bundle.getBundleContext(), factory, bindingsValuesProvidersByContext,\n                scriptEngineFactory, annotatedClass, exporterAnnotation.selector(), exporterAnnotation.name(), accessor, baseOptions);\n        Dictionary<String, Object> registrationProps = new Hashtable<>();\n        registrationProps.put(\"sling.servlet.resourceTypes\", resourceType);\n        registrationProps.put(\"sling.servlet.selectors\", exporterAnnotation.selector());\n        registrationProps.put(\"sling.servlet.extensions\", exporterAnnotation.extensions());\n        registrationProps.put(PROP_EXPORTER_SERVLET_CLASS, annotatedClass.getName());\n        registrationProps.put(PROP_EXPORTER_SERVLET_NAME, exporterAnnotation.name());\n\n        log.debug(\"registering servlet for {}, {}, {}\", new Object[]{resourceType, exporterAnnotation.selector(), exporterAnnotation.extensions()});\n\n        ServiceRegistration reg = bundleContext.registerService(Servlet.class.getName(), servlet, registrationProps);\n        regs.add(reg);\n    }\n}",
        "reject_response": "private void registerExporter(Bundle bundle, Class<?> annotatedClass, String resourceType, Exporter exporterAnnotation,\n                              List<ServiceRegistration> regs, ExportServlet.ExportedObjectAccessor accessor) {\n    if (accessor != null) {\n        Map<String, String> baseOptions = getOptions(exporterAnnotation);\n        ExportServlet servlet = new ExportServlet(bundle.getBundleContext(), factory, bindingsValuesProvidersByContext,\n                scriptEngineFactory, annotatedClass, exporterAnnotation.selector(), exporterAnnotation.name(), accessor, baseOptions);\n        Dictionary<String, Object> registrationProps = new Hashtable<>();\n        registrationProps.put(\"sling.servlet.resourceTypes\", resourceType);\n        registrationProps.put(\"sling.servlet.selectors\", exporterAnnotation.selector());\n        registrationProps.put(\"sling.servlet.extensions\", exporterAnnotation.extensions());\n        registrationProps.put(PROP_EXPORTER_SERVLET_CLASS, annotatedClass.getName());\n        registrationProps.put(PROP_EXPORTER_SERVLET_NAME, exporterAnnotation.name());\n\n        log.info(\"registering servlet for {}, {}, {}\", new Object[]{resourceType, exporterAnnotation.selector(), exporterAnnotation.extensions()});\n\n        ServiceRegistration reg = bundleContext.registerService(Servlet.class.getName(), servlet, registrationProps);\n        regs.add(reg);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2441,
        "instruction": "@Test\npublic void testSetupMDC()\n{\n  // The test does not test MDC properties that are passed via environment variables\n\n  String appenderName = \"mdcTestAppender\";\n  String service = \"test\";\n  String application = \"my application\";\n  String args = \"log4j.appender.mdcTestAppender=com.datatorrent.stram.util.LoggerUtilTest$TestAppender\"\n      + \",log4j.appender.mdcTestAppender.layout=org.apache.log4j.PatternLayout\"\n      + \",log4j.appender.mdcTestAppender.layout.ConversionPattern=%d %d{Z} [%t] %-5p (%F:%L) - %m%n\";\n\n  System.setProperty(APPLICATION_NAME.getLongName(), application);\n  LoggerUtil.setupMDC(service);\n\n  Logger logger = LogManager.getLogger(LoggerUtilTest.class);\n  LoggerUtil.addAppenders(logger, new String[] {appenderName}, args, \",\");\n  TestAppender appender = (TestAppender)logger.getAppender(appenderName);\n\n  assertEquals(service, appender.mdcProperties.get(\"apex.service\"));\n  String node = StramClientUtils.getHostName();\n  assertEquals(node == null ? \"unknown\" : node, appender.mdcProperties.get(\"apex.node\"));\n  assertEquals(application, appender.mdcProperties.get(\"apex.application\"));\n\n  assertTrue(LoggerUtil.removeAppender(logger, appenderName));\n}",
        "accept_response": "@Test\npublic void testSetupMDC()\n{\n  // The test does not test MDC properties that are passed via environment variables\n\n  String appenderName = \"mdcTestAppender\";\n  String service = \"test\";\n  String application = \"my application\";\n  String args = \"log4j.appender.mdcTestAppender=com.datatorrent.stram.util.LoggerUtilTest$TestAppender\"\n      + \",log4j.appender.mdcTestAppender.layout=org.apache.log4j.PatternLayout\"\n      + \",log4j.appender.mdcTestAppender.layout.ConversionPattern=%d %d{Z} [%t] %-5p (%F:%L) - %m%n\";\n\n  System.setProperty(APPLICATION_NAME.getLongName(), application);\n  LoggerUtil.setupMDC(service);\n\n  Logger logger = LogManager.getLogger(LoggerUtilTest.class);\n  LoggerUtil.addAppenders(logger, new String[] {appenderName}, args, \",\");\n  TestAppender appender = (TestAppender)logger.getAppender(appenderName);\n\n  LoggerUtilTest.logger.info(args);\n  assertEquals(service, appender.mdcProperties.get(\"apex.service\"));\n  String node = StramClientUtils.getHostName();\n  assertEquals(node == null ? \"unknown\" : node, appender.mdcProperties.get(\"apex.node\"));\n  assertEquals(application, appender.mdcProperties.get(\"apex.application\"));\n\n  assertTrue(LoggerUtil.removeAppender(logger, appenderName));\n}",
        "reject_response": "@Test\npublic void testSetupMDC()\n{\n  // The test does not test MDC properties that are passed via environment variables\n\n  String appenderName = \"mdcTestAppender\";\n  String service = \"test\";\n  String application = \"my application\";\n  String args = \"log4j.appender.mdcTestAppender=com.datatorrent.stram.util.LoggerUtilTest$TestAppender\"\n      + \",log4j.appender.mdcTestAppender.layout=org.apache.log4j.PatternLayout\"\n      + \",log4j.appender.mdcTestAppender.layout.ConversionPattern=%d %d{Z} [%t] %-5p (%F:%L) - %m%n\";\n\n  System.setProperty(APPLICATION_NAME.getLongName(), application);\n  LoggerUtil.setupMDC(service);\n\n  Logger logger = LogManager.getLogger(LoggerUtilTest.class);\n  LoggerUtil.addAppenders(logger, new String[] {appenderName}, args, \",\");\n  TestAppender appender = (TestAppender)logger.getAppender(appenderName);\n\n  logger.info(args);\n  assertEquals(service, appender.mdcProperties.get(\"apex.service\"));\n  String node = StramClientUtils.getHostName();\n  assertEquals(node == null ? \"unknown\" : node, appender.mdcProperties.get(\"apex.node\"));\n  assertEquals(application, appender.mdcProperties.get(\"apex.application\"));\n\n  assertTrue(LoggerUtil.removeAppender(logger, appenderName));\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3159,
        "instruction": "protected void handleErrorInDevMode(HttpServletResponse response, int code, Exception e) {\n    try {\n        List<Throwable> chain = new ArrayList<>();\n        Throwable cur = e;\n        chain.add(cur);\n        while ((cur = cur.getCause()) != null) {\n            chain.add(cur);\n        }\n\n        Writer writer = new StringWriter();\n        template.process(createReportData(e, chain), writer);\n\n        response.setContentType(\"text/html\");\n        response.getWriter().write(writer.toString());\n        response.getWriter().close();\n    } catch (Exception exp) {\n        try {\n            LOG.debug(\"Cannot show problem report!\", exp);\n            response.sendError(code, \"Unable to show problem report:\\n\" + exp + \"\\n\\n\" + LocationUtils.getLocation(exp));\n        } catch (IOException ex) {\n            // we're already sending an error, not much else we can do if more stuff breaks\n        }\n    }\n}",
        "accept_response": "protected void handleErrorInDevMode(HttpServletResponse response, int code, Exception e) {\n    LOG.debug(\"Exception occurred during processing request: {}\", e.getMessage(), e);\n    try {\n        List<Throwable> chain = new ArrayList<>();\n        Throwable cur = e;\n        chain.add(cur);\n        while ((cur = cur.getCause()) != null) {\n            chain.add(cur);\n        }\n\n        Writer writer = new StringWriter();\n        template.process(createReportData(e, chain), writer);\n\n        response.setContentType(\"text/html\");\n        response.getWriter().write(writer.toString());\n        response.getWriter().close();\n    } catch (Exception exp) {\n        try {\n            LOG.debug(\"Cannot show problem report!\", exp);\n            response.sendError(code, \"Unable to show problem report:\\n\" + exp + \"\\n\\n\" + LocationUtils.getLocation(exp));\n        } catch (IOException ex) {\n            // we're already sending an error, not much else we can do if more stuff breaks\n        }\n    }\n}",
        "reject_response": "protected void handleErrorInDevMode(HttpServletResponse response, int code, Exception e) {\n    LOG.debug(\"Exception occurred during processing request: {}\", e, e.getMessage());\n    try {\n        List<Throwable> chain = new ArrayList<>();\n        Throwable cur = e;\n        chain.add(cur);\n        while ((cur = cur.getCause()) != null) {\n            chain.add(cur);\n        }\n\n        Writer writer = new StringWriter();\n        template.process(createReportData(e, chain), writer);\n\n        response.setContentType(\"text/html\");\n        response.getWriter().write(writer.toString());\n        response.getWriter().close();\n    } catch (Exception exp) {\n        try {\n            LOG.debug(\"Cannot show problem report!\", exp);\n            response.sendError(code, \"Unable to show problem report:\\n\" + exp + \"\\n\\n\" + LocationUtils.getLocation(exp));\n        } catch (IOException ex) {\n            // we're already sending an error, not much else we can do if more stuff breaks\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3068,
        "instruction": "@SuppressWarnings(\"unchecked\")\nvoid deleteReplica(ClusterState clusterState, ZkNodeProps message, NamedList results, Runnable onComplete)\n        throws KeeperException, InterruptedException {\n  boolean parallel = message.getBool(\"parallel\", false);\n\n  //If a count is specified the strategy needs be different\n  if (message.getStr(COUNT_PROP) != null) {\n    deleteReplicaBasedOnCount(clusterState, message, results, onComplete, parallel);\n    return;\n  }\n\n\n  ocmh.checkRequired(message, COLLECTION_PROP, SHARD_ID_PROP, REPLICA_PROP);\n  String collectionName = message.getStr(COLLECTION_PROP);\n  String shard = message.getStr(SHARD_ID_PROP);\n  String replicaName = message.getStr(REPLICA_PROP);\n\n  DocCollection coll = clusterState.getCollection(collectionName);\n  Slice slice = coll.getSlice(shard);\n  if (slice == null) {\n    throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Invalid shard name : \" +  shard + \" in collection : \" +  collectionName);\n  }\n\n  deleteCore(slice, collectionName, replicaName, message, shard, results, onComplete,  parallel);\n\n}",
        "accept_response": "@SuppressWarnings(\"unchecked\")\nvoid deleteReplica(ClusterState clusterState, ZkNodeProps message, NamedList results, Runnable onComplete)\n        throws KeeperException, InterruptedException {\n  log.debug(\"deleteReplica() : {}\", Utils.toJSONString(message));\n  boolean parallel = message.getBool(\"parallel\", false);\n\n  //If a count is specified the strategy needs be different\n  if (message.getStr(COUNT_PROP) != null) {\n    deleteReplicaBasedOnCount(clusterState, message, results, onComplete, parallel);\n    return;\n  }\n\n\n  ocmh.checkRequired(message, COLLECTION_PROP, SHARD_ID_PROP, REPLICA_PROP);\n  String collectionName = message.getStr(COLLECTION_PROP);\n  String shard = message.getStr(SHARD_ID_PROP);\n  String replicaName = message.getStr(REPLICA_PROP);\n\n  DocCollection coll = clusterState.getCollection(collectionName);\n  Slice slice = coll.getSlice(shard);\n  if (slice == null) {\n    throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Invalid shard name : \" +  shard + \" in collection : \" +  collectionName);\n  }\n\n  deleteCore(slice, collectionName, replicaName, message, shard, results, onComplete,  parallel);\n\n}",
        "reject_response": "@SuppressWarnings(\"unchecked\")\nvoid deleteReplica(ClusterState clusterState, ZkNodeProps message, NamedList results, Runnable onComplete)\n        throws KeeperException, InterruptedException {\n  log.info(\"deleteReplica() : {}\", Utils.toJSONString(message));\n  boolean parallel = message.getBool(\"parallel\", false);\n\n  //If a count is specified the strategy needs be different\n  if (message.getStr(COUNT_PROP) != null) {\n    deleteReplicaBasedOnCount(clusterState, message, results, onComplete, parallel);\n    return;\n  }\n\n\n  ocmh.checkRequired(message, COLLECTION_PROP, SHARD_ID_PROP, REPLICA_PROP);\n  String collectionName = message.getStr(COLLECTION_PROP);\n  String shard = message.getStr(SHARD_ID_PROP);\n  String replicaName = message.getStr(REPLICA_PROP);\n\n  DocCollection coll = clusterState.getCollection(collectionName);\n  Slice slice = coll.getSlice(shard);\n  if (slice == null) {\n    throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Invalid shard name : \" +  shard + \" in collection : \" +  collectionName);\n  }\n\n  deleteCore(slice, collectionName, replicaName, message, shard, results, onComplete,  parallel);\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3187,
        "instruction": "@Override\npublic void failApplicationAttempt(ApplicationAttemptId attemptId)\n    throws YarnException, IOException {\n  FailApplicationAttemptRequest request =\n      Records.newRecord(FailApplicationAttemptRequest.class);\n  request.setApplicationAttemptId(attemptId);\n  rmClient.failApplicationAttempt(request);\n}",
        "accept_response": "@Override\npublic void failApplicationAttempt(ApplicationAttemptId attemptId)\n    throws YarnException, IOException {\n  LOG.info(\"Failing application attempt {}.\", attemptId);\n  FailApplicationAttemptRequest request =\n      Records.newRecord(FailApplicationAttemptRequest.class);\n  request.setApplicationAttemptId(attemptId);\n  rmClient.failApplicationAttempt(request);\n}",
        "reject_response": "@Override\npublic void failApplicationAttempt(ApplicationAttemptId attemptId)\n    throws YarnException, IOException {\n  LOG.info(\"Failing application attempt \" + attemptId);\n  FailApplicationAttemptRequest request =\n      Records.newRecord(FailApplicationAttemptRequest.class);\n  request.setApplicationAttemptId(attemptId);\n  rmClient.failApplicationAttempt(request);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3188,
        "instruction": "@VisibleForTesting\norg.apache.hadoop.security.token.Token<TimelineDelegationTokenIdentifier>\n    getTimelineDelegationToken() throws IOException, YarnException {\n  try {\n    // Only reachable when both security and timeline service are enabled.\n    if (timelineClient == null) {\n      synchronized (this) {\n        if (timelineClient == null) {\n          TimelineClient tlClient = createTimelineClient();\n          tlClient.init(getConfig());\n          tlClient.start();\n          // Assign value to timeline client variable only\n          // when it is fully initiated. In order to avoid\n          // other threads to see partially initialized object.\n          this.timelineClient = tlClient;\n        }\n      }\n    }\n    return timelineClient.getDelegationToken(timelineDTRenewer);\n  } catch (Exception e) {\n    if (timelineServiceBestEffort) {\n      return null;\n    }\n    throw new IOException(e);\n  } catch (NoClassDefFoundError e) {\n    NoClassDefFoundError wrappedError = new NoClassDefFoundError(\n        e.getMessage() + \". It appears that the timeline client \"\n            + \"failed to initiate because an incompatible dependency \"\n            + \"in classpath. If timeline service is optional to this \"\n            + \"client, try to work around by setting \"\n            + YarnConfiguration.TIMELINE_SERVICE_ENABLED\n            + \" to false in client configuration.\");\n    wrappedError.setStackTrace(e.getStackTrace());\n    throw wrappedError;\n  }\n}",
        "accept_response": "@VisibleForTesting\norg.apache.hadoop.security.token.Token<TimelineDelegationTokenIdentifier>\n    getTimelineDelegationToken() throws IOException, YarnException {\n  try {\n    // Only reachable when both security and timeline service are enabled.\n    if (timelineClient == null) {\n      synchronized (this) {\n        if (timelineClient == null) {\n          TimelineClient tlClient = createTimelineClient();\n          tlClient.init(getConfig());\n          tlClient.start();\n          // Assign value to timeline client variable only\n          // when it is fully initiated. In order to avoid\n          // other threads to see partially initialized object.\n          this.timelineClient = tlClient;\n        }\n      }\n    }\n    return timelineClient.getDelegationToken(timelineDTRenewer);\n  } catch (Exception e) {\n    if (timelineServiceBestEffort) {\n      LOG.warn(\"Failed to get delegation token from the timeline server: {}\", e.getMessage());\n      return null;\n    }\n    throw new IOException(e);\n  } catch (NoClassDefFoundError e) {\n    NoClassDefFoundError wrappedError = new NoClassDefFoundError(\n        e.getMessage() + \". It appears that the timeline client \"\n            + \"failed to initiate because an incompatible dependency \"\n            + \"in classpath. If timeline service is optional to this \"\n            + \"client, try to work around by setting \"\n            + YarnConfiguration.TIMELINE_SERVICE_ENABLED\n            + \" to false in client configuration.\");\n    wrappedError.setStackTrace(e.getStackTrace());\n    throw wrappedError;\n  }\n}",
        "reject_response": "@VisibleForTesting\norg.apache.hadoop.security.token.Token<TimelineDelegationTokenIdentifier>\n    getTimelineDelegationToken() throws IOException, YarnException {\n  try {\n    // Only reachable when both security and timeline service are enabled.\n    if (timelineClient == null) {\n      synchronized (this) {\n        if (timelineClient == null) {\n          TimelineClient tlClient = createTimelineClient();\n          tlClient.init(getConfig());\n          tlClient.start();\n          // Assign value to timeline client variable only\n          // when it is fully initiated. In order to avoid\n          // other threads to see partially initialized object.\n          this.timelineClient = tlClient;\n        }\n      }\n    }\n    return timelineClient.getDelegationToken(timelineDTRenewer);\n  } catch (Exception e) {\n    if (timelineServiceBestEffort) {\n      LOG.warn(\"Failed to get delegation token from the timeline server: \"\n          + e.getMessage());\n      return null;\n    }\n    throw new IOException(e);\n  } catch (NoClassDefFoundError e) {\n    NoClassDefFoundError wrappedError = new NoClassDefFoundError(\n        e.getMessage() + \". It appears that the timeline client \"\n            + \"failed to initiate because an incompatible dependency \"\n            + \"in classpath. If timeline service is optional to this \"\n            + \"client, try to work around by setting \"\n            + YarnConfiguration.TIMELINE_SERVICE_ENABLED\n            + \" to false in client configuration.\");\n    wrappedError.setStackTrace(e.getStackTrace());\n    throw wrappedError;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2533,
        "instruction": "@VisibleForTesting\nvoid setupMessagingPipeline(InetAddressAndPort from, int useMessagingVersion, int maxMessagingVersion, ChannelPipeline pipeline)\n{\n    handshakeTimeout.cancel(true);\n    // record the \"true\" endpoint, i.e. the one the peer is identified with, as opposed to the socket it connected over\n    instance().versions.set(from, maxMessagingVersion);\n\n    BufferPool.setRecycleWhenFreeForCurrentThread(false);\n    BufferPoolAllocator allocator = GlobalBufferPoolAllocator.instance;\n    if (initiate.type == ConnectionType.LARGE_MESSAGES)\n    {\n        // for large messages, swap the global pool allocator for a local one, to optimise utilisation of chunks\n        allocator = new LocalBufferPoolAllocator(pipeline.channel().eventLoop());\n        pipeline.channel().config().setAllocator(allocator);\n    }\n\n    FrameDecoder frameDecoder;\n    switch (initiate.framing)\n    {\n        case LZ4:\n        {\n            if (useMessagingVersion >= VERSION_40)\n                frameDecoder = FrameDecoderLZ4.fast(allocator);\n            else\n                frameDecoder = new FrameDecoderLegacyLZ4(allocator, useMessagingVersion);\n            break;\n        }\n        case CRC:\n        {\n            if (useMessagingVersion >= VERSION_40)\n            {\n                frameDecoder = FrameDecoderCrc.create(allocator);\n                break;\n            }\n        }\n        case UNPROTECTED:\n        {\n            if (useMessagingVersion >= VERSION_40)\n                frameDecoder = new FrameDecoderUnprotected(allocator);\n            else\n                frameDecoder = new FrameDecoderLegacy(allocator, useMessagingVersion);\n            break;\n        }\n        default:\n            throw new AssertionError();\n    }\n\n    frameDecoder.addLastTo(pipeline);\n\n    InboundMessageHandler handler =\n        settings.handlers.apply(from).createHandler(frameDecoder, initiate.type, pipeline.channel(), useMessagingVersion);\n\n\n    pipeline.addLast(\"deserialize\", handler);\n\n    pipeline.remove(this);\n}",
        "accept_response": "@VisibleForTesting\nvoid setupMessagingPipeline(InetAddressAndPort from, int useMessagingVersion, int maxMessagingVersion, ChannelPipeline pipeline)\n{\n    handshakeTimeout.cancel(true);\n    // record the \"true\" endpoint, i.e. the one the peer is identified with, as opposed to the socket it connected over\n    instance().versions.set(from, maxMessagingVersion);\n\n    BufferPool.setRecycleWhenFreeForCurrentThread(false);\n    BufferPoolAllocator allocator = GlobalBufferPoolAllocator.instance;\n    if (initiate.type == ConnectionType.LARGE_MESSAGES)\n    {\n        // for large messages, swap the global pool allocator for a local one, to optimise utilisation of chunks\n        allocator = new LocalBufferPoolAllocator(pipeline.channel().eventLoop());\n        pipeline.channel().config().setAllocator(allocator);\n    }\n\n    FrameDecoder frameDecoder;\n    switch (initiate.framing)\n    {\n        case LZ4:\n        {\n            if (useMessagingVersion >= VERSION_40)\n                frameDecoder = FrameDecoderLZ4.fast(allocator);\n            else\n                frameDecoder = new FrameDecoderLegacyLZ4(allocator, useMessagingVersion);\n            break;\n        }\n        case CRC:\n        {\n            if (useMessagingVersion >= VERSION_40)\n            {\n                frameDecoder = FrameDecoderCrc.create(allocator);\n                break;\n            }\n        }\n        case UNPROTECTED:\n        {\n            if (useMessagingVersion >= VERSION_40)\n                frameDecoder = new FrameDecoderUnprotected(allocator);\n            else\n                frameDecoder = new FrameDecoderLegacy(allocator, useMessagingVersion);\n            break;\n        }\n        default:\n            throw new AssertionError();\n    }\n\n    frameDecoder.addLastTo(pipeline);\n\n    InboundMessageHandler handler =\n        settings.handlers.apply(from).createHandler(frameDecoder, initiate.type, pipeline.channel(), useMessagingVersion);\n\n    logger.info(\"{} messaging connection established, version = {}, framing = {}, encryption = {}\",\n                handler.id(true),\n                useMessagingVersion,\n                initiate.framing,\n                pipeline.get(\"ssl\") != null ? encryptionLogStatement(pipeline.channel(), settings.encryption) : \"disabled\");\n\n    pipeline.addLast(\"deserialize\", handler);\n\n    pipeline.remove(this);\n}",
        "reject_response": "@VisibleForTesting\nvoid setupMessagingPipeline(InetAddressAndPort from, int useMessagingVersion, int maxMessagingVersion, ChannelPipeline pipeline)\n{\n    handshakeTimeout.cancel(true);\n    // record the \"true\" endpoint, i.e. the one the peer is identified with, as opposed to the socket it connected over\n    instance().versions.set(from, maxMessagingVersion);\n\n    BufferPool.setRecycleWhenFreeForCurrentThread(false);\n    BufferPoolAllocator allocator = GlobalBufferPoolAllocator.instance;\n    if (initiate.type == ConnectionType.LARGE_MESSAGES)\n    {\n        // for large messages, swap the global pool allocator for a local one, to optimise utilisation of chunks\n        allocator = new LocalBufferPoolAllocator(pipeline.channel().eventLoop());\n        pipeline.channel().config().setAllocator(allocator);\n    }\n\n    FrameDecoder frameDecoder;\n    switch (initiate.framing)\n    {\n        case LZ4:\n        {\n            if (useMessagingVersion >= VERSION_40)\n                frameDecoder = FrameDecoderLZ4.fast(allocator);\n            else\n                frameDecoder = new FrameDecoderLegacyLZ4(allocator, useMessagingVersion);\n            break;\n        }\n        case CRC:\n        {\n            if (useMessagingVersion >= VERSION_40)\n            {\n                frameDecoder = FrameDecoderCrc.create(allocator);\n                break;\n            }\n        }\n        case UNPROTECTED:\n        {\n            if (useMessagingVersion >= VERSION_40)\n                frameDecoder = new FrameDecoderUnprotected(allocator);\n            else\n                frameDecoder = new FrameDecoderLegacy(allocator, useMessagingVersion);\n            break;\n        }\n        default:\n            throw new AssertionError();\n    }\n\n    frameDecoder.addLastTo(pipeline);\n\n    InboundMessageHandler handler =\n        settings.handlers.apply(from).createHandler(frameDecoder, initiate.type, pipeline.channel(), useMessagingVersion);\n\n    logger.info(\"{} connection established, version = {}, framing = {}, encryption = {}\",\n\n    pipeline.addLast(\"deserialize\", handler);\n\n    pipeline.remove(this);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2633,
        "instruction": "private void registerPulseUrlToManager(JMXConnector connection)\n    throws IOException, AttributeNotFoundException, InstanceNotFoundException, MBeanException,\n    ReflectionException, MalformedObjectNameException, InvalidAttributeValueException {\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n  // Register Pulse URL if not already present in the JMX Manager\n  if (connection != null) {\n    MBeanServerConnection mbsc = connection.getMBeanServerConnection();\n\n    Set<ObjectName> mbeans = mbsc.queryNames(this.MBEAN_OBJECT_NAME_MEMBER_MANAGER, null);\n\n    for (ObjectName mbeanName : mbeans) {\n      String presentUrl =\n          (String) mbsc.getAttribute(mbeanName, PulseConstants.MBEAN_MANAGER_ATTRIBUTE_PULSEURL);\n      String pulseWebAppUrl = repository.getPulseWebAppUrl();\n      if (pulseWebAppUrl != null && (presentUrl == null || !pulseWebAppUrl.equals(presentUrl))) {\n        logger.debug(resourceBundle.getString(\"LOG_MSG_SETTING_APP_URL_TO_MANAGER\"));\n        Attribute pulseUrlAttr =\n            new Attribute(PulseConstants.MBEAN_MANAGER_ATTRIBUTE_PULSEURL, pulseWebAppUrl);\n        mbsc.setAttribute(mbeanName, pulseUrlAttr);\n      } else {\n        logger.debug(resourceBundle.getString(\"LOG_MSG_APP_URL_ALREADY_PRESENT_IN_MANAGER\"));\n      }\n    }\n  }\n}",
        "accept_response": "private void registerPulseUrlToManager(JMXConnector connection)\n    throws IOException, AttributeNotFoundException, InstanceNotFoundException, MBeanException,\n    ReflectionException, MalformedObjectNameException, InvalidAttributeValueException {\n  logger.info(resourceBundle.getString(\"LOG_MSG_REGISTERING_APP_URL_TO_MANAGER\"));\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n  // Register Pulse URL if not already present in the JMX Manager\n  if (connection != null) {\n    MBeanServerConnection mbsc = connection.getMBeanServerConnection();\n\n    Set<ObjectName> mbeans = mbsc.queryNames(this.MBEAN_OBJECT_NAME_MEMBER_MANAGER, null);\n\n    for (ObjectName mbeanName : mbeans) {\n      String presentUrl =\n          (String) mbsc.getAttribute(mbeanName, PulseConstants.MBEAN_MANAGER_ATTRIBUTE_PULSEURL);\n      String pulseWebAppUrl = repository.getPulseWebAppUrl();\n      if (pulseWebAppUrl != null && (presentUrl == null || !pulseWebAppUrl.equals(presentUrl))) {\n        logger.debug(resourceBundle.getString(\"LOG_MSG_SETTING_APP_URL_TO_MANAGER\"));\n        Attribute pulseUrlAttr =\n            new Attribute(PulseConstants.MBEAN_MANAGER_ATTRIBUTE_PULSEURL, pulseWebAppUrl);\n        mbsc.setAttribute(mbeanName, pulseUrlAttr);\n      } else {\n        logger.debug(resourceBundle.getString(\"LOG_MSG_APP_URL_ALREADY_PRESENT_IN_MANAGER\"));\n      }\n    }\n  }\n}",
        "reject_response": "private void registerPulseUrlToManager(JMXConnector connection)\n    throws IOException, AttributeNotFoundException, InstanceNotFoundException, MBeanException,\n    ReflectionException, MalformedObjectNameException, InvalidAttributeValueException {\n  if (LOGGER.infoEnabled()) {\n    LOGGER.info(resourceBundle.getString(\"LOG_MSG_REGISTERING_APP_URL_TO_MANAGER\"));\n  }\n\n  // Reference to repository\n  Repository repository = Repository.get();\n\n  // Register Pulse URL if not already present in the JMX Manager\n  if (connection != null) {\n    MBeanServerConnection mbsc = connection.getMBeanServerConnection();\n\n    Set<ObjectName> mbeans = mbsc.queryNames(this.MBEAN_OBJECT_NAME_MEMBER_MANAGER, null);\n\n    for (ObjectName mbeanName : mbeans) {\n      String presentUrl =\n          (String) mbsc.getAttribute(mbeanName, PulseConstants.MBEAN_MANAGER_ATTRIBUTE_PULSEURL);\n      String pulseWebAppUrl = repository.getPulseWebAppUrl();\n      if (pulseWebAppUrl != null && (presentUrl == null || !pulseWebAppUrl.equals(presentUrl))) {\n        logger.debug(resourceBundle.getString(\"LOG_MSG_SETTING_APP_URL_TO_MANAGER\"));\n        Attribute pulseUrlAttr =\n            new Attribute(PulseConstants.MBEAN_MANAGER_ATTRIBUTE_PULSEURL, pulseWebAppUrl);\n        mbsc.setAttribute(mbeanName, pulseUrlAttr);\n      } else {\n        logger.debug(resourceBundle.getString(\"LOG_MSG_APP_URL_ALREADY_PRESENT_IN_MANAGER\"));\n      }\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2938,
        "instruction": "@Override\npublic int run(String[] args) throws Exception {\n    Options options = new Options();\n\n    options.addOption(OPTION_CUBE_NAME);\n    options.addOption(OPTION_SEGMENT_ID);\n    options.addOption(OPTION_PARTITION_FILE_PATH);\n    options.addOption(OPTION_CUBOID_MODE);\n    options.addOption(OPTION_HBASE_CONF_PATH);\n    parseOptions(options, args);\n\n    partitionFilePath = new Path(getOptionValue(OPTION_PARTITION_FILE_PATH));\n\n    String cubeName = getOptionValue(OPTION_CUBE_NAME).toUpperCase(Locale.ROOT);\n    CubeManager cubeMgr = CubeManager.getInstance(KylinConfig.getInstanceFromEnv());\n    cube = cubeMgr.getCube(cubeName);\n    cubeDesc = cube.getDescriptor();\n    kylinConfig = cube.getConfig();\n    segmentID = getOptionValue(OPTION_SEGMENT_ID);\n    cuboidModeName = getOptionValue(OPTION_CUBOID_MODE);\n    hbaseConfPath = getOptionValue(OPTION_HBASE_CONF_PATH);\n    CubeSegment cubeSegment = cube.getSegmentById(segmentID);\n\n    byte[][] splitKeys;\n    Map<Long, Double> cuboidSizeMap = new CubeStatsReader(cubeSegment, kylinConfig).getCuboidSizeMap();\n\n    // for cube planner, will keep cuboidSizeMap unchanged if cube planner is disabled\n    Set<Long> buildingCuboids = cube.getCuboidsByMode(cuboidModeName);\n    if (buildingCuboids != null && !buildingCuboids.isEmpty()) {\n        Map<Long, Double> optimizedCuboidSizeMap = Maps.newHashMapWithExpectedSize(buildingCuboids.size());\n        for (Long cuboid : buildingCuboids) {\n            Double cuboidSize = cuboidSizeMap.get(cuboid);\n            if (cuboidSize == null) {\n                cuboidSize = 0.0;\n            }\n            optimizedCuboidSizeMap.put(cuboid, cuboidSize);\n        }\n        cuboidSizeMap = optimizedCuboidSizeMap;\n    }\n\n    splitKeys = getRegionSplitsFromCuboidStatistics(cuboidSizeMap, kylinConfig, cubeSegment,\n            partitionFilePath.getParent());\n\n    CubeHTableUtil.createHTable(cubeSegment, splitKeys);\n\n    // export configuration in advance to avoid connecting to hbase from spark\n    if (cubeDesc.getEngineType()== IEngineAware.ID_SPARK){\n        exportHBaseConfiguration(cubeSegment.getStorageLocationIdentifier());\n    }\n    return 0;\n}",
        "accept_response": "@Override\npublic int run(String[] args) throws Exception {\n    Options options = new Options();\n\n    options.addOption(OPTION_CUBE_NAME);\n    options.addOption(OPTION_SEGMENT_ID);\n    options.addOption(OPTION_PARTITION_FILE_PATH);\n    options.addOption(OPTION_CUBOID_MODE);\n    options.addOption(OPTION_HBASE_CONF_PATH);\n    parseOptions(options, args);\n\n    partitionFilePath = new Path(getOptionValue(OPTION_PARTITION_FILE_PATH));\n\n    String cubeName = getOptionValue(OPTION_CUBE_NAME).toUpperCase(Locale.ROOT);\n    CubeManager cubeMgr = CubeManager.getInstance(KylinConfig.getInstanceFromEnv());\n    cube = cubeMgr.getCube(cubeName);\n    cubeDesc = cube.getDescriptor();\n    kylinConfig = cube.getConfig();\n    segmentID = getOptionValue(OPTION_SEGMENT_ID);\n    cuboidModeName = getOptionValue(OPTION_CUBOID_MODE);\n    hbaseConfPath = getOptionValue(OPTION_HBASE_CONF_PATH);\n    CubeSegment cubeSegment = cube.getSegmentById(segmentID);\n\n    byte[][] splitKeys;\n    Map<Long, Double> cuboidSizeMap = new CubeStatsReader(cubeSegment, kylinConfig).getCuboidSizeMap();\n\n    // for cube planner, will keep cuboidSizeMap unchanged if cube planner is disabled\n    Set<Long> buildingCuboids = cube.getCuboidsByMode(cuboidModeName);\n    if (buildingCuboids != null && !buildingCuboids.isEmpty()) {\n        Map<Long, Double> optimizedCuboidSizeMap = Maps.newHashMapWithExpectedSize(buildingCuboids.size());\n        for (Long cuboid : buildingCuboids) {\n            Double cuboidSize = cuboidSizeMap.get(cuboid);\n            if (cuboidSize == null) {\n                logger.warn(\"{} cuboid's size is null will replace by 0\", cuboid);\n                cuboidSize = 0.0;\n            }\n            optimizedCuboidSizeMap.put(cuboid, cuboidSize);\n        }\n        cuboidSizeMap = optimizedCuboidSizeMap;\n    }\n\n    splitKeys = getRegionSplitsFromCuboidStatistics(cuboidSizeMap, kylinConfig, cubeSegment,\n            partitionFilePath.getParent());\n\n    CubeHTableUtil.createHTable(cubeSegment, splitKeys);\n\n    // export configuration in advance to avoid connecting to hbase from spark\n    if (cubeDesc.getEngineType()== IEngineAware.ID_SPARK){\n        exportHBaseConfiguration(cubeSegment.getStorageLocationIdentifier());\n    }\n    return 0;\n}",
        "reject_response": "@Override\npublic int run(String[] args) throws Exception {\n    Options options = new Options();\n\n    options.addOption(OPTION_CUBE_NAME);\n    options.addOption(OPTION_SEGMENT_ID);\n    options.addOption(OPTION_PARTITION_FILE_PATH);\n    options.addOption(OPTION_CUBOID_MODE);\n    options.addOption(OPTION_HBASE_CONF_PATH);\n    parseOptions(options, args);\n\n    partitionFilePath = new Path(getOptionValue(OPTION_PARTITION_FILE_PATH));\n\n    String cubeName = getOptionValue(OPTION_CUBE_NAME).toUpperCase(Locale.ROOT);\n    CubeManager cubeMgr = CubeManager.getInstance(KylinConfig.getInstanceFromEnv());\n    cube = cubeMgr.getCube(cubeName);\n    cubeDesc = cube.getDescriptor();\n    kylinConfig = cube.getConfig();\n    segmentID = getOptionValue(OPTION_SEGMENT_ID);\n    cuboidModeName = getOptionValue(OPTION_CUBOID_MODE);\n    hbaseConfPath = getOptionValue(OPTION_HBASE_CONF_PATH);\n    CubeSegment cubeSegment = cube.getSegmentById(segmentID);\n\n    byte[][] splitKeys;\n    Map<Long, Double> cuboidSizeMap = new CubeStatsReader(cubeSegment, kylinConfig).getCuboidSizeMap();\n\n    // for cube planner, will keep cuboidSizeMap unchanged if cube planner is disabled\n    Set<Long> buildingCuboids = cube.getCuboidsByMode(cuboidModeName);\n    if (buildingCuboids != null && !buildingCuboids.isEmpty()) {\n        Map<Long, Double> optimizedCuboidSizeMap = Maps.newHashMapWithExpectedSize(buildingCuboids.size());\n        for (Long cuboid : buildingCuboids) {\n            Double cuboidSize = cuboidSizeMap.get(cuboid);\n            if (cuboidSize == null) {\n                logger.warn(\"{0} cuboid's size is null will replace by 0\", cuboid);\n                cuboidSize = 0.0;\n            }\n            optimizedCuboidSizeMap.put(cuboid, cuboidSize);\n        }\n        cuboidSizeMap = optimizedCuboidSizeMap;\n    }\n\n    splitKeys = getRegionSplitsFromCuboidStatistics(cuboidSizeMap, kylinConfig, cubeSegment,\n            partitionFilePath.getParent());\n\n    CubeHTableUtil.createHTable(cubeSegment, splitKeys);\n\n    // export configuration in advance to avoid connecting to hbase from spark\n    if (cubeDesc.getEngineType()== IEngineAware.ID_SPARK){\n        exportHBaseConfiguration(cubeSegment.getStorageLocationIdentifier());\n    }\n    return 0;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2720,
        "instruction": "protected Path closeRecoveredEditsWriter(RecoveredEditsWriter editsWriter,\n    List<IOException> thrown) throws IOException {\n  try {\n    editsWriter.writer.close();\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not close recovered edits at \" + editsWriter.path;\n    LOG.error(errorMsg, ioe);\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  final String msg = \"Closed recovered edits writer path=\" + editsWriter.path + \" (wrote \"\n    + editsWriter.editsWritten + \" edits, skipped \" + editsWriter.editsSkipped + \" edits in \" + (\n    editsWriter.nanosSpent / 1000 / 1000) + \" ms)\";\n  LOG.info(msg);\n  updateStatusWithMsg(msg);\n  if (editsWriter.editsWritten == 0) {\n    // just remove the empty recovered.edits file\n    if (walSplitter.walFS.exists(editsWriter.path)\n        && !walSplitter.walFS.delete(editsWriter.path, false)) {\n      final String errorMsg = \"Failed deleting empty \" + editsWriter.path;\n      LOG.warn(errorMsg);\n      updateStatusWithMsg(errorMsg);\n      throw new IOException(\"Failed deleting empty  \" + editsWriter.path);\n    }\n    return null;\n  }\n\n  Path dst = getCompletedRecoveredEditsFilePath(editsWriter.path,\n    regionMaximumEditLogSeqNum.get(Bytes.toString(editsWriter.encodedRegionName)));\n  try {\n    if (!dst.equals(editsWriter.path) && walSplitter.walFS.exists(dst)) {\n      deleteOneWithFewerEntries(editsWriter, dst);\n    }\n    // Skip the unit tests which create a splitter that reads and\n    // writes the data without touching disk.\n    // TestHLogSplit#testThreading is an example.\n    if (walSplitter.walFS.exists(editsWriter.path)) {\n      if (!walSplitter.walFS.rename(editsWriter.path, dst)) {\n        final String errorMsg =\n          \"Failed renaming recovered edits \" + editsWriter.path + \" to \" + dst;\n        updateStatusWithMsg(errorMsg);\n        throw new IOException(errorMsg);\n      }\n      final String renameEditMsg = \"Rename recovered edits \" + editsWriter.path + \" to \" + dst;\n      updateStatusWithMsg(renameEditMsg);\n    }\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not rename recovered edits \" + editsWriter.path\n      + \" to \" + dst;\n    LOG.error(errorMsg, ioe);\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  return dst;\n}",
        "accept_response": "protected Path closeRecoveredEditsWriter(RecoveredEditsWriter editsWriter,\n    List<IOException> thrown) throws IOException {\n  try {\n    editsWriter.writer.close();\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not close recovered edits at \" + editsWriter.path;\n    LOG.error(errorMsg, ioe);\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  final String msg = \"Closed recovered edits writer path=\" + editsWriter.path + \" (wrote \"\n    + editsWriter.editsWritten + \" edits, skipped \" + editsWriter.editsSkipped + \" edits in \" + (\n    editsWriter.nanosSpent / 1000 / 1000) + \" ms)\";\n  LOG.info(msg);\n  updateStatusWithMsg(msg);\n  if (editsWriter.editsWritten == 0) {\n    // just remove the empty recovered.edits file\n    if (walSplitter.walFS.exists(editsWriter.path)\n        && !walSplitter.walFS.delete(editsWriter.path, false)) {\n      final String errorMsg = \"Failed deleting empty \" + editsWriter.path;\n      LOG.warn(errorMsg);\n      updateStatusWithMsg(errorMsg);\n      throw new IOException(\"Failed deleting empty  \" + editsWriter.path);\n    }\n    return null;\n  }\n\n  Path dst = getCompletedRecoveredEditsFilePath(editsWriter.path,\n    regionMaximumEditLogSeqNum.get(Bytes.toString(editsWriter.encodedRegionName)));\n  try {\n    if (!dst.equals(editsWriter.path) && walSplitter.walFS.exists(dst)) {\n      deleteOneWithFewerEntries(editsWriter, dst);\n    }\n    // Skip the unit tests which create a splitter that reads and\n    // writes the data without touching disk.\n    // TestHLogSplit#testThreading is an example.\n    if (walSplitter.walFS.exists(editsWriter.path)) {\n      if (!walSplitter.walFS.rename(editsWriter.path, dst)) {\n        final String errorMsg =\n          \"Failed renaming recovered edits \" + editsWriter.path + \" to \" + dst;\n        updateStatusWithMsg(errorMsg);\n        throw new IOException(errorMsg);\n      }\n      final String renameEditMsg = \"Rename recovered edits \" + editsWriter.path + \" to \" + dst;\n      LOG.info(renameEditMsg);\n      updateStatusWithMsg(renameEditMsg);\n    }\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not rename recovered edits \" + editsWriter.path\n      + \" to \" + dst;\n    LOG.error(errorMsg, ioe);\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  return dst;\n}",
        "reject_response": "protected Path closeRecoveredEditsWriter(RecoveredEditsWriter editsWriter,\n    List<IOException> thrown) throws IOException {\n  try {\n    editsWriter.writer.close();\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not close recovered edits at \" + editsWriter.path;\n    LOG.error(errorMsg, ioe);\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  final String msg = \"Closed recovered edits writer path=\" + editsWriter.path + \" (wrote \"\n    + editsWriter.editsWritten + \" edits, skipped \" + editsWriter.editsSkipped + \" edits in \" + (\n    editsWriter.nanosSpent / 1000 / 1000) + \" ms)\";\n  LOG.info(msg);\n  updateStatusWithMsg(msg);\n  if (editsWriter.editsWritten == 0) {\n    // just remove the empty recovered.edits file\n    if (walSplitter.walFS.exists(editsWriter.path)\n        && !walSplitter.walFS.delete(editsWriter.path, false)) {\n      final String errorMsg = \"Failed deleting empty \" + editsWriter.path;\n      LOG.warn(errorMsg);\n      updateStatusWithMsg(errorMsg);\n      throw new IOException(\"Failed deleting empty  \" + editsWriter.path);\n    }\n    return null;\n  }\n\n  Path dst = getCompletedRecoveredEditsFilePath(editsWriter.path,\n    regionMaximumEditLogSeqNum.get(Bytes.toString(editsWriter.encodedRegionName)));\n  try {\n    if (!dst.equals(editsWriter.path) && walSplitter.walFS.exists(dst)) {\n      deleteOneWithFewerEntries(editsWriter, dst);\n    }\n    // Skip the unit tests which create a splitter that reads and\n    // writes the data without touching disk.\n    // TestHLogSplit#testThreading is an example.\n    if (walSplitter.walFS.exists(editsWriter.path)) {\n      if (!walSplitter.walFS.rename(editsWriter.path, dst)) {\n        final String errorMsg =\n          \"Failed renaming recovered edits \" + editsWriter.path + \" to \" + dst;\n        updateStatusWithMsg(errorMsg);\n        throw new IOException(errorMsg);\n      }\n      final String renameEditMsg = \"Rename recovered edits \" + editsWriter.path + \" to \" + dst;\n      LOG.info(\"Rename recovered edits {} to {}\", editsWriter.path, dst);\n      updateStatusWithMsg(renameEditMsg);\n    }\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not rename recovered edits \" + editsWriter.path\n      + \" to \" + dst;\n    LOG.error(errorMsg, ioe);\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  return dst;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2894,
        "instruction": "public void evaluate(MailAdapter mail, Node startNode)\n        throws SieveException {\n    final SieveContext context = new BaseSieveContext(commandManager,\n            comparatorManager, testManager);\n    try {\n        // Ensure that the context is set on the mail\n        mail.setContext(context);\n\n        SieveParserVisitor visitor = new SieveParserVisitorImpl(context);\n        try {\n            // Evaluate the Nodes\n            startNode.jjtAccept(visitor, mail);\n\n        } catch (StopException ex) {\n            // Stop is OK\n        } catch (SieveException ex) {\n            throw ex;\n        }\n\n        // If after evaluating all of the nodes or stopping, implicitKeep is\n        // still\n        // in effect, add a Keep to the list of Actions.\n        if (context.getCommandStateManager().isImplicitKeep())\n            mail.addAction(new ActionKeep());\n\n        // Execute the List of Actions\n        try {\n            mail.executeActions();\n        } catch (SieveException ex) {\n            LOGGER.error(\"Evaluation failed.\", ex);\n            throw ex;\n        }\n    } finally {\n        // Tidy up by ensuring that a reference to the context is not held by the adapter.\n        // This prevents leaks when the adapter stores the context in a thread local variable.\n        mail.setContext(null);\n    }\n}",
        "accept_response": "public void evaluate(MailAdapter mail, Node startNode)\n        throws SieveException {\n    final SieveContext context = new BaseSieveContext(commandManager,\n            comparatorManager, testManager);\n    try {\n        // Ensure that the context is set on the mail\n        mail.setContext(context);\n\n        SieveParserVisitor visitor = new SieveParserVisitorImpl(context);\n        try {\n            // Evaluate the Nodes\n            startNode.jjtAccept(visitor, mail);\n\n        } catch (StopException ex) {\n            // Stop is OK\n        } catch (SieveException ex) {\n            LOGGER.error(\"Evaluation failed.\", ex);\n            throw ex;\n        }\n\n        // If after evaluating all of the nodes or stopping, implicitKeep is\n        // still\n        // in effect, add a Keep to the list of Actions.\n        if (context.getCommandStateManager().isImplicitKeep())\n            mail.addAction(new ActionKeep());\n\n        // Execute the List of Actions\n        try {\n            mail.executeActions();\n        } catch (SieveException ex) {\n            LOGGER.error(\"Evaluation failed.\", ex);\n            throw ex;\n        }\n    } finally {\n        // Tidy up by ensuring that a reference to the context is not held by the adapter.\n        // This prevents leaks when the adapter stores the context in a thread local variable.\n        mail.setContext(null);\n    }\n}",
        "reject_response": "public void evaluate(MailAdapter mail, Node startNode)\n        throws SieveException {\n    final SieveContext context = new BaseSieveContext(commandManager,\n            comparatorManager, testManager);\n    try {\n        // Ensure that the context is set on the mail\n        mail.setContext(context);\n\n        SieveParserVisitor visitor = new SieveParserVisitorImpl(context);\n        try {\n            // Evaluate the Nodes\n            startNode.jjtAccept(visitor, mail);\n\n        } catch (StopException ex) {\n            // Stop is OK\n        } catch (SieveException ex) {\n            if (log.isErrorEnabled())\n                log.error(\"Evaluation failed. Reason: \" + ex.getMessage());\n            if (log.isDebugEnabled())\n                log.debug(\"Evaluation failed.\", ex);\n            throw ex;\n        }\n\n        // If after evaluating all of the nodes or stopping, implicitKeep is\n        // still\n        // in effect, add a Keep to the list of Actions.\n        if (context.getCommandStateManager().isImplicitKeep())\n            mail.addAction(new ActionKeep());\n\n        // Execute the List of Actions\n        try {\n            mail.executeActions();\n        } catch (SieveException ex) {\n            LOGGER.error(\"Evaluation failed.\", ex);\n            throw ex;\n        }\n    } finally {\n        // Tidy up by ensuring that a reference to the context is not held by the adapter.\n        // This prevents leaks when the adapter stores the context in a thread local variable.\n        mail.setContext(null);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2382,
        "instruction": "@Override\npublic Collection<Text> listSplits(String tableName) throws TableNotFoundException, AccumuloSecurityException {\n\n  checkArgument(tableName != null, \"tableName is null\");\n\n  String tableId = Tables.getTableId(context.getInstance(), tableName);\n\n  TreeMap<KeyExtent,String> tabletLocations = new TreeMap<KeyExtent,String>();\n\n  while (true) {\n    try {\n      tabletLocations.clear();\n      // the following method throws AccumuloException for some conditions that should be retried\n      MetadataServicer.forTableId(context, tableId).getTabletLocations(tabletLocations);\n      break;\n    } catch (AccumuloSecurityException ase) {\n      throw ase;\n    } catch (Exception e) {\n      if (!Tables.exists(context.getInstance(), tableId)) {\n        throw new TableNotFoundException(tableId, tableName, null);\n      }\n\n      if (e instanceof RuntimeException && e.getCause() instanceof AccumuloSecurityException) {\n        throw (AccumuloSecurityException) e.getCause();\n      }\n\n      UtilWaitThread.sleep(3000);\n    }\n  }\n\n  ArrayList<Text> endRows = new ArrayList<Text>(tabletLocations.size());\n\n  for (KeyExtent ke : tabletLocations.keySet())\n    if (ke.getEndRow() != null)\n      endRows.add(ke.getEndRow());\n\n  return endRows;\n}",
        "accept_response": "@Override\npublic Collection<Text> listSplits(String tableName) throws TableNotFoundException, AccumuloSecurityException {\n\n  checkArgument(tableName != null, \"tableName is null\");\n\n  String tableId = Tables.getTableId(context.getInstance(), tableName);\n\n  TreeMap<KeyExtent,String> tabletLocations = new TreeMap<KeyExtent,String>();\n\n  while (true) {\n    try {\n      tabletLocations.clear();\n      // the following method throws AccumuloException for some conditions that should be retried\n      MetadataServicer.forTableId(context, tableId).getTabletLocations(tabletLocations);\n      break;\n    } catch (AccumuloSecurityException ase) {\n      throw ase;\n    } catch (Exception e) {\n      if (!Tables.exists(context.getInstance(), tableId)) {\n        throw new TableNotFoundException(tableId, tableName, null);\n      }\n\n      if (e instanceof RuntimeException && e.getCause() instanceof AccumuloSecurityException) {\n        throw (AccumuloSecurityException) e.getCause();\n      }\n\n      log.info(\"{} ... retrying ...\", e.getMessage());\n      UtilWaitThread.sleep(3000);\n    }\n  }\n\n  ArrayList<Text> endRows = new ArrayList<Text>(tabletLocations.size());\n\n  for (KeyExtent ke : tabletLocations.keySet())\n    if (ke.getEndRow() != null)\n      endRows.add(ke.getEndRow());\n\n  return endRows;\n}",
        "reject_response": "@Override\npublic Collection<Text> listSplits(String tableName) throws TableNotFoundException, AccumuloSecurityException {\n\n  checkArgument(tableName != null, \"tableName is null\");\n\n  String tableId = Tables.getTableId(context.getInstance(), tableName);\n\n  TreeMap<KeyExtent,String> tabletLocations = new TreeMap<KeyExtent,String>();\n\n  while (true) {\n    try {\n      tabletLocations.clear();\n      // the following method throws AccumuloException for some conditions that should be retried\n      MetadataServicer.forTableId(context, tableId).getTabletLocations(tabletLocations);\n      break;\n    } catch (AccumuloSecurityException ase) {\n      throw ase;\n    } catch (Exception e) {\n      if (!Tables.exists(context.getInstance(), tableId)) {\n        throw new TableNotFoundException(tableId, tableName, null);\n      }\n\n      if (e instanceof RuntimeException && e.getCause() instanceof AccumuloSecurityException) {\n        throw (AccumuloSecurityException) e.getCause();\n      }\n\n      log.info(e.getMessage() + \" ... retrying ...\");\n      UtilWaitThread.sleep(3000);\n    }\n  }\n\n  ArrayList<Text> endRows = new ArrayList<Text>(tabletLocations.size());\n\n  for (KeyExtent ke : tabletLocations.keySet())\n    if (ke.getEndRow() != null)\n      endRows.add(ke.getEndRow());\n\n  return endRows;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2712,
        "instruction": "public boolean isMergeable() {\n  if (!isAvailable()) {\n    LOG.debug(\"Region \" + this\n        + \" is not mergeable because it is closing or closed\");\n    return false;\n  }\n  if (hasReferences()) {\n    return false;\n  }\n\n  return true;\n}",
        "accept_response": "public boolean isMergeable() {\n  if (!isAvailable()) {\n    LOG.debug(\"Region \" + this\n        + \" is not mergeable because it is closing or closed\");\n    return false;\n  }\n  if (hasReferences()) {\n    LOG.debug(\"Region \" + this\n        + \" is not mergeable because it has references\");\n    return false;\n  }\n\n  return true;\n}",
        "reject_response": "public boolean isMergeable() {\n  if (!isAvailable()) {\n    LOG.debug(\"Region \" + this\n        + \" is not mergeable because it is closing or closed\");\n    return false;\n  }\n  if (hasReferences()) {\n    LOG.debug(\"Region \" + getRegionInfo().getRegionNameAsString()\n    return false;\n  }\n\n  return true;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2813,
        "instruction": "public void inject(GridDeployment dep, Class<?> depCls, Object target) throws IgniteCheckedException {\n    assert target != null;\n\n    if (log.isDebugEnabled())\n\n    // Unwrap Proxy object.\n    target = unwrapTarget(target);\n\n    inject(target, GridResourceIoc.AnnotationSet.GENERIC, dep, depCls);\n}",
        "accept_response": "public void inject(GridDeployment dep, Class<?> depCls, Object target) throws IgniteCheckedException {\n    assert target != null;\n\n    if (log.isDebugEnabled())\n        log.debug(S.toString(\"Injecting resources\", \"target\", target, true));\n\n    // Unwrap Proxy object.\n    target = unwrapTarget(target);\n\n    inject(target, GridResourceIoc.AnnotationSet.GENERIC, dep, depCls);\n}",
        "reject_response": "public void inject(GridDeployment dep, Class<?> depCls, Object target) throws IgniteCheckedException {\n    assert target != null;\n\n    if (log.isDebugEnabled())\n        log.debug(\"Injecting resources: \" + target);\n\n    // Unwrap Proxy object.\n    target = unwrapTarget(target);\n\n    inject(target, GridResourceIoc.AnnotationSet.GENERIC, dep, depCls);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2423,
        "instruction": "protected void configureInactivityMonitor(String heartBeatConfig) throws ProtocolException {\n\n    String[] keepAliveOpts = heartBeatConfig.split(Stomp.COMMA);\n\n    if (keepAliveOpts == null || keepAliveOpts.length != 2) {\n        throw new ProtocolException(\"Invalid heart-beat header:\" + heartBeatConfig, true);\n    } else {\n\n        try {\n            hbReadInterval = (Long.parseLong(keepAliveOpts[0]));\n            hbWriteInterval = Long.parseLong(keepAliveOpts[1]);\n        } catch(NumberFormatException e) {\n            throw new ProtocolException(\"Invalid heart-beat header:\" + heartBeatConfig, true);\n        }\n\n        try {\n            StompInactivityMonitor monitor = this.stompTransport.getInactivityMonitor();\n            monitor.setReadCheckTime((long) (hbReadInterval * hbGracePeriodMultiplier));\n            monitor.setInitialDelayTime(Math.min(hbReadInterval, hbWriteInterval));\n            monitor.setWriteCheckTime(hbWriteInterval);\n            monitor.startMonitoring();\n        } catch(Exception ex) {\n            hbReadInterval = 0;\n            hbWriteInterval = 0;\n        }\n    }\n}",
        "accept_response": "protected void configureInactivityMonitor(String heartBeatConfig) throws ProtocolException {\n\n    String[] keepAliveOpts = heartBeatConfig.split(Stomp.COMMA);\n\n    if (keepAliveOpts == null || keepAliveOpts.length != 2) {\n        throw new ProtocolException(\"Invalid heart-beat header:\" + heartBeatConfig, true);\n    } else {\n\n        try {\n            hbReadInterval = (Long.parseLong(keepAliveOpts[0]));\n            hbWriteInterval = Long.parseLong(keepAliveOpts[1]);\n        } catch(NumberFormatException e) {\n            throw new ProtocolException(\"Invalid heart-beat header:\" + heartBeatConfig, true);\n        }\n\n        try {\n            StompInactivityMonitor monitor = this.stompTransport.getInactivityMonitor();\n            monitor.setReadCheckTime((long) (hbReadInterval * hbGracePeriodMultiplier));\n            monitor.setInitialDelayTime(Math.min(hbReadInterval, hbWriteInterval));\n            monitor.setWriteCheckTime(hbWriteInterval);\n            monitor.startMonitoring();\n        } catch(Exception ex) {\n            hbReadInterval = 0;\n            hbWriteInterval = 0;\n        }\n\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Stomp Connect heartbeat conf RW[{},{}]\", hbReadInterval, hbWriteInterval);\n        }\n    }\n}",
        "reject_response": "protected void configureInactivityMonitor(String heartBeatConfig) throws ProtocolException {\n\n    String[] keepAliveOpts = heartBeatConfig.split(Stomp.COMMA);\n\n    if (keepAliveOpts == null || keepAliveOpts.length != 2) {\n        throw new ProtocolException(\"Invalid heart-beat header:\" + heartBeatConfig, true);\n    } else {\n\n        try {\n            hbReadInterval = (Long.parseLong(keepAliveOpts[0]));\n            hbWriteInterval = Long.parseLong(keepAliveOpts[1]);\n        } catch(NumberFormatException e) {\n            throw new ProtocolException(\"Invalid heart-beat header:\" + heartBeatConfig, true);\n        }\n\n        try {\n            StompInactivityMonitor monitor = this.stompTransport.getInactivityMonitor();\n            monitor.setReadCheckTime((long) (hbReadInterval * hbGracePeriodMultiplier));\n            monitor.setInitialDelayTime(Math.min(hbReadInterval, hbWriteInterval));\n            monitor.setWriteCheckTime(hbWriteInterval);\n            monitor.startMonitoring();\n        } catch(Exception ex) {\n            hbReadInterval = 0;\n            hbWriteInterval = 0;\n        }\n\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Stomp Connect heartbeat conf RW[\" + hbReadInterval + \",\" + hbWriteInterval + \"]\");\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2880,
        "instruction": "private void _parse(UpdateSink sink, Reader r) {\n    SPARQLParser11 parser = null ;\n    try {\n        parser = new SPARQLParser11(r) ;\n        parser.setUpdateSink(sink) ;\n        parser.UpdateUnit() ;\n    }\n    catch (org.apache.jena.sparql.lang.sparql_11.ParseException ex)\n    {\n        throw new QueryParseException(ex.getMessage(),\n                                      ex.currentToken.beginLine,\n                                      ex.currentToken.beginColumn) ;\n    }\n    catch (org.apache.jena.sparql.lang.sparql_11.TokenMgrError tErr)\n    {\n        // Last valid token : not the same as token error message - but this should not happen\n        int col = parser.token.endColumn ;\n        int line = parser.token.endLine ;\n        throw new QueryParseException(tErr.getMessage(), line, col) ; }\n\n    catch (UpdateException ex) { throw ex ; }\n    catch (JenaException ex)  { throw new QueryException(ex.getMessage(), ex) ; }\n    catch (Error err)\n    {\n        // The token stream can throw errors.\n        throw new QueryParseException(err.getMessage(), err, -1, -1) ;\n    }\n    catch (Throwable th)\n    {\n        throw new QueryException(th.getMessage(), th) ;\n    }\n}",
        "accept_response": "private void _parse(UpdateSink sink, Reader r) {\n    SPARQLParser11 parser = null ;\n    try {\n        parser = new SPARQLParser11(r) ;\n        parser.setUpdateSink(sink) ;\n        parser.UpdateUnit() ;\n    }\n    catch (org.apache.jena.sparql.lang.sparql_11.ParseException ex)\n    {\n        throw new QueryParseException(ex.getMessage(),\n                                      ex.currentToken.beginLine,\n                                      ex.currentToken.beginColumn) ;\n    }\n    catch (org.apache.jena.sparql.lang.sparql_11.TokenMgrError tErr)\n    {\n        // Last valid token : not the same as token error message - but this should not happen\n        int col = parser.token.endColumn ;\n        int line = parser.token.endLine ;\n        throw new QueryParseException(tErr.getMessage(), line, col) ; }\n\n    catch (UpdateException ex) { throw ex ; }\n    catch (JenaException ex)  { throw new QueryException(ex.getMessage(), ex) ; }\n    catch (Error err)\n    {\n        // The token stream can throw errors.\n        throw new QueryParseException(err.getMessage(), err, -1, -1) ;\n    }\n    catch (Throwable th)\n    {\n        Log.error(this, \"Unexpected throwable: \",th) ;\n        throw new QueryException(th.getMessage(), th) ;\n    }\n}",
        "reject_response": "private void _parse(UpdateSink sink, Reader r) {\n    SPARQLParser11 parser = null ;\n    try {\n        parser = new SPARQLParser11(r) ;\n        parser.setUpdateSink(sink) ;\n        parser.UpdateUnit() ;\n    }\n    catch (org.apache.jena.sparql.lang.sparql_11.ParseException ex)\n    {\n        throw new QueryParseException(ex.getMessage(),\n                                      ex.currentToken.beginLine,\n                                      ex.currentToken.beginColumn) ;\n    }\n    catch (org.apache.jena.sparql.lang.sparql_11.TokenMgrError tErr)\n    {\n        // Last valid token : not the same as token error message - but this should not happen\n        int col = parser.token.endColumn ;\n        int line = parser.token.endLine ;\n        throw new QueryParseException(tErr.getMessage(), line, col) ; }\n\n    catch (UpdateException ex) { throw ex ; }\n    catch (JenaException ex)  { throw new QueryException(ex.getMessage(), ex) ; }\n    catch (Error err)\n    {\n        // The token stream can throw errors.\n        throw new QueryParseException(err.getMessage(), err, -1, -1) ;\n    }\n    catch (Throwable th)\n    {\n        Log.fatal(this, \"Unexpected throwable: \",th) ;\n        throw new QueryException(th.getMessage(), th) ;\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2939,
        "instruction": "@Scheduled(cron = \"${kylin.metadata.ops-cron:0 0 0 * * *}\")\npublic void routineTask() {\n    opsCronTimeout = KylinConfig.getInstanceFromEnv().getRoutineOpsTaskTimeOut();\n    CURRENT_FUTURE.remove();\n    EpochManager epochManager = EpochManager.getInstance();\n    try {\n        log.info(\"Start to work\");\n        long startTime = System.currentTimeMillis();\n        MetricsGroup.hostTagCounterInc(MetricsName.METADATA_OPS_CRON, MetricsCategory.GLOBAL, GLOBAL);\n        try (SetThreadName ignored = new SetThreadName(\"RoutineOpsWorker\")) {\n            if (epochManager.checkEpochOwner(EpochManager.GLOBAL)) {\n                executeTask(() -> backupService.backupAll(), \"MetadataBackup\", startTime);\n                executeTask(RoutineTool::cleanQueryHistories, \"QueryHistoriesCleanup\", startTime);\n                executeTask(RoutineTool::cleanStreamingStats, \"StreamingStatsCleanup\", startTime);\n                executeTask(RoutineTool::deleteRawRecItems, \"RawRecItemsDeletion\", startTime);\n                executeTask(RoutineTool::cleanGlobalSourceUsage, \"SourceUsageCleanup\", startTime);\n                executeTask(() -> projectService.cleanupAcl(), \"AclCleanup\", startTime);\n            }\n            executeTask(() -> projectService.garbageCleanup(getRemainingTime(startTime)), \"ProjectGarbageCleanup\",\n                    startTime);\n            executeTask(() -> newFastRoutineTool().execute(new String[] { \"-c\" }), \"HdfsCleanup\", startTime);\n        }\n    } catch (InterruptedException e) {\n        log.warn(\"Routine task execution interrupted\", e);\n        Thread.currentThread().interrupt();\n    } catch (TimeoutException e) {\n        log.warn(\"Routine task execution timeout\", e);\n        if (CURRENT_FUTURE.get() != null) {\n            CURRENT_FUTURE.get().cancel(true);\n        }\n    }\n    MetricsGroup.hostTagCounterInc(MetricsName.METADATA_OPS_CRON_SUCCESS, MetricsCategory.GLOBAL, GLOBAL);\n}",
        "accept_response": "@Scheduled(cron = \"${kylin.metadata.ops-cron:0 0 0 * * *}\")\npublic void routineTask() {\n    opsCronTimeout = KylinConfig.getInstanceFromEnv().getRoutineOpsTaskTimeOut();\n    CURRENT_FUTURE.remove();\n    EpochManager epochManager = EpochManager.getInstance();\n    try {\n        log.info(\"Start to work\");\n        long startTime = System.currentTimeMillis();\n        MetricsGroup.hostTagCounterInc(MetricsName.METADATA_OPS_CRON, MetricsCategory.GLOBAL, GLOBAL);\n        try (SetThreadName ignored = new SetThreadName(\"RoutineOpsWorker\")) {\n            if (epochManager.checkEpochOwner(EpochManager.GLOBAL)) {\n                executeTask(() -> backupService.backupAll(), \"MetadataBackup\", startTime);\n                executeTask(RoutineTool::cleanQueryHistories, \"QueryHistoriesCleanup\", startTime);\n                executeTask(RoutineTool::cleanStreamingStats, \"StreamingStatsCleanup\", startTime);\n                executeTask(RoutineTool::deleteRawRecItems, \"RawRecItemsDeletion\", startTime);\n                executeTask(RoutineTool::cleanGlobalSourceUsage, \"SourceUsageCleanup\", startTime);\n                executeTask(() -> projectService.cleanupAcl(), \"AclCleanup\", startTime);\n            }\n            executeTask(() -> projectService.garbageCleanup(getRemainingTime(startTime)), \"ProjectGarbageCleanup\",\n                    startTime);\n            executeTask(() -> newFastRoutineTool().execute(new String[] { \"-c\" }), \"HdfsCleanup\", startTime);\n            log.info(\"Finish to work, cost {}ms\", System.currentTimeMillis() - startTime);\n        }\n    } catch (InterruptedException e) {\n        log.warn(\"Routine task execution interrupted\", e);\n        Thread.currentThread().interrupt();\n    } catch (TimeoutException e) {\n        log.warn(\"Routine task execution timeout\", e);\n        if (CURRENT_FUTURE.get() != null) {\n            CURRENT_FUTURE.get().cancel(true);\n        }\n    }\n    MetricsGroup.hostTagCounterInc(MetricsName.METADATA_OPS_CRON_SUCCESS, MetricsCategory.GLOBAL, GLOBAL);\n}",
        "reject_response": "@Scheduled(cron = \"${kylin.metadata.ops-cron:0 0 0 * * *}\")\npublic void routineTask() {\n    opsCronTimeout = KylinConfig.getInstanceFromEnv().getRoutineOpsTaskTimeOut();\n    CURRENT_FUTURE.remove();\n    EpochManager epochManager = EpochManager.getInstance();\n    try {\n        log.info(\"Start to work\");\n        long startTime = System.currentTimeMillis();\n        MetricsGroup.hostTagCounterInc(MetricsName.METADATA_OPS_CRON, MetricsCategory.GLOBAL, GLOBAL);\n        try (SetThreadName ignored = new SetThreadName(\"RoutineOpsWorker\")) {\n            if (epochManager.checkEpochOwner(EpochManager.GLOBAL)) {\n                executeTask(() -> backupService.backupAll(), \"MetadataBackup\", startTime);\n                executeTask(RoutineTool::cleanQueryHistories, \"QueryHistoriesCleanup\", startTime);\n                executeTask(RoutineTool::cleanStreamingStats, \"StreamingStatsCleanup\", startTime);\n                executeTask(RoutineTool::deleteRawRecItems, \"RawRecItemsDeletion\", startTime);\n                executeTask(RoutineTool::cleanGlobalSourceUsage, \"SourceUsageCleanup\", startTime);\n                executeTask(() -> projectService.cleanupAcl(), \"AclCleanup\", startTime);\n            }\n            executeTask(() -> projectService.garbageCleanup(getRemainingTime(startTime)), \"ProjectGarbageCleanup\",\n                    startTime);\n            executeTask(() -> newFastRoutineTool().execute(new String[] { \"-c\" }), \"HdfsCleanup\", startTime);\n            log.info(\"Finish to work\");\n        }\n    } catch (InterruptedException e) {\n        log.warn(\"Routine task execution interrupted\", e);\n        Thread.currentThread().interrupt();\n    } catch (TimeoutException e) {\n        log.warn(\"Routine task execution timeout\", e);\n        if (CURRENT_FUTURE.get() != null) {\n            CURRENT_FUTURE.get().cancel(true);\n        }\n    }\n    MetricsGroup.hostTagCounterInc(MetricsName.METADATA_OPS_CRON_SUCCESS, MetricsCategory.GLOBAL, GLOBAL);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3170,
        "instruction": "@Override\nprotected void serviceStart() throws Exception {\n\n  // NodeManager is the last service to start, so NodeId is available.\n  this.nodeId = this.context.getNodeId();\n  this.httpPort = this.context.getHttpPort();\n  this.nodeManagerVersionId = YarnVersionInfo.getVersion();\n  try {\n    // Registration has to be in start so that ContainerManager can get the\n    // perNM tokens needed to authenticate ContainerTokens.\n    this.resourceTracker = getRMClient();\n    registerWithRM();\n    super.serviceStart();\n    startStatusUpdater();\n  } catch (Exception e) {\n    String errorMessage = \"Unexpected error starting NodeStatusUpdater\";\n    LOG.error(errorMessage, e);\n    throw new YarnRuntimeException(e);\n  }\n}",
        "accept_response": "@Override\nprotected void serviceStart() throws Exception {\n\n  // NodeManager is the last service to start, so NodeId is available.\n  this.nodeId = this.context.getNodeId();\n  LOG.info(\"Node ID assigned is : {}.\", this.nodeId);\n  this.httpPort = this.context.getHttpPort();\n  this.nodeManagerVersionId = YarnVersionInfo.getVersion();\n  try {\n    // Registration has to be in start so that ContainerManager can get the\n    // perNM tokens needed to authenticate ContainerTokens.\n    this.resourceTracker = getRMClient();\n    registerWithRM();\n    super.serviceStart();\n    startStatusUpdater();\n  } catch (Exception e) {\n    String errorMessage = \"Unexpected error starting NodeStatusUpdater\";\n    LOG.error(errorMessage, e);\n    throw new YarnRuntimeException(e);\n  }\n}",
        "reject_response": "@Override\nprotected void serviceStart() throws Exception {\n\n  // NodeManager is the last service to start, so NodeId is available.\n  this.nodeId = this.context.getNodeId();\n  LOG.info(\"Node ID assigned is : \" + this.nodeId);\n  this.httpPort = this.context.getHttpPort();\n  this.nodeManagerVersionId = YarnVersionInfo.getVersion();\n  try {\n    // Registration has to be in start so that ContainerManager can get the\n    // perNM tokens needed to authenticate ContainerTokens.\n    this.resourceTracker = getRMClient();\n    registerWithRM();\n    super.serviceStart();\n    startStatusUpdater();\n  } catch (Exception e) {\n    String errorMessage = \"Unexpected error starting NodeStatusUpdater\";\n    LOG.error(errorMessage, e);\n    throw new YarnRuntimeException(e);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2995,
        "instruction": "@Override\npublic List<String> lookupResource(ResourceLookupContext context) throws Exception {\n\n  List<String> ret \t\t   = new ArrayList<String>();\n  String \tserviceName  \t   = getServiceName();\n  String\tserviceType\t\t   = getServiceType();\n  Map<String,String> configs = getConfigs();\n  if(LOG.isDebugEnabled()) {\n    LOG.debug(\"==> RangerServicePresto.lookupResource() Context: (\" + context + \")\");\n  }\n  if (context != null) {\n    try {\n      if (!configs.containsKey(HadoopConfigHolder.RANGER_LOGIN_PASSWORD)) {\n        configs.put(HadoopConfigHolder.RANGER_LOGIN_PASSWORD, null);\n      }\n      ret  = PrestoResourceManager.getPrestoResources(serviceName, serviceType, configs,context);\n    } catch (Exception e) {\n      LOG.error( \"<==RangerServicePresto.lookupResource() Error : \" + e);\n      throw e;\n    }\n  }\n  return ret;\n}",
        "accept_response": "@Override\npublic List<String> lookupResource(ResourceLookupContext context) throws Exception {\n\n  List<String> ret \t\t   = new ArrayList<String>();\n  String \tserviceName  \t   = getServiceName();\n  String\tserviceType\t\t   = getServiceType();\n  Map<String,String> configs = getConfigs();\n  if(LOG.isDebugEnabled()) {\n    LOG.debug(\"==> RangerServicePresto.lookupResource() Context: (\" + context + \")\");\n  }\n  if (context != null) {\n    try {\n      if (!configs.containsKey(HadoopConfigHolder.RANGER_LOGIN_PASSWORD)) {\n        configs.put(HadoopConfigHolder.RANGER_LOGIN_PASSWORD, null);\n      }\n      ret  = PrestoResourceManager.getPrestoResources(serviceName, serviceType, configs,context);\n    } catch (Exception e) {\n      LOG.error( \"<==RangerServicePresto.lookupResource() Error : \" + e);\n      throw e;\n    }\n  }\n  if(LOG.isDebugEnabled()) {\n    LOG.debug(\"<== RangerServicePresto.lookupResource() Response: (\" + ret + \")\");\n  }\n  return ret;\n}",
        "reject_response": "@Override\npublic List<String> lookupResource(ResourceLookupContext context) throws Exception {\n\n  List<String> ret \t\t   = new ArrayList<String>();\n  String \tserviceName  \t   = getServiceName();\n  String\tserviceType\t\t   = getServiceType();\n  Map<String,String> configs = getConfigs();\n  if(LOG.isDebugEnabled()) {\n    LOG.debug(\"==> RangerServicePresto.lookupResource() Context: (\" + context + \")\");\n  }\n  if (context != null) {\n    try {\n      if (!configs.containsKey(HadoopConfigHolder.RANGER_LOGIN_PASSWORD)) {\n        configs.put(HadoopConfigHolder.RANGER_LOGIN_PASSWORD, null);\n      }\n      ret  = PrestoResourceManager.getPrestoResources(serviceName, serviceType, configs,context);\n    } catch (Exception e) {\n      LOG.error( \"<==RangerServicePresto.lookupResource() Error : \" + e);\n      throw e;\n    }\n  }\n  if(LOG.isDebugEnabled()) {\n    LOG.debug(\"<== RangerServicePresto.lookupResource Response: (\" + ret + \")\");\n  }\n  return ret;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3194,
        "instruction": "@VisibleForTesting\nprotected CollectorNodemanagerProtocol getNMCollectorService() {\n  if (nmCollectorService == null) {\n    synchronized (this) {\n      if (nmCollectorService == null) {\n        Configuration conf = getConfig();\n        InetSocketAddress nmCollectorServiceAddress = conf.getSocketAddr(\n            YarnConfiguration.NM_BIND_HOST,\n            YarnConfiguration.NM_COLLECTOR_SERVICE_ADDRESS,\n            YarnConfiguration.DEFAULT_NM_COLLECTOR_SERVICE_ADDRESS,\n            YarnConfiguration.DEFAULT_NM_COLLECTOR_SERVICE_PORT);\n        final YarnRPC rpc = YarnRPC.create(conf);\n\n        // TODO Security settings.\n        nmCollectorService = (CollectorNodemanagerProtocol) rpc.getProxy(\n            CollectorNodemanagerProtocol.class,\n            nmCollectorServiceAddress, conf);\n      }\n    }\n  }\n  return nmCollectorService;\n}",
        "accept_response": "@VisibleForTesting\nprotected CollectorNodemanagerProtocol getNMCollectorService() {\n  if (nmCollectorService == null) {\n    synchronized (this) {\n      if (nmCollectorService == null) {\n        Configuration conf = getConfig();\n        InetSocketAddress nmCollectorServiceAddress = conf.getSocketAddr(\n            YarnConfiguration.NM_BIND_HOST,\n            YarnConfiguration.NM_COLLECTOR_SERVICE_ADDRESS,\n            YarnConfiguration.DEFAULT_NM_COLLECTOR_SERVICE_ADDRESS,\n            YarnConfiguration.DEFAULT_NM_COLLECTOR_SERVICE_PORT);\n        LOG.info(\"nmCollectorServiceAddress: {}\", nmCollectorServiceAddress);\n        final YarnRPC rpc = YarnRPC.create(conf);\n\n        // TODO Security settings.\n        nmCollectorService = (CollectorNodemanagerProtocol) rpc.getProxy(\n            CollectorNodemanagerProtocol.class,\n            nmCollectorServiceAddress, conf);\n      }\n    }\n  }\n  return nmCollectorService;\n}",
        "reject_response": "@VisibleForTesting\nprotected CollectorNodemanagerProtocol getNMCollectorService() {\n  if (nmCollectorService == null) {\n    synchronized (this) {\n      if (nmCollectorService == null) {\n        Configuration conf = getConfig();\n        InetSocketAddress nmCollectorServiceAddress = conf.getSocketAddr(\n            YarnConfiguration.NM_BIND_HOST,\n            YarnConfiguration.NM_COLLECTOR_SERVICE_ADDRESS,\n            YarnConfiguration.DEFAULT_NM_COLLECTOR_SERVICE_ADDRESS,\n            YarnConfiguration.DEFAULT_NM_COLLECTOR_SERVICE_PORT);\n        LOG.info(\"nmCollectorServiceAddress: \" + nmCollectorServiceAddress);\n        final YarnRPC rpc = YarnRPC.create(conf);\n\n        // TODO Security settings.\n        nmCollectorService = (CollectorNodemanagerProtocol) rpc.getProxy(\n            CollectorNodemanagerProtocol.class,\n            nmCollectorServiceAddress, conf);\n      }\n    }\n  }\n  return nmCollectorService;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2410,
        "instruction": "private void loadFilters() {\n  sortFilters();\n\n  List<Input> toRemoveInputList = new ArrayList<Input>();\n  for (Input input : inputManager.getInputList()) {\n    for (Map<String, Object> map : filterConfigList) {\n      if (map == null) {\n        continue;\n      }\n      mergeBlocks(globalConfigs, map);\n\n      String value = (String) map.get(\"filter\");\n      if (StringUtils.isEmpty(value)) {\n        LOG.error(\"Filter block doesn't have filter element\");\n        continue;\n      }\n      Filter filter = (Filter) AliasUtil.getClassInstance(value, AliasType.FILTER);\n      if (filter == null) {\n        LOG.error(\"Filter object could not be found\");\n        continue;\n      }\n      filter.loadConfig(map);\n      filter.setInput(input);\n\n      if (filter.isEnabled()) {\n        filter.setOutputManager(outputManager);\n        input.addFilter(filter);\n        filter.logConfgs(Level.INFO);\n      } else {\n        LOG.debug(\"Ignoring filter \" + filter.getShortDescription() + \" for input \" + input.getShortDescription());\n      }\n    }\n\n    if (input.getFirstFilter() == null) {\n      toRemoveInputList.add(input);\n    }\n  }\n\n  for (Input toRemoveInput : toRemoveInputList) {\n    inputManager.removeInput(toRemoveInput);\n  }\n}",
        "accept_response": "private void loadFilters() {\n  sortFilters();\n\n  List<Input> toRemoveInputList = new ArrayList<Input>();\n  for (Input input : inputManager.getInputList()) {\n    for (Map<String, Object> map : filterConfigList) {\n      if (map == null) {\n        continue;\n      }\n      mergeBlocks(globalConfigs, map);\n\n      String value = (String) map.get(\"filter\");\n      if (StringUtils.isEmpty(value)) {\n        LOG.error(\"Filter block doesn't have filter element\");\n        continue;\n      }\n      Filter filter = (Filter) AliasUtil.getClassInstance(value, AliasType.FILTER);\n      if (filter == null) {\n        LOG.error(\"Filter object could not be found\");\n        continue;\n      }\n      filter.loadConfig(map);\n      filter.setInput(input);\n\n      if (filter.isEnabled()) {\n        filter.setOutputManager(outputManager);\n        input.addFilter(filter);\n        filter.logConfgs(Level.INFO);\n      } else {\n        LOG.debug(\"Ignoring filter \" + filter.getShortDescription() + \" for input \" + input.getShortDescription());\n      }\n    }\n\n    if (input.getFirstFilter() == null) {\n      toRemoveInputList.add(input);\n    }\n  }\n\n  for (Input toRemoveInput : toRemoveInputList) {\n    LOG.warn(\"There are no filters, we will ignore this input. \" + toRemoveInput.getShortDescription());\n    inputManager.removeInput(toRemoveInput);\n  }\n}",
        "reject_response": "private void loadFilters() {\n  sortFilters();\n\n  List<Input> toRemoveInputList = new ArrayList<Input>();\n  for (Input input : inputManager.getInputList()) {\n    for (Map<String, Object> map : filterConfigList) {\n      if (map == null) {\n        continue;\n      }\n      mergeBlocks(globalConfigs, map);\n\n      String value = (String) map.get(\"filter\");\n      if (StringUtils.isEmpty(value)) {\n        LOG.error(\"Filter block doesn't have filter element\");\n        continue;\n      }\n      Filter filter = (Filter) AliasUtil.getClassInstance(value, AliasType.FILTER);\n      if (filter == null) {\n        LOG.error(\"Filter object could not be found\");\n        continue;\n      }\n      filter.loadConfig(map);\n      filter.setInput(input);\n\n      if (filter.isEnabled()) {\n        filter.setOutputManager(outputManager);\n        input.addFilter(filter);\n        filter.logConfgs(Level.INFO);\n      } else {\n        LOG.debug(\"Ignoring filter \" + filter.getShortDescription() + \" for input \" + input.getShortDescription());\n      }\n    }\n\n    if (input.getFirstFilter() == null) {\n      toRemoveInputList.add(input);\n    }\n  }\n\n  for (Input toRemoveInput : toRemoveInputList) {\n    logger.warn(\"There are no filters, we will ignore this input. \"\n      + toRemoveInput.getShortDescription());\n    inputMgr.removeInput(toRemoveInput);\n    inputManager.removeInput(toRemoveInput);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2801,
        "instruction": "@Override\npublic void execute(\n        final HttpRequest request,\n        final AsyncEntityProducer entityProducer,\n        final AsyncExecChain.Scope scope,\n        final AsyncExecChain chain,\n        final AsyncExecCallback asyncExecCallback) throws HttpException, IOException {\n    final String exchangeId = scope.exchangeId;\n    final HttpRoute route = scope.route;\n    final CancellableDependency operation = scope.cancellableDependency;\n    final HttpClientContext clientContext = scope.clientContext;\n    final AsyncExecRuntime execRuntime = scope.execRuntime;\n\n    final AtomicInteger messageCountDown = new AtomicInteger(2);\n    final AsyncClientExchangeHandler internalExchangeHandler = new AsyncClientExchangeHandler() {\n\n        private final AtomicReference<AsyncDataConsumer> entityConsumerRef = new AtomicReference<>(null);\n\n        @Override\n        public void releaseResources() {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.getAndSet(null);\n            if (entityConsumer != null) {\n                entityConsumer.releaseResources();\n            }\n        }\n\n        @Override\n        public void failed(final Exception cause) {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.getAndSet(null);\n            if (entityConsumer != null) {\n                entityConsumer.releaseResources();\n            }\n            execRuntime.markConnectionNonReusable();\n            asyncExecCallback.failed(cause);\n        }\n\n        @Override\n        public void cancel() {\n            failed(new InterruptedIOException());\n        }\n\n        @Override\n        public void produceRequest(\n                final RequestChannel channel,\n                final HttpContext context) throws HttpException, IOException {\n            channel.sendRequest(request, entityProducer, context);\n            if (entityProducer == null) {\n                messageCountDown.decrementAndGet();\n            }\n        }\n\n        @Override\n        public int available() {\n            return entityProducer.available();\n        }\n\n        @Override\n        public void produce(final DataStreamChannel channel) throws IOException {\n            entityProducer.produce(new DataStreamChannel() {\n\n                @Override\n                public void requestOutput() {\n                    channel.requestOutput();\n                }\n\n                @Override\n                public int write(final ByteBuffer src) throws IOException {\n                    return channel.write(src);\n                }\n\n                @Override\n                public void endStream(final List<? extends Header> trailers) throws IOException {\n                    channel.endStream(trailers);\n                    if (messageCountDown.decrementAndGet() <= 0) {\n                        asyncExecCallback.completed();\n                    }\n                }\n\n                @Override\n                public void endStream() throws IOException {\n                    channel.endStream();\n                    if (messageCountDown.decrementAndGet() <= 0) {\n                        asyncExecCallback.completed();\n                    }\n                }\n\n            });\n        }\n\n        @Override\n        public void consumeInformation(\n                final HttpResponse response,\n                final HttpContext context) throws HttpException, IOException {\n            asyncExecCallback.handleInformationResponse(response);\n        }\n\n        @Override\n        public void consumeResponse(\n                final HttpResponse response,\n                final EntityDetails entityDetails,\n                final HttpContext context) throws HttpException, IOException {\n            entityConsumerRef.set(asyncExecCallback.handleResponse(response, entityDetails));\n            if (response.getCode() >= HttpStatus.SC_CLIENT_ERROR) {\n                messageCountDown.decrementAndGet();\n            }\n            final TimeValue keepAliveDuration = keepAliveStrategy.getKeepAliveDuration(response, clientContext);\n            Object userToken = clientContext.getUserToken();\n            if (userToken == null) {\n                userToken = userTokenHandler.getUserToken(route, clientContext);\n                clientContext.setAttribute(HttpClientContext.USER_TOKEN, userToken);\n            }\n            execRuntime.markConnectionReusable(userToken, keepAliveDuration);\n            if (entityDetails == null) {\n                execRuntime.validateConnection();\n                if (messageCountDown.decrementAndGet() <= 0) {\n                    asyncExecCallback.completed();\n                }\n            }\n        }\n\n        @Override\n        public void updateCapacity(final CapacityChannel capacityChannel) throws IOException {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.get();\n            if (entityConsumer != null) {\n                entityConsumer.updateCapacity(capacityChannel);\n            } else {\n                capacityChannel.update(Integer.MAX_VALUE);\n            }\n        }\n\n        @Override\n        public void consume(final ByteBuffer src) throws IOException {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.get();\n            if (entityConsumer != null) {\n                entityConsumer.consume(src);\n            }\n        }\n\n        @Override\n        public void streamEnd(final List<? extends Header> trailers) throws HttpException, IOException {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.getAndSet(null);\n            if (entityConsumer != null) {\n                entityConsumer.streamEnd(trailers);\n            } else {\n                execRuntime.validateConnection();\n            }\n            if (messageCountDown.decrementAndGet() <= 0) {\n                asyncExecCallback.completed();\n            }\n        }\n\n    };\n\n    if (LOG.isDebugEnabled()) {\n        operation.setDependency(execRuntime.execute(\n                exchangeId,\n                new LoggingAsyncClientExchangeHandler(LOG, exchangeId, internalExchangeHandler),\n                clientContext));\n    } else {\n        operation.setDependency(execRuntime.execute(exchangeId, internalExchangeHandler, clientContext));\n    }\n}",
        "accept_response": "@Override\npublic void execute(\n        final HttpRequest request,\n        final AsyncEntityProducer entityProducer,\n        final AsyncExecChain.Scope scope,\n        final AsyncExecChain chain,\n        final AsyncExecCallback asyncExecCallback) throws HttpException, IOException {\n    final String exchangeId = scope.exchangeId;\n    final HttpRoute route = scope.route;\n    final CancellableDependency operation = scope.cancellableDependency;\n    final HttpClientContext clientContext = scope.clientContext;\n    final AsyncExecRuntime execRuntime = scope.execRuntime;\n\n    if (LOG.isDebugEnabled()) {\n        LOG.debug(\"{}: executing {}\", exchangeId, new RequestLine(request));\n    }\n\n    final AtomicInteger messageCountDown = new AtomicInteger(2);\n    final AsyncClientExchangeHandler internalExchangeHandler = new AsyncClientExchangeHandler() {\n\n        private final AtomicReference<AsyncDataConsumer> entityConsumerRef = new AtomicReference<>(null);\n\n        @Override\n        public void releaseResources() {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.getAndSet(null);\n            if (entityConsumer != null) {\n                entityConsumer.releaseResources();\n            }\n        }\n\n        @Override\n        public void failed(final Exception cause) {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.getAndSet(null);\n            if (entityConsumer != null) {\n                entityConsumer.releaseResources();\n            }\n            execRuntime.markConnectionNonReusable();\n            asyncExecCallback.failed(cause);\n        }\n\n        @Override\n        public void cancel() {\n            failed(new InterruptedIOException());\n        }\n\n        @Override\n        public void produceRequest(\n                final RequestChannel channel,\n                final HttpContext context) throws HttpException, IOException {\n            channel.sendRequest(request, entityProducer, context);\n            if (entityProducer == null) {\n                messageCountDown.decrementAndGet();\n            }\n        }\n\n        @Override\n        public int available() {\n            return entityProducer.available();\n        }\n\n        @Override\n        public void produce(final DataStreamChannel channel) throws IOException {\n            entityProducer.produce(new DataStreamChannel() {\n\n                @Override\n                public void requestOutput() {\n                    channel.requestOutput();\n                }\n\n                @Override\n                public int write(final ByteBuffer src) throws IOException {\n                    return channel.write(src);\n                }\n\n                @Override\n                public void endStream(final List<? extends Header> trailers) throws IOException {\n                    channel.endStream(trailers);\n                    if (messageCountDown.decrementAndGet() <= 0) {\n                        asyncExecCallback.completed();\n                    }\n                }\n\n                @Override\n                public void endStream() throws IOException {\n                    channel.endStream();\n                    if (messageCountDown.decrementAndGet() <= 0) {\n                        asyncExecCallback.completed();\n                    }\n                }\n\n            });\n        }\n\n        @Override\n        public void consumeInformation(\n                final HttpResponse response,\n                final HttpContext context) throws HttpException, IOException {\n            asyncExecCallback.handleInformationResponse(response);\n        }\n\n        @Override\n        public void consumeResponse(\n                final HttpResponse response,\n                final EntityDetails entityDetails,\n                final HttpContext context) throws HttpException, IOException {\n            entityConsumerRef.set(asyncExecCallback.handleResponse(response, entityDetails));\n            if (response.getCode() >= HttpStatus.SC_CLIENT_ERROR) {\n                messageCountDown.decrementAndGet();\n            }\n            final TimeValue keepAliveDuration = keepAliveStrategy.getKeepAliveDuration(response, clientContext);\n            Object userToken = clientContext.getUserToken();\n            if (userToken == null) {\n                userToken = userTokenHandler.getUserToken(route, clientContext);\n                clientContext.setAttribute(HttpClientContext.USER_TOKEN, userToken);\n            }\n            execRuntime.markConnectionReusable(userToken, keepAliveDuration);\n            if (entityDetails == null) {\n                execRuntime.validateConnection();\n                if (messageCountDown.decrementAndGet() <= 0) {\n                    asyncExecCallback.completed();\n                }\n            }\n        }\n\n        @Override\n        public void updateCapacity(final CapacityChannel capacityChannel) throws IOException {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.get();\n            if (entityConsumer != null) {\n                entityConsumer.updateCapacity(capacityChannel);\n            } else {\n                capacityChannel.update(Integer.MAX_VALUE);\n            }\n        }\n\n        @Override\n        public void consume(final ByteBuffer src) throws IOException {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.get();\n            if (entityConsumer != null) {\n                entityConsumer.consume(src);\n            }\n        }\n\n        @Override\n        public void streamEnd(final List<? extends Header> trailers) throws HttpException, IOException {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.getAndSet(null);\n            if (entityConsumer != null) {\n                entityConsumer.streamEnd(trailers);\n            } else {\n                execRuntime.validateConnection();\n            }\n            if (messageCountDown.decrementAndGet() <= 0) {\n                asyncExecCallback.completed();\n            }\n        }\n\n    };\n\n    if (LOG.isDebugEnabled()) {\n        operation.setDependency(execRuntime.execute(\n                exchangeId,\n                new LoggingAsyncClientExchangeHandler(LOG, exchangeId, internalExchangeHandler),\n                clientContext));\n    } else {\n        operation.setDependency(execRuntime.execute(exchangeId, internalExchangeHandler, clientContext));\n    }\n}",
        "reject_response": "@Override\npublic void execute(\n        final HttpRequest request,\n        final AsyncEntityProducer entityProducer,\n        final AsyncExecChain.Scope scope,\n        final AsyncExecChain chain,\n        final AsyncExecCallback asyncExecCallback) throws HttpException, IOException {\n    final String exchangeId = scope.exchangeId;\n    final HttpRoute route = scope.route;\n    final CancellableDependency operation = scope.cancellableDependency;\n    final HttpClientContext clientContext = scope.clientContext;\n    final AsyncExecRuntime execRuntime = scope.execRuntime;\n\n    if (LOG.isDebugEnabled()) {\n    if (log.isDebugEnabled()) {\n        log.debug(\"{}: executing {}\", exchangeId, new RequestLine(request));\n    }\n\n    final AtomicInteger messageCountDown = new AtomicInteger(2);\n    final AsyncClientExchangeHandler internalExchangeHandler = new AsyncClientExchangeHandler() {\n\n        private final AtomicReference<AsyncDataConsumer> entityConsumerRef = new AtomicReference<>(null);\n\n        @Override\n        public void releaseResources() {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.getAndSet(null);\n            if (entityConsumer != null) {\n                entityConsumer.releaseResources();\n            }\n        }\n\n        @Override\n        public void failed(final Exception cause) {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.getAndSet(null);\n            if (entityConsumer != null) {\n                entityConsumer.releaseResources();\n            }\n            execRuntime.markConnectionNonReusable();\n            asyncExecCallback.failed(cause);\n        }\n\n        @Override\n        public void cancel() {\n            failed(new InterruptedIOException());\n        }\n\n        @Override\n        public void produceRequest(\n                final RequestChannel channel,\n                final HttpContext context) throws HttpException, IOException {\n            channel.sendRequest(request, entityProducer, context);\n            if (entityProducer == null) {\n                messageCountDown.decrementAndGet();\n            }\n        }\n\n        @Override\n        public int available() {\n            return entityProducer.available();\n        }\n\n        @Override\n        public void produce(final DataStreamChannel channel) throws IOException {\n            entityProducer.produce(new DataStreamChannel() {\n\n                @Override\n                public void requestOutput() {\n                    channel.requestOutput();\n                }\n\n                @Override\n                public int write(final ByteBuffer src) throws IOException {\n                    return channel.write(src);\n                }\n\n                @Override\n                public void endStream(final List<? extends Header> trailers) throws IOException {\n                    channel.endStream(trailers);\n                    if (messageCountDown.decrementAndGet() <= 0) {\n                        asyncExecCallback.completed();\n                    }\n                }\n\n                @Override\n                public void endStream() throws IOException {\n                    channel.endStream();\n                    if (messageCountDown.decrementAndGet() <= 0) {\n                        asyncExecCallback.completed();\n                    }\n                }\n\n            });\n        }\n\n        @Override\n        public void consumeInformation(\n                final HttpResponse response,\n                final HttpContext context) throws HttpException, IOException {\n            asyncExecCallback.handleInformationResponse(response);\n        }\n\n        @Override\n        public void consumeResponse(\n                final HttpResponse response,\n                final EntityDetails entityDetails,\n                final HttpContext context) throws HttpException, IOException {\n            entityConsumerRef.set(asyncExecCallback.handleResponse(response, entityDetails));\n            if (response.getCode() >= HttpStatus.SC_CLIENT_ERROR) {\n                messageCountDown.decrementAndGet();\n            }\n            final TimeValue keepAliveDuration = keepAliveStrategy.getKeepAliveDuration(response, clientContext);\n            Object userToken = clientContext.getUserToken();\n            if (userToken == null) {\n                userToken = userTokenHandler.getUserToken(route, clientContext);\n                clientContext.setAttribute(HttpClientContext.USER_TOKEN, userToken);\n            }\n            execRuntime.markConnectionReusable(userToken, keepAliveDuration);\n            if (entityDetails == null) {\n                execRuntime.validateConnection();\n                if (messageCountDown.decrementAndGet() <= 0) {\n                    asyncExecCallback.completed();\n                }\n            }\n        }\n\n        @Override\n        public void updateCapacity(final CapacityChannel capacityChannel) throws IOException {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.get();\n            if (entityConsumer != null) {\n                entityConsumer.updateCapacity(capacityChannel);\n            } else {\n                capacityChannel.update(Integer.MAX_VALUE);\n            }\n        }\n\n        @Override\n        public void consume(final ByteBuffer src) throws IOException {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.get();\n            if (entityConsumer != null) {\n                entityConsumer.consume(src);\n            }\n        }\n\n        @Override\n        public void streamEnd(final List<? extends Header> trailers) throws HttpException, IOException {\n            final AsyncDataConsumer entityConsumer = entityConsumerRef.getAndSet(null);\n            if (entityConsumer != null) {\n                entityConsumer.streamEnd(trailers);\n            } else {\n                execRuntime.validateConnection();\n            }\n            if (messageCountDown.decrementAndGet() <= 0) {\n                asyncExecCallback.completed();\n            }\n        }\n\n    };\n\n    if (LOG.isDebugEnabled()) {\n        operation.setDependency(execRuntime.execute(\n                exchangeId,\n                new LoggingAsyncClientExchangeHandler(LOG, exchangeId, internalExchangeHandler),\n                clientContext));\n    } else {\n        operation.setDependency(execRuntime.execute(exchangeId, internalExchangeHandler, clientContext));\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2891,
        "instruction": "@SuppressWarnings(\"resource\")\npublic Iterator<ArpaNgram> iterator() {\n\n  try {\n    final Scanner scanner;\n\n    if (arpaFile.getName().endsWith(\"gz\")) {\n      InputStream in = new GZIPInputStream(\n          new FileInputStream(arpaFile));\n      scanner = new Scanner(in);\n    } else {\n      scanner = new Scanner(arpaFile);\n    }\n\n    // Eat initial header lines\n    while (scanner.hasNextLine()) {\n      String line = scanner.nextLine();\n      if (NGRAM_HEADER.matches(line)) {\n        break;\n      }\n    }\n\n    return new Iterator<ArpaNgram>() {\n\n      String nextLine = null;\n      int ngramOrder = 1;\n      //    int id = 0;\n\n      public boolean hasNext() {\n\n        if (scanner.hasNext()) {\n\n          String line = scanner.nextLine();\n\n          boolean lineIsHeader = NGRAM_HEADER.matches(line) || NGRAM_END.matches(line);\n\n          while (lineIsHeader || BLANK_LINE.matches(line)) {\n\n            if (lineIsHeader) {\n              ngramOrder++;\n            }\n\n            if (scanner.hasNext()) {\n              line = scanner.nextLine().trim();\n              lineIsHeader = NGRAM_HEADER.matches(line) || NGRAM_END.matches(line);\n            } else {\n              nextLine = null;\n              return false;\n            }\n          }\n\n          nextLine = line;\n          return true;\n\n        } else {\n          nextLine = null;\n          return false;\n        }\n\n      }\n\n      public ArpaNgram next() {\n        if (nextLine!=null) {\n\n          String[] parts = Regex.spaces.split(nextLine);\n\n          float value = Float.valueOf(parts[0]);\n\n          int word = Vocabulary.id(parts[ngramOrder]);\n\n          int[] context = new int[ngramOrder-1];\n          for (int i=1; i<ngramOrder; i++) {\n            context[i-1] = Vocabulary.id(parts[i]);\n          }\n\n          float backoff;\n          if (parts.length > ngramOrder+1) {\n            backoff = Float.valueOf(parts[parts.length-1]);\n          } else {\n            backoff = ArpaNgram.DEFAULT_BACKOFF;\n          }\n\n          nextLine = null;\n          return new ArpaNgram(word, context, value, backoff);\n\n        } else {\n          throw new NoSuchElementException();\n        }\n      }\n\n      public void remove() {\n        throw new UnsupportedOperationException();\n      }\n\n    };\n  } catch (IOException e) {\n    LOG.error(e.getMessage(), e);\n    return null;\n  }\n}",
        "accept_response": "@SuppressWarnings(\"resource\")\npublic Iterator<ArpaNgram> iterator() {\n\n  try {\n    final Scanner scanner;\n\n    if (arpaFile.getName().endsWith(\"gz\")) {\n      InputStream in = new GZIPInputStream(\n          new FileInputStream(arpaFile));\n      scanner = new Scanner(in);\n    } else {\n      scanner = new Scanner(arpaFile);\n    }\n\n    // Eat initial header lines\n    while (scanner.hasNextLine()) {\n      String line = scanner.nextLine();\n      LOG.debug(\"Discarding line: {}\", line);\n      if (NGRAM_HEADER.matches(line)) {\n        break;\n      }\n    }\n\n    return new Iterator<ArpaNgram>() {\n\n      String nextLine = null;\n      int ngramOrder = 1;\n      //    int id = 0;\n\n      public boolean hasNext() {\n\n        if (scanner.hasNext()) {\n\n          String line = scanner.nextLine();\n\n          boolean lineIsHeader = NGRAM_HEADER.matches(line) || NGRAM_END.matches(line);\n\n          while (lineIsHeader || BLANK_LINE.matches(line)) {\n\n            if (lineIsHeader) {\n              ngramOrder++;\n            }\n\n            if (scanner.hasNext()) {\n              line = scanner.nextLine().trim();\n              lineIsHeader = NGRAM_HEADER.matches(line) || NGRAM_END.matches(line);\n            } else {\n              nextLine = null;\n              return false;\n            }\n          }\n\n          nextLine = line;\n          return true;\n\n        } else {\n          nextLine = null;\n          return false;\n        }\n\n      }\n\n      public ArpaNgram next() {\n        if (nextLine!=null) {\n\n          String[] parts = Regex.spaces.split(nextLine);\n\n          float value = Float.valueOf(parts[0]);\n\n          int word = Vocabulary.id(parts[ngramOrder]);\n\n          int[] context = new int[ngramOrder-1];\n          for (int i=1; i<ngramOrder; i++) {\n            context[i-1] = Vocabulary.id(parts[i]);\n          }\n\n          float backoff;\n          if (parts.length > ngramOrder+1) {\n            backoff = Float.valueOf(parts[parts.length-1]);\n          } else {\n            backoff = ArpaNgram.DEFAULT_BACKOFF;\n          }\n\n          nextLine = null;\n          return new ArpaNgram(word, context, value, backoff);\n\n        } else {\n          throw new NoSuchElementException();\n        }\n      }\n\n      public void remove() {\n        throw new UnsupportedOperationException();\n      }\n\n    };\n  } catch (IOException e) {\n    LOG.error(e.getMessage(), e);\n    return null;\n  }\n}",
        "reject_response": "@SuppressWarnings(\"resource\")\npublic Iterator<ArpaNgram> iterator() {\n\n  try {\n    final Scanner scanner;\n\n    if (arpaFile.getName().endsWith(\"gz\")) {\n      InputStream in = new GZIPInputStream(\n          new FileInputStream(arpaFile));\n      scanner = new Scanner(in);\n    } else {\n      scanner = new Scanner(arpaFile);\n    }\n\n    // Eat initial header lines\n    while (scanner.hasNextLine()) {\n      String line = scanner.nextLine();\n      logger.finest(\"Discarding line: \" + line);\n      if (NGRAM_HEADER.matches(line)) {\n        break;\n      }\n    }\n\n    return new Iterator<ArpaNgram>() {\n\n      String nextLine = null;\n      int ngramOrder = 1;\n      //    int id = 0;\n\n      public boolean hasNext() {\n\n        if (scanner.hasNext()) {\n\n          String line = scanner.nextLine();\n\n          boolean lineIsHeader = NGRAM_HEADER.matches(line) || NGRAM_END.matches(line);\n\n          while (lineIsHeader || BLANK_LINE.matches(line)) {\n\n            if (lineIsHeader) {\n              ngramOrder++;\n            }\n\n            if (scanner.hasNext()) {\n              line = scanner.nextLine().trim();\n              lineIsHeader = NGRAM_HEADER.matches(line) || NGRAM_END.matches(line);\n            } else {\n              nextLine = null;\n              return false;\n            }\n          }\n\n          nextLine = line;\n          return true;\n\n        } else {\n          nextLine = null;\n          return false;\n        }\n\n      }\n\n      public ArpaNgram next() {\n        if (nextLine!=null) {\n\n          String[] parts = Regex.spaces.split(nextLine);\n\n          float value = Float.valueOf(parts[0]);\n\n          int word = Vocabulary.id(parts[ngramOrder]);\n\n          int[] context = new int[ngramOrder-1];\n          for (int i=1; i<ngramOrder; i++) {\n            context[i-1] = Vocabulary.id(parts[i]);\n          }\n\n          float backoff;\n          if (parts.length > ngramOrder+1) {\n            backoff = Float.valueOf(parts[parts.length-1]);\n          } else {\n            backoff = ArpaNgram.DEFAULT_BACKOFF;\n          }\n\n          nextLine = null;\n          return new ArpaNgram(word, context, value, backoff);\n\n        } else {\n          throw new NoSuchElementException();\n        }\n      }\n\n      public void remove() {\n        throw new UnsupportedOperationException();\n      }\n\n    };\n  } catch (IOException e) {\n    LOG.error(e.getMessage(), e);\n    return null;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2374,
        "instruction": "Map<List<String>,Long> calculateUsage() {\n\n  Map<List<Integer>,Long> usage = new HashMap<List<Integer>,Long>();\n\n  for (Entry<String,Integer[]> entry : tableFiles.entrySet()) {\n    List<Integer> key = Arrays.asList(entry.getValue());\n    Long size = fileSizes.get(entry.getKey());\n\n    Long tablesUsage = usage.get(key);\n    if (tablesUsage == null)\n      tablesUsage = 0l;\n\n    tablesUsage += size;\n\n    usage.put(key, tablesUsage);\n\n  }\n\n  Map<List<String>,Long> externalUsage = new HashMap<List<String>,Long>();\n\n  for (Entry<List<Integer>,Long> entry : usage.entrySet()) {\n    List<String> externalKey = new ArrayList<String>();\n    List<Integer> key = entry.getKey();\n    for (int i = 0; i < key.size(); i++)\n      if (key.get(i) != 0)\n        externalKey.add(externalIds.get(i));\n\n    externalUsage.put(externalKey, entry.getValue());\n  }\n\n  return externalUsage;\n}",
        "accept_response": "Map<List<String>,Long> calculateUsage() {\n\n  Map<List<Integer>,Long> usage = new HashMap<List<Integer>,Long>();\n\n  for (Entry<String,Integer[]> entry : tableFiles.entrySet()) {\n    if (log.isTraceEnabled()) {\n      log.trace(\"fileSizes \" + fileSizes + \" key \" + entry.getKey());\n    }\n    List<Integer> key = Arrays.asList(entry.getValue());\n    Long size = fileSizes.get(entry.getKey());\n\n    Long tablesUsage = usage.get(key);\n    if (tablesUsage == null)\n      tablesUsage = 0l;\n\n    tablesUsage += size;\n\n    usage.put(key, tablesUsage);\n\n  }\n\n  Map<List<String>,Long> externalUsage = new HashMap<List<String>,Long>();\n\n  for (Entry<List<Integer>,Long> entry : usage.entrySet()) {\n    List<String> externalKey = new ArrayList<String>();\n    List<Integer> key = entry.getKey();\n    for (int i = 0; i < key.size(); i++)\n      if (key.get(i) != 0)\n        externalKey.add(externalIds.get(i));\n\n    externalUsage.put(externalKey, entry.getValue());\n  }\n\n  return externalUsage;\n}",
        "reject_response": "Map<List<String>,Long> calculateUsage() {\n\n  Map<List<Integer>,Long> usage = new HashMap<List<Integer>,Long>();\n\n  for (Entry<String,Integer[]> entry : tableFiles.entrySet()) {\n    if (log.isTraceEnabled()) {\n    log.info(\"fileSizes \" + fileSizes + \" key \" + Arrays.asList(entry.getKey()));\n    }\n    List<Integer> key = Arrays.asList(entry.getValue());\n    Long size = fileSizes.get(entry.getKey());\n\n    Long tablesUsage = usage.get(key);\n    if (tablesUsage == null)\n      tablesUsage = 0l;\n\n    tablesUsage += size;\n\n    usage.put(key, tablesUsage);\n\n  }\n\n  Map<List<String>,Long> externalUsage = new HashMap<List<String>,Long>();\n\n  for (Entry<List<Integer>,Long> entry : usage.entrySet()) {\n    List<String> externalKey = new ArrayList<String>();\n    List<Integer> key = entry.getKey();\n    for (int i = 0; i < key.size(); i++)\n      if (key.get(i) != 0)\n        externalKey.add(externalIds.get(i));\n\n    externalUsage.put(externalKey, entry.getValue());\n  }\n\n  return externalUsage;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3157,
        "instruction": "public final void internalRenderComponent()\n{\n\tfinal IMarkupFragment markup = getMarkup();\n\tif (markup == null)\n\t{\n\t\tthrow new MarkupException(\"Markup not found. Component: \" + toString());\n\t}\n\n\tfinal MarkupStream markupStream = new MarkupStream(markup);\n\n\t// Get mutable copy of next tag\n\tfinal ComponentTag openTag = markupStream.getTag();\n\tfinal ComponentTag tag = openTag.mutable();\n\n\t// Call any tag handler\n\tonComponentTag(tag);\n\n\t// If we're an openclose tag\n\tif (!tag.isOpenClose() && !tag.isOpen())\n\t{\n\t\t// We were something other than <tag> or <tag/>\n\t\tmarkupStream.throwMarkupException(\"Method renderComponent called on bad markup element: \" +\n\t\t\ttag);\n\t}\n\n\tif (tag.isOpenClose() && openTag.isOpen())\n\t{\n\t\tmarkupStream.throwMarkupException(\"You can not modify a open tag to open-close: \" + tag);\n\t}\n\n\ttry\n\t{\n\t\t// Render open tag\n\t\tboolean renderBodyOnly = getRenderBodyOnly();\n\t\tif (renderBodyOnly)\n\t\t{\n\t\t\tExceptionSettings.NotRenderableErrorStrategy notRenderableErrorStrategy = ExceptionSettings.NotRenderableErrorStrategy.LOG_WARNING;\n\t\t\tif (Application.exists())\n\t\t\t{\n\t\t\t\tnotRenderableErrorStrategy = getApplication().getExceptionSettings().getNotRenderableErrorStrategy();\n\t\t\t}\n\n\t\t\tif (getFlag(FLAG_OUTPUT_MARKUP_ID))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Markup id set on a component that renders its body only. \" +\n\t\t\t\t                               \"Markup id: %s, component id: %s.\", getMarkupId(), getId());\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (getFlag(FLAG_PLACEHOLDER))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Placeholder tag set on a component that renders its body only. \" +\n\t\t\t\t                               \"Component id: %s.\", getId());\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t}\n\t\telse\n\t\t{\n\t\t\trenderComponentTag(tag);\n\t\t}\n\t\tmarkupStream.next();\n\n\t\t// Render the body only if open-body-close. Do not render if open-close.\n\t\tif (tag.isOpen())\n\t\t{\n\t\t\t// Render the body. The default strategy will simply call the component's\n\t\t\t// onComponentTagBody() implementation.\n\t\t\tgetMarkupSourcingStrategy().onComponentTagBody(this, markupStream, tag);\n\n\t\t\t// Render close tag\n\t\t\tif (openTag.isOpen())\n\t\t\t{\n\t\t\t\trenderClosingComponentTag(markupStream, tag, renderBodyOnly);\n\t\t\t}\n\t\t\telse if (renderBodyOnly == false)\n\t\t\t{\n\t\t\t\tif (needToRenderTag(openTag))\n\t\t\t\t{\n\t\t\t\t\t// Close the manually opened tag. And since the user might have changed the\n\t\t\t\t\t// tag name ...\n\t\t\t\t\tgetResponse().write(tag.syntheticCloseTagString());\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tcatch (WicketRuntimeException wre)\n\t{\n\t\tthrow wre;\n\t}\n\tcatch (RuntimeException re)\n\t{\n\t\tthrow new WicketRuntimeException(\"Exception in rendering component: \" + this, re);\n\t}\n}",
        "accept_response": "public final void internalRenderComponent()\n{\n\tfinal IMarkupFragment markup = getMarkup();\n\tif (markup == null)\n\t{\n\t\tthrow new MarkupException(\"Markup not found. Component: \" + toString());\n\t}\n\n\tfinal MarkupStream markupStream = new MarkupStream(markup);\n\n\t// Get mutable copy of next tag\n\tfinal ComponentTag openTag = markupStream.getTag();\n\tfinal ComponentTag tag = openTag.mutable();\n\n\t// Call any tag handler\n\tonComponentTag(tag);\n\n\t// If we're an openclose tag\n\tif (!tag.isOpenClose() && !tag.isOpen())\n\t{\n\t\t// We were something other than <tag> or <tag/>\n\t\tmarkupStream.throwMarkupException(\"Method renderComponent called on bad markup element: \" +\n\t\t\ttag);\n\t}\n\n\tif (tag.isOpenClose() && openTag.isOpen())\n\t{\n\t\tmarkupStream.throwMarkupException(\"You can not modify a open tag to open-close: \" + tag);\n\t}\n\n\ttry\n\t{\n\t\t// Render open tag\n\t\tboolean renderBodyOnly = getRenderBodyOnly();\n\t\tif (renderBodyOnly)\n\t\t{\n\t\t\tExceptionSettings.NotRenderableErrorStrategy notRenderableErrorStrategy = ExceptionSettings.NotRenderableErrorStrategy.LOG_WARNING;\n\t\t\tif (Application.exists())\n\t\t\t{\n\t\t\t\tnotRenderableErrorStrategy = getApplication().getExceptionSettings().getNotRenderableErrorStrategy();\n\t\t\t}\n\n\t\t\tif (getFlag(FLAG_OUTPUT_MARKUP_ID))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Markup id set on a component that renders its body only. \" +\n\t\t\t\t                               \"Markup id: %s, component id: %s.\", getMarkupId(), getId());\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t\tif (getFlag(FLAG_PLACEHOLDER))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Placeholder tag set on a component that renders its body only. \" +\n\t\t\t\t                               \"Component id: %s.\", getId());\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t}\n\t\telse\n\t\t{\n\t\t\trenderComponentTag(tag);\n\t\t}\n\t\tmarkupStream.next();\n\n\t\t// Render the body only if open-body-close. Do not render if open-close.\n\t\tif (tag.isOpen())\n\t\t{\n\t\t\t// Render the body. The default strategy will simply call the component's\n\t\t\t// onComponentTagBody() implementation.\n\t\t\tgetMarkupSourcingStrategy().onComponentTagBody(this, markupStream, tag);\n\n\t\t\t// Render close tag\n\t\t\tif (openTag.isOpen())\n\t\t\t{\n\t\t\t\trenderClosingComponentTag(markupStream, tag, renderBodyOnly);\n\t\t\t}\n\t\t\telse if (renderBodyOnly == false)\n\t\t\t{\n\t\t\t\tif (needToRenderTag(openTag))\n\t\t\t\t{\n\t\t\t\t\t// Close the manually opened tag. And since the user might have changed the\n\t\t\t\t\t// tag name ...\n\t\t\t\t\tgetResponse().write(tag.syntheticCloseTagString());\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tcatch (WicketRuntimeException wre)\n\t{\n\t\tthrow wre;\n\t}\n\tcatch (RuntimeException re)\n\t{\n\t\tthrow new WicketRuntimeException(\"Exception in rendering component: \" + this, re);\n\t}\n}",
        "reject_response": "public final void internalRenderComponent()\n{\n\tfinal IMarkupFragment markup = getMarkup();\n\tif (markup == null)\n\t{\n\t\tthrow new MarkupException(\"Markup not found. Component: \" + toString());\n\t}\n\n\tfinal MarkupStream markupStream = new MarkupStream(markup);\n\n\t// Get mutable copy of next tag\n\tfinal ComponentTag openTag = markupStream.getTag();\n\tfinal ComponentTag tag = openTag.mutable();\n\n\t// Call any tag handler\n\tonComponentTag(tag);\n\n\t// If we're an openclose tag\n\tif (!tag.isOpenClose() && !tag.isOpen())\n\t{\n\t\t// We were something other than <tag> or <tag/>\n\t\tmarkupStream.throwMarkupException(\"Method renderComponent called on bad markup element: \" +\n\t\t\ttag);\n\t}\n\n\tif (tag.isOpenClose() && openTag.isOpen())\n\t{\n\t\tmarkupStream.throwMarkupException(\"You can not modify a open tag to open-close: \" + tag);\n\t}\n\n\ttry\n\t{\n\t\t// Render open tag\n\t\tboolean renderBodyOnly = getRenderBodyOnly();\n\t\tif (renderBodyOnly)\n\t\t{\n\t\t\tExceptionSettings.NotRenderableErrorStrategy notRenderableErrorStrategy = ExceptionSettings.NotRenderableErrorStrategy.LOG_WARNING;\n\t\t\tif (Application.exists())\n\t\t\t{\n\t\t\t\tnotRenderableErrorStrategy = getApplication().getExceptionSettings().getNotRenderableErrorStrategy();\n\t\t\t}\n\n\t\t\tif (getFlag(FLAG_OUTPUT_MARKUP_ID))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Markup id set on a component that renders its body only. \" +\n\t\t\t\t                               \"Markup id: %s, component id: %s.\", getMarkupId(), getId());\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(String.format(\n\t\t\t\t\t\"Markup id set on a component that renders its body only. \"\n\t\t\t\t\t\t+ \"Markup id: %s, component id: %s.\", getMarkupId(), getId()));\n\t\t\t}\n\t\t\tif (getFlag(FLAG_PLACEHOLDER))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Placeholder tag set on a component that renders its body only. \" +\n\t\t\t\t                               \"Component id: %s.\", getId());\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t}\n\t\telse\n\t\t{\n\t\t\trenderComponentTag(tag);\n\t\t}\n\t\tmarkupStream.next();\n\n\t\t// Render the body only if open-body-close. Do not render if open-close.\n\t\tif (tag.isOpen())\n\t\t{\n\t\t\t// Render the body. The default strategy will simply call the component's\n\t\t\t// onComponentTagBody() implementation.\n\t\t\tgetMarkupSourcingStrategy().onComponentTagBody(this, markupStream, tag);\n\n\t\t\t// Render close tag\n\t\t\tif (openTag.isOpen())\n\t\t\t{\n\t\t\t\trenderClosingComponentTag(markupStream, tag, renderBodyOnly);\n\t\t\t}\n\t\t\telse if (renderBodyOnly == false)\n\t\t\t{\n\t\t\t\tif (needToRenderTag(openTag))\n\t\t\t\t{\n\t\t\t\t\t// Close the manually opened tag. And since the user might have changed the\n\t\t\t\t\t// tag name ...\n\t\t\t\t\tgetResponse().write(tag.syntheticCloseTagString());\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tcatch (WicketRuntimeException wre)\n\t{\n\t\tthrow wre;\n\t}\n\tcatch (RuntimeException re)\n\t{\n\t\tthrow new WicketRuntimeException(\"Exception in rendering component: \" + this, re);\n\t}\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2494,
        "instruction": "@Override\npublic boolean apply(Object deploymentPlanItem, AssemblyTemplateConstructor atc) {\n    if (!(deploymentPlanItem instanceof Service)) return false;\n\n    String type = lookupType(deploymentPlanItem);\n    if (type==null) return false;\n\n\n    Object old = atc.getInstantiator();\n    if (old!=null && !old.equals(BrooklynAssemblyTemplateInstantiator.class)) {\n        log.warn(\"Can't mix Brooklyn entities with non-Brooklyn entities (at present): \"+old);\n        return false;\n    }\n\n    // TODO should we build up a new type, BrooklynEntityComponentTemplate here\n    // complete w EntitySpec -- ie merge w BrooklynComponentTemplateResolver ?\n\n    Builder<? extends PlatformComponentTemplate> builder = PlatformComponentTemplate.builder();\n    builder.type( type.indexOf(':')==-1 ? \"brooklyn:\"+type : type );\n\n    // currently instantiator must be brooklyn at the ATC level\n    // optionally would be nice to support multiple/mixed instantiators,\n    // ie at the component level, perhaps with the first one responsible for building the app\n    atc.instantiator(BrooklynAssemblyTemplateInstantiator.class);\n\n    String name = ((Service)deploymentPlanItem).getName();\n    if (!Strings.isBlank(name)) builder.name(name);\n\n    // configuration\n    Map<String, Object> attrs = MutableMap.copyOf( ((Service)deploymentPlanItem).getCustomAttributes() );\n\n    if (attrs.containsKey(\"id\"))\n        builder.customAttribute(\"planId\", attrs.remove(\"id\"));\n\n    Object location = attrs.remove(\"location\");\n    if (location!=null)\n        builder.customAttribute(\"location\", location);\n    Object locations = attrs.remove(\"locations\");\n    if (locations!=null)\n        builder.customAttribute(\"locations\", locations);\n    Object iconUrl = attrs.remove(BrooklynConfigKeys.ICON_URL.getName());\n    if (iconUrl!=null)\n        builder.customAttribute(BrooklynConfigKeys.ICON_URL.getName(), iconUrl);\n\n    MutableMap<Object, Object> brooklynFlags = MutableMap.of();\n    Object origBrooklynFlags = attrs.remove(BrooklynCampReservedKeys.BROOKLYN_FLAGS);\n    if (origBrooklynFlags!=null) {\n        if (!(origBrooklynFlags instanceof Map))\n            throw new IllegalArgumentException(\"brooklyn.flags must be a map of brooklyn flags\");\n        brooklynFlags.putAll((Map<?,?>)origBrooklynFlags);\n    }\n\n    addCustomMapAttributeIfNonNull(builder, attrs, BrooklynCampReservedKeys.BROOKLYN_CONFIG);\n    addCustomListAttributeIfNonNull(builder, attrs, BrooklynCampReservedKeys.BROOKLYN_POLICIES);\n    addCustomListAttributeIfNonNull(builder, attrs, BrooklynCampReservedKeys.BROOKLYN_ENRICHERS);\n    addCustomListAttributeIfNonNull(builder, attrs, BrooklynCampReservedKeys.BROOKLYN_INITIALIZERS);\n    addCustomListAttributeIfNonNull(builder, attrs, BrooklynCampReservedKeys.BROOKLYN_CHILDREN);\n    addCustomListAttributeIfNonNull(builder, attrs, BrooklynCampReservedKeys.BROOKLYN_PARAMETERS);\n    addCustomMapAttributeIfNonNull(builder, attrs, BrooklynCampReservedKeys.BROOKLYN_CATALOG);\n    addCustomListAttributeIfNonNull(builder, attrs, BrooklynCampReservedKeys.BROOKLYN_TAGS);\n\n    brooklynFlags.putAll(attrs);\n    if (!brooklynFlags.isEmpty()) {\n        builder.customAttribute(BrooklynCampReservedKeys.BROOKLYN_FLAGS, brooklynFlags);\n    }\n\n    atc.add(builder.build());\n\n    return true;\n}",
        "accept_response": "@Override\npublic boolean apply(Object deploymentPlanItem, AssemblyTemplateConstructor atc) {\n    if (!(deploymentPlanItem instanceof Service)) return false;\n\n    String type = lookupType(deploymentPlanItem);\n    if (type==null) return false;\n\n    log.trace(\"Item {} being instantiated with {}\", deploymentPlanItem, type);\n\n    Object old = atc.getInstantiator();\n    if (old!=null && !old.equals(BrooklynAssemblyTemplateInstantiator.class)) {\n        log.warn(\"Can't mix Brooklyn entities with non-Brooklyn entities (at present): \"+old);\n        return false;\n    }\n\n    // TODO should we build up a new type, BrooklynEntityComponentTemplate here\n    // complete w EntitySpec -- ie merge w BrooklynComponentTemplateResolver ?\n\n    Builder<? extends PlatformComponentTemplate> builder = PlatformComponentTemplate.builder();\n    builder.type( type.indexOf(':')==-1 ? \"brooklyn:\"+type : type );\n\n    // currently instantiator must be brooklyn at the ATC level\n    // optionally would be nice to support multiple/mixed instantiators,\n    // ie at the component level, perhaps with the first one responsible for building the app\n    atc.instantiator(BrooklynAssemblyTemplateInstantiator.class);\n\n    String name = ((Service)deploymentPlanItem).getName();\n    if (!Strings.isBlank(name)) builder.name(name);\n\n    // configuration\n    Map<String, Object> attrs = MutableMap.copyOf( ((Service)deploymentPlanItem).getCustomAttributes() );\n\n    if (attrs.containsKey(\"id\"))\n        builder.customAttribute(\"planId\", attrs.remove(\"id\"));\n\n    Object location = attrs.remove(\"location\");\n    if (location!=null)\n        builder.customAttribute(\"location\", location);\n    Object locations = attrs.remove(\"locations\");\n    if (locations!=null)\n        builder.customAttribute(\"locations\", locations);\n    Object iconUrl = attrs.remove(BrooklynConfigKeys.ICON_URL.getName());\n    if (iconUrl!=null)\n        builder.customAttribute(BrooklynConfigKeys.ICON_URL.getName(), iconUrl);\n\n    MutableMap<Object, Object> brooklynFlags = MutableMap.of();\n    Object origBrooklynFlags = attrs.remove(BrooklynCampReservedKeys.BROOKLYN_FLAGS);\n    if (origBrooklynFlags!=null) {\n        if (!(origBrooklynFlags instanceof Map))\n            throw new IllegalArgumentException(\"brooklyn.flags must be a map of brooklyn flags\");\n        brooklynFlags.putAll((Map<?,?>)origBrooklynFlags);\n    }\n\n    addCustomMapAttributeIfNonNull(builder, attrs, BrooklynCampReservedKeys.BROOKLYN_CONFIG);\n    addCustomListAttributeIfNonNull(builder, attrs, BrooklynCampReservedKeys.BROOKLYN_POLICIES);\n    addCustomListAttributeIfNonNull(builder, attrs, BrooklynCampReservedKeys.BROOKLYN_ENRICHERS);\n    addCustomListAttributeIfNonNull(builder, attrs, BrooklynCampReservedKeys.BROOKLYN_INITIALIZERS);\n    addCustomListAttributeIfNonNull(builder, attrs, BrooklynCampReservedKeys.BROOKLYN_CHILDREN);\n    addCustomListAttributeIfNonNull(builder, attrs, BrooklynCampReservedKeys.BROOKLYN_PARAMETERS);\n    addCustomMapAttributeIfNonNull(builder, attrs, BrooklynCampReservedKeys.BROOKLYN_CATALOG);\n    addCustomListAttributeIfNonNull(builder, attrs, BrooklynCampReservedKeys.BROOKLYN_TAGS);\n\n    brooklynFlags.putAll(attrs);\n    if (!brooklynFlags.isEmpty()) {\n        builder.customAttribute(BrooklynCampReservedKeys.BROOKLYN_FLAGS, brooklynFlags);\n    }\n\n    atc.add(builder.build());\n\n    return true;\n}",
        "reject_response": "@Override\npublic boolean apply(Object deploymentPlanItem, AssemblyTemplateConstructor atc) {\n    if (!(deploymentPlanItem instanceof Service)) return false;\n\n    String type = lookupType(deploymentPlanItem);\n    if (type==null) return false;\n\n    log.debug(\"Item \"+deploymentPlanItem+\" being instantiated with \"+type);\n\n    Object old = atc.getInstantiator();\n    if (old!=null && !old.equals(BrooklynAssemblyTemplateInstantiator.class)) {\n        log.warn(\"Can't mix Brooklyn entities with non-Brooklyn entities (at present): \"+old);\n        return false;\n    }\n\n    // TODO should we build up a new type, BrooklynEntityComponentTemplate here\n    // complete w EntitySpec -- ie merge w BrooklynComponentTemplateResolver ?\n\n    Builder<? extends PlatformComponentTemplate> builder = PlatformComponentTemplate.builder();\n    builder.type( type.indexOf(':')==-1 ? \"brooklyn:\"+type : type );\n\n    // currently instantiator must be brooklyn at the ATC level\n    // optionally would be nice to support multiple/mixed instantiators,\n    // ie at the component level, perhaps with the first one responsible for building the app\n    atc.instantiator(BrooklynAssemblyTemplateInstantiator.class);\n\n    String name = ((Service)deploymentPlanItem).getName();\n    if (!Strings.isBlank(name)) builder.name(name);\n\n    // configuration\n    Map<String, Object> attrs = MutableMap.copyOf( ((Service)deploymentPlanItem).getCustomAttributes() );\n\n    if (attrs.containsKey(\"id\"))\n        builder.customAttribute(\"planId\", attrs.remove(\"id\"));\n\n    Object location = attrs.remove(\"location\");\n    if (location!=null)\n        builder.customAttribute(\"location\", location);\n    Object locations = attrs.remove(\"locations\");\n    if (locations!=null)\n        builder.customAttribute(\"locations\", locations);\n    Object iconUrl = attrs.remove(BrooklynConfigKeys.ICON_URL.getName());\n    if (iconUrl!=null)\n        builder.customAttribute(BrooklynConfigKeys.ICON_URL.getName(), iconUrl);\n\n    MutableMap<Object, Object> brooklynFlags = MutableMap.of();\n    Object origBrooklynFlags = attrs.remove(BrooklynCampReservedKeys.BROOKLYN_FLAGS);\n    if (origBrooklynFlags!=null) {\n        if (!(origBrooklynFlags instanceof Map))\n            throw new IllegalArgumentException(\"brooklyn.flags must be a map of brooklyn flags\");\n        brooklynFlags.putAll((Map<?,?>)origBrooklynFlags);\n    }\n\n    addCustomMapAttributeIfNonNull(builder, attrs, BrooklynCampReservedKeys.BROOKLYN_CONFIG);\n    addCustomListAttributeIfNonNull(builder, attrs, BrooklynCampReservedKeys.BROOKLYN_POLICIES);\n    addCustomListAttributeIfNonNull(builder, attrs, BrooklynCampReservedKeys.BROOKLYN_ENRICHERS);\n    addCustomListAttributeIfNonNull(builder, attrs, BrooklynCampReservedKeys.BROOKLYN_INITIALIZERS);\n    addCustomListAttributeIfNonNull(builder, attrs, BrooklynCampReservedKeys.BROOKLYN_CHILDREN);\n    addCustomListAttributeIfNonNull(builder, attrs, BrooklynCampReservedKeys.BROOKLYN_PARAMETERS);\n    addCustomMapAttributeIfNonNull(builder, attrs, BrooklynCampReservedKeys.BROOKLYN_CATALOG);\n    addCustomListAttributeIfNonNull(builder, attrs, BrooklynCampReservedKeys.BROOKLYN_TAGS);\n\n    brooklynFlags.putAll(attrs);\n    if (!brooklynFlags.isEmpty()) {\n        builder.customAttribute(BrooklynCampReservedKeys.BROOKLYN_FLAGS, brooklynFlags);\n    }\n\n    atc.add(builder.build());\n\n    return true;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2387,
        "instruction": "@Override\npublic void invalidateCache(KeyExtent failedExtent) {\n  wLock.lock();\n  try {\n    badExtents.add(failedExtent);\n  } finally {\n    wLock.unlock();\n  }\n  if (log.isTraceEnabled())\n}",
        "accept_response": "@Override\npublic void invalidateCache(KeyExtent failedExtent) {\n  wLock.lock();\n  try {\n    badExtents.add(failedExtent);\n  } finally {\n    wLock.unlock();\n  }\n  if (log.isTraceEnabled())\n    log.trace(\"Invalidated extent={}\", failedExtent);\n}",
        "reject_response": "@Override\npublic void invalidateCache(KeyExtent failedExtent) {\n  wLock.lock();\n  try {\n    badExtents.add(failedExtent);\n  } finally {\n    wLock.unlock();\n  }\n  if (log.isTraceEnabled())\n    log.trace(\"Invalidated extent=\" + failedExtent);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2838,
        "instruction": "@Nullable public IgniteInternalFuture<AffinityTopologyVersion> affinityReadyFuture(AffinityTopologyVersion ver) {\n    GridDhtPartitionsExchangeFuture lastInitializedFut0 = lastInitializedFut;\n\n    if (lastInitializedFut0 != null && lastInitializedFut0.initialVersion().compareTo(ver) == 0) {\n        if (log.isTraceEnabled())\n\n        return lastInitializedFut0;\n    }\n\n    AffinityTopologyVersion topVer = exchFuts.readyTopVer();\n\n    if (topVer.compareTo(ver) >= 0) {\n        if (log.isTraceEnabled())\n            log.trace(\"Return finished future for topology ready future [ver=\" + ver + \", topVer=\" + topVer + ']');\n\n        return null;\n    }\n\n    GridFutureAdapter<AffinityTopologyVersion> fut = F.addIfAbsent(readyFuts, ver,\n        new AffinityReadyFuture(ver));\n\n    if (log.isDebugEnabled())\n        log.debug(\"Created topology ready future [ver=\" + ver + \", fut=\" + fut + ']');\n\n    topVer = exchFuts.readyTopVer();\n\n    if (topVer.compareTo(ver) >= 0) {\n        if (log.isTraceEnabled())\n            log.trace(\"Completing created topology ready future \" +\n                \"[ver=\" + topVer + \", topVer=\" + topVer + \", fut=\" + fut + ']');\n\n        fut.onDone(topVer);\n    }\n    else if (stopErr != null)\n        fut.onDone(stopErr);\n\n    return fut;\n}",
        "accept_response": "@Nullable public IgniteInternalFuture<AffinityTopologyVersion> affinityReadyFuture(AffinityTopologyVersion ver) {\n    GridDhtPartitionsExchangeFuture lastInitializedFut0 = lastInitializedFut;\n\n    if (lastInitializedFut0 != null && lastInitializedFut0.initialVersion().compareTo(ver) == 0) {\n        if (log.isTraceEnabled())\n            log.trace(\"Return lastInitializedFut for topology ready future \" +\n                \"[ver=\" + ver + \", fut=\" + lastInitializedFut0 + ']');\n\n        return lastInitializedFut0;\n    }\n\n    AffinityTopologyVersion topVer = exchFuts.readyTopVer();\n\n    if (topVer.compareTo(ver) >= 0) {\n        if (log.isTraceEnabled())\n            log.trace(\"Return finished future for topology ready future [ver=\" + ver + \", topVer=\" + topVer + ']');\n\n        return null;\n    }\n\n    GridFutureAdapter<AffinityTopologyVersion> fut = F.addIfAbsent(readyFuts, ver,\n        new AffinityReadyFuture(ver));\n\n    if (log.isDebugEnabled())\n        log.debug(\"Created topology ready future [ver=\" + ver + \", fut=\" + fut + ']');\n\n    topVer = exchFuts.readyTopVer();\n\n    if (topVer.compareTo(ver) >= 0) {\n        if (log.isTraceEnabled())\n            log.trace(\"Completing created topology ready future \" +\n                \"[ver=\" + topVer + \", topVer=\" + topVer + \", fut=\" + fut + ']');\n\n        fut.onDone(topVer);\n    }\n    else if (stopErr != null)\n        fut.onDone(stopErr);\n\n    return fut;\n}",
        "reject_response": "@Nullable public IgniteInternalFuture<AffinityTopologyVersion> affinityReadyFuture(AffinityTopologyVersion ver) {\n    GridDhtPartitionsExchangeFuture lastInitializedFut0 = lastInitializedFut;\n\n    if (lastInitializedFut0 != null && lastInitializedFut0.initialVersion().compareTo(ver) == 0) {\n        if (log.isTraceEnabled())\n        if (log.isDebugEnabled())\n            log.debug(\"Return lastInitializedFut for topology ready future \" +\n\n        return lastInitializedFut0;\n    }\n\n    AffinityTopologyVersion topVer = exchFuts.readyTopVer();\n\n    if (topVer.compareTo(ver) >= 0) {\n        if (log.isTraceEnabled())\n            log.trace(\"Return finished future for topology ready future [ver=\" + ver + \", topVer=\" + topVer + ']');\n\n        return null;\n    }\n\n    GridFutureAdapter<AffinityTopologyVersion> fut = F.addIfAbsent(readyFuts, ver,\n        new AffinityReadyFuture(ver));\n\n    if (log.isDebugEnabled())\n        log.debug(\"Created topology ready future [ver=\" + ver + \", fut=\" + fut + ']');\n\n    topVer = exchFuts.readyTopVer();\n\n    if (topVer.compareTo(ver) >= 0) {\n        if (log.isTraceEnabled())\n            log.trace(\"Completing created topology ready future \" +\n                \"[ver=\" + topVer + \", topVer=\" + topVer + \", fut=\" + fut + ']');\n\n        fut.onDone(topVer);\n    }\n    else if (stopErr != null)\n        fut.onDone(stopErr);\n\n    return fut;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2737,
        "instruction": "private void scheduleIdleScanTask() {\n  if (!running) {\n    return;\n  }\n  TimerTask idleScanTask = new TimerTask(){\n    @Override\n    public void run() {\n      if (!running) {\n        return;\n      }\n      try {\n        closeIdle(false);\n      } finally {\n        // explicitly reschedule so next execution occurs relative\n        // to the end of this scan, not the beginning\n        scheduleIdleScanTask();\n      }\n    }\n  };\n  idleScanTimer.schedule(idleScanTask, idleScanInterval);\n}",
        "accept_response": "private void scheduleIdleScanTask() {\n  if (!running) {\n    return;\n  }\n  TimerTask idleScanTask = new TimerTask(){\n    @Override\n    public void run() {\n      if (!running) {\n        return;\n      }\n      LOG.debug(\"{}: task running\", Thread.currentThread().getName());\n      try {\n        closeIdle(false);\n      } finally {\n        // explicitly reschedule so next execution occurs relative\n        // to the end of this scan, not the beginning\n        scheduleIdleScanTask();\n      }\n    }\n  };\n  idleScanTimer.schedule(idleScanTask, idleScanInterval);\n}",
        "reject_response": "private void scheduleIdleScanTask() {\n  if (!running) {\n    return;\n  }\n  TimerTask idleScanTask = new TimerTask(){\n    @Override\n    public void run() {\n      if (!running) {\n        return;\n      }\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(Thread.currentThread().getName()+\": task running\");\n      }\n      try {\n        closeIdle(false);\n      } finally {\n        // explicitly reschedule so next execution occurs relative\n        // to the end of this scan, not the beginning\n        scheduleIdleScanTask();\n      }\n    }\n  };\n  idleScanTimer.schedule(idleScanTask, idleScanInterval);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2555,
        "instruction": "private List<BundleJob> findBundles(Entity entity, String clusterName) throws FalconException {\n    Cluster cluster = STORE.get(EntityType.CLUSTER, clusterName);\n    List<BundleJob> filteredJobs = new ArrayList<BundleJob>();\n    try {\n        List<BundleJob> jobs = OozieClientFactory.get(cluster.getName()).getBundleJobsInfo(OozieClient.FILTER_NAME\n            + \"=\" + EntityUtil.getWorkflowName(entity) + \";\", 0, 256);\n        if (jobs != null) {\n            for (BundleJob job : jobs) {\n                // Path is extracted twice as to handle changes in hadoop configurations for nameservices.\n                if (EntityUtil.isStagingPath(cluster, entity,\n                        new Path((new Path(job.getAppPath())).toUri().getPath()))) {\n                    //Load bundle as coord info is not returned in getBundleJobsInfo()\n                    BundleJob bundle = getBundleInfo(clusterName, job.getId());\n                    filteredJobs.add(bundle);\n                }\n            }\n        }\n    } catch (OozieClientException e) {\n        throw new FalconException(e);\n    }\n    return filteredJobs;\n}",
        "accept_response": "private List<BundleJob> findBundles(Entity entity, String clusterName) throws FalconException {\n    Cluster cluster = STORE.get(EntityType.CLUSTER, clusterName);\n    List<BundleJob> filteredJobs = new ArrayList<BundleJob>();\n    try {\n        List<BundleJob> jobs = OozieClientFactory.get(cluster.getName()).getBundleJobsInfo(OozieClient.FILTER_NAME\n            + \"=\" + EntityUtil.getWorkflowName(entity) + \";\", 0, 256);\n        if (jobs != null) {\n            for (BundleJob job : jobs) {\n                // Path is extracted twice as to handle changes in hadoop configurations for nameservices.\n                if (EntityUtil.isStagingPath(cluster, entity,\n                        new Path((new Path(job.getAppPath())).toUri().getPath()))) {\n                    //Load bundle as coord info is not returned in getBundleJobsInfo()\n                    BundleJob bundle = getBundleInfo(clusterName, job.getId());\n                    filteredJobs.add(bundle);\n                    LOG.trace(\"Found bundle {} with app path {} and status {}\",\n                            job.getId(), job.getAppPath(), job.getStatus());\n                }\n            }\n        }\n    } catch (OozieClientException e) {\n        throw new FalconException(e);\n    }\n    return filteredJobs;\n}",
        "reject_response": "private List<BundleJob> findBundles(Entity entity, String clusterName) throws FalconException {\n    Cluster cluster = STORE.get(EntityType.CLUSTER, clusterName);\n    List<BundleJob> filteredJobs = new ArrayList<BundleJob>();\n    try {\n        List<BundleJob> jobs = OozieClientFactory.get(cluster.getName()).getBundleJobsInfo(OozieClient.FILTER_NAME\n            + \"=\" + EntityUtil.getWorkflowName(entity) + \";\", 0, 256);\n        if (jobs != null) {\n            for (BundleJob job : jobs) {\n                // Path is extracted twice as to handle changes in hadoop configurations for nameservices.\n                if (EntityUtil.isStagingPath(cluster, entity,\n                        new Path((new Path(job.getAppPath())).toUri().getPath()))) {\n                    //Load bundle as coord info is not returned in getBundleJobsInfo()\n                    BundleJob bundle = getBundleInfo(clusterName, job.getId());\n                    filteredJobs.add(bundle);\n                    LOG.debug(\"Found bundle {} with app path {} and status {}\",\n                }\n            }\n        }\n    } catch (OozieClientException e) {\n        throw new FalconException(e);\n    }\n    return filteredJobs;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2573,
        "instruction": "@Override\n@CronTarget(jobName = JobName.UPDATE_SMS_OUTBOUND_WITH_CAMPAIGN_MESSAGE)\npublic void storeTemplateMessageIntoSmsOutBoundTable() throws JobExecutionException {\n    final Collection<SmsCampaign> smsCampaignDataCollection = this.smsCampaignRepository.findByTriggerTypeAndStatus(\n            SmsCampaignTriggerType.SCHEDULE.getValue(), SmsCampaignStatus.ACTIVE.getValue());\n    if (smsCampaignDataCollection != null) {\n        for (SmsCampaign smsCampaign : smsCampaignDataCollection) {\n            LocalDateTime tenantDateNow = tenantDateTime();\n            LocalDateTime nextTriggerDate = smsCampaign.getNextTriggerDate();\n\n            if (nextTriggerDate.isBefore(tenantDateNow)) {\n                insertDirectCampaignIntoSmsOutboundTable(smsCampaign);\n                this.updateTriggerDates(smsCampaign.getId());\n            }\n        }\n    }\n}",
        "accept_response": "@Override\n@CronTarget(jobName = JobName.UPDATE_SMS_OUTBOUND_WITH_CAMPAIGN_MESSAGE)\npublic void storeTemplateMessageIntoSmsOutBoundTable() throws JobExecutionException {\n    final Collection<SmsCampaign> smsCampaignDataCollection = this.smsCampaignRepository.findByTriggerTypeAndStatus(\n            SmsCampaignTriggerType.SCHEDULE.getValue(), SmsCampaignStatus.ACTIVE.getValue());\n    if (smsCampaignDataCollection != null) {\n        for (SmsCampaign smsCampaign : smsCampaignDataCollection) {\n            LocalDateTime tenantDateNow = tenantDateTime();\n            LocalDateTime nextTriggerDate = smsCampaign.getNextTriggerDate();\n\n            logger.info(\"tenant time {} trigger time {} {}\",\n                new Object[] { tenantDateNow, nextTriggerDate, JobName.UPDATE_SMS_OUTBOUND_WITH_CAMPAIGN_MESSAGE.name() });\n            if (nextTriggerDate.isBefore(tenantDateNow)) {\n                insertDirectCampaignIntoSmsOutboundTable(smsCampaign);\n                this.updateTriggerDates(smsCampaign.getId());\n            }\n        }\n    }\n}",
        "reject_response": "@Override\n@CronTarget(jobName = JobName.UPDATE_SMS_OUTBOUND_WITH_CAMPAIGN_MESSAGE)\npublic void storeTemplateMessageIntoSmsOutBoundTable() throws JobExecutionException {\n    final Collection<SmsCampaign> smsCampaignDataCollection = this.smsCampaignRepository.findByTriggerTypeAndStatus(\n            SmsCampaignTriggerType.SCHEDULE.getValue(), SmsCampaignStatus.ACTIVE.getValue());\n    if (smsCampaignDataCollection != null) {\n        for (SmsCampaign smsCampaign : smsCampaignDataCollection) {\n            LocalDateTime tenantDateNow = tenantDateTime();\n            LocalDateTime nextTriggerDate = smsCampaign.getNextTriggerDate();\n\n            logger.info(\"tenant time \" + tenantDateNow.toString() + \" trigger time \" + nextTriggerDate.toString() + JobName.UPDATE_SMS_OUTBOUND_WITH_CAMPAIGN_MESSAGE.name());\n            if (nextTriggerDate.isBefore(tenantDateNow)) {\n                insertDirectCampaignIntoSmsOutboundTable(smsCampaign);\n                this.updateTriggerDates(smsCampaign.getId());\n            }\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2779,
        "instruction": "private ClassicHttpResponse handleCacheHit(\n        final HttpHost target,\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain,\n        final HttpCacheEntry entry) throws IOException, HttpException {\n    final HttpClientContext context  = scope.clientContext;\n    context.setAttribute(HttpCoreContext.HTTP_REQUEST, request);\n    recordCacheHit(target, request);\n    final Date now = getCurrentDate();\n    if (suitabilityChecker.canCachedResponseBeUsed(target, request, entry, now)) {\n        LOG.debug(\"Cache hit\");\n        try {\n            return convert(generateCachedResponse(request, context, entry, now), scope);\n        } catch (final ResourceIOException ex) {\n            recordCacheFailure(target, request);\n            if (!mayCallBackend(request)) {\n                return convert(generateGatewayTimeout(context), scope);\n            }\n            setResponseStatus(scope.clientContext, CacheResponseStatus.FAILURE);\n            return chain.proceed(request, scope);\n        }\n    } else if (!mayCallBackend(request)) {\n        LOG.debug(\"Cache entry not suitable but only-if-cached requested\");\n        return convert(generateGatewayTimeout(context), scope);\n    } else if (!(entry.getStatus() == HttpStatus.SC_NOT_MODIFIED && !suitabilityChecker.isConditional(request))) {\n        LOG.debug(\"Revalidating cache entry\");\n        try {\n            if (cacheRevalidator != null\n                    && !staleResponseNotAllowed(request, entry, now)\n                    && validityPolicy.mayReturnStaleWhileRevalidating(entry, now)) {\n                final String exchangeId = ExecSupport.getNextExchangeId();\n                final ExecChain.Scope fork = new ExecChain.Scope(\n                        exchangeId,\n                        scope.route,\n                        scope.originalRequest,\n                        scope.execRuntime.fork(null),\n                        HttpClientContext.create());\n                final SimpleHttpResponse response = generateCachedResponse(request, context, entry, now);\n                cacheRevalidator.revalidateCacheEntry(\n                        responseCache.generateKey(target, request, entry),\n                        new DefaultCacheRevalidator.RevalidationCall() {\n\n                    @Override\n                    public ClassicHttpResponse execute() throws HttpException, IOException {\n                        return revalidateCacheEntry(target, request, fork, chain, entry);\n                    }\n\n                });\n                return convert(response, scope);\n            }\n            return revalidateCacheEntry(target, request, scope, chain, entry);\n        } catch (final IOException ioex) {\n            return convert(handleRevalidationFailure(request, context, entry, now), scope);\n        }\n    } else {\n        LOG.debug(\"Cache entry not usable; calling backend\");\n        return callBackend(target, request, scope, chain);\n    }\n}",
        "accept_response": "private ClassicHttpResponse handleCacheHit(\n        final HttpHost target,\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain,\n        final HttpCacheEntry entry) throws IOException, HttpException {\n    final HttpClientContext context  = scope.clientContext;\n    context.setAttribute(HttpCoreContext.HTTP_REQUEST, request);\n    recordCacheHit(target, request);\n    final Date now = getCurrentDate();\n    if (suitabilityChecker.canCachedResponseBeUsed(target, request, entry, now)) {\n        LOG.debug(\"Cache hit\");\n        try {\n            return convert(generateCachedResponse(request, context, entry, now), scope);\n        } catch (final ResourceIOException ex) {\n            recordCacheFailure(target, request);\n            if (!mayCallBackend(request)) {\n                return convert(generateGatewayTimeout(context), scope);\n            }\n            setResponseStatus(scope.clientContext, CacheResponseStatus.FAILURE);\n            return chain.proceed(request, scope);\n        }\n    } else if (!mayCallBackend(request)) {\n        LOG.debug(\"Cache entry not suitable but only-if-cached requested\");\n        return convert(generateGatewayTimeout(context), scope);\n    } else if (!(entry.getStatus() == HttpStatus.SC_NOT_MODIFIED && !suitabilityChecker.isConditional(request))) {\n        LOG.debug(\"Revalidating cache entry\");\n        try {\n            if (cacheRevalidator != null\n                    && !staleResponseNotAllowed(request, entry, now)\n                    && validityPolicy.mayReturnStaleWhileRevalidating(entry, now)) {\n                LOG.debug(\"Serving stale with asynchronous revalidation\");\n                final String exchangeId = ExecSupport.getNextExchangeId();\n                final ExecChain.Scope fork = new ExecChain.Scope(\n                        exchangeId,\n                        scope.route,\n                        scope.originalRequest,\n                        scope.execRuntime.fork(null),\n                        HttpClientContext.create());\n                final SimpleHttpResponse response = generateCachedResponse(request, context, entry, now);\n                cacheRevalidator.revalidateCacheEntry(\n                        responseCache.generateKey(target, request, entry),\n                        new DefaultCacheRevalidator.RevalidationCall() {\n\n                    @Override\n                    public ClassicHttpResponse execute() throws HttpException, IOException {\n                        return revalidateCacheEntry(target, request, fork, chain, entry);\n                    }\n\n                });\n                return convert(response, scope);\n            }\n            return revalidateCacheEntry(target, request, scope, chain, entry);\n        } catch (final IOException ioex) {\n            return convert(handleRevalidationFailure(request, context, entry, now), scope);\n        }\n    } else {\n        LOG.debug(\"Cache entry not usable; calling backend\");\n        return callBackend(target, request, scope, chain);\n    }\n}",
        "reject_response": "private ClassicHttpResponse handleCacheHit(\n        final HttpHost target,\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain,\n        final HttpCacheEntry entry) throws IOException, HttpException {\n    final HttpClientContext context  = scope.clientContext;\n    context.setAttribute(HttpCoreContext.HTTP_REQUEST, request);\n    recordCacheHit(target, request);\n    final Date now = getCurrentDate();\n    if (suitabilityChecker.canCachedResponseBeUsed(target, request, entry, now)) {\n        LOG.debug(\"Cache hit\");\n        try {\n            return convert(generateCachedResponse(request, context, entry, now), scope);\n        } catch (final ResourceIOException ex) {\n            recordCacheFailure(target, request);\n            if (!mayCallBackend(request)) {\n                return convert(generateGatewayTimeout(context), scope);\n            }\n            setResponseStatus(scope.clientContext, CacheResponseStatus.FAILURE);\n            return chain.proceed(request, scope);\n        }\n    } else if (!mayCallBackend(request)) {\n        LOG.debug(\"Cache entry not suitable but only-if-cached requested\");\n        return convert(generateGatewayTimeout(context), scope);\n    } else if (!(entry.getStatus() == HttpStatus.SC_NOT_MODIFIED && !suitabilityChecker.isConditional(request))) {\n        LOG.debug(\"Revalidating cache entry\");\n        try {\n            if (cacheRevalidator != null\n                    && !staleResponseNotAllowed(request, entry, now)\n                    && validityPolicy.mayReturnStaleWhileRevalidating(entry, now)) {\n                log.debug(\"Serving stale with asynchronous revalidation\");\n                final String exchangeId = ExecSupport.getNextExchangeId();\n                final ExecChain.Scope fork = new ExecChain.Scope(\n                        exchangeId,\n                        scope.route,\n                        scope.originalRequest,\n                        scope.execRuntime.fork(null),\n                        HttpClientContext.create());\n                final SimpleHttpResponse response = generateCachedResponse(request, context, entry, now);\n                cacheRevalidator.revalidateCacheEntry(\n                        responseCache.generateKey(target, request, entry),\n                        new DefaultCacheRevalidator.RevalidationCall() {\n\n                    @Override\n                    public ClassicHttpResponse execute() throws HttpException, IOException {\n                        return revalidateCacheEntry(target, request, fork, chain, entry);\n                    }\n\n                });\n                return convert(response, scope);\n            }\n            return revalidateCacheEntry(target, request, scope, chain, entry);\n        } catch (final IOException ioex) {\n            return convert(handleRevalidationFailure(request, context, entry, now), scope);\n        }\n    } else {\n        LOG.debug(\"Cache entry not usable; calling backend\");\n        return callBackend(target, request, scope, chain);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2691,
        "instruction": "@GET\n@Produces(MediaType.APPLICATION_OCTET_STREAM)\npublic Response read(@Context final ServletContext servletContext,\n                     @Context HttpHeaders headers) throws Exception {\n    // Convert headers into a regular map\n    Map<String, String> params = convertToCaseInsensitiveMap(headers.getRequestHeaders());\n\n\n    ProtocolData protData = new ProtocolData(params);\n    SecuredHDFS.verifyToken(protData, servletContext);\n    Bridge bridge;\n    float sampleRatio = protData.getStatsSampleRatio();\n    if (sampleRatio > 0) {\n        bridge = new ReadSamplingBridge(protData);\n    } else {\n        bridge = new ReadBridge(protData);\n    }\n    String dataDir = protData.getDataSource();\n    // THREAD-SAFE parameter has precedence\n    boolean isThreadSafe = protData.isThreadSafe() && bridge.isThreadSafe();\n    LOG.debug(\"Request for \" + dataDir + \" will be handled \"\n            + (isThreadSafe ? \"without\" : \"with\") + \" synchronization\");\n\n    return readResponse(bridge, protData, isThreadSafe);\n}",
        "accept_response": "@GET\n@Produces(MediaType.APPLICATION_OCTET_STREAM)\npublic Response read(@Context final ServletContext servletContext,\n                     @Context HttpHeaders headers) throws Exception {\n    // Convert headers into a regular map\n    Map<String, String> params = convertToCaseInsensitiveMap(headers.getRequestHeaders());\n\n    LOG.debug(\"started with parameters: \" + params);\n\n    ProtocolData protData = new ProtocolData(params);\n    SecuredHDFS.verifyToken(protData, servletContext);\n    Bridge bridge;\n    float sampleRatio = protData.getStatsSampleRatio();\n    if (sampleRatio > 0) {\n        bridge = new ReadSamplingBridge(protData);\n    } else {\n        bridge = new ReadBridge(protData);\n    }\n    String dataDir = protData.getDataSource();\n    // THREAD-SAFE parameter has precedence\n    boolean isThreadSafe = protData.isThreadSafe() && bridge.isThreadSafe();\n    LOG.debug(\"Request for \" + dataDir + \" will be handled \"\n            + (isThreadSafe ? \"without\" : \"with\") + \" synchronization\");\n\n    return readResponse(bridge, protData, isThreadSafe);\n}",
        "reject_response": "@GET\n@Produces(MediaType.APPLICATION_OCTET_STREAM)\npublic Response read(@Context final ServletContext servletContext,\n                     @Context HttpHeaders headers) throws Exception {\n    // Convert headers into a regular map\n    Map<String, String> params = convertToCaseInsensitiveMap(headers.getRequestHeaders());\n\n    Log.debug(\"started with parameters: \" + params);\n\n    ProtocolData protData = new ProtocolData(params);\n    SecuredHDFS.verifyToken(protData, servletContext);\n    Bridge bridge;\n    float sampleRatio = protData.getStatsSampleRatio();\n    if (sampleRatio > 0) {\n        bridge = new ReadSamplingBridge(protData);\n    } else {\n        bridge = new ReadBridge(protData);\n    }\n    String dataDir = protData.getDataSource();\n    // THREAD-SAFE parameter has precedence\n    boolean isThreadSafe = protData.isThreadSafe() && bridge.isThreadSafe();\n    LOG.debug(\"Request for \" + dataDir + \" will be handled \"\n            + (isThreadSafe ? \"without\" : \"with\") + \" synchronization\");\n\n    return readResponse(bridge, protData, isThreadSafe);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2536,
        "instruction": "public static Future<Boolean> flushLargestMemtable()\n{\n    float largestRatio = 0f;\n    Memtable largest = null;\n    float liveOnHeap = 0, liveOffHeap = 0;\n    for (ColumnFamilyStore cfs : ColumnFamilyStore.all())\n    {\n        // we take a reference to the current main memtable for the CF prior to snapping its ownership ratios\n        // to ensure we have some ordering guarantee for performing the switchMemtableIf(), i.e. we will only\n        // swap if the memtables we are measuring here haven't already been swapped by the time we try to swap them\n        Memtable current = cfs.getTracker().getView().getCurrentMemtable();\n\n        // find the total ownership ratio for the memtable and all SecondaryIndexes owned by this CF,\n        // both on- and off-heap, and select the largest of the two ratios to weight this CF\n        float onHeap = 0f, offHeap = 0f;\n        onHeap += current.getAllocator().onHeap().ownershipRatio();\n        offHeap += current.getAllocator().offHeap().ownershipRatio();\n\n        for (ColumnFamilyStore indexCfs : cfs.indexManager.getAllIndexColumnFamilyStores())\n        {\n            MemtableAllocator allocator = indexCfs.getTracker().getView().getCurrentMemtable().getAllocator();\n            onHeap += allocator.onHeap().ownershipRatio();\n            offHeap += allocator.offHeap().ownershipRatio();\n        }\n\n        float ratio = Math.max(onHeap, offHeap);\n        if (ratio > largestRatio)\n        {\n            largest = current;\n            largestRatio = ratio;\n        }\n\n        liveOnHeap += onHeap;\n        liveOffHeap += offHeap;\n    }\n\n    Promise<Boolean> returnFuture = new AsyncPromise<>();\n\n    if (largest != null)\n    {\n        float usedOnHeap = Memtable.MEMORY_POOL.onHeap.usedRatio();\n        float usedOffHeap = Memtable.MEMORY_POOL.offHeap.usedRatio();\n        float flushingOnHeap = Memtable.MEMORY_POOL.onHeap.reclaimingRatio();\n        float flushingOffHeap = Memtable.MEMORY_POOL.offHeap.reclaimingRatio();\n        float thisOnHeap = largest.getAllocator().onHeap().ownershipRatio();\n        float thisOffHeap = largest.getAllocator().offHeap().ownershipRatio();\n\n        Future<CommitLogPosition> flushFuture = largest.cfs.switchMemtableIfCurrent(largest);\n        flushFuture.addListener(() -> {\n            try\n            {\n                flushFuture.get();\n                returnFuture.trySuccess(true);\n            }\n            catch (Throwable t)\n            {\n                returnFuture.tryFailure(t);\n            }\n        });\n    }\n    else\n    {\n        logger.debug(\"Flushing of largest memtable, not done, no memtable found\");\n\n        returnFuture.trySuccess(false);\n    }\n\n    return returnFuture;\n}",
        "accept_response": "public static Future<Boolean> flushLargestMemtable()\n{\n    float largestRatio = 0f;\n    Memtable largest = null;\n    float liveOnHeap = 0, liveOffHeap = 0;\n    for (ColumnFamilyStore cfs : ColumnFamilyStore.all())\n    {\n        // we take a reference to the current main memtable for the CF prior to snapping its ownership ratios\n        // to ensure we have some ordering guarantee for performing the switchMemtableIf(), i.e. we will only\n        // swap if the memtables we are measuring here haven't already been swapped by the time we try to swap them\n        Memtable current = cfs.getTracker().getView().getCurrentMemtable();\n\n        // find the total ownership ratio for the memtable and all SecondaryIndexes owned by this CF,\n        // both on- and off-heap, and select the largest of the two ratios to weight this CF\n        float onHeap = 0f, offHeap = 0f;\n        onHeap += current.getAllocator().onHeap().ownershipRatio();\n        offHeap += current.getAllocator().offHeap().ownershipRatio();\n\n        for (ColumnFamilyStore indexCfs : cfs.indexManager.getAllIndexColumnFamilyStores())\n        {\n            MemtableAllocator allocator = indexCfs.getTracker().getView().getCurrentMemtable().getAllocator();\n            onHeap += allocator.onHeap().ownershipRatio();\n            offHeap += allocator.offHeap().ownershipRatio();\n        }\n\n        float ratio = Math.max(onHeap, offHeap);\n        if (ratio > largestRatio)\n        {\n            largest = current;\n            largestRatio = ratio;\n        }\n\n        liveOnHeap += onHeap;\n        liveOffHeap += offHeap;\n    }\n\n    Promise<Boolean> returnFuture = new AsyncPromise<>();\n\n    if (largest != null)\n    {\n        float usedOnHeap = Memtable.MEMORY_POOL.onHeap.usedRatio();\n        float usedOffHeap = Memtable.MEMORY_POOL.offHeap.usedRatio();\n        float flushingOnHeap = Memtable.MEMORY_POOL.onHeap.reclaimingRatio();\n        float flushingOffHeap = Memtable.MEMORY_POOL.offHeap.reclaimingRatio();\n        float thisOnHeap = largest.getAllocator().onHeap().ownershipRatio();\n        float thisOffHeap = largest.getAllocator().offHeap().ownershipRatio();\n        logger.info(\"Flushing largest {} to free up room. Used total: {}, live: {}, flushing: {}, this: {}\",\n                     largest.cfs, ratio(usedOnHeap, usedOffHeap), ratio(liveOnHeap, liveOffHeap),\n                     ratio(flushingOnHeap, flushingOffHeap), ratio(thisOnHeap, thisOffHeap));\n\n        Future<CommitLogPosition> flushFuture = largest.cfs.switchMemtableIfCurrent(largest);\n        flushFuture.addListener(() -> {\n            try\n            {\n                flushFuture.get();\n                returnFuture.trySuccess(true);\n            }\n            catch (Throwable t)\n            {\n                returnFuture.tryFailure(t);\n            }\n        });\n    }\n    else\n    {\n        logger.debug(\"Flushing of largest memtable, not done, no memtable found\");\n\n        returnFuture.trySuccess(false);\n    }\n\n    return returnFuture;\n}",
        "reject_response": "public static Future<Boolean> flushLargestMemtable()\n{\n    float largestRatio = 0f;\n    Memtable largest = null;\n    float liveOnHeap = 0, liveOffHeap = 0;\n    for (ColumnFamilyStore cfs : ColumnFamilyStore.all())\n    {\n        // we take a reference to the current main memtable for the CF prior to snapping its ownership ratios\n        // to ensure we have some ordering guarantee for performing the switchMemtableIf(), i.e. we will only\n        // swap if the memtables we are measuring here haven't already been swapped by the time we try to swap them\n        Memtable current = cfs.getTracker().getView().getCurrentMemtable();\n\n        // find the total ownership ratio for the memtable and all SecondaryIndexes owned by this CF,\n        // both on- and off-heap, and select the largest of the two ratios to weight this CF\n        float onHeap = 0f, offHeap = 0f;\n        onHeap += current.getAllocator().onHeap().ownershipRatio();\n        offHeap += current.getAllocator().offHeap().ownershipRatio();\n\n        for (ColumnFamilyStore indexCfs : cfs.indexManager.getAllIndexColumnFamilyStores())\n        {\n            MemtableAllocator allocator = indexCfs.getTracker().getView().getCurrentMemtable().getAllocator();\n            onHeap += allocator.onHeap().ownershipRatio();\n            offHeap += allocator.offHeap().ownershipRatio();\n        }\n\n        float ratio = Math.max(onHeap, offHeap);\n        if (ratio > largestRatio)\n        {\n            largest = current;\n            largestRatio = ratio;\n        }\n\n        liveOnHeap += onHeap;\n        liveOffHeap += offHeap;\n    }\n\n    Promise<Boolean> returnFuture = new AsyncPromise<>();\n\n    if (largest != null)\n    {\n        float usedOnHeap = Memtable.MEMORY_POOL.onHeap.usedRatio();\n        float usedOffHeap = Memtable.MEMORY_POOL.offHeap.usedRatio();\n        float flushingOnHeap = Memtable.MEMORY_POOL.onHeap.reclaimingRatio();\n        float flushingOffHeap = Memtable.MEMORY_POOL.offHeap.reclaimingRatio();\n        float thisOnHeap = largest.getAllocator().onHeap().ownershipRatio();\n        float thisOffHeap = largest.getAllocator().offHeap().ownershipRatio();\n        logger.debug(\"Flushing largest {} to free up room. Used total: {}, live: {}, flushing: {}, this: {}\",\n\n        Future<CommitLogPosition> flushFuture = largest.cfs.switchMemtableIfCurrent(largest);\n        flushFuture.addListener(() -> {\n            try\n            {\n                flushFuture.get();\n                returnFuture.trySuccess(true);\n            }\n            catch (Throwable t)\n            {\n                returnFuture.tryFailure(t);\n            }\n        });\n    }\n    else\n    {\n        logger.debug(\"Flushing of largest memtable, not done, no memtable found\");\n\n        returnFuture.trySuccess(false);\n    }\n\n    return returnFuture;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2586,
        "instruction": "public void updateInputChannel(InputChannelDeploymentDescriptor icdd) throws IOException, InterruptedException {\n\tsynchronized (requestLock) {\n\t\tif (isReleased) {\n\t\t\t// There was a race with a task failure/cancel\n\t\t\treturn;\n\t\t}\n\n\t\tfinal IntermediateResultPartitionID partitionId = icdd.getConsumedPartitionId().getPartitionId();\n\n\t\tInputChannel current = inputChannels.get(partitionId);\n\n\t\tif (current instanceof UnknownInputChannel) {\n\n\t\t\tUnknownInputChannel unknownChannel = (UnknownInputChannel) current;\n\n\t\t\tInputChannel newChannel;\n\n\t\t\tResultPartitionLocation partitionLocation = icdd.getConsumedPartitionLocation();\n\n\t\t\tif (partitionLocation.isLocal()) {\n\t\t\t\tnewChannel = unknownChannel.toLocalInputChannel();\n\t\t\t}\n\t\t\telse if (partitionLocation.isRemote()) {\n\t\t\t\tnewChannel = unknownChannel.toRemoteInputChannel(partitionLocation.getConnectionId());\n\n\t\t\t\tif (this.isCreditBased) {\n\t\t\t\t\tcheckState(this.networkBufferPool != null, \"Bug in input gate setup logic: \" +\n\t\t\t\t\t\t\"global buffer pool has not been set for this input gate.\");\n\t\t\t\t\t((RemoteInputChannel) newChannel).assignExclusiveSegments(\n\t\t\t\t\t\tnetworkBufferPool.requestMemorySegments(networkBuffersPerChannel));\n\t\t\t\t}\n\t\t\t}\n\t\t\telse {\n\t\t\t\tthrow new IllegalStateException(\"Tried to update unknown channel with unknown channel.\");\n\t\t\t}\n\n\n\t\t\tinputChannels.put(partitionId, newChannel);\n\n\t\t\tif (requestedPartitionsFlag) {\n\t\t\t\tnewChannel.requestSubpartition(consumedSubpartitionIndex);\n\t\t\t}\n\n\t\t\tfor (TaskEvent event : pendingEvents) {\n\t\t\t\tnewChannel.sendTaskEvent(event);\n\t\t\t}\n\n\t\t\tif (--numberOfUninitializedChannels == 0) {\n\t\t\t\tpendingEvents.clear();\n\t\t\t}\n\t\t}\n\t}\n}",
        "accept_response": "public void updateInputChannel(InputChannelDeploymentDescriptor icdd) throws IOException, InterruptedException {\n\tsynchronized (requestLock) {\n\t\tif (isReleased) {\n\t\t\t// There was a race with a task failure/cancel\n\t\t\treturn;\n\t\t}\n\n\t\tfinal IntermediateResultPartitionID partitionId = icdd.getConsumedPartitionId().getPartitionId();\n\n\t\tInputChannel current = inputChannels.get(partitionId);\n\n\t\tif (current instanceof UnknownInputChannel) {\n\n\t\t\tUnknownInputChannel unknownChannel = (UnknownInputChannel) current;\n\n\t\t\tInputChannel newChannel;\n\n\t\t\tResultPartitionLocation partitionLocation = icdd.getConsumedPartitionLocation();\n\n\t\t\tif (partitionLocation.isLocal()) {\n\t\t\t\tnewChannel = unknownChannel.toLocalInputChannel();\n\t\t\t}\n\t\t\telse if (partitionLocation.isRemote()) {\n\t\t\t\tnewChannel = unknownChannel.toRemoteInputChannel(partitionLocation.getConnectionId());\n\n\t\t\t\tif (this.isCreditBased) {\n\t\t\t\t\tcheckState(this.networkBufferPool != null, \"Bug in input gate setup logic: \" +\n\t\t\t\t\t\t\"global buffer pool has not been set for this input gate.\");\n\t\t\t\t\t((RemoteInputChannel) newChannel).assignExclusiveSegments(\n\t\t\t\t\t\tnetworkBufferPool.requestMemorySegments(networkBuffersPerChannel));\n\t\t\t\t}\n\t\t\t}\n\t\t\telse {\n\t\t\t\tthrow new IllegalStateException(\"Tried to update unknown channel with unknown channel.\");\n\t\t\t}\n\n\t\t\tLOG.debug(\"{}: Updated unknown input channel to {}.\", owningTaskName, newChannel);\n\n\t\t\tinputChannels.put(partitionId, newChannel);\n\n\t\t\tif (requestedPartitionsFlag) {\n\t\t\t\tnewChannel.requestSubpartition(consumedSubpartitionIndex);\n\t\t\t}\n\n\t\t\tfor (TaskEvent event : pendingEvents) {\n\t\t\t\tnewChannel.sendTaskEvent(event);\n\t\t\t}\n\n\t\t\tif (--numberOfUninitializedChannels == 0) {\n\t\t\t\tpendingEvents.clear();\n\t\t\t}\n\t\t}\n\t}\n}",
        "reject_response": "public void updateInputChannel(InputChannelDeploymentDescriptor icdd) throws IOException, InterruptedException {\n\tsynchronized (requestLock) {\n\t\tif (isReleased) {\n\t\t\t// There was a race with a task failure/cancel\n\t\t\treturn;\n\t\t}\n\n\t\tfinal IntermediateResultPartitionID partitionId = icdd.getConsumedPartitionId().getPartitionId();\n\n\t\tInputChannel current = inputChannels.get(partitionId);\n\n\t\tif (current instanceof UnknownInputChannel) {\n\n\t\t\tUnknownInputChannel unknownChannel = (UnknownInputChannel) current;\n\n\t\t\tInputChannel newChannel;\n\n\t\t\tResultPartitionLocation partitionLocation = icdd.getConsumedPartitionLocation();\n\n\t\t\tif (partitionLocation.isLocal()) {\n\t\t\t\tnewChannel = unknownChannel.toLocalInputChannel();\n\t\t\t}\n\t\t\telse if (partitionLocation.isRemote()) {\n\t\t\t\tnewChannel = unknownChannel.toRemoteInputChannel(partitionLocation.getConnectionId());\n\n\t\t\t\tif (this.isCreditBased) {\n\t\t\t\t\tcheckState(this.networkBufferPool != null, \"Bug in input gate setup logic: \" +\n\t\t\t\t\t\t\"global buffer pool has not been set for this input gate.\");\n\t\t\t\t\t((RemoteInputChannel) newChannel).assignExclusiveSegments(\n\t\t\t\t\t\tnetworkBufferPool.requestMemorySegments(networkBuffersPerChannel));\n\t\t\t\t}\n\t\t\t}\n\t\t\telse {\n\t\t\t\tthrow new IllegalStateException(\"Tried to update unknown channel with unknown channel.\");\n\t\t\t}\n\n\t\t\tLOG.debug(\"Updated unknown input channel to {}.\", newChannel);\n\n\t\t\tinputChannels.put(partitionId, newChannel);\n\n\t\t\tif (requestedPartitionsFlag) {\n\t\t\t\tnewChannel.requestSubpartition(consumedSubpartitionIndex);\n\t\t\t}\n\n\t\t\tfor (TaskEvent event : pendingEvents) {\n\t\t\t\tnewChannel.sendTaskEvent(event);\n\t\t\t}\n\n\t\t\tif (--numberOfUninitializedChannels == 0) {\n\t\t\t\tpendingEvents.clear();\n\t\t\t}\n\t\t}\n\t}\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2508,
        "instruction": "@Override\npublic RemoteFileOperations<FTPFile> createRemoteFileOperations() throws Exception {\n    // configure ftp client\n    FTPSClient client = getFtpsClient();\n\n    if (client == null) {\n        // must use a new client if not explicit configured to use a custom client\n        client = (FTPSClient) createFtpClient();\n    }\n\n    // use configured buffer size which is larger and therefore faster (as the default is no buffer)\n    if (getConfiguration().getReceiveBufferSize() > 0) {\n        client.setBufferSize(getConfiguration().getReceiveBufferSize());\n    }\n    // set any endpoint configured timeouts\n    if (getConfiguration().getConnectTimeout() > -1) {\n        client.setConnectTimeout(getConfiguration().getConnectTimeout());\n    }\n    if (getConfiguration().getSoTimeout() > -1) {\n        soTimeout = getConfiguration().getSoTimeout();\n    }\n    dataTimeout = getConfiguration().getTimeout();\n\n    if (ftpClientParameters != null) {\n        Map<String, Object> localParameters = new HashMap<>(ftpClientParameters);\n        // setting soTimeout has to be done later on FTPClient (after it has connected)\n        Object timeout = localParameters.remove(\"soTimeout\");\n        if (timeout != null) {\n            soTimeout = getCamelContext().getTypeConverter().convertTo(int.class, timeout);\n        }\n        // and we want to keep data timeout so we can log it later\n        timeout = localParameters.remove(\"dataTimeout\");\n        if (timeout != null) {\n            dataTimeout = getCamelContext().getTypeConverter().convertTo(int.class, timeout);\n        }\n        setProperties(client, localParameters);\n    }\n\n    if (ftpClientConfigParameters != null) {\n        // client config is optional so create a new one if we have parameter for it\n        if (ftpClientConfig == null) {\n            ftpClientConfig = new FTPClientConfig();\n        }\n        Map<String, Object> localConfigParameters = new HashMap<>(ftpClientConfigParameters);\n        setProperties(ftpClientConfig, localConfigParameters);\n    }\n\n    if (dataTimeout > 0) {\n        client.setDataTimeout(dataTimeout);\n    }\n\n    FtpsOperations operations = new FtpsOperations(client, getFtpClientConfig());\n    operations.setEndpoint(this);\n    return operations;\n}",
        "accept_response": "@Override\npublic RemoteFileOperations<FTPFile> createRemoteFileOperations() throws Exception {\n    // configure ftp client\n    FTPSClient client = getFtpsClient();\n\n    if (client == null) {\n        // must use a new client if not explicit configured to use a custom client\n        client = (FTPSClient) createFtpClient();\n    }\n\n    // use configured buffer size which is larger and therefore faster (as the default is no buffer)\n    if (getConfiguration().getReceiveBufferSize() > 0) {\n        client.setBufferSize(getConfiguration().getReceiveBufferSize());\n    }\n    // set any endpoint configured timeouts\n    if (getConfiguration().getConnectTimeout() > -1) {\n        client.setConnectTimeout(getConfiguration().getConnectTimeout());\n    }\n    if (getConfiguration().getSoTimeout() > -1) {\n        soTimeout = getConfiguration().getSoTimeout();\n    }\n    dataTimeout = getConfiguration().getTimeout();\n\n    if (ftpClientParameters != null) {\n        Map<String, Object> localParameters = new HashMap<>(ftpClientParameters);\n        // setting soTimeout has to be done later on FTPClient (after it has connected)\n        Object timeout = localParameters.remove(\"soTimeout\");\n        if (timeout != null) {\n            soTimeout = getCamelContext().getTypeConverter().convertTo(int.class, timeout);\n        }\n        // and we want to keep data timeout so we can log it later\n        timeout = localParameters.remove(\"dataTimeout\");\n        if (timeout != null) {\n            dataTimeout = getCamelContext().getTypeConverter().convertTo(int.class, timeout);\n        }\n        setProperties(client, localParameters);\n    }\n\n    if (ftpClientConfigParameters != null) {\n        // client config is optional so create a new one if we have parameter for it\n        if (ftpClientConfig == null) {\n            ftpClientConfig = new FTPClientConfig();\n        }\n        Map<String, Object> localConfigParameters = new HashMap<>(ftpClientConfigParameters);\n        setProperties(ftpClientConfig, localConfigParameters);\n    }\n\n    if (dataTimeout > 0) {\n        client.setDataTimeout(dataTimeout);\n    }\n\n    if (log.isDebugEnabled()) {\n        log.debug(\"Created FTPSClient [connectTimeout: {}, soTimeout: {}, dataTimeout: {}, bufferSize: {}\"\n                        + \", receiveDataSocketBufferSize: {}, sendDataSocketBufferSize: {}]: {}\",\n                new Object[]{client.getConnectTimeout(), getSoTimeout(), dataTimeout, client.getBufferSize(),\n                        client.getReceiveDataSocketBufferSize(), client.getSendDataSocketBufferSize(), client});\n    }\n\n    FtpsOperations operations = new FtpsOperations(client, getFtpClientConfig());\n    operations.setEndpoint(this);\n    return operations;\n}",
        "reject_response": "@Override\npublic RemoteFileOperations<FTPFile> createRemoteFileOperations() throws Exception {\n    // configure ftp client\n    FTPSClient client = getFtpsClient();\n\n    if (client == null) {\n        // must use a new client if not explicit configured to use a custom client\n        client = (FTPSClient) createFtpClient();\n    }\n\n    // use configured buffer size which is larger and therefore faster (as the default is no buffer)\n    if (getConfiguration().getReceiveBufferSize() > 0) {\n        client.setBufferSize(getConfiguration().getReceiveBufferSize());\n    }\n    // set any endpoint configured timeouts\n    if (getConfiguration().getConnectTimeout() > -1) {\n        client.setConnectTimeout(getConfiguration().getConnectTimeout());\n    }\n    if (getConfiguration().getSoTimeout() > -1) {\n        soTimeout = getConfiguration().getSoTimeout();\n    }\n    dataTimeout = getConfiguration().getTimeout();\n\n    if (ftpClientParameters != null) {\n        Map<String, Object> localParameters = new HashMap<>(ftpClientParameters);\n        // setting soTimeout has to be done later on FTPClient (after it has connected)\n        Object timeout = localParameters.remove(\"soTimeout\");\n        if (timeout != null) {\n            soTimeout = getCamelContext().getTypeConverter().convertTo(int.class, timeout);\n        }\n        // and we want to keep data timeout so we can log it later\n        timeout = localParameters.remove(\"dataTimeout\");\n        if (timeout != null) {\n            dataTimeout = getCamelContext().getTypeConverter().convertTo(int.class, timeout);\n        }\n        setProperties(client, localParameters);\n    }\n\n    if (ftpClientConfigParameters != null) {\n        // client config is optional so create a new one if we have parameter for it\n        if (ftpClientConfig == null) {\n            ftpClientConfig = new FTPClientConfig();\n        }\n        Map<String, Object> localConfigParameters = new HashMap<>(ftpClientConfigParameters);\n        setProperties(ftpClientConfig, localConfigParameters);\n    }\n\n    if (dataTimeout > 0) {\n        client.setDataTimeout(dataTimeout);\n    }\n\n    if (log.isDebugEnabled()) {\n        log.debug(\"Created FTPClient [connectTimeout: {}, soTimeout: {}, dataTimeout: {}, bufferSize: {}\"\n    }\n\n    FtpsOperations operations = new FtpsOperations(client, getFtpClientConfig());\n    operations.setEndpoint(this);\n    return operations;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3074,
        "instruction": "private void deleteExecutorDirs(String[] dirs) {\n  for (String localDir : dirs) {\n    try {\n      JavaUtils.deleteRecursively(new File(localDir));\n    } catch (Exception e) {\n      logger.error(\"Failed to delete directory: \" + localDir, e);\n    }\n  }\n}",
        "accept_response": "private void deleteExecutorDirs(String[] dirs) {\n  for (String localDir : dirs) {\n    try {\n      JavaUtils.deleteRecursively(new File(localDir));\n      logger.debug(\"Successfully cleaned up directory: {}\", localDir);\n    } catch (Exception e) {\n      logger.error(\"Failed to delete directory: \" + localDir, e);\n    }\n  }\n}",
        "reject_response": "private void deleteExecutorDirs(String[] dirs) {\n  for (String localDir : dirs) {\n    try {\n      JavaUtils.deleteRecursively(new File(localDir));\n      logger.debug(\"Successfully cleaned up directory: \" + localDir);\n    } catch (Exception e) {\n      logger.error(\"Failed to delete directory: \" + localDir, e);\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2800,
        "instruction": "@Override\npublic int write(final ByteBuffer src) throws IOException {\n    final int byteWritten = session.write(src);\n    if (byteWritten > 0 && wireLog.isEnabled()) {\n        final ByteBuffer b = src.duplicate();\n        final int p = b.position();\n        b.limit(p);\n        b.position(p - byteWritten);\n        wireLog.output(b);\n    }\n    return byteWritten;\n}",
        "accept_response": "@Override\npublic int write(final ByteBuffer src) throws IOException {\n    final int byteWritten = session.write(src);\n    if (log.isDebugEnabled()) {\n        log.debug(\"{} {} bytes written\", session, byteWritten);\n    }\n    if (byteWritten > 0 && wireLog.isEnabled()) {\n        final ByteBuffer b = src.duplicate();\n        final int p = b.position();\n        b.limit(p);\n        b.position(p - byteWritten);\n        wireLog.output(b);\n    }\n    return byteWritten;\n}",
        "reject_response": "@Override\npublic int write(final ByteBuffer src) throws IOException {\n    final int byteWritten = session.write(src);\n    if (log.isDebugEnabled()) {\n        log.debug(session + \" \" + byteWritten + \" bytes written\");\n    }\n    if (byteWritten > 0 && wireLog.isEnabled()) {\n        final ByteBuffer b = src.duplicate();\n        final int p = b.position();\n        b.limit(p);\n        b.position(p - byteWritten);\n        wireLog.output(b);\n    }\n    return byteWritten;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2831,
        "instruction": "private SegmentArchiveResult archiveSegment(long absIdx) throws IgniteCheckedException {\n    long segIdx = absIdx % dsCfg.getWalSegments();\n\n    File origFile = new File(walWorkDir, FileDescriptor.fileName(segIdx));\n\n    String name = FileDescriptor.fileName(absIdx);\n\n    File dstTmpFile = new File(walArchiveDir, name + \".tmp\");\n\n    File dstFile = new File(walArchiveDir, name);\n\n    if (log.isInfoEnabled())\n\n    try {\n        Files.deleteIfExists(dstTmpFile.toPath());\n\n        Files.copy(origFile.toPath(), dstTmpFile.toPath());\n\n        Files.move(dstTmpFile.toPath(), dstFile.toPath());\n\n        if (mode == WALMode.FSYNC) {\n            try (FileIO f0 = ioFactory.create(dstFile, CREATE, READ, WRITE)) {\n                f0.force();\n            }\n        }\n    }\n    catch (IOException e) {\n        throw new IgniteCheckedException(\"Failed to archive WAL segment [\" +\n            \"srcFile=\" + origFile.getAbsolutePath() +\n            \", dstFile=\" + dstTmpFile.getAbsolutePath() + ']', e);\n    }\n\n    if (log.isInfoEnabled())\n        log.info(\"Copied file [src=\" + origFile.getAbsolutePath() +\n            \", dst=\" + dstFile.getAbsolutePath() + ']');\n\n    return new SegmentArchiveResult(absIdx, origFile, dstFile);\n}",
        "accept_response": "private SegmentArchiveResult archiveSegment(long absIdx) throws IgniteCheckedException {\n    long segIdx = absIdx % dsCfg.getWalSegments();\n\n    File origFile = new File(walWorkDir, FileDescriptor.fileName(segIdx));\n\n    String name = FileDescriptor.fileName(absIdx);\n\n    File dstTmpFile = new File(walArchiveDir, name + \".tmp\");\n\n    File dstFile = new File(walArchiveDir, name);\n\n    if (log.isInfoEnabled())\n        log.info(\"Starting to copy WAL segment [absIdx=\" + absIdx + \", segIdx=\" + segIdx +\n            \", origFile=\" + origFile.getAbsolutePath() + \", dstFile=\" + dstFile.getAbsolutePath() + ']');\n\n    try {\n        Files.deleteIfExists(dstTmpFile.toPath());\n\n        Files.copy(origFile.toPath(), dstTmpFile.toPath());\n\n        Files.move(dstTmpFile.toPath(), dstFile.toPath());\n\n        if (mode == WALMode.FSYNC) {\n            try (FileIO f0 = ioFactory.create(dstFile, CREATE, READ, WRITE)) {\n                f0.force();\n            }\n        }\n    }\n    catch (IOException e) {\n        throw new IgniteCheckedException(\"Failed to archive WAL segment [\" +\n            \"srcFile=\" + origFile.getAbsolutePath() +\n            \", dstFile=\" + dstTmpFile.getAbsolutePath() + ']', e);\n    }\n\n    if (log.isInfoEnabled())\n        log.info(\"Copied file [src=\" + origFile.getAbsolutePath() +\n            \", dst=\" + dstFile.getAbsolutePath() + ']');\n\n    return new SegmentArchiveResult(absIdx, origFile, dstFile);\n}",
        "reject_response": "private SegmentArchiveResult archiveSegment(long absIdx) throws IgniteCheckedException {\n    long segIdx = absIdx % dsCfg.getWalSegments();\n\n    File origFile = new File(walWorkDir, FileDescriptor.fileName(segIdx));\n\n    String name = FileDescriptor.fileName(absIdx);\n\n    File dstTmpFile = new File(walArchiveDir, name + \".tmp\");\n\n    File dstFile = new File(walArchiveDir, name);\n\n    if (log.isInfoEnabled())\n    if (log.isDebugEnabled())\n        log.debug(\"Starting to copy WAL segment [absIdx=\" + absIdx + \", segIdx=\" + segIdx +\n\n    try {\n        Files.deleteIfExists(dstTmpFile.toPath());\n\n        Files.copy(origFile.toPath(), dstTmpFile.toPath());\n\n        Files.move(dstTmpFile.toPath(), dstFile.toPath());\n\n        if (mode == WALMode.FSYNC) {\n            try (FileIO f0 = ioFactory.create(dstFile, CREATE, READ, WRITE)) {\n                f0.force();\n            }\n        }\n    }\n    catch (IOException e) {\n        throw new IgniteCheckedException(\"Failed to archive WAL segment [\" +\n            \"srcFile=\" + origFile.getAbsolutePath() +\n            \", dstFile=\" + dstTmpFile.getAbsolutePath() + ']', e);\n    }\n\n    if (log.isInfoEnabled())\n        log.info(\"Copied file [src=\" + origFile.getAbsolutePath() +\n            \", dst=\" + dstFile.getAbsolutePath() + ']');\n\n    return new SegmentArchiveResult(absIdx, origFile, dstFile);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2972,
        "instruction": "public boolean nextKeyValue() throws IOException, InterruptedException {\n  boolean recordFound = false;\n\n  while (!recordFound) {\n    // no more records left\n    if (current >= total) { return false; }\n\n    try {\n      checkRead();\n      current ++;\n\n      try {\n        currentValue = recordReader.read();\n      } catch (RecordMaterializationException e) {\n        // this might throw, but it's fatal if it does.\n        unmaterializableRecordCounter.incErrors(e);\n        if (LOG.isDebugEnabled()) LOG.debug(\"skipping a corrupt record\");\n        continue;\n      }\n\n      if (recordReader.shouldSkipCurrentRecord()) {\n        // this record is being filtered via the filter2 package\n        if (LOG.isDebugEnabled()) LOG.debug(\"skipping record\");\n        continue;\n      }\n\n      if (currentValue == null) {\n        // only happens with FilteredRecordReader at end of block\n        current = totalCountLoadedSoFar;\n        if (LOG.isDebugEnabled()) LOG.debug(\"filtered record reader reached end of block\");\n        continue;\n      }\n\n      recordFound = true;\n\n    } catch (RuntimeException e) {\n      throw new ParquetDecodingException(format(\"Can not read value at %d in block %d in file %s\", current, currentBlock, reader.getPath()), e);\n    }\n  }\n  return true;\n}",
        "accept_response": "public boolean nextKeyValue() throws IOException, InterruptedException {\n  boolean recordFound = false;\n\n  while (!recordFound) {\n    // no more records left\n    if (current >= total) { return false; }\n\n    try {\n      checkRead();\n      current ++;\n\n      try {\n        currentValue = recordReader.read();\n      } catch (RecordMaterializationException e) {\n        // this might throw, but it's fatal if it does.\n        unmaterializableRecordCounter.incErrors(e);\n        if (LOG.isDebugEnabled()) LOG.debug(\"skipping a corrupt record\");\n        continue;\n      }\n\n      if (recordReader.shouldSkipCurrentRecord()) {\n        // this record is being filtered via the filter2 package\n        if (LOG.isDebugEnabled()) LOG.debug(\"skipping record\");\n        continue;\n      }\n\n      if (currentValue == null) {\n        // only happens with FilteredRecordReader at end of block\n        current = totalCountLoadedSoFar;\n        if (LOG.isDebugEnabled()) LOG.debug(\"filtered record reader reached end of block\");\n        continue;\n      }\n\n      recordFound = true;\n\n      if (LOG.isDebugEnabled()) LOG.debug(\"read value: {}\", currentValue);\n    } catch (RuntimeException e) {\n      throw new ParquetDecodingException(format(\"Can not read value at %d in block %d in file %s\", current, currentBlock, reader.getPath()), e);\n    }\n  }\n  return true;\n}",
        "reject_response": "public boolean nextKeyValue() throws IOException, InterruptedException {\n  boolean recordFound = false;\n\n  while (!recordFound) {\n    // no more records left\n    if (current >= total) { return false; }\n\n    try {\n      checkRead();\n      current ++;\n\n      try {\n        currentValue = recordReader.read();\n      } catch (RecordMaterializationException e) {\n        // this might throw, but it's fatal if it does.\n        unmaterializableRecordCounter.incErrors(e);\n        if (LOG.isDebugEnabled()) LOG.debug(\"skipping a corrupt record\");\n        continue;\n      }\n\n      if (recordReader.shouldSkipCurrentRecord()) {\n        // this record is being filtered via the filter2 package\n        if (LOG.isDebugEnabled()) LOG.debug(\"skipping record\");\n        continue;\n      }\n\n      if (currentValue == null) {\n        // only happens with FilteredRecordReader at end of block\n        current = totalCountLoadedSoFar;\n        if (LOG.isDebugEnabled()) LOG.debug(\"filtered record reader reached end of block\");\n        continue;\n      }\n\n      recordFound = true;\n\n      LOG.debug(\"read value: {}\", currentValue);\n    } catch (RuntimeException e) {\n      throw new ParquetDecodingException(format(\"Can not read value at %d in block %d in file %s\", current, currentBlock, reader.getPath()), e);\n    }\n  }\n  return true;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2781,
        "instruction": "private ClassicHttpResponse handleCacheHit(\n        final HttpHost target,\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain,\n        final HttpCacheEntry entry) throws IOException, HttpException {\n    final HttpClientContext context  = scope.clientContext;\n    context.setAttribute(HttpCoreContext.HTTP_REQUEST, request);\n    recordCacheHit(target, request);\n    final Date now = getCurrentDate();\n    if (suitabilityChecker.canCachedResponseBeUsed(target, request, entry, now)) {\n        LOG.debug(\"Cache hit\");\n        try {\n            return convert(generateCachedResponse(request, context, entry, now), scope);\n        } catch (final ResourceIOException ex) {\n            recordCacheFailure(target, request);\n            if (!mayCallBackend(request)) {\n                return convert(generateGatewayTimeout(context), scope);\n            }\n            setResponseStatus(scope.clientContext, CacheResponseStatus.FAILURE);\n            return chain.proceed(request, scope);\n        }\n    } else if (!mayCallBackend(request)) {\n        return convert(generateGatewayTimeout(context), scope);\n    } else if (!(entry.getStatus() == HttpStatus.SC_NOT_MODIFIED && !suitabilityChecker.isConditional(request))) {\n        LOG.debug(\"Revalidating cache entry\");\n        try {\n            if (cacheRevalidator != null\n                    && !staleResponseNotAllowed(request, entry, now)\n                    && validityPolicy.mayReturnStaleWhileRevalidating(entry, now)) {\n                LOG.debug(\"Serving stale with asynchronous revalidation\");\n                final String exchangeId = ExecSupport.getNextExchangeId();\n                final ExecChain.Scope fork = new ExecChain.Scope(\n                        exchangeId,\n                        scope.route,\n                        scope.originalRequest,\n                        scope.execRuntime.fork(null),\n                        HttpClientContext.create());\n                final SimpleHttpResponse response = generateCachedResponse(request, context, entry, now);\n                cacheRevalidator.revalidateCacheEntry(\n                        responseCache.generateKey(target, request, entry),\n                        new DefaultCacheRevalidator.RevalidationCall() {\n\n                    @Override\n                    public ClassicHttpResponse execute() throws HttpException, IOException {\n                        return revalidateCacheEntry(target, request, fork, chain, entry);\n                    }\n\n                });\n                return convert(response, scope);\n            }\n            return revalidateCacheEntry(target, request, scope, chain, entry);\n        } catch (final IOException ioex) {\n            return convert(handleRevalidationFailure(request, context, entry, now), scope);\n        }\n    } else {\n        LOG.debug(\"Cache entry not usable; calling backend\");\n        return callBackend(target, request, scope, chain);\n    }\n}",
        "accept_response": "private ClassicHttpResponse handleCacheHit(\n        final HttpHost target,\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain,\n        final HttpCacheEntry entry) throws IOException, HttpException {\n    final HttpClientContext context  = scope.clientContext;\n    context.setAttribute(HttpCoreContext.HTTP_REQUEST, request);\n    recordCacheHit(target, request);\n    final Date now = getCurrentDate();\n    if (suitabilityChecker.canCachedResponseBeUsed(target, request, entry, now)) {\n        LOG.debug(\"Cache hit\");\n        try {\n            return convert(generateCachedResponse(request, context, entry, now), scope);\n        } catch (final ResourceIOException ex) {\n            recordCacheFailure(target, request);\n            if (!mayCallBackend(request)) {\n                return convert(generateGatewayTimeout(context), scope);\n            }\n            setResponseStatus(scope.clientContext, CacheResponseStatus.FAILURE);\n            return chain.proceed(request, scope);\n        }\n    } else if (!mayCallBackend(request)) {\n        LOG.debug(\"Cache entry not suitable but only-if-cached requested\");\n        return convert(generateGatewayTimeout(context), scope);\n    } else if (!(entry.getStatus() == HttpStatus.SC_NOT_MODIFIED && !suitabilityChecker.isConditional(request))) {\n        LOG.debug(\"Revalidating cache entry\");\n        try {\n            if (cacheRevalidator != null\n                    && !staleResponseNotAllowed(request, entry, now)\n                    && validityPolicy.mayReturnStaleWhileRevalidating(entry, now)) {\n                LOG.debug(\"Serving stale with asynchronous revalidation\");\n                final String exchangeId = ExecSupport.getNextExchangeId();\n                final ExecChain.Scope fork = new ExecChain.Scope(\n                        exchangeId,\n                        scope.route,\n                        scope.originalRequest,\n                        scope.execRuntime.fork(null),\n                        HttpClientContext.create());\n                final SimpleHttpResponse response = generateCachedResponse(request, context, entry, now);\n                cacheRevalidator.revalidateCacheEntry(\n                        responseCache.generateKey(target, request, entry),\n                        new DefaultCacheRevalidator.RevalidationCall() {\n\n                    @Override\n                    public ClassicHttpResponse execute() throws HttpException, IOException {\n                        return revalidateCacheEntry(target, request, fork, chain, entry);\n                    }\n\n                });\n                return convert(response, scope);\n            }\n            return revalidateCacheEntry(target, request, scope, chain, entry);\n        } catch (final IOException ioex) {\n            return convert(handleRevalidationFailure(request, context, entry, now), scope);\n        }\n    } else {\n        LOG.debug(\"Cache entry not usable; calling backend\");\n        return callBackend(target, request, scope, chain);\n    }\n}",
        "reject_response": "private ClassicHttpResponse handleCacheHit(\n        final HttpHost target,\n        final ClassicHttpRequest request,\n        final ExecChain.Scope scope,\n        final ExecChain chain,\n        final HttpCacheEntry entry) throws IOException, HttpException {\n    final HttpClientContext context  = scope.clientContext;\n    context.setAttribute(HttpCoreContext.HTTP_REQUEST, request);\n    recordCacheHit(target, request);\n    final Date now = getCurrentDate();\n    if (suitabilityChecker.canCachedResponseBeUsed(target, request, entry, now)) {\n        LOG.debug(\"Cache hit\");\n        try {\n            return convert(generateCachedResponse(request, context, entry, now), scope);\n        } catch (final ResourceIOException ex) {\n            recordCacheFailure(target, request);\n            if (!mayCallBackend(request)) {\n                return convert(generateGatewayTimeout(context), scope);\n            }\n            setResponseStatus(scope.clientContext, CacheResponseStatus.FAILURE);\n            return chain.proceed(request, scope);\n        }\n    } else if (!mayCallBackend(request)) {\n        log.debug(\"Cache entry not suitable but only-if-cached requested\");\n        return convert(generateGatewayTimeout(context), scope);\n    } else if (!(entry.getStatus() == HttpStatus.SC_NOT_MODIFIED && !suitabilityChecker.isConditional(request))) {\n        LOG.debug(\"Revalidating cache entry\");\n        try {\n            if (cacheRevalidator != null\n                    && !staleResponseNotAllowed(request, entry, now)\n                    && validityPolicy.mayReturnStaleWhileRevalidating(entry, now)) {\n                LOG.debug(\"Serving stale with asynchronous revalidation\");\n                final String exchangeId = ExecSupport.getNextExchangeId();\n                final ExecChain.Scope fork = new ExecChain.Scope(\n                        exchangeId,\n                        scope.route,\n                        scope.originalRequest,\n                        scope.execRuntime.fork(null),\n                        HttpClientContext.create());\n                final SimpleHttpResponse response = generateCachedResponse(request, context, entry, now);\n                cacheRevalidator.revalidateCacheEntry(\n                        responseCache.generateKey(target, request, entry),\n                        new DefaultCacheRevalidator.RevalidationCall() {\n\n                    @Override\n                    public ClassicHttpResponse execute() throws HttpException, IOException {\n                        return revalidateCacheEntry(target, request, fork, chain, entry);\n                    }\n\n                });\n                return convert(response, scope);\n            }\n            return revalidateCacheEntry(target, request, scope, chain, entry);\n        } catch (final IOException ioex) {\n            return convert(handleRevalidationFailure(request, context, entry, now), scope);\n        }\n    } else {\n        LOG.debug(\"Cache entry not usable; calling backend\");\n        return callBackend(target, request, scope, chain);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2594,
        "instruction": "public void registerTaskManager(final TaskExecutorConnection taskExecutorConnection, SlotReport initialSlotReport) {\n\tcheckInit();\n\n\n\t// we identify task managers by their instance id\n\tif (taskManagerRegistrations.containsKey(taskExecutorConnection.getInstanceID())) {\n\t\treportSlotStatus(taskExecutorConnection.getInstanceID(), initialSlotReport);\n\t} else {\n\t\t// first register the TaskManager\n\t\tArrayList<SlotID> reportedSlots = new ArrayList<>();\n\n\t\tfor (SlotStatus slotStatus : initialSlotReport) {\n\t\t\treportedSlots.add(slotStatus.getSlotID());\n\t\t}\n\n\t\tTaskManagerRegistration taskManagerRegistration = new TaskManagerRegistration(\n\t\t\ttaskExecutorConnection,\n\t\t\treportedSlots);\n\n\t\ttaskManagerRegistrations.put(taskExecutorConnection.getInstanceID(), taskManagerRegistration);\n\n\t\t// next register the new slots\n\t\tfor (SlotStatus slotStatus : initialSlotReport) {\n\t\t\tregisterSlot(\n\t\t\t\tslotStatus.getSlotID(),\n\t\t\t\tslotStatus.getAllocationID(),\n\t\t\t\tslotStatus.getJobID(),\n\t\t\t\tslotStatus.getResourceProfile(),\n\t\t\t\ttaskExecutorConnection);\n\t\t}\n\t}\n\n}",
        "accept_response": "public void registerTaskManager(final TaskExecutorConnection taskExecutorConnection, SlotReport initialSlotReport) {\n\tcheckInit();\n\n\tLOG.debug(\"Registering TaskManager {} under {} at the SlotManager.\", taskExecutorConnection.getResourceID(), taskExecutorConnection.getInstanceID());\n\n\t// we identify task managers by their instance id\n\tif (taskManagerRegistrations.containsKey(taskExecutorConnection.getInstanceID())) {\n\t\treportSlotStatus(taskExecutorConnection.getInstanceID(), initialSlotReport);\n\t} else {\n\t\t// first register the TaskManager\n\t\tArrayList<SlotID> reportedSlots = new ArrayList<>();\n\n\t\tfor (SlotStatus slotStatus : initialSlotReport) {\n\t\t\treportedSlots.add(slotStatus.getSlotID());\n\t\t}\n\n\t\tTaskManagerRegistration taskManagerRegistration = new TaskManagerRegistration(\n\t\t\ttaskExecutorConnection,\n\t\t\treportedSlots);\n\n\t\ttaskManagerRegistrations.put(taskExecutorConnection.getInstanceID(), taskManagerRegistration);\n\n\t\t// next register the new slots\n\t\tfor (SlotStatus slotStatus : initialSlotReport) {\n\t\t\tregisterSlot(\n\t\t\t\tslotStatus.getSlotID(),\n\t\t\t\tslotStatus.getAllocationID(),\n\t\t\t\tslotStatus.getJobID(),\n\t\t\t\tslotStatus.getResourceProfile(),\n\t\t\t\ttaskExecutorConnection);\n\t\t}\n\t}\n\n}",
        "reject_response": "public void registerTaskManager(final TaskExecutorConnection taskExecutorConnection, SlotReport initialSlotReport) {\n\tcheckInit();\n\n\tLOG.info(\"Registering TaskManager {} under {} at the SlotManager.\", taskExecutorConnection.getResourceID(), taskExecutorConnection.getInstanceID());\n\n\t// we identify task managers by their instance id\n\tif (taskManagerRegistrations.containsKey(taskExecutorConnection.getInstanceID())) {\n\t\treportSlotStatus(taskExecutorConnection.getInstanceID(), initialSlotReport);\n\t} else {\n\t\t// first register the TaskManager\n\t\tArrayList<SlotID> reportedSlots = new ArrayList<>();\n\n\t\tfor (SlotStatus slotStatus : initialSlotReport) {\n\t\t\treportedSlots.add(slotStatus.getSlotID());\n\t\t}\n\n\t\tTaskManagerRegistration taskManagerRegistration = new TaskManagerRegistration(\n\t\t\ttaskExecutorConnection,\n\t\t\treportedSlots);\n\n\t\ttaskManagerRegistrations.put(taskExecutorConnection.getInstanceID(), taskManagerRegistration);\n\n\t\t// next register the new slots\n\t\tfor (SlotStatus slotStatus : initialSlotReport) {\n\t\t\tregisterSlot(\n\t\t\t\tslotStatus.getSlotID(),\n\t\t\t\tslotStatus.getAllocationID(),\n\t\t\t\tslotStatus.getJobID(),\n\t\t\t\tslotStatus.getResourceProfile(),\n\t\t\t\ttaskExecutorConnection);\n\t\t}\n\t}\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2821,
        "instruction": "@SuppressWarnings(\"unchecked\")\nprotected void runQuery(GridCacheQueryInfo qryInfo) {\n    assert qryInfo != null;\n    assert qryInfo.query().type() != SCAN || !qryInfo.local() : qryInfo;\n\n    if (!enterBusy()) {\n        if (cctx.localNodeId().equals(qryInfo.senderId()))\n            throw new IllegalStateException(\"Failed to process query request (grid is stopping).\");\n\n        return; // Ignore remote requests when when node is stopping.\n    }\n\n    try {\n        boolean loc = qryInfo.local();\n\n        QueryResult<K, V> res = null;\n\n        if (log.isDebugEnabled())\n            log.debug(\"Running query: \" + qryInfo);\n\n        boolean rmvIter = true;\n\n        try {\n            // Preparing query closures.\n            IgniteClosure<Cache.Entry<K, V>, Object> trans =\n                (IgniteClosure<Cache.Entry<K, V>, Object>)qryInfo.transformer();\n\n            IgniteReducer<Cache.Entry<K, V>, Object> rdc = (IgniteReducer<Cache.Entry<K, V>, Object>)qryInfo.reducer();\n\n            injectResources(trans);\n            injectResources(rdc);\n\n            GridCacheQueryAdapter<?> qry = qryInfo.query();\n\n            int pageSize = qry.pageSize();\n\n            boolean incBackups = qry.includeBackups();\n\n            String taskName = cctx.kernalContext().task().resolveTaskName(qry.taskHash());\n\n            IgniteSpiCloseableIterator<IgniteBiTuple<K, V>> iter;\n            GridCacheQueryType type;\n\n            res = loc ?\n                executeQuery(qry, qryInfo.arguments(), loc, qry.subjectId(), taskName,\n                    recipient(qryInfo.senderId(), qryInfo.requestId())) :\n                queryResult(qryInfo, taskName);\n\n            if (res == null)\n                return;\n\n            iter = res.iterator(recipient(qryInfo.senderId(), qryInfo.requestId()));\n            type = res.type();\n\n            final GridCacheAdapter<K, V> cache = cctx.cache();\n\n            if (log.isDebugEnabled())\n                log.debug(\"Received index iterator [iterHasNext=\" + iter.hasNext() +\n                    \", cacheSize=\" + cache.size() + ']');\n\n            int cnt = 0;\n\n            boolean stop = false;\n            boolean pageSent = false;\n\n            Collection<Object> data = new ArrayList<>(pageSize);\n\n            AffinityTopologyVersion topVer = cctx.affinity().affinityTopologyVersion();\n\n            final boolean statsEnabled = cctx.config().isStatisticsEnabled();\n\n            final boolean readEvt = cctx.gridEvents().isRecordable(EVT_CACHE_QUERY_OBJECT_READ);\n\n            while (!Thread.currentThread().isInterrupted() && iter.hasNext()) {\n                long start = statsEnabled ? System.nanoTime() : 0L;\n\n                IgniteBiTuple<K, V> row = iter.next();\n\n                // Query is cancelled.\n                if (row == null) {\n                    onPageReady(loc, qryInfo, null, true, null);\n\n                    break;\n                }\n\n                final K key = row.getKey();\n\n                // Filter backups for SCAN queries, if it isn't partition scan.\n                // Other types are filtered in indexing manager.\n                if (!cctx.isReplicated() && qry.type() == SCAN && qry.partition() == null &&\n                    cctx.config().getCacheMode() != LOCAL && !incBackups &&\n                    !cctx.affinity().primary(cctx.localNode(), key, topVer)) {\n                    if (log.isDebugEnabled())\n                        log.debug(\"Ignoring backup element [row=\" + row +\n                            \", cacheMode=\" + cctx.config().getCacheMode() + \", incBackups=\" + incBackups +\n                            \", primary=\" + cctx.affinity().primary(cctx.localNode(), key, topVer) + ']');\n\n                    continue;\n                }\n\n                V val = row.getValue();\n\n                if (log.isDebugEnabled()) {\n                    ClusterNode primaryNode = CU.primaryNode(cctx, key);\n\n                }\n\n                if (val == null) {\n                    if (log.isDebugEnabled())\n                        log.debug(S.toString(\"Unsuitable record value\", \"val\", val, true));\n\n                    continue;\n                }\n\n                if (statsEnabled) {\n                    CacheMetricsImpl metrics = cctx.cache().metrics0();\n\n                    metrics.onRead(true);\n\n                    metrics.addGetTimeNanos(System.nanoTime() - start);\n                }\n\n                K key0 = null;\n                V val0 = null;\n\n                if (readEvt) {\n                    key0 = (K)cctx.unwrapBinaryIfNeeded(key, qry.keepBinary());\n                    val0 = (V)cctx.unwrapBinaryIfNeeded(val, qry.keepBinary());\n\n                    switch (type) {\n                        case SQL:\n                            cctx.gridEvents().record(new CacheQueryReadEvent<>(\n                                cctx.localNode(),\n                                \"SQL query entry read.\",\n                                EVT_CACHE_QUERY_OBJECT_READ,\n                                CacheQueryType.SQL.name(),\n                                cctx.namex(),\n                                qry.queryClassName(),\n                                qry.clause(),\n                                null,\n                                null,\n                                qryInfo.arguments(),\n                                qry.subjectId(),\n                                taskName,\n                                key0,\n                                val0,\n                                null,\n                                null));\n\n                            break;\n\n                        case TEXT:\n                            cctx.gridEvents().record(new CacheQueryReadEvent<>(\n                                cctx.localNode(),\n                                \"Full text query entry read.\",\n                                EVT_CACHE_QUERY_OBJECT_READ,\n                                CacheQueryType.FULL_TEXT.name(),\n                                cctx.namex(),\n                                qry.queryClassName(),\n                                qry.clause(),\n                                null,\n                                null,\n                                null,\n                                qry.subjectId(),\n                                taskName,\n                                key0,\n                                val0,\n                                null,\n                                null));\n\n                            break;\n\n                        case SCAN:\n                            cctx.gridEvents().record(new CacheQueryReadEvent<>(\n                                cctx.localNode(),\n                                \"Scan query entry read.\",\n                                EVT_CACHE_QUERY_OBJECT_READ,\n                                CacheQueryType.SCAN.name(),\n                                cctx.namex(),\n                                null,\n                                null,\n                                qry.scanFilter(),\n                                null,\n                                null,\n                                qry.subjectId(),\n                                taskName,\n                                key0,\n                                val0,\n                                null,\n                                null));\n\n                            break;\n                    }\n                }\n\n                if (rdc != null || trans != null) {\n                    if (key0 == null)\n                        key0 = (K)cctx.unwrapBinaryIfNeeded(key, qry.keepBinary());\n                    if (val0 == null)\n                        val0 = (V)cctx.unwrapBinaryIfNeeded(val, qry.keepBinary());\n\n                    Cache.Entry<K, V> entry = new CacheEntryImpl(key0, val0);\n\n                    // Reduce.\n                    if (rdc != null) {\n                        if (!rdc.collect(entry) || !iter.hasNext()) {\n                            onPageReady(loc, qryInfo, Collections.singletonList(rdc.reduce()), true, null);\n\n                            pageSent = true;\n\n                            break;\n                        }\n                        else\n                            continue;\n                    }\n\n                    data.add(trans != null ? trans.apply(entry) :\n                        !loc ? new GridCacheQueryResponseEntry<>(key, val) : F.t(key, val));\n                }\n                else\n                    data.add(!loc ? new GridCacheQueryResponseEntry<>(key, val) : F.t(key, val));\n\n                if (!loc) {\n                    if (++cnt == pageSize || !iter.hasNext()) {\n                        boolean finished = !iter.hasNext();\n\n                        onPageReady(loc, qryInfo, data, finished, null);\n\n                        pageSent = true;\n\n                        if (!finished)\n                            rmvIter = false;\n\n                        if (!qryInfo.allPages())\n                            return;\n\n                        data = new ArrayList<>(pageSize);\n\n                        if (stop)\n                            break; // while\n                    }\n                }\n            }\n\n            if (!pageSent) {\n                if (rdc == null)\n                    onPageReady(loc, qryInfo, data, true, null);\n                else\n                    onPageReady(loc, qryInfo, Collections.singletonList(rdc.reduce()), true, null);\n            }\n        }\n        catch (Throwable e) {\n            if (!X.hasCause(e, GridDhtUnreservedPartitionException.class))\n                U.error(log, \"Failed to run query [qry=\" + qryInfo + \", node=\" + cctx.nodeId() + \"]\", e);\n\n            onPageReady(loc, qryInfo, null, true, e);\n\n            if (e instanceof Error)\n                throw (Error)e;\n        }\n        finally {\n            if (loc) {\n                // Local iterators are always removed.\n                if (res != null) {\n                    try {\n                        res.closeIfNotShared(recipient(qryInfo.senderId(), qryInfo.requestId()));\n                    }\n                    catch (IgniteCheckedException e) {\n                        if (!X.hasCause(e, GridDhtUnreservedPartitionException.class))\n                            U.error(log, \"Failed to close local iterator [qry=\" + qryInfo + \", node=\" +\n                                cctx.nodeId() + \"]\", e);\n                    }\n                }\n            }\n            else if (rmvIter)\n                removeQueryResult(qryInfo.senderId(), qryInfo.requestId());\n        }\n    }\n    finally {\n        leaveBusy();\n    }\n}",
        "accept_response": "@SuppressWarnings(\"unchecked\")\nprotected void runQuery(GridCacheQueryInfo qryInfo) {\n    assert qryInfo != null;\n    assert qryInfo.query().type() != SCAN || !qryInfo.local() : qryInfo;\n\n    if (!enterBusy()) {\n        if (cctx.localNodeId().equals(qryInfo.senderId()))\n            throw new IllegalStateException(\"Failed to process query request (grid is stopping).\");\n\n        return; // Ignore remote requests when when node is stopping.\n    }\n\n    try {\n        boolean loc = qryInfo.local();\n\n        QueryResult<K, V> res = null;\n\n        if (log.isDebugEnabled())\n            log.debug(\"Running query: \" + qryInfo);\n\n        boolean rmvIter = true;\n\n        try {\n            // Preparing query closures.\n            IgniteClosure<Cache.Entry<K, V>, Object> trans =\n                (IgniteClosure<Cache.Entry<K, V>, Object>)qryInfo.transformer();\n\n            IgniteReducer<Cache.Entry<K, V>, Object> rdc = (IgniteReducer<Cache.Entry<K, V>, Object>)qryInfo.reducer();\n\n            injectResources(trans);\n            injectResources(rdc);\n\n            GridCacheQueryAdapter<?> qry = qryInfo.query();\n\n            int pageSize = qry.pageSize();\n\n            boolean incBackups = qry.includeBackups();\n\n            String taskName = cctx.kernalContext().task().resolveTaskName(qry.taskHash());\n\n            IgniteSpiCloseableIterator<IgniteBiTuple<K, V>> iter;\n            GridCacheQueryType type;\n\n            res = loc ?\n                executeQuery(qry, qryInfo.arguments(), loc, qry.subjectId(), taskName,\n                    recipient(qryInfo.senderId(), qryInfo.requestId())) :\n                queryResult(qryInfo, taskName);\n\n            if (res == null)\n                return;\n\n            iter = res.iterator(recipient(qryInfo.senderId(), qryInfo.requestId()));\n            type = res.type();\n\n            final GridCacheAdapter<K, V> cache = cctx.cache();\n\n            if (log.isDebugEnabled())\n                log.debug(\"Received index iterator [iterHasNext=\" + iter.hasNext() +\n                    \", cacheSize=\" + cache.size() + ']');\n\n            int cnt = 0;\n\n            boolean stop = false;\n            boolean pageSent = false;\n\n            Collection<Object> data = new ArrayList<>(pageSize);\n\n            AffinityTopologyVersion topVer = cctx.affinity().affinityTopologyVersion();\n\n            final boolean statsEnabled = cctx.config().isStatisticsEnabled();\n\n            final boolean readEvt = cctx.gridEvents().isRecordable(EVT_CACHE_QUERY_OBJECT_READ);\n\n            while (!Thread.currentThread().isInterrupted() && iter.hasNext()) {\n                long start = statsEnabled ? System.nanoTime() : 0L;\n\n                IgniteBiTuple<K, V> row = iter.next();\n\n                // Query is cancelled.\n                if (row == null) {\n                    onPageReady(loc, qryInfo, null, true, null);\n\n                    break;\n                }\n\n                final K key = row.getKey();\n\n                // Filter backups for SCAN queries, if it isn't partition scan.\n                // Other types are filtered in indexing manager.\n                if (!cctx.isReplicated() && qry.type() == SCAN && qry.partition() == null &&\n                    cctx.config().getCacheMode() != LOCAL && !incBackups &&\n                    !cctx.affinity().primary(cctx.localNode(), key, topVer)) {\n                    if (log.isDebugEnabled())\n                        log.debug(\"Ignoring backup element [row=\" + row +\n                            \", cacheMode=\" + cctx.config().getCacheMode() + \", incBackups=\" + incBackups +\n                            \", primary=\" + cctx.affinity().primary(cctx.localNode(), key, topVer) + ']');\n\n                    continue;\n                }\n\n                V val = row.getValue();\n\n                if (log.isDebugEnabled()) {\n                    ClusterNode primaryNode = CU.primaryNode(cctx, key);\n\n                    log.debug(S.toString(\"Record\",\n                        \"key\", key, true,\n                        \"val\", val, true,\n                        \"incBackups\", incBackups, false,\n                        \"priNode\", primaryNode != null ? U.id8(primaryNode.id()) : null, false,\n                        \"node\", U.id8(cctx.localNode().id()), false));\n                }\n\n                if (val == null) {\n                    if (log.isDebugEnabled())\n                        log.debug(S.toString(\"Unsuitable record value\", \"val\", val, true));\n\n                    continue;\n                }\n\n                if (statsEnabled) {\n                    CacheMetricsImpl metrics = cctx.cache().metrics0();\n\n                    metrics.onRead(true);\n\n                    metrics.addGetTimeNanos(System.nanoTime() - start);\n                }\n\n                K key0 = null;\n                V val0 = null;\n\n                if (readEvt) {\n                    key0 = (K)cctx.unwrapBinaryIfNeeded(key, qry.keepBinary());\n                    val0 = (V)cctx.unwrapBinaryIfNeeded(val, qry.keepBinary());\n\n                    switch (type) {\n                        case SQL:\n                            cctx.gridEvents().record(new CacheQueryReadEvent<>(\n                                cctx.localNode(),\n                                \"SQL query entry read.\",\n                                EVT_CACHE_QUERY_OBJECT_READ,\n                                CacheQueryType.SQL.name(),\n                                cctx.namex(),\n                                qry.queryClassName(),\n                                qry.clause(),\n                                null,\n                                null,\n                                qryInfo.arguments(),\n                                qry.subjectId(),\n                                taskName,\n                                key0,\n                                val0,\n                                null,\n                                null));\n\n                            break;\n\n                        case TEXT:\n                            cctx.gridEvents().record(new CacheQueryReadEvent<>(\n                                cctx.localNode(),\n                                \"Full text query entry read.\",\n                                EVT_CACHE_QUERY_OBJECT_READ,\n                                CacheQueryType.FULL_TEXT.name(),\n                                cctx.namex(),\n                                qry.queryClassName(),\n                                qry.clause(),\n                                null,\n                                null,\n                                null,\n                                qry.subjectId(),\n                                taskName,\n                                key0,\n                                val0,\n                                null,\n                                null));\n\n                            break;\n\n                        case SCAN:\n                            cctx.gridEvents().record(new CacheQueryReadEvent<>(\n                                cctx.localNode(),\n                                \"Scan query entry read.\",\n                                EVT_CACHE_QUERY_OBJECT_READ,\n                                CacheQueryType.SCAN.name(),\n                                cctx.namex(),\n                                null,\n                                null,\n                                qry.scanFilter(),\n                                null,\n                                null,\n                                qry.subjectId(),\n                                taskName,\n                                key0,\n                                val0,\n                                null,\n                                null));\n\n                            break;\n                    }\n                }\n\n                if (rdc != null || trans != null) {\n                    if (key0 == null)\n                        key0 = (K)cctx.unwrapBinaryIfNeeded(key, qry.keepBinary());\n                    if (val0 == null)\n                        val0 = (V)cctx.unwrapBinaryIfNeeded(val, qry.keepBinary());\n\n                    Cache.Entry<K, V> entry = new CacheEntryImpl(key0, val0);\n\n                    // Reduce.\n                    if (rdc != null) {\n                        if (!rdc.collect(entry) || !iter.hasNext()) {\n                            onPageReady(loc, qryInfo, Collections.singletonList(rdc.reduce()), true, null);\n\n                            pageSent = true;\n\n                            break;\n                        }\n                        else\n                            continue;\n                    }\n\n                    data.add(trans != null ? trans.apply(entry) :\n                        !loc ? new GridCacheQueryResponseEntry<>(key, val) : F.t(key, val));\n                }\n                else\n                    data.add(!loc ? new GridCacheQueryResponseEntry<>(key, val) : F.t(key, val));\n\n                if (!loc) {\n                    if (++cnt == pageSize || !iter.hasNext()) {\n                        boolean finished = !iter.hasNext();\n\n                        onPageReady(loc, qryInfo, data, finished, null);\n\n                        pageSent = true;\n\n                        if (!finished)\n                            rmvIter = false;\n\n                        if (!qryInfo.allPages())\n                            return;\n\n                        data = new ArrayList<>(pageSize);\n\n                        if (stop)\n                            break; // while\n                    }\n                }\n            }\n\n            if (!pageSent) {\n                if (rdc == null)\n                    onPageReady(loc, qryInfo, data, true, null);\n                else\n                    onPageReady(loc, qryInfo, Collections.singletonList(rdc.reduce()), true, null);\n            }\n        }\n        catch (Throwable e) {\n            if (!X.hasCause(e, GridDhtUnreservedPartitionException.class))\n                U.error(log, \"Failed to run query [qry=\" + qryInfo + \", node=\" + cctx.nodeId() + \"]\", e);\n\n            onPageReady(loc, qryInfo, null, true, e);\n\n            if (e instanceof Error)\n                throw (Error)e;\n        }\n        finally {\n            if (loc) {\n                // Local iterators are always removed.\n                if (res != null) {\n                    try {\n                        res.closeIfNotShared(recipient(qryInfo.senderId(), qryInfo.requestId()));\n                    }\n                    catch (IgniteCheckedException e) {\n                        if (!X.hasCause(e, GridDhtUnreservedPartitionException.class))\n                            U.error(log, \"Failed to close local iterator [qry=\" + qryInfo + \", node=\" +\n                                cctx.nodeId() + \"]\", e);\n                    }\n                }\n            }\n            else if (rmvIter)\n                removeQueryResult(qryInfo.senderId(), qryInfo.requestId());\n        }\n    }\n    finally {\n        leaveBusy();\n    }\n}",
        "reject_response": "@SuppressWarnings(\"unchecked\")\nprotected void runQuery(GridCacheQueryInfo qryInfo) {\n    assert qryInfo != null;\n    assert qryInfo.query().type() != SCAN || !qryInfo.local() : qryInfo;\n\n    if (!enterBusy()) {\n        if (cctx.localNodeId().equals(qryInfo.senderId()))\n            throw new IllegalStateException(\"Failed to process query request (grid is stopping).\");\n\n        return; // Ignore remote requests when when node is stopping.\n    }\n\n    try {\n        boolean loc = qryInfo.local();\n\n        QueryResult<K, V> res = null;\n\n        if (log.isDebugEnabled())\n            log.debug(\"Running query: \" + qryInfo);\n\n        boolean rmvIter = true;\n\n        try {\n            // Preparing query closures.\n            IgniteClosure<Cache.Entry<K, V>, Object> trans =\n                (IgniteClosure<Cache.Entry<K, V>, Object>)qryInfo.transformer();\n\n            IgniteReducer<Cache.Entry<K, V>, Object> rdc = (IgniteReducer<Cache.Entry<K, V>, Object>)qryInfo.reducer();\n\n            injectResources(trans);\n            injectResources(rdc);\n\n            GridCacheQueryAdapter<?> qry = qryInfo.query();\n\n            int pageSize = qry.pageSize();\n\n            boolean incBackups = qry.includeBackups();\n\n            String taskName = cctx.kernalContext().task().resolveTaskName(qry.taskHash());\n\n            IgniteSpiCloseableIterator<IgniteBiTuple<K, V>> iter;\n            GridCacheQueryType type;\n\n            res = loc ?\n                executeQuery(qry, qryInfo.arguments(), loc, qry.subjectId(), taskName,\n                    recipient(qryInfo.senderId(), qryInfo.requestId())) :\n                queryResult(qryInfo, taskName);\n\n            if (res == null)\n                return;\n\n            iter = res.iterator(recipient(qryInfo.senderId(), qryInfo.requestId()));\n            type = res.type();\n\n            final GridCacheAdapter<K, V> cache = cctx.cache();\n\n            if (log.isDebugEnabled())\n                log.debug(\"Received index iterator [iterHasNext=\" + iter.hasNext() +\n                    \", cacheSize=\" + cache.size() + ']');\n\n            int cnt = 0;\n\n            boolean stop = false;\n            boolean pageSent = false;\n\n            Collection<Object> data = new ArrayList<>(pageSize);\n\n            AffinityTopologyVersion topVer = cctx.affinity().affinityTopologyVersion();\n\n            final boolean statsEnabled = cctx.config().isStatisticsEnabled();\n\n            final boolean readEvt = cctx.gridEvents().isRecordable(EVT_CACHE_QUERY_OBJECT_READ);\n\n            while (!Thread.currentThread().isInterrupted() && iter.hasNext()) {\n                long start = statsEnabled ? System.nanoTime() : 0L;\n\n                IgniteBiTuple<K, V> row = iter.next();\n\n                // Query is cancelled.\n                if (row == null) {\n                    onPageReady(loc, qryInfo, null, true, null);\n\n                    break;\n                }\n\n                final K key = row.getKey();\n\n                // Filter backups for SCAN queries, if it isn't partition scan.\n                // Other types are filtered in indexing manager.\n                if (!cctx.isReplicated() && qry.type() == SCAN && qry.partition() == null &&\n                    cctx.config().getCacheMode() != LOCAL && !incBackups &&\n                    !cctx.affinity().primary(cctx.localNode(), key, topVer)) {\n                    if (log.isDebugEnabled())\n                        log.debug(\"Ignoring backup element [row=\" + row +\n                            \", cacheMode=\" + cctx.config().getCacheMode() + \", incBackups=\" + incBackups +\n                            \", primary=\" + cctx.affinity().primary(cctx.localNode(), key, topVer) + ']');\n\n                    continue;\n                }\n\n                V val = row.getValue();\n\n                if (log.isDebugEnabled()) {\n                    ClusterNode primaryNode = CU.primaryNode(cctx, key);\n\n                    log.debug(\"Record [key=\" + key +\n                        \", val=\" + val +\n                        \", incBackups=\" + incBackups +\n                        \", priNode=\" + (primaryNode != null ? U.id8(primaryNode.id()) : null) +\n                        \", node=\" + U.id8(cctx.localNode().id()) + ']');\n                }\n\n                if (val == null) {\n                    if (log.isDebugEnabled())\n                        log.debug(S.toString(\"Unsuitable record value\", \"val\", val, true));\n\n                    continue;\n                }\n\n                if (statsEnabled) {\n                    CacheMetricsImpl metrics = cctx.cache().metrics0();\n\n                    metrics.onRead(true);\n\n                    metrics.addGetTimeNanos(System.nanoTime() - start);\n                }\n\n                K key0 = null;\n                V val0 = null;\n\n                if (readEvt) {\n                    key0 = (K)cctx.unwrapBinaryIfNeeded(key, qry.keepBinary());\n                    val0 = (V)cctx.unwrapBinaryIfNeeded(val, qry.keepBinary());\n\n                    switch (type) {\n                        case SQL:\n                            cctx.gridEvents().record(new CacheQueryReadEvent<>(\n                                cctx.localNode(),\n                                \"SQL query entry read.\",\n                                EVT_CACHE_QUERY_OBJECT_READ,\n                                CacheQueryType.SQL.name(),\n                                cctx.namex(),\n                                qry.queryClassName(),\n                                qry.clause(),\n                                null,\n                                null,\n                                qryInfo.arguments(),\n                                qry.subjectId(),\n                                taskName,\n                                key0,\n                                val0,\n                                null,\n                                null));\n\n                            break;\n\n                        case TEXT:\n                            cctx.gridEvents().record(new CacheQueryReadEvent<>(\n                                cctx.localNode(),\n                                \"Full text query entry read.\",\n                                EVT_CACHE_QUERY_OBJECT_READ,\n                                CacheQueryType.FULL_TEXT.name(),\n                                cctx.namex(),\n                                qry.queryClassName(),\n                                qry.clause(),\n                                null,\n                                null,\n                                null,\n                                qry.subjectId(),\n                                taskName,\n                                key0,\n                                val0,\n                                null,\n                                null));\n\n                            break;\n\n                        case SCAN:\n                            cctx.gridEvents().record(new CacheQueryReadEvent<>(\n                                cctx.localNode(),\n                                \"Scan query entry read.\",\n                                EVT_CACHE_QUERY_OBJECT_READ,\n                                CacheQueryType.SCAN.name(),\n                                cctx.namex(),\n                                null,\n                                null,\n                                qry.scanFilter(),\n                                null,\n                                null,\n                                qry.subjectId(),\n                                taskName,\n                                key0,\n                                val0,\n                                null,\n                                null));\n\n                            break;\n                    }\n                }\n\n                if (rdc != null || trans != null) {\n                    if (key0 == null)\n                        key0 = (K)cctx.unwrapBinaryIfNeeded(key, qry.keepBinary());\n                    if (val0 == null)\n                        val0 = (V)cctx.unwrapBinaryIfNeeded(val, qry.keepBinary());\n\n                    Cache.Entry<K, V> entry = new CacheEntryImpl(key0, val0);\n\n                    // Reduce.\n                    if (rdc != null) {\n                        if (!rdc.collect(entry) || !iter.hasNext()) {\n                            onPageReady(loc, qryInfo, Collections.singletonList(rdc.reduce()), true, null);\n\n                            pageSent = true;\n\n                            break;\n                        }\n                        else\n                            continue;\n                    }\n\n                    data.add(trans != null ? trans.apply(entry) :\n                        !loc ? new GridCacheQueryResponseEntry<>(key, val) : F.t(key, val));\n                }\n                else\n                    data.add(!loc ? new GridCacheQueryResponseEntry<>(key, val) : F.t(key, val));\n\n                if (!loc) {\n                    if (++cnt == pageSize || !iter.hasNext()) {\n                        boolean finished = !iter.hasNext();\n\n                        onPageReady(loc, qryInfo, data, finished, null);\n\n                        pageSent = true;\n\n                        if (!finished)\n                            rmvIter = false;\n\n                        if (!qryInfo.allPages())\n                            return;\n\n                        data = new ArrayList<>(pageSize);\n\n                        if (stop)\n                            break; // while\n                    }\n                }\n            }\n\n            if (!pageSent) {\n                if (rdc == null)\n                    onPageReady(loc, qryInfo, data, true, null);\n                else\n                    onPageReady(loc, qryInfo, Collections.singletonList(rdc.reduce()), true, null);\n            }\n        }\n        catch (Throwable e) {\n            if (!X.hasCause(e, GridDhtUnreservedPartitionException.class))\n                U.error(log, \"Failed to run query [qry=\" + qryInfo + \", node=\" + cctx.nodeId() + \"]\", e);\n\n            onPageReady(loc, qryInfo, null, true, e);\n\n            if (e instanceof Error)\n                throw (Error)e;\n        }\n        finally {\n            if (loc) {\n                // Local iterators are always removed.\n                if (res != null) {\n                    try {\n                        res.closeIfNotShared(recipient(qryInfo.senderId(), qryInfo.requestId()));\n                    }\n                    catch (IgniteCheckedException e) {\n                        if (!X.hasCause(e, GridDhtUnreservedPartitionException.class))\n                            U.error(log, \"Failed to close local iterator [qry=\" + qryInfo + \", node=\" +\n                                cctx.nodeId() + \"]\", e);\n                    }\n                }\n            }\n            else if (rmvIter)\n                removeQueryResult(qryInfo.senderId(), qryInfo.requestId());\n        }\n    }\n    finally {\n        leaveBusy();\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3021,
        "instruction": "public void updateTopicUnitSubFlag(final String topic, final boolean hasUnitSub) {\n    TopicConfig topicConfig = this.topicConfigTable.get(topic);\n    if (topicConfig != null) {\n        int oldTopicSysFlag = topicConfig.getTopicSysFlag();\n        if (hasUnitSub) {\n            topicConfig.setTopicSysFlag(TopicSysFlag.setUnitSubFlag(oldTopicSysFlag));\n        }\n\n\n        this.topicConfigTable.put(topic, topicConfig);\n\n        this.dataVersion.nextVersion();\n\n        this.persist();\n        this.brokerController.registerBrokerAll(false, true);\n    }\n}",
        "accept_response": "public void updateTopicUnitSubFlag(final String topic, final boolean hasUnitSub) {\n    TopicConfig topicConfig = this.topicConfigTable.get(topic);\n    if (topicConfig != null) {\n        int oldTopicSysFlag = topicConfig.getTopicSysFlag();\n        if (hasUnitSub) {\n            topicConfig.setTopicSysFlag(TopicSysFlag.setUnitSubFlag(oldTopicSysFlag));\n        }\n\n        log.info(\"update topic sys flag. oldTopicSysFlag={}, newTopicSysFlag\", oldTopicSysFlag,\n                topicConfig.getTopicSysFlag());\n\n        this.topicConfigTable.put(topic, topicConfig);\n\n        this.dataVersion.nextVersion();\n\n        this.persist();\n        this.brokerController.registerBrokerAll(false, true);\n    }\n}",
        "reject_response": "public void updateTopicUnitSubFlag(final String topic, final boolean hasUnitSub) {\n    TopicConfig topicConfig = this.topicConfigTable.get(topic);\n    if (topicConfig != null) {\n        int oldTopicSysFlag = topicConfig.getTopicSysFlag();\n        if (hasUnitSub) {\n            topicConfig.setTopicSysFlag(TopicSysFlag.setUnitSubFlag(oldTopicSysFlag));\n        }\n\n        LOG.info(\"update topic sys flag. oldTopicSysFlag={}, newTopicSysFlag\", oldTopicSysFlag,\n            topicConfig.getTopicSysFlag());\n\n        this.topicConfigTable.put(topic, topicConfig);\n\n        this.dataVersion.nextVersion();\n\n        this.persist();\n        this.brokerController.registerBrokerAll(false, true);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2651,
        "instruction": "private void refresh(Member m) {\n\n  Random r = new Random(System.currentTimeMillis());\n\n  m.setUptime(System.currentTimeMillis());\n  m.setQueueBacklog(\"\" + Math.abs(r.nextInt(500)));\n  m.setCurrentHeapSize(Math.abs(r.nextInt(Math.abs((int) m.getMaxHeapSize()))));\n  m.setTotalDiskUsage(Math.abs(r.nextInt(100)));\n\n  double cpuUsage = r.nextDouble() * 100;\n  m.getCpuUsageSamples().add(cpuUsage);\n  m.setCpuUsage(cpuUsage);\n\n  m.getHeapUsageSamples().add(m.getCurrentHeapSize());\n  m.setLoadAverage((double) Math.abs(r.nextInt(100)));\n  m.setNumThreads(Math.abs(r.nextInt(100)));\n  m.setGarbageCollectionCount((long) Math.abs(r.nextInt(100)));\n  m.getGarbageCollectionSamples().add(m.getGarbageCollectionCount());\n\n  m.setTotalFileDescriptorOpen((long) Math.abs(r.nextInt(100)));\n\n  m.setThroughputWrites(Math.abs(r.nextInt(10)));\n  m.getThroughputWritesTrend().add(m.getThroughputWrites());\n\n  m.setGetsRate(Math.abs(r.nextInt(5000)));\n  m.getGetsPerSecond().add(m.getGetsRate());\n\n  m.setPutsRate(Math.abs(r.nextInt(5000)));\n  m.getPutsPerSecond().add(m.getPutsRate());\n\n  Alert[] alerts = cluster.getAlertsList();\n  List<Alert> alertsList = Arrays.asList(alerts);\n\n  if (r.nextBoolean()) {\n    // Generate alerts\n    if (r.nextBoolean()) {\n      if (r.nextInt(10) > 5) {\n        alertsList.add(createAlert(Alert.SEVERE, m.getName(), alertsList.size()));\n        if (alertsList.size() > ALERTS_MAX_SIZE) {\n          alertsList.remove(0);\n        }\n      }\n    }\n\n    if (r.nextBoolean()) {\n      if (r.nextInt(10) > 5) {\n        alertsList.add(createAlert(Alert.ERROR, m.getName(), alertsList.size()));\n        if (alertsList.size() > ALERTS_MAX_SIZE) {\n          alertsList.remove(0);\n        }\n      }\n    }\n\n    if (r.nextBoolean()) {\n      if (r.nextInt(10) > 5) {\n        alertsList.add(createAlert(Alert.WARNING, m.getName(), alertsList.size()));\n        if (alertsList.size() > ALERTS_MAX_SIZE) {\n          alertsList.remove(0);\n        }\n      }\n    }\n  }\n}",
        "accept_response": "private void refresh(Member m) {\n  logger.info(\"{} : {}\", resourceBundle.getString(\"LOG_MSG_REFRESHING_MEMBER_DATA\"), m.getName());\n\n  Random r = new Random(System.currentTimeMillis());\n\n  m.setUptime(System.currentTimeMillis());\n  m.setQueueBacklog(\"\" + Math.abs(r.nextInt(500)));\n  m.setCurrentHeapSize(Math.abs(r.nextInt(Math.abs((int) m.getMaxHeapSize()))));\n  m.setTotalDiskUsage(Math.abs(r.nextInt(100)));\n\n  double cpuUsage = r.nextDouble() * 100;\n  m.getCpuUsageSamples().add(cpuUsage);\n  m.setCpuUsage(cpuUsage);\n\n  m.getHeapUsageSamples().add(m.getCurrentHeapSize());\n  m.setLoadAverage((double) Math.abs(r.nextInt(100)));\n  m.setNumThreads(Math.abs(r.nextInt(100)));\n  m.setGarbageCollectionCount((long) Math.abs(r.nextInt(100)));\n  m.getGarbageCollectionSamples().add(m.getGarbageCollectionCount());\n\n  m.setTotalFileDescriptorOpen((long) Math.abs(r.nextInt(100)));\n\n  m.setThroughputWrites(Math.abs(r.nextInt(10)));\n  m.getThroughputWritesTrend().add(m.getThroughputWrites());\n\n  m.setGetsRate(Math.abs(r.nextInt(5000)));\n  m.getGetsPerSecond().add(m.getGetsRate());\n\n  m.setPutsRate(Math.abs(r.nextInt(5000)));\n  m.getPutsPerSecond().add(m.getPutsRate());\n\n  Alert[] alerts = cluster.getAlertsList();\n  List<Alert> alertsList = Arrays.asList(alerts);\n\n  if (r.nextBoolean()) {\n    // Generate alerts\n    if (r.nextBoolean()) {\n      if (r.nextInt(10) > 5) {\n        alertsList.add(createAlert(Alert.SEVERE, m.getName(), alertsList.size()));\n        if (alertsList.size() > ALERTS_MAX_SIZE) {\n          alertsList.remove(0);\n        }\n      }\n    }\n\n    if (r.nextBoolean()) {\n      if (r.nextInt(10) > 5) {\n        alertsList.add(createAlert(Alert.ERROR, m.getName(), alertsList.size()));\n        if (alertsList.size() > ALERTS_MAX_SIZE) {\n          alertsList.remove(0);\n        }\n      }\n    }\n\n    if (r.nextBoolean()) {\n      if (r.nextInt(10) > 5) {\n        alertsList.add(createAlert(Alert.WARNING, m.getName(), alertsList.size()));\n        if (alertsList.size() > ALERTS_MAX_SIZE) {\n          alertsList.remove(0);\n        }\n      }\n    }\n  }\n}",
        "reject_response": "private void refresh(Member m) {\n  if (LOGGER.infoEnabled()) {\n    LOGGER.info(resourceBundle.getString(\"LOG_MSG_REFRESHING_MEMBER_DATA\") + \" : \" + m.getName());\n  }\n\n  Random r = new Random(System.currentTimeMillis());\n\n  m.setUptime(System.currentTimeMillis());\n  m.setQueueBacklog(\"\" + Math.abs(r.nextInt(500)));\n  m.setCurrentHeapSize(Math.abs(r.nextInt(Math.abs((int) m.getMaxHeapSize()))));\n  m.setTotalDiskUsage(Math.abs(r.nextInt(100)));\n\n  double cpuUsage = r.nextDouble() * 100;\n  m.getCpuUsageSamples().add(cpuUsage);\n  m.setCpuUsage(cpuUsage);\n\n  m.getHeapUsageSamples().add(m.getCurrentHeapSize());\n  m.setLoadAverage((double) Math.abs(r.nextInt(100)));\n  m.setNumThreads(Math.abs(r.nextInt(100)));\n  m.setGarbageCollectionCount((long) Math.abs(r.nextInt(100)));\n  m.getGarbageCollectionSamples().add(m.getGarbageCollectionCount());\n\n  m.setTotalFileDescriptorOpen((long) Math.abs(r.nextInt(100)));\n\n  m.setThroughputWrites(Math.abs(r.nextInt(10)));\n  m.getThroughputWritesTrend().add(m.getThroughputWrites());\n\n  m.setGetsRate(Math.abs(r.nextInt(5000)));\n  m.getGetsPerSecond().add(m.getGetsRate());\n\n  m.setPutsRate(Math.abs(r.nextInt(5000)));\n  m.getPutsPerSecond().add(m.getPutsRate());\n\n  Alert[] alerts = cluster.getAlertsList();\n  List<Alert> alertsList = Arrays.asList(alerts);\n\n  if (r.nextBoolean()) {\n    // Generate alerts\n    if (r.nextBoolean()) {\n      if (r.nextInt(10) > 5) {\n        alertsList.add(createAlert(Alert.SEVERE, m.getName(), alertsList.size()));\n        if (alertsList.size() > ALERTS_MAX_SIZE) {\n          alertsList.remove(0);\n        }\n      }\n    }\n\n    if (r.nextBoolean()) {\n      if (r.nextInt(10) > 5) {\n        alertsList.add(createAlert(Alert.ERROR, m.getName(), alertsList.size()));\n        if (alertsList.size() > ALERTS_MAX_SIZE) {\n          alertsList.remove(0);\n        }\n      }\n    }\n\n    if (r.nextBoolean()) {\n      if (r.nextInt(10) > 5) {\n        alertsList.add(createAlert(Alert.WARNING, m.getName(), alertsList.size()));\n        if (alertsList.size() > ALERTS_MAX_SIZE) {\n          alertsList.remove(0);\n        }\n      }\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2693,
        "instruction": "Response readResponse(final Bridge bridge, ProtocolData protData,\n                      final boolean threadSafe) {\n    final int fragment = protData.getDataFragment();\n    final String dataDir = protData.getDataSource();\n\n    // Creating an internal streaming class\n    // which will iterate the records and put them on the\n    // output stream\n    final StreamingOutput streaming = new StreamingOutput() {\n        @Override\n        public void write(final OutputStream out) throws IOException,\n                WebApplicationException {\n            long recordCount = 0;\n\n            if (!threadSafe) {\n                lock(dataDir);\n            }\n            try {\n\n                if (!bridge.beginIteration()) {\n                    return;\n                }\n\n                Writable record;\n                DataOutputStream dos = new DataOutputStream(out);\n                LOG.debug(\"Starting streaming fragment \" + fragment\n                        + \" of resource \" + dataDir);\n                while ((record = bridge.getNext()) != null) {\n                    record.write(dos);\n                    ++recordCount;\n                }\n            } catch (ClientAbortException e) {\n                // Occurs whenever client (HAWQ) decides the end the\n                // connection\n                LOG.error(\"Remote connection closed by HAWQ\", e);\n            } catch (Exception e) {\n                LOG.error(\"Exception thrown when streaming\", e);\n                throw new IOException(e.getMessage());\n            } finally {\n                LOG.debug(\"Stopped streaming fragment \" + fragment\n                        + \" of resource \" + dataDir + \", \" + recordCount\n                        + \" records.\");\n                if (!threadSafe) {\n                    unlock(dataDir);\n                }\n            }\n        }\n    };\n\n    return Response.ok(streaming, MediaType.APPLICATION_OCTET_STREAM).build();\n}",
        "accept_response": "Response readResponse(final Bridge bridge, ProtocolData protData,\n                      final boolean threadSafe) {\n    final int fragment = protData.getDataFragment();\n    final String dataDir = protData.getDataSource();\n\n    // Creating an internal streaming class\n    // which will iterate the records and put them on the\n    // output stream\n    final StreamingOutput streaming = new StreamingOutput() {\n        @Override\n        public void write(final OutputStream out) throws IOException,\n                WebApplicationException {\n            long recordCount = 0;\n\n            if (!threadSafe) {\n                lock(dataDir);\n            }\n            try {\n\n                if (!bridge.beginIteration()) {\n                    return;\n                }\n\n                Writable record;\n                DataOutputStream dos = new DataOutputStream(out);\n                LOG.debug(\"Starting streaming fragment \" + fragment\n                        + \" of resource \" + dataDir);\n                while ((record = bridge.getNext()) != null) {\n                    record.write(dos);\n                    ++recordCount;\n                }\n                LOG.debug(\"Finished streaming fragment \" + fragment\n                        + \" of resource \" + dataDir + \", \" + recordCount\n                        + \" records.\");\n            } catch (ClientAbortException e) {\n                // Occurs whenever client (HAWQ) decides the end the\n                // connection\n                LOG.error(\"Remote connection closed by HAWQ\", e);\n            } catch (Exception e) {\n                LOG.error(\"Exception thrown when streaming\", e);\n                throw new IOException(e.getMessage());\n            } finally {\n                LOG.debug(\"Stopped streaming fragment \" + fragment\n                        + \" of resource \" + dataDir + \", \" + recordCount\n                        + \" records.\");\n                if (!threadSafe) {\n                    unlock(dataDir);\n                }\n            }\n        }\n    };\n\n    return Response.ok(streaming, MediaType.APPLICATION_OCTET_STREAM).build();\n}",
        "reject_response": "Response readResponse(final Bridge bridge, ProtocolData protData,\n                      final boolean threadSafe) {\n    final int fragment = protData.getDataFragment();\n    final String dataDir = protData.getDataSource();\n\n    // Creating an internal streaming class\n    // which will iterate the records and put them on the\n    // output stream\n    final StreamingOutput streaming = new StreamingOutput() {\n        @Override\n        public void write(final OutputStream out) throws IOException,\n                WebApplicationException {\n            long recordCount = 0;\n\n            if (!threadSafe) {\n                lock(dataDir);\n            }\n            try {\n\n                if (!bridge.beginIteration()) {\n                    return;\n                }\n\n                Writable record;\n                DataOutputStream dos = new DataOutputStream(out);\n                LOG.debug(\"Starting streaming fragment \" + fragment\n                        + \" of resource \" + dataDir);\n                while ((record = bridge.getNext()) != null) {\n                    record.write(dos);\n                    ++recordCount;\n                }\n                Log.debug(\"Finished streaming fragment \" + fragment\n            } catch (ClientAbortException e) {\n                // Occurs whenever client (HAWQ) decides the end the\n                // connection\n                LOG.error(\"Remote connection closed by HAWQ\", e);\n            } catch (Exception e) {\n                LOG.error(\"Exception thrown when streaming\", e);\n                throw new IOException(e.getMessage());\n            } finally {\n                LOG.debug(\"Stopped streaming fragment \" + fragment\n                        + \" of resource \" + dataDir + \", \" + recordCount\n                        + \" records.\");\n                if (!threadSafe) {\n                    unlock(dataDir);\n                }\n            }\n        }\n    };\n\n    return Response.ok(streaming, MediaType.APPLICATION_OCTET_STREAM).build();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2856,
        "instruction": "private void logReadSuccess(CassandraIdAndPath cassandraIdAndPath) {\n    GhostMailbox.logger()\n        .addField(GhostMailbox.MAILBOX_NAME, cassandraIdAndPath.getMailboxPath())\n        .addField(TYPE, \"readSuccess\")\n        .addField(GhostMailbox.MAILBOX_ID, cassandraIdAndPath.getCassandraId())\n}",
        "accept_response": "private void logReadSuccess(CassandraIdAndPath cassandraIdAndPath) {\n    GhostMailbox.logger()\n        .addField(GhostMailbox.MAILBOX_NAME, cassandraIdAndPath.getMailboxPath())\n        .addField(TYPE, \"readSuccess\")\n        .addField(GhostMailbox.MAILBOX_ID, cassandraIdAndPath.getCassandraId())\n        .log(logger -> logger.debug(\"Read mailbox succeeded\"));\n}",
        "reject_response": "private void logReadSuccess(CassandraIdAndPath cassandraIdAndPath) {\n    GhostMailbox.logger()\n        .addField(GhostMailbox.MAILBOX_NAME, cassandraIdAndPath.getMailboxPath())\n        .addField(TYPE, \"readSuccess\")\n        .addField(GhostMailbox.MAILBOX_ID, cassandraIdAndPath.getCassandraId())\n        .log(logger -> logger.info(\"Read mailbox succeeded\"));\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3130,
        "instruction": "private static HistoryEvent getNextEvent(CodedInputStream inputStream)\n    throws IOException {\n  boolean isAtEnd = inputStream.isAtEnd();\n  if (isAtEnd) {\n    return null;\n  }\n  int eventTypeOrdinal = -1;\n  try {\n    eventTypeOrdinal = inputStream.readFixed32();\n  } catch (EOFException eof) {\n    return null;\n  }\n  if (eventTypeOrdinal < 0 || eventTypeOrdinal >=\n      HistoryEventType.values().length) {\n    // Corrupt data\n    // reached end\n    throw new IOException(\"Corrupt data found when trying to read next event type\"\n        + \", eventTypeOrdinal=\" + eventTypeOrdinal);\n  }\n  HistoryEventType eventType = HistoryEventType.values()[eventTypeOrdinal];\n  HistoryEvent event;\n  switch (eventType) {\n    case AM_LAUNCHED:\n      event = new AMLaunchedEvent();\n      break;\n    case AM_STARTED:\n      event = new AMStartedEvent();\n      break;\n    case DAG_SUBMITTED:\n      event = new DAGSubmittedEvent();\n      break;\n    case DAG_INITIALIZED:\n      event = new DAGInitializedEvent();\n      break;\n    case DAG_STARTED:\n      event = new DAGStartedEvent();\n      break;\n    case DAG_COMMIT_STARTED:\n      event = new DAGCommitStartedEvent();\n      break;\n    case DAG_FINISHED:\n      event = new DAGFinishedEvent();\n      break;\n    case DAG_KILL_REQUEST:\n      event = new DAGKillRequestEvent();\n      break;\n    case CONTAINER_LAUNCHED:\n      event = new ContainerLaunchedEvent();\n      break;\n    case CONTAINER_STOPPED:\n      event = new ContainerStoppedEvent();\n      break;\n    case VERTEX_INITIALIZED:\n      event = new VertexInitializedEvent();\n      break;\n    case VERTEX_CONFIGURE_DONE:\n      event = new VertexConfigurationDoneEvent();\n      break;\n    case VERTEX_STARTED:\n      event = new VertexStartedEvent();\n      break;\n    case VERTEX_COMMIT_STARTED:\n      event = new VertexCommitStartedEvent();\n      break;\n    case VERTEX_GROUP_COMMIT_STARTED:\n      event = new VertexGroupCommitStartedEvent();\n      break;\n    case VERTEX_GROUP_COMMIT_FINISHED:\n      event = new VertexGroupCommitFinishedEvent();\n      break;\n    case VERTEX_FINISHED:\n      event = new VertexFinishedEvent();\n      break;\n    case TASK_STARTED:\n      event = new TaskStartedEvent();\n      break;\n    case TASK_FINISHED:\n      event = new TaskFinishedEvent();\n      break;\n    case TASK_ATTEMPT_STARTED:\n      event = new TaskAttemptStartedEvent();\n      break;\n    case TASK_ATTEMPT_FINISHED:\n      event = new TaskAttemptFinishedEvent();\n      break;\n    default:\n      throw new IOException(\"Invalid data found, unknown event type \"\n          + eventType);\n\n  }\n  try {\n    event.fromProtoStream(inputStream);\n  } catch (EOFException eof) {\n    return null;\n  }\n  return event;\n}",
        "accept_response": "private static HistoryEvent getNextEvent(CodedInputStream inputStream)\n    throws IOException {\n  boolean isAtEnd = inputStream.isAtEnd();\n  if (isAtEnd) {\n    return null;\n  }\n  int eventTypeOrdinal = -1;\n  try {\n    eventTypeOrdinal = inputStream.readFixed32();\n  } catch (EOFException eof) {\n    return null;\n  }\n  if (eventTypeOrdinal < 0 || eventTypeOrdinal >=\n      HistoryEventType.values().length) {\n    // Corrupt data\n    // reached end\n    throw new IOException(\"Corrupt data found when trying to read next event type\"\n        + \", eventTypeOrdinal=\" + eventTypeOrdinal);\n  }\n  HistoryEventType eventType = HistoryEventType.values()[eventTypeOrdinal];\n  HistoryEvent event;\n  switch (eventType) {\n    case AM_LAUNCHED:\n      event = new AMLaunchedEvent();\n      break;\n    case AM_STARTED:\n      event = new AMStartedEvent();\n      break;\n    case DAG_SUBMITTED:\n      event = new DAGSubmittedEvent();\n      break;\n    case DAG_INITIALIZED:\n      event = new DAGInitializedEvent();\n      break;\n    case DAG_STARTED:\n      event = new DAGStartedEvent();\n      break;\n    case DAG_COMMIT_STARTED:\n      event = new DAGCommitStartedEvent();\n      break;\n    case DAG_FINISHED:\n      event = new DAGFinishedEvent();\n      break;\n    case DAG_KILL_REQUEST:\n      event = new DAGKillRequestEvent();\n      break;\n    case CONTAINER_LAUNCHED:\n      event = new ContainerLaunchedEvent();\n      break;\n    case CONTAINER_STOPPED:\n      event = new ContainerStoppedEvent();\n      break;\n    case VERTEX_INITIALIZED:\n      event = new VertexInitializedEvent();\n      break;\n    case VERTEX_CONFIGURE_DONE:\n      event = new VertexConfigurationDoneEvent();\n      break;\n    case VERTEX_STARTED:\n      event = new VertexStartedEvent();\n      break;\n    case VERTEX_COMMIT_STARTED:\n      event = new VertexCommitStartedEvent();\n      break;\n    case VERTEX_GROUP_COMMIT_STARTED:\n      event = new VertexGroupCommitStartedEvent();\n      break;\n    case VERTEX_GROUP_COMMIT_FINISHED:\n      event = new VertexGroupCommitFinishedEvent();\n      break;\n    case VERTEX_FINISHED:\n      event = new VertexFinishedEvent();\n      break;\n    case TASK_STARTED:\n      event = new TaskStartedEvent();\n      break;\n    case TASK_FINISHED:\n      event = new TaskFinishedEvent();\n      break;\n    case TASK_ATTEMPT_STARTED:\n      event = new TaskAttemptStartedEvent();\n      break;\n    case TASK_ATTEMPT_FINISHED:\n      event = new TaskAttemptFinishedEvent();\n      break;\n    default:\n      throw new IOException(\"Invalid data found, unknown event type \"\n          + eventType);\n\n  }\n  try {\n    event.fromProtoStream(inputStream);\n  } catch (EOFException eof) {\n    return null;\n  }\n    LOG.debug(\"Parsed event from input stream, eventType={}, event={}\",\n      eventType, event);\n  return event;\n}",
        "reject_response": "private static HistoryEvent getNextEvent(CodedInputStream inputStream)\n    throws IOException {\n  boolean isAtEnd = inputStream.isAtEnd();\n  if (isAtEnd) {\n    return null;\n  }\n  int eventTypeOrdinal = -1;\n  try {\n    eventTypeOrdinal = inputStream.readFixed32();\n  } catch (EOFException eof) {\n    return null;\n  }\n  if (eventTypeOrdinal < 0 || eventTypeOrdinal >=\n      HistoryEventType.values().length) {\n    // Corrupt data\n    // reached end\n    throw new IOException(\"Corrupt data found when trying to read next event type\"\n        + \", eventTypeOrdinal=\" + eventTypeOrdinal);\n  }\n  HistoryEventType eventType = HistoryEventType.values()[eventTypeOrdinal];\n  HistoryEvent event;\n  switch (eventType) {\n    case AM_LAUNCHED:\n      event = new AMLaunchedEvent();\n      break;\n    case AM_STARTED:\n      event = new AMStartedEvent();\n      break;\n    case DAG_SUBMITTED:\n      event = new DAGSubmittedEvent();\n      break;\n    case DAG_INITIALIZED:\n      event = new DAGInitializedEvent();\n      break;\n    case DAG_STARTED:\n      event = new DAGStartedEvent();\n      break;\n    case DAG_COMMIT_STARTED:\n      event = new DAGCommitStartedEvent();\n      break;\n    case DAG_FINISHED:\n      event = new DAGFinishedEvent();\n      break;\n    case DAG_KILL_REQUEST:\n      event = new DAGKillRequestEvent();\n      break;\n    case CONTAINER_LAUNCHED:\n      event = new ContainerLaunchedEvent();\n      break;\n    case CONTAINER_STOPPED:\n      event = new ContainerStoppedEvent();\n      break;\n    case VERTEX_INITIALIZED:\n      event = new VertexInitializedEvent();\n      break;\n    case VERTEX_CONFIGURE_DONE:\n      event = new VertexConfigurationDoneEvent();\n      break;\n    case VERTEX_STARTED:\n      event = new VertexStartedEvent();\n      break;\n    case VERTEX_COMMIT_STARTED:\n      event = new VertexCommitStartedEvent();\n      break;\n    case VERTEX_GROUP_COMMIT_STARTED:\n      event = new VertexGroupCommitStartedEvent();\n      break;\n    case VERTEX_GROUP_COMMIT_FINISHED:\n      event = new VertexGroupCommitFinishedEvent();\n      break;\n    case VERTEX_FINISHED:\n      event = new VertexFinishedEvent();\n      break;\n    case TASK_STARTED:\n      event = new TaskStartedEvent();\n      break;\n    case TASK_FINISHED:\n      event = new TaskFinishedEvent();\n      break;\n    case TASK_ATTEMPT_STARTED:\n      event = new TaskAttemptStartedEvent();\n      break;\n    case TASK_ATTEMPT_FINISHED:\n      event = new TaskAttemptFinishedEvent();\n      break;\n    default:\n      throw new IOException(\"Invalid data found, unknown event type \"\n          + eventType);\n\n  }\n  try {\n    event.fromProtoStream(inputStream);\n  } catch (EOFException eof) {\n    return null;\n  }\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"Parsed event from input stream\"\n        + \", eventType=\" + eventType\n        + \", event=\" + event.toString());\n  }\n  return event;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2566,
        "instruction": "private void handleFinancialActivityAccountDataIntegrityIssues(final JsonCommand command, final Throwable realCause, final Exception dve) {\n    if (realCause.getMessage().contains(\"financial_activity_type\")) {\n        final Integer financialActivityId = command\n                .integerValueSansLocaleOfParameterNamed(FinancialActivityAccountsJsonInputParams.FINANCIAL_ACTIVITY_ID.getValue());\n        throw new DuplicateFinancialActivityAccountFoundException(financialActivityId);\n    }\n\n    throw new PlatformDataIntegrityException(\"error.msg.glAccount.unknown.data.integrity.issue\",\n            \"Unknown data integrity issue with resource GL Account: \" + realCause.getMessage());\n}",
        "accept_response": "private void handleFinancialActivityAccountDataIntegrityIssues(final JsonCommand command, final Throwable realCause, final Exception dve) {\n    if (realCause.getMessage().contains(\"financial_activity_type\")) {\n        final Integer financialActivityId = command\n                .integerValueSansLocaleOfParameterNamed(FinancialActivityAccountsJsonInputParams.FINANCIAL_ACTIVITY_ID.getValue());\n        throw new DuplicateFinancialActivityAccountFoundException(financialActivityId);\n    }\n\n    logger.error(\"Error occured.\", dve);\n    throw new PlatformDataIntegrityException(\"error.msg.glAccount.unknown.data.integrity.issue\",\n            \"Unknown data integrity issue with resource GL Account: \" + realCause.getMessage());\n}",
        "reject_response": "private void handleFinancialActivityAccountDataIntegrityIssues(final JsonCommand command, final Throwable realCause, final Exception dve) {\n    if (realCause.getMessage().contains(\"financial_activity_type\")) {\n        final Integer financialActivityId = command\n                .integerValueSansLocaleOfParameterNamed(FinancialActivityAccountsJsonInputParams.FINANCIAL_ACTIVITY_ID.getValue());\n        throw new DuplicateFinancialActivityAccountFoundException(financialActivityId);\n    }\n\n    logger.error(dve.getMessage(), dve);\n    throw new PlatformDataIntegrityException(\"error.msg.glAccount.unknown.data.integrity.issue\",\n            \"Unknown data integrity issue with resource GL Account: \" + realCause.getMessage());\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3032,
        "instruction": "private void sendQueuedEvent(FullMessage<PackageMessage> fMessage) {\n    long offset = fMessage.getInfo().getOffset();\n    PackageMessage message = fMessage.getMessage();\n    final Event queuedEvent = DistributionEvent.eventPackageQueued(message, message.getPubAgentName());\n    eventAdmin.postEvent(queuedEvent);\n}",
        "accept_response": "private void sendQueuedEvent(FullMessage<PackageMessage> fMessage) {\n    long offset = fMessage.getInfo().getOffset();\n    LOG.debug(\"Queueing message package-id={}, offset={}\", fMessage.getMessage().getPkgId(), offset);\n    PackageMessage message = fMessage.getMessage();\n    final Event queuedEvent = DistributionEvent.eventPackageQueued(message, message.getPubAgentName());\n    eventAdmin.postEvent(queuedEvent);\n}",
        "reject_response": "private void sendQueuedEvent(FullMessage<PackageMessage> fMessage) {\n    long offset = fMessage.getInfo().getOffset();\n    LOG.info(\"Queueing message package-id={}, offset={}\", fMessage.getMessage().getPkgId(), offset);\n    PackageMessage message = fMessage.getMessage();\n    final Event queuedEvent = DistributionEvent.eventPackageQueued(message, message.getPubAgentName());\n    eventAdmin.postEvent(queuedEvent);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2584,
        "instruction": "private String getAddSql(final List<ResultsetColumnHeaderData> columnHeaders, final String datatable, final String fkName,\n        final Long appTableId, final Map<String, String> queryParams) {\n\n    final Map<String, String> affectedColumns = getAffectedColumns(columnHeaders, queryParams, fkName);\n\n    String pValueWrite = \"\";\n    String addSql = \"\";\n    final String singleQuote = \"'\";\n\n    String insertColumns = \"\";\n    String selectColumns = \"\";\n    String columnName = \"\";\n    String pValue = null;\n    for (final ResultsetColumnHeaderData pColumnHeader : columnHeaders) {\n        final String key = pColumnHeader.getColumnName();\n        if (affectedColumns.containsKey(key)) {\n            pValue = affectedColumns.get(key);\n            if (StringUtils.isEmpty(pValue)) {\n                pValueWrite = \"null\";\n            } else {\n                if (\"bit\".equalsIgnoreCase(pColumnHeader.getColumnType())) {\n                    pValueWrite = BooleanUtils.toString(BooleanUtils.toBooleanObject(pValue), \"1\", \"0\", \"null\");\n                } else {\n                    pValueWrite = singleQuote + this.genericDataService.replace(pValue, singleQuote, singleQuote + singleQuote)\n                            + singleQuote;\n                }\n\n            }\n            columnName = \"`\" + key + \"`\";\n            insertColumns += \", \" + columnName;\n            selectColumns += \",\" + pValueWrite + \" as \" + columnName;\n        }\n    }\n\n    addSql = \"insert into `\" + datatable + \"` (`\" + fkName + \"` \" + insertColumns + \")\" + \" select \" + appTableId + \" as id\"\n            + selectColumns;\n\n\n    return addSql;\n}",
        "accept_response": "private String getAddSql(final List<ResultsetColumnHeaderData> columnHeaders, final String datatable, final String fkName,\n        final Long appTableId, final Map<String, String> queryParams) {\n\n    final Map<String, String> affectedColumns = getAffectedColumns(columnHeaders, queryParams, fkName);\n\n    String pValueWrite = \"\";\n    String addSql = \"\";\n    final String singleQuote = \"'\";\n\n    String insertColumns = \"\";\n    String selectColumns = \"\";\n    String columnName = \"\";\n    String pValue = null;\n    for (final ResultsetColumnHeaderData pColumnHeader : columnHeaders) {\n        final String key = pColumnHeader.getColumnName();\n        if (affectedColumns.containsKey(key)) {\n            pValue = affectedColumns.get(key);\n            if (StringUtils.isEmpty(pValue)) {\n                pValueWrite = \"null\";\n            } else {\n                if (\"bit\".equalsIgnoreCase(pColumnHeader.getColumnType())) {\n                    pValueWrite = BooleanUtils.toString(BooleanUtils.toBooleanObject(pValue), \"1\", \"0\", \"null\");\n                } else {\n                    pValueWrite = singleQuote + this.genericDataService.replace(pValue, singleQuote, singleQuote + singleQuote)\n                            + singleQuote;\n                }\n\n            }\n            columnName = \"`\" + key + \"`\";\n            insertColumns += \", \" + columnName;\n            selectColumns += \",\" + pValueWrite + \" as \" + columnName;\n        }\n    }\n\n    addSql = \"insert into `\" + datatable + \"` (`\" + fkName + \"` \" + insertColumns + \")\" + \" select \" + appTableId + \" as id\"\n            + selectColumns;\n\n    logger.info(\"{}\", addSql);\n\n    return addSql;\n}",
        "reject_response": "private String getAddSql(final List<ResultsetColumnHeaderData> columnHeaders, final String datatable, final String fkName,\n        final Long appTableId, final Map<String, String> queryParams) {\n\n    final Map<String, String> affectedColumns = getAffectedColumns(columnHeaders, queryParams, fkName);\n\n    String pValueWrite = \"\";\n    String addSql = \"\";\n    final String singleQuote = \"'\";\n\n    String insertColumns = \"\";\n    String selectColumns = \"\";\n    String columnName = \"\";\n    String pValue = null;\n    for (final ResultsetColumnHeaderData pColumnHeader : columnHeaders) {\n        final String key = pColumnHeader.getColumnName();\n        if (affectedColumns.containsKey(key)) {\n            pValue = affectedColumns.get(key);\n            if (StringUtils.isEmpty(pValue)) {\n                pValueWrite = \"null\";\n            } else {\n                if (\"bit\".equalsIgnoreCase(pColumnHeader.getColumnType())) {\n                    pValueWrite = BooleanUtils.toString(BooleanUtils.toBooleanObject(pValue), \"1\", \"0\", \"null\");\n                } else {\n                    pValueWrite = singleQuote + this.genericDataService.replace(pValue, singleQuote, singleQuote + singleQuote)\n                            + singleQuote;\n                }\n\n            }\n            columnName = \"`\" + key + \"`\";\n            insertColumns += \", \" + columnName;\n            selectColumns += \",\" + pValueWrite + \" as \" + columnName;\n        }\n    }\n\n    addSql = \"insert into `\" + datatable + \"` (`\" + fkName + \"` \" + insertColumns + \")\" + \" select \" + appTableId + \" as id\"\n            + selectColumns;\n\n    logger.info(addSql);\n\n    return addSql;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2965,
        "instruction": "@Override\npublic int run(String[] args) throws IOException {\n  if (args.length < 1) {\n    System.err.println(\"Usage: DeduplicationJob <crawldb> [-group <none|host|domain>] [-compareOrder <score>,<fetchTime>,<httpsOverHttp>,<urlLength>]\");\n    return 1;\n  }\n\n  String group = \"none\";\n  Path crawlDb = new Path(args[0]);\n  String compareOrder = \"score,fetchTime,urlLength\";\n\n  for (int i = 1; i < args.length; i++) {\n    if (args[i].equals(\"-group\"))\n      group = args[++i];\n    if (args[i].equals(\"-compareOrder\")) {\n      compareOrder = args[++i];\n\n      if (compareOrder.indexOf(\"score\") == -1 ||\n          compareOrder.indexOf(\"fetchTime\") == -1 ||\n          compareOrder.indexOf(\"urlLength\") == -1) {\n        System.err.println(\"DeduplicationJob: compareOrder must contain score, fetchTime and urlLength.\");\n        return 1;\n      }\n    }\n  }\n\n  SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");\n  long start = System.currentTimeMillis();\n  LOG.info(\"DeduplicationJob: starting at \" + sdf.format(start));\n\n  Path tempDir = new Path(crawlDb, \"dedup-temp-\"\n      + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));\n\n  Job job = NutchJob.getInstance(getConf());\n  Configuration conf = job.getConfiguration();\n  job.setJobName(\"Deduplication on \" + crawlDb);\n  conf.set(DEDUPLICATION_GROUP_MODE, group);\n  conf.set(DEDUPLICATION_COMPARE_ORDER, compareOrder);\n  job.setJarByClass(DeduplicationJob.class);\n\n  FileInputFormat.addInputPath(job, new Path(crawlDb, CrawlDb.CURRENT_NAME));\n  job.setInputFormatClass(SequenceFileInputFormat.class);\n\n  FileOutputFormat.setOutputPath(job, tempDir);\n  job.setOutputFormatClass(SequenceFileOutputFormat.class);\n\n  job.setMapOutputKeyClass(BytesWritable.class);\n  job.setMapOutputValueClass(CrawlDatum.class);\n\n  job.setOutputKeyClass(Text.class);\n  job.setOutputValueClass(CrawlDatum.class);\n\n  job.setMapperClass(DBFilter.class);\n  job.setReducerClass(DedupReducer.class);\n\n  FileSystem fs = tempDir.getFileSystem(getConf());\n  try {\n    boolean success = job.waitForCompletion(true);\n    if (!success) {\n      String message = \"Crawl job did not succeed, job status:\"\n          + job.getStatus().getState() + \", reason: \"\n          + job.getStatus().getFailureInfo();\n      LOG.error(message);\n      fs.delete(tempDir, true);\n      throw new RuntimeException(message);\n    }\n    CounterGroup g = job.getCounters().getGroup(\"DeduplicationJobStatus\");\n    if (g != null) {\n      Counter counter = g.findCounter(\"Documents marked as duplicate\");\n      long dups = counter.getValue();\n      LOG.info(\"Deduplication: \" + (int) dups\n          + \" documents marked as duplicates\");\n    }\n  } catch (IOException | InterruptedException | ClassNotFoundException e) {\n    LOG.error(\"DeduplicationJob: \" + StringUtils.stringifyException(e));\n    fs.delete(tempDir, true);\n    return -1;\n  }\n\n  // merge with existing crawl db\n\n  Job mergeJob = CrawlDb.createJob(getConf(), crawlDb);\n  FileInputFormat.addInputPath(mergeJob, tempDir);\n  mergeJob.setReducerClass(StatusUpdateReducer.class);\n  mergeJob.setJarByClass(DeduplicationJob.class);\n\n  fs = crawlDb.getFileSystem(getConf());\n  Path outPath = FileOutputFormat.getOutputPath(job);\n  Path lock = CrawlDb.lock(getConf(), crawlDb, false);\n  try {\n    boolean success = mergeJob.waitForCompletion(true);\n    if (!success) {\n      String message = \"Crawl job did not succeed, job status:\"\n          + mergeJob.getStatus().getState() + \", reason: \"\n          + mergeJob.getStatus().getFailureInfo();\n      LOG.error(message);\n      fs.delete(tempDir, true);\n      NutchJob.cleanupAfterFailure(outPath, lock, fs);\n      throw new RuntimeException(message);\n    }\n  } catch (IOException | InterruptedException | ClassNotFoundException e) {\n    LOG.error(\"DeduplicationMergeJob: \" + StringUtils.stringifyException(e));\n    fs.delete(tempDir, true);\n    NutchJob.cleanupAfterFailure(outPath, lock, fs);\n    return -1;\n  }\n\n  CrawlDb.install(mergeJob, crawlDb);\n\n  // clean up\n  fs.delete(tempDir, true);\n\n  long end = System.currentTimeMillis();\n  LOG.info(\"Deduplication finished at \" + sdf.format(end) + \", elapsed: \"\n      + TimingUtil.elapsedTime(start, end));\n\n  return 0;\n}",
        "accept_response": "@Override\npublic int run(String[] args) throws IOException {\n  if (args.length < 1) {\n    System.err.println(\"Usage: DeduplicationJob <crawldb> [-group <none|host|domain>] [-compareOrder <score>,<fetchTime>,<httpsOverHttp>,<urlLength>]\");\n    return 1;\n  }\n\n  String group = \"none\";\n  Path crawlDb = new Path(args[0]);\n  String compareOrder = \"score,fetchTime,urlLength\";\n\n  for (int i = 1; i < args.length; i++) {\n    if (args[i].equals(\"-group\"))\n      group = args[++i];\n    if (args[i].equals(\"-compareOrder\")) {\n      compareOrder = args[++i];\n\n      if (compareOrder.indexOf(\"score\") == -1 ||\n          compareOrder.indexOf(\"fetchTime\") == -1 ||\n          compareOrder.indexOf(\"urlLength\") == -1) {\n        System.err.println(\"DeduplicationJob: compareOrder must contain score, fetchTime and urlLength.\");\n        return 1;\n      }\n    }\n  }\n\n  SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");\n  long start = System.currentTimeMillis();\n  LOG.info(\"DeduplicationJob: starting at \" + sdf.format(start));\n\n  Path tempDir = new Path(crawlDb, \"dedup-temp-\"\n      + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));\n\n  Job job = NutchJob.getInstance(getConf());\n  Configuration conf = job.getConfiguration();\n  job.setJobName(\"Deduplication on \" + crawlDb);\n  conf.set(DEDUPLICATION_GROUP_MODE, group);\n  conf.set(DEDUPLICATION_COMPARE_ORDER, compareOrder);\n  job.setJarByClass(DeduplicationJob.class);\n\n  FileInputFormat.addInputPath(job, new Path(crawlDb, CrawlDb.CURRENT_NAME));\n  job.setInputFormatClass(SequenceFileInputFormat.class);\n\n  FileOutputFormat.setOutputPath(job, tempDir);\n  job.setOutputFormatClass(SequenceFileOutputFormat.class);\n\n  job.setMapOutputKeyClass(BytesWritable.class);\n  job.setMapOutputValueClass(CrawlDatum.class);\n\n  job.setOutputKeyClass(Text.class);\n  job.setOutputValueClass(CrawlDatum.class);\n\n  job.setMapperClass(DBFilter.class);\n  job.setReducerClass(DedupReducer.class);\n\n  FileSystem fs = tempDir.getFileSystem(getConf());\n  try {\n    boolean success = job.waitForCompletion(true);\n    if (!success) {\n      String message = \"Crawl job did not succeed, job status:\"\n          + job.getStatus().getState() + \", reason: \"\n          + job.getStatus().getFailureInfo();\n      LOG.error(message);\n      fs.delete(tempDir, true);\n      throw new RuntimeException(message);\n    }\n    CounterGroup g = job.getCounters().getGroup(\"DeduplicationJobStatus\");\n    if (g != null) {\n      Counter counter = g.findCounter(\"Documents marked as duplicate\");\n      long dups = counter.getValue();\n      LOG.info(\"Deduplication: \" + (int) dups\n          + \" documents marked as duplicates\");\n    }\n  } catch (IOException | InterruptedException | ClassNotFoundException e) {\n    LOG.error(\"DeduplicationJob: \" + StringUtils.stringifyException(e));\n    fs.delete(tempDir, true);\n    return -1;\n  }\n\n  // merge with existing crawl db\n  LOG.info(\"Deduplication: Updating status of duplicate urls into crawl db.\");\n\n  Job mergeJob = CrawlDb.createJob(getConf(), crawlDb);\n  FileInputFormat.addInputPath(mergeJob, tempDir);\n  mergeJob.setReducerClass(StatusUpdateReducer.class);\n  mergeJob.setJarByClass(DeduplicationJob.class);\n\n  fs = crawlDb.getFileSystem(getConf());\n  Path outPath = FileOutputFormat.getOutputPath(job);\n  Path lock = CrawlDb.lock(getConf(), crawlDb, false);\n  try {\n    boolean success = mergeJob.waitForCompletion(true);\n    if (!success) {\n      String message = \"Crawl job did not succeed, job status:\"\n          + mergeJob.getStatus().getState() + \", reason: \"\n          + mergeJob.getStatus().getFailureInfo();\n      LOG.error(message);\n      fs.delete(tempDir, true);\n      NutchJob.cleanupAfterFailure(outPath, lock, fs);\n      throw new RuntimeException(message);\n    }\n  } catch (IOException | InterruptedException | ClassNotFoundException e) {\n    LOG.error(\"DeduplicationMergeJob: \" + StringUtils.stringifyException(e));\n    fs.delete(tempDir, true);\n    NutchJob.cleanupAfterFailure(outPath, lock, fs);\n    return -1;\n  }\n\n  CrawlDb.install(mergeJob, crawlDb);\n\n  // clean up\n  fs.delete(tempDir, true);\n\n  long end = System.currentTimeMillis();\n  LOG.info(\"Deduplication finished at \" + sdf.format(end) + \", elapsed: \"\n      + TimingUtil.elapsedTime(start, end));\n\n  return 0;\n}",
        "reject_response": "@Override\npublic int run(String[] args) throws IOException {\n  if (args.length < 1) {\n    System.err.println(\"Usage: DeduplicationJob <crawldb> [-group <none|host|domain>] [-compareOrder <score>,<fetchTime>,<httpsOverHttp>,<urlLength>]\");\n    return 1;\n  }\n\n  String group = \"none\";\n  Path crawlDb = new Path(args[0]);\n  String compareOrder = \"score,fetchTime,urlLength\";\n\n  for (int i = 1; i < args.length; i++) {\n    if (args[i].equals(\"-group\"))\n      group = args[++i];\n    if (args[i].equals(\"-compareOrder\")) {\n      compareOrder = args[++i];\n\n      if (compareOrder.indexOf(\"score\") == -1 ||\n          compareOrder.indexOf(\"fetchTime\") == -1 ||\n          compareOrder.indexOf(\"urlLength\") == -1) {\n        System.err.println(\"DeduplicationJob: compareOrder must contain score, fetchTime and urlLength.\");\n        return 1;\n      }\n    }\n  }\n\n  SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");\n  long start = System.currentTimeMillis();\n  LOG.info(\"DeduplicationJob: starting at \" + sdf.format(start));\n\n  Path tempDir = new Path(crawlDb, \"dedup-temp-\"\n      + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));\n\n  Job job = NutchJob.getInstance(getConf());\n  Configuration conf = job.getConfiguration();\n  job.setJobName(\"Deduplication on \" + crawlDb);\n  conf.set(DEDUPLICATION_GROUP_MODE, group);\n  conf.set(DEDUPLICATION_COMPARE_ORDER, compareOrder);\n  job.setJarByClass(DeduplicationJob.class);\n\n  FileInputFormat.addInputPath(job, new Path(crawlDb, CrawlDb.CURRENT_NAME));\n  job.setInputFormatClass(SequenceFileInputFormat.class);\n\n  FileOutputFormat.setOutputPath(job, tempDir);\n  job.setOutputFormatClass(SequenceFileOutputFormat.class);\n\n  job.setMapOutputKeyClass(BytesWritable.class);\n  job.setMapOutputValueClass(CrawlDatum.class);\n\n  job.setOutputKeyClass(Text.class);\n  job.setOutputValueClass(CrawlDatum.class);\n\n  job.setMapperClass(DBFilter.class);\n  job.setReducerClass(DedupReducer.class);\n\n  FileSystem fs = tempDir.getFileSystem(getConf());\n  try {\n    boolean success = job.waitForCompletion(true);\n    if (!success) {\n      String message = \"Crawl job did not succeed, job status:\"\n          + job.getStatus().getState() + \", reason: \"\n          + job.getStatus().getFailureInfo();\n      LOG.error(message);\n      fs.delete(tempDir, true);\n      throw new RuntimeException(message);\n    }\n    CounterGroup g = job.getCounters().getGroup(\"DeduplicationJobStatus\");\n    if (g != null) {\n      Counter counter = g.findCounter(\"Documents marked as duplicate\");\n      long dups = counter.getValue();\n      LOG.info(\"Deduplication: \" + (int) dups\n          + \" documents marked as duplicates\");\n    }\n  } catch (IOException | InterruptedException | ClassNotFoundException e) {\n    LOG.error(\"DeduplicationJob: \" + StringUtils.stringifyException(e));\n    fs.delete(tempDir, true);\n    return -1;\n  }\n\n  // merge with existing crawl db\n  if (LOG.isInfoEnabled()) {\n    LOG.info(\"Deduplication: Updating status of duplicate urls into crawl db.\");\n  }\n\n  Job mergeJob = CrawlDb.createJob(getConf(), crawlDb);\n  FileInputFormat.addInputPath(mergeJob, tempDir);\n  mergeJob.setReducerClass(StatusUpdateReducer.class);\n  mergeJob.setJarByClass(DeduplicationJob.class);\n\n  fs = crawlDb.getFileSystem(getConf());\n  Path outPath = FileOutputFormat.getOutputPath(job);\n  Path lock = CrawlDb.lock(getConf(), crawlDb, false);\n  try {\n    boolean success = mergeJob.waitForCompletion(true);\n    if (!success) {\n      String message = \"Crawl job did not succeed, job status:\"\n          + mergeJob.getStatus().getState() + \", reason: \"\n          + mergeJob.getStatus().getFailureInfo();\n      LOG.error(message);\n      fs.delete(tempDir, true);\n      NutchJob.cleanupAfterFailure(outPath, lock, fs);\n      throw new RuntimeException(message);\n    }\n  } catch (IOException | InterruptedException | ClassNotFoundException e) {\n    LOG.error(\"DeduplicationMergeJob: \" + StringUtils.stringifyException(e));\n    fs.delete(tempDir, true);\n    NutchJob.cleanupAfterFailure(outPath, lock, fs);\n    return -1;\n  }\n\n  CrawlDb.install(mergeJob, crawlDb);\n\n  // clean up\n  fs.delete(tempDir, true);\n\n  long end = System.currentTimeMillis();\n  LOG.info(\"Deduplication finished at \" + sdf.format(end) + \", elapsed: \"\n      + TimingUtil.elapsedTime(start, end));\n\n  return 0;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3011,
        "instruction": "void onStart() {\n\n  LOG.log(Level.FINEST, \"YARN registration: begin\");\n\n  this.resourceManager.init(this.yarnConf);\n  this.resourceManager.start();\n\n  this.nodeManager.init(this.yarnConf);\n  this.nodeManager.start();\n\n  LOG.log(Level.FINEST, \"YARN registration: registered with RM and NM\");\n\n  try {\n\n    this.registration.setRegistration(this.resourceManager.registerApplicationMaster(\n        AM_REGISTRATION_HOST, AM_REGISTRATION_PORT, this.trackingURLProvider.getTrackingUrl()));\n\n\n    final FileSystem fs = FileSystem.get(this.yarnConf);\n    final Path outputFileName = new Path(this.jobSubmissionDirectory, this.reefFileNames.getDriverHttpEndpoint());\n\n    try (final FSDataOutputStream out = fs.create(outputFileName)) {\n      out.writeBytes(this.trackingURLProvider.getTrackingUrl() + '\\n');\n    }\n  } catch (final YarnException | IOException e) {\n    LOG.log(Level.WARNING, \"Unable to register application master.\", e);\n    onRuntimeError(e);\n  }\n\n  LOG.log(Level.FINEST, \"YARN registration: done: {0}\", this.registration);\n}",
        "accept_response": "void onStart() {\n\n  LOG.log(Level.FINEST, \"YARN registration: begin\");\n\n  this.resourceManager.init(this.yarnConf);\n  this.resourceManager.start();\n\n  this.nodeManager.init(this.yarnConf);\n  this.nodeManager.start();\n\n  LOG.log(Level.FINEST, \"YARN registration: registered with RM and NM\");\n\n  try {\n\n    this.registration.setRegistration(this.resourceManager.registerApplicationMaster(\n        AM_REGISTRATION_HOST, AM_REGISTRATION_PORT, this.trackingURLProvider.getTrackingUrl()));\n\n    LOG.log(Level.FINE, \"YARN registration: AM registered: {0}\", this.registration);\n\n    final FileSystem fs = FileSystem.get(this.yarnConf);\n    final Path outputFileName = new Path(this.jobSubmissionDirectory, this.reefFileNames.getDriverHttpEndpoint());\n\n    try (final FSDataOutputStream out = fs.create(outputFileName)) {\n      out.writeBytes(this.trackingURLProvider.getTrackingUrl() + '\\n');\n    }\n  } catch (final YarnException | IOException e) {\n    LOG.log(Level.WARNING, \"Unable to register application master.\", e);\n    onRuntimeError(e);\n  }\n\n  LOG.log(Level.FINEST, \"YARN registration: done: {0}\", this.registration);\n}",
        "reject_response": "void onStart() {\n\n  LOG.log(Level.FINEST, \"YARN registration: begin\");\n\n  this.resourceManager.init(this.yarnConf);\n  this.resourceManager.start();\n\n  this.nodeManager.init(this.yarnConf);\n  this.nodeManager.start();\n\n  LOG.log(Level.FINEST, \"YARN registration: registered with RM and NM\");\n\n  try {\n\n    this.registration.setRegistration(this.resourceManager.registerApplicationMaster(\n        AM_REGISTRATION_HOST, AM_REGISTRATION_PORT, this.trackingURLProvider.getTrackingUrl()));\n\n    LOG.log(Level.FINE, \"YARN registration: {0}\", this.registration);\n\n    final FileSystem fs = FileSystem.get(this.yarnConf);\n    final Path outputFileName = new Path(this.jobSubmissionDirectory, this.reefFileNames.getDriverHttpEndpoint());\n\n    try (final FSDataOutputStream out = fs.create(outputFileName)) {\n      out.writeBytes(this.trackingURLProvider.getTrackingUrl() + '\\n');\n    }\n  } catch (final YarnException | IOException e) {\n    LOG.log(Level.WARNING, \"Unable to register application master.\", e);\n    onRuntimeError(e);\n  }\n\n  LOG.log(Level.FINEST, \"YARN registration: done: {0}\", this.registration);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3088,
        "instruction": "private Map emitNewPartitionBatch(TransactionAttempt attempt, TridentCollector collector, Partition partition, Map lastMeta) {\n    try {\n        return failFastEmitNewPartitionBatch(attempt, collector, partition, lastMeta);\n    } catch (FailedFetchException e) {\n        if (lastMeta == null) {\n            return null;\n        } else {\n            Map ret = new HashMap();\n            ret.put(\"offset\", lastMeta.get(\"nextOffset\"));\n            ret.put(\"nextOffset\", lastMeta.get(\"nextOffset\"));\n            ret.put(\"partition\", partition.partition);\n            ret.put(\"broker\", ImmutableMap.of(\"host\", partition.host.host, \"port\", partition.host.port));\n            ret.put(\"topic\", partition.topic);\n            ret.put(\"topology\", ImmutableMap.of(\"name\", _topologyName, \"id\", _topologyInstanceId));\n            return ret;\n        }\n    }\n}",
        "accept_response": "private Map emitNewPartitionBatch(TransactionAttempt attempt, TridentCollector collector, Partition partition, Map lastMeta) {\n    try {\n        return failFastEmitNewPartitionBatch(attempt, collector, partition, lastMeta);\n    } catch (FailedFetchException e) {\n        LOG.warn(\"Failed to fetch from partition \" + partition, e);\n        if (lastMeta == null) {\n            return null;\n        } else {\n            Map ret = new HashMap();\n            ret.put(\"offset\", lastMeta.get(\"nextOffset\"));\n            ret.put(\"nextOffset\", lastMeta.get(\"nextOffset\"));\n            ret.put(\"partition\", partition.partition);\n            ret.put(\"broker\", ImmutableMap.of(\"host\", partition.host.host, \"port\", partition.host.port));\n            ret.put(\"topic\", partition.topic);\n            ret.put(\"topology\", ImmutableMap.of(\"name\", _topologyName, \"id\", _topologyInstanceId));\n            return ret;\n        }\n    }\n}",
        "reject_response": "private Map emitNewPartitionBatch(TransactionAttempt attempt, TridentCollector collector, Partition partition, Map lastMeta) {\n    try {\n        return failFastEmitNewPartitionBatch(attempt, collector, partition, lastMeta);\n    } catch (FailedFetchException e) {\n        LOG.warn(\"Failed to fetch from partition \" + partition);\n        if (lastMeta == null) {\n            return null;\n        } else {\n            Map ret = new HashMap();\n            ret.put(\"offset\", lastMeta.get(\"nextOffset\"));\n            ret.put(\"nextOffset\", lastMeta.get(\"nextOffset\"));\n            ret.put(\"partition\", partition.partition);\n            ret.put(\"broker\", ImmutableMap.of(\"host\", partition.host.host, \"port\", partition.host.port));\n            ret.put(\"topic\", partition.topic);\n            ret.put(\"topology\", ImmutableMap.of(\"name\", _topologyName, \"id\", _topologyInstanceId));\n            return ret;\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3146,
        "instruction": "@Override\nprotected void serviceStop() throws Exception {\n  if (this.webApp != null) {\n    this.webApp.stop();\n  }\n  super.serviceStop();\n}",
        "accept_response": "@Override\nprotected void serviceStop() throws Exception {\n  if (this.webApp != null) {\n    LOG.debug(\"Stopping WebApp\");\n    this.webApp.stop();\n  }\n  super.serviceStop();\n}",
        "reject_response": "@Override\nprotected void serviceStop() throws Exception {\n  if (this.webApp != null) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Stopping WebApp\");\n    }\n    this.webApp.stop();\n  }\n  super.serviceStop();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3258,
        "instruction": "public void register(ZKMBeanInfo bean, ZKMBeanInfo parent) throws JMException {\n    assert bean != null;\n    String path = null;\n    if (parent != null) {\n        path = mapBean2Path.get(parent);\n        assert path != null;\n    }\n    path = makeFullPath(path, parent);\n    if (bean.isHidden()) {\n        return;\n    }\n    ObjectName oname = makeObjectName(path, bean);\n    try {\n        synchronized (LOCK) {\n            mBeanServer.registerMBean(bean, oname);\n            mapBean2Path.put(bean, path);\n        }\n    } catch (JMException e) {\n        throw e;\n    }\n}",
        "accept_response": "public void register(ZKMBeanInfo bean, ZKMBeanInfo parent) throws JMException {\n    assert bean != null;\n    String path = null;\n    if (parent != null) {\n        path = mapBean2Path.get(parent);\n        assert path != null;\n    }\n    path = makeFullPath(path, parent);\n    if (bean.isHidden()) {\n        return;\n    }\n    ObjectName oname = makeObjectName(path, bean);\n    try {\n        synchronized (LOCK) {\n            mBeanServer.registerMBean(bean, oname);\n            mapBean2Path.put(bean, path);\n        }\n    } catch (JMException e) {\n        LOG.warn(\"Failed to register MBean {}\", bean.getName());\n        throw e;\n    }\n}",
        "reject_response": "public void register(ZKMBeanInfo bean, ZKMBeanInfo parent) throws JMException {\n    assert bean != null;\n    String path = null;\n    if (parent != null) {\n        path = mapBean2Path.get(parent);\n        assert path != null;\n    }\n    path = makeFullPath(path, parent);\n    if (bean.isHidden()) {\n        return;\n    }\n    ObjectName oname = makeObjectName(path, bean);\n    try {\n        synchronized (LOCK) {\n            mBeanServer.registerMBean(bean, oname);\n            mapBean2Path.put(bean, path);\n        }\n    } catch (JMException e) {\n        LOG.warn(\"Failed to register MBean \" + bean.getName());\n        throw e;\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2401,
        "instruction": "private void configureViewLogging(ViewEntity viewDefinition,ClassLoader cl) {\n  URL resourceURL = cl.getResource(VIEW_LOG_FILE);\n  if( null != resourceURL ){\n    PropertyConfigurator.configure(resourceURL);\n  }\n}",
        "accept_response": "private void configureViewLogging(ViewEntity viewDefinition,ClassLoader cl) {\n  URL resourceURL = cl.getResource(VIEW_LOG_FILE);\n  if( null != resourceURL ){\n    LOG.info(\"setting up logging for view {} as per property file {}\",viewDefinition.getName(), resourceURL);\n    PropertyConfigurator.configure(resourceURL);\n  }\n}",
        "reject_response": "private void configureViewLogging(ViewEntity viewDefinition,ClassLoader cl) {\n  URL resourceURL = cl.getResource(VIEW_LOG_FILE);\n  if( null != resourceURL ){\n    LOG.debug(\"setting up logging for view {} as per property file {}\",viewDefinition.getName(), resourceURL);\n    PropertyConfigurator.configure(resourceURL);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2659,
        "instruction": "public void logout() {\n  Subject currentUser = getSubject();\n  if (currentUser == null) {\n    return;\n  }\n\n  try {\n    currentUser.logout();\n  } catch (ShiroException e) {\n    logger.info(e.getMessage(), e);\n    throw new GemFireSecurityException(e.getMessage(), e);\n  }\n  // clean out Shiro's thread local content\n  ThreadContext.remove();\n}",
        "accept_response": "public void logout() {\n  Subject currentUser = getSubject();\n  if (currentUser == null) {\n    return;\n  }\n\n  try {\n    logger.debug(\"Logging out \" + currentUser.getPrincipal());\n    currentUser.logout();\n  } catch (ShiroException e) {\n    logger.info(e.getMessage(), e);\n    throw new GemFireSecurityException(e.getMessage(), e);\n  }\n  // clean out Shiro's thread local content\n  ThreadContext.remove();\n}",
        "reject_response": "public void logout() {\n  Subject currentUser = getSubject();\n  if (currentUser == null) {\n    return;\n  }\n\n  try {\n    logger.info(\"Logging out \" + currentUser.getPrincipal());\n    currentUser.logout();\n  } catch (ShiroException e) {\n    logger.info(e.getMessage(), e);\n    throw new GemFireSecurityException(e.getMessage(), e);\n  }\n  // clean out Shiro's thread local content\n  ThreadContext.remove();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2372,
        "instruction": "private void transitionToFailed(long tid, Repo<T> op, Exception e) {\n  String tidStr = String.format(\"%016x\", tid);\n  log.warn(\"Failed to execute Repo, tid=\" + tidStr, e);\n  store.setProperty(tid, EXCEPTION_PROP, e);\n  store.setStatus(tid, TStatus.FAILED_IN_PROGRESS);\n}",
        "accept_response": "private void transitionToFailed(long tid, Repo<T> op, Exception e) {\n  String tidStr = String.format(\"%016x\", tid);\n  log.warn(\"Failed to execute Repo, tid=\" + tidStr, e);\n  store.setProperty(tid, EXCEPTION_PROP, e);\n  store.setStatus(tid, TStatus.FAILED_IN_PROGRESS);\n  log.info(\"Updated status for Repo with tid=\" + tidStr + \" to FAILED_IN_PROGRESS\");\n}",
        "reject_response": "private void transitionToFailed(long tid, Repo<T> op, Exception e) {\n  String tidStr = String.format(\"%016x\", tid);\n  log.warn(\"Failed to execute Repo, tid=\" + tidStr, e);\n  store.setProperty(tid, EXCEPTION_PROP, e);\n  store.setStatus(tid, TStatus.FAILED_IN_PROGRESS);\n  log.warn(\"Failed to execute Repo, tid=\" + String.format(\"%016x\", tid), e);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2694,
        "instruction": "Response readResponse(final Bridge bridge, ProtocolData protData,\n                      final boolean threadSafe) {\n    final int fragment = protData.getDataFragment();\n    final String dataDir = protData.getDataSource();\n\n    // Creating an internal streaming class\n    // which will iterate the records and put them on the\n    // output stream\n    final StreamingOutput streaming = new StreamingOutput() {\n        @Override\n        public void write(final OutputStream out) throws IOException,\n                WebApplicationException {\n            long recordCount = 0;\n\n            if (!threadSafe) {\n                lock(dataDir);\n            }\n            try {\n\n                if (!bridge.beginIteration()) {\n                    return;\n                }\n\n                Writable record;\n                DataOutputStream dos = new DataOutputStream(out);\n                while ((record = bridge.getNext()) != null) {\n                    record.write(dos);\n                    ++recordCount;\n                }\n                LOG.debug(\"Finished streaming fragment \" + fragment\n                        + \" of resource \" + dataDir + \", \" + recordCount\n                        + \" records.\");\n            } catch (ClientAbortException e) {\n                // Occurs whenever client (HAWQ) decides the end the\n                // connection\n                LOG.error(\"Remote connection closed by HAWQ\", e);\n            } catch (Exception e) {\n                LOG.error(\"Exception thrown when streaming\", e);\n                throw new IOException(e.getMessage());\n            } finally {\n                LOG.debug(\"Stopped streaming fragment \" + fragment\n                        + \" of resource \" + dataDir + \", \" + recordCount\n                        + \" records.\");\n                if (!threadSafe) {\n                    unlock(dataDir);\n                }\n            }\n        }\n    };\n\n    return Response.ok(streaming, MediaType.APPLICATION_OCTET_STREAM).build();\n}",
        "accept_response": "Response readResponse(final Bridge bridge, ProtocolData protData,\n                      final boolean threadSafe) {\n    final int fragment = protData.getDataFragment();\n    final String dataDir = protData.getDataSource();\n\n    // Creating an internal streaming class\n    // which will iterate the records and put them on the\n    // output stream\n    final StreamingOutput streaming = new StreamingOutput() {\n        @Override\n        public void write(final OutputStream out) throws IOException,\n                WebApplicationException {\n            long recordCount = 0;\n\n            if (!threadSafe) {\n                lock(dataDir);\n            }\n            try {\n\n                if (!bridge.beginIteration()) {\n                    return;\n                }\n\n                Writable record;\n                DataOutputStream dos = new DataOutputStream(out);\n                LOG.debug(\"Starting streaming fragment \" + fragment\n                        + \" of resource \" + dataDir);\n                while ((record = bridge.getNext()) != null) {\n                    record.write(dos);\n                    ++recordCount;\n                }\n                LOG.debug(\"Finished streaming fragment \" + fragment\n                        + \" of resource \" + dataDir + \", \" + recordCount\n                        + \" records.\");\n            } catch (ClientAbortException e) {\n                // Occurs whenever client (HAWQ) decides the end the\n                // connection\n                LOG.error(\"Remote connection closed by HAWQ\", e);\n            } catch (Exception e) {\n                LOG.error(\"Exception thrown when streaming\", e);\n                throw new IOException(e.getMessage());\n            } finally {\n                LOG.debug(\"Stopped streaming fragment \" + fragment\n                        + \" of resource \" + dataDir + \", \" + recordCount\n                        + \" records.\");\n                if (!threadSafe) {\n                    unlock(dataDir);\n                }\n            }\n        }\n    };\n\n    return Response.ok(streaming, MediaType.APPLICATION_OCTET_STREAM).build();\n}",
        "reject_response": "Response readResponse(final Bridge bridge, ProtocolData protData,\n                      final boolean threadSafe) {\n    final int fragment = protData.getDataFragment();\n    final String dataDir = protData.getDataSource();\n\n    // Creating an internal streaming class\n    // which will iterate the records and put them on the\n    // output stream\n    final StreamingOutput streaming = new StreamingOutput() {\n        @Override\n        public void write(final OutputStream out) throws IOException,\n                WebApplicationException {\n            long recordCount = 0;\n\n            if (!threadSafe) {\n                lock(dataDir);\n            }\n            try {\n\n                if (!bridge.beginIteration()) {\n                    return;\n                }\n\n                Writable record;\n                DataOutputStream dos = new DataOutputStream(out);\n                Log.debug(\"Starting streaming fragment \" + fragment\n                while ((record = bridge.getNext()) != null) {\n                    record.write(dos);\n                    ++recordCount;\n                }\n                LOG.debug(\"Finished streaming fragment \" + fragment\n                        + \" of resource \" + dataDir + \", \" + recordCount\n                        + \" records.\");\n            } catch (ClientAbortException e) {\n                // Occurs whenever client (HAWQ) decides the end the\n                // connection\n                LOG.error(\"Remote connection closed by HAWQ\", e);\n            } catch (Exception e) {\n                LOG.error(\"Exception thrown when streaming\", e);\n                throw new IOException(e.getMessage());\n            } finally {\n                LOG.debug(\"Stopped streaming fragment \" + fragment\n                        + \" of resource \" + dataDir + \", \" + recordCount\n                        + \" records.\");\n                if (!threadSafe) {\n                    unlock(dataDir);\n                }\n            }\n        }\n    };\n\n    return Response.ok(streaming, MediaType.APPLICATION_OCTET_STREAM).build();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2556,
        "instruction": "@Override\npublic void run() {\n    executor = new ScheduledThreadPoolExecutor(10);\n    List<Future> futures = new ArrayList<>();\n    try {\n        for (Entity entity : entityBacklogs.keySet()) {\n            futures.add(executor.submit(new BacklogCalcService(entity, entityBacklogs.get(entity))));\n        }\n        waitForFuturesToComplete(futures);\n    } finally {\n        executor.shutdown();\n    }\n}",
        "accept_response": "@Override\npublic void run() {\n    LOG.debug(\"Starting periodic check for backlog\");\n    executor = new ScheduledThreadPoolExecutor(10);\n    List<Future> futures = new ArrayList<>();\n    try {\n        for (Entity entity : entityBacklogs.keySet()) {\n            futures.add(executor.submit(new BacklogCalcService(entity, entityBacklogs.get(entity))));\n        }\n        waitForFuturesToComplete(futures);\n    } finally {\n        executor.shutdown();\n    }\n}",
        "reject_response": "@Override\npublic void run() {\n    LOG.debug(\"BacklogMetricEmitter running for entities\");\n    executor = new ScheduledThreadPoolExecutor(10);\n    List<Future> futures = new ArrayList<>();\n    try {\n        for (Entity entity : entityBacklogs.keySet()) {\n            futures.add(executor.submit(new BacklogCalcService(entity, entityBacklogs.get(entity))));\n        }\n        waitForFuturesToComplete(futures);\n    } finally {\n        executor.shutdown();\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2945,
        "instruction": "@VisibleForTesting\nvoid createSplits(Map<String, Set<OneBlockInfo>> nodeToBlocks,\n                   Map<OneBlockInfo, String[]> blockToNodes,\n                   Map<String, List<OneBlockInfo>> rackToBlocks,\n                   long totLength,\n                   long maxSize,\n                   long minSizeNode,\n                   long minSizeRack,\n                   List<InputSplit> splits\n                  ) {\n  ArrayList<OneBlockInfo> validBlocks = new ArrayList<OneBlockInfo>();\n  long curSplitSize = 0;\n\n  int totalNodes = nodeToBlocks.size();\n  long totalLength = totLength;\n\n  Multiset<String> splitsPerNode = HashMultiset.create();\n  Set<String> completedNodes = new HashSet<String>();\n\n  while(true) {\n    for (Iterator<Map.Entry<String, Set<OneBlockInfo>>> iter = nodeToBlocks\n        .entrySet().iterator(); iter.hasNext();) {\n      Map.Entry<String, Set<OneBlockInfo>> one = iter.next();\n\n      String node = one.getKey();\n\n      // Skip the node if it has previously been marked as completed.\n      if (completedNodes.contains(node)) {\n        continue;\n      }\n\n      Set<OneBlockInfo> blocksInCurrentNode = one.getValue();\n\n      // for each block, copy it into validBlocks. Delete it from\n      // blockToNodes so that the same block does not appear in\n      // two different splits.\n      Iterator<OneBlockInfo> oneBlockIter = blocksInCurrentNode.iterator();\n      while (oneBlockIter.hasNext()) {\n        OneBlockInfo oneblock = oneBlockIter.next();\n\n        // Remove all blocks which may already have been assigned to other\n        // splits.\n        if(!blockToNodes.containsKey(oneblock)) {\n          oneBlockIter.remove();\n          continue;\n        }\n\n        validBlocks.add(oneblock);\n        blockToNodes.remove(oneblock);\n        curSplitSize += oneblock.length;\n\n        // if the accumulated split size exceeds the maximum, then\n        // create this split.\n        if (maxSize != 0 && curSplitSize >= maxSize) {\n          // create an input split and add it to the splits array\n          addCreatedSplit(splits, Collections.singleton(node), validBlocks);\n          totalLength -= curSplitSize;\n          curSplitSize = 0;\n\n          splitsPerNode.add(node);\n\n          // Remove entries from blocksInNode so that we don't walk these\n          // again.\n          blocksInCurrentNode.removeAll(validBlocks);\n          validBlocks.clear();\n\n          // Done creating a single split for this node. Move on to the next\n          // node so that splits are distributed across nodes.\n          break;\n        }\n\n      }\n      if (validBlocks.size() != 0) {\n        // This implies that the last few blocks (or all in case maxSize=0)\n        // were not part of a split. The node is complete.\n\n        // if there were any blocks left over and their combined size is\n        // larger than minSplitNode, then combine them into one split.\n        // Otherwise add them back to the unprocessed pool. It is likely\n        // that they will be combined with other blocks from the\n        // same rack later on.\n        // This condition also kicks in when max split size is not set. All\n        // blocks on a node will be grouped together into a single split.\n        if (minSizeNode != 0 && curSplitSize >= minSizeNode\n            && splitsPerNode.count(node) == 0) {\n          // haven't created any split on this machine. so its ok to add a\n          // smaller one for parallelism. Otherwise group it in the rack for\n          // balanced size create an input split and add it to the splits\n          // array\n          addCreatedSplit(splits, Collections.singleton(node), validBlocks);\n          totalLength -= curSplitSize;\n          splitsPerNode.add(node);\n          // Remove entries from blocksInNode so that we don't walk this again.\n          blocksInCurrentNode.removeAll(validBlocks);\n          // The node is done. This was the last set of blocks for this node.\n        } else {\n          // Put the unplaced blocks back into the pool for later rack-allocation.\n          for (OneBlockInfo oneblock : validBlocks) {\n            blockToNodes.put(oneblock, oneblock.hosts);\n          }\n        }\n        validBlocks.clear();\n        curSplitSize = 0;\n        completedNodes.add(node);\n      } else { // No in-flight blocks.\n        if (blocksInCurrentNode.size() == 0) {\n          // Node is done. All blocks were fit into node-local splits.\n          completedNodes.add(node);\n        } // else Run through the node again.\n      }\n    }\n\n    // Check if node-local assignments are complete.\n    if (completedNodes.size() == totalNodes || totalLength == 0) {\n      // All nodes have been walked over and marked as completed or all blocks\n      // have been assigned. The rest should be handled via rackLock assignment.\n      break;\n    }\n  }\n\n  // if blocks in a rack are below the specified minimum size, then keep them\n  // in 'overflow'. After the processing of all racks is complete, these\n  // overflow blocks will be combined into splits.\n  ArrayList<OneBlockInfo> overflowBlocks = new ArrayList<OneBlockInfo>();\n  Set<String> racks = new HashSet<String>();\n\n  // Process all racks over and over again until there is no more work to do.\n  while (blockToNodes.size() > 0) {\n\n    // Create one split for this rack before moving over to the next rack.\n    // Come back to this rack after creating a single split for each of the\n    // remaining racks.\n    // Process one rack location at a time, Combine all possible blocks that\n    // reside on this rack as one split. (constrained by minimum and maximum\n    // split size).\n\n    // iterate over all racks\n    for (Iterator<Map.Entry<String, List<OneBlockInfo>>> iter =\n         rackToBlocks.entrySet().iterator(); iter.hasNext();) {\n\n      Map.Entry<String, List<OneBlockInfo>> one = iter.next();\n      racks.add(one.getKey());\n      List<OneBlockInfo> blocks = one.getValue();\n\n      // for each block, copy it into validBlocks. Delete it from\n      // blockToNodes so that the same block does not appear in\n      // two different splits.\n      boolean createdSplit = false;\n      for (OneBlockInfo oneblock : blocks) {\n        if (blockToNodes.containsKey(oneblock)) {\n          validBlocks.add(oneblock);\n          blockToNodes.remove(oneblock);\n          curSplitSize += oneblock.length;\n\n          // if the accumulated split size exceeds the maximum, then\n          // create this split.\n          if (maxSize != 0 && curSplitSize >= maxSize) {\n            // create an input split and add it to the splits array\n            addCreatedSplit(splits, getHosts(racks), validBlocks);\n            createdSplit = true;\n            break;\n          }\n        }\n      }\n\n      // if we created a split, then just go to the next rack\n      if (createdSplit) {\n        curSplitSize = 0;\n        validBlocks.clear();\n        racks.clear();\n        continue;\n      }\n\n      if (!validBlocks.isEmpty()) {\n        if (minSizeRack != 0 && curSplitSize >= minSizeRack) {\n          // if there is a minimum size specified, then create a single split\n          // otherwise, store these blocks into overflow data structure\n          addCreatedSplit(splits, getHosts(racks), validBlocks);\n        } else {\n          // There were a few blocks in this rack that\n      \t// remained to be processed. Keep them in 'overflow' block list.\n      \t// These will be combined later.\n          overflowBlocks.addAll(validBlocks);\n        }\n      }\n      curSplitSize = 0;\n      validBlocks.clear();\n      racks.clear();\n    }\n  }\n\n  assert blockToNodes.isEmpty();\n  assert curSplitSize == 0;\n  assert validBlocks.isEmpty();\n  assert racks.isEmpty();\n\n  // Process all overflow blocks\n  for (OneBlockInfo oneblock : overflowBlocks) {\n    validBlocks.add(oneblock);\n    curSplitSize += oneblock.length;\n\n    // This might cause an exiting rack location to be re-added,\n    // but it should be ok.\n    for (int i = 0; i < oneblock.racks.length; i++) {\n      racks.add(oneblock.racks[i]);\n    }\n\n    // if the accumulated split size exceeds the maximum, then\n    // create this split.\n    if (maxSize != 0 && curSplitSize >= maxSize) {\n      // create an input split and add it to the splits array\n      addCreatedSplit(splits, getHosts(racks), validBlocks);\n      curSplitSize = 0;\n      validBlocks.clear();\n      racks.clear();\n    }\n  }\n\n  // Process any remaining blocks, if any.\n  if (!validBlocks.isEmpty()) {\n    addCreatedSplit(splits, getHosts(racks), validBlocks);\n  }\n}",
        "accept_response": "@VisibleForTesting\nvoid createSplits(Map<String, Set<OneBlockInfo>> nodeToBlocks,\n                   Map<OneBlockInfo, String[]> blockToNodes,\n                   Map<String, List<OneBlockInfo>> rackToBlocks,\n                   long totLength,\n                   long maxSize,\n                   long minSizeNode,\n                   long minSizeRack,\n                   List<InputSplit> splits\n                  ) {\n  ArrayList<OneBlockInfo> validBlocks = new ArrayList<OneBlockInfo>();\n  long curSplitSize = 0;\n\n  int totalNodes = nodeToBlocks.size();\n  long totalLength = totLength;\n\n  Multiset<String> splitsPerNode = HashMultiset.create();\n  Set<String> completedNodes = new HashSet<String>();\n\n  while(true) {\n    for (Iterator<Map.Entry<String, Set<OneBlockInfo>>> iter = nodeToBlocks\n        .entrySet().iterator(); iter.hasNext();) {\n      Map.Entry<String, Set<OneBlockInfo>> one = iter.next();\n\n      String node = one.getKey();\n\n      // Skip the node if it has previously been marked as completed.\n      if (completedNodes.contains(node)) {\n        continue;\n      }\n\n      Set<OneBlockInfo> blocksInCurrentNode = one.getValue();\n\n      // for each block, copy it into validBlocks. Delete it from\n      // blockToNodes so that the same block does not appear in\n      // two different splits.\n      Iterator<OneBlockInfo> oneBlockIter = blocksInCurrentNode.iterator();\n      while (oneBlockIter.hasNext()) {\n        OneBlockInfo oneblock = oneBlockIter.next();\n\n        // Remove all blocks which may already have been assigned to other\n        // splits.\n        if(!blockToNodes.containsKey(oneblock)) {\n          oneBlockIter.remove();\n          continue;\n        }\n\n        validBlocks.add(oneblock);\n        blockToNodes.remove(oneblock);\n        curSplitSize += oneblock.length;\n\n        // if the accumulated split size exceeds the maximum, then\n        // create this split.\n        if (maxSize != 0 && curSplitSize >= maxSize) {\n          // create an input split and add it to the splits array\n          addCreatedSplit(splits, Collections.singleton(node), validBlocks);\n          totalLength -= curSplitSize;\n          curSplitSize = 0;\n\n          splitsPerNode.add(node);\n\n          // Remove entries from blocksInNode so that we don't walk these\n          // again.\n          blocksInCurrentNode.removeAll(validBlocks);\n          validBlocks.clear();\n\n          // Done creating a single split for this node. Move on to the next\n          // node so that splits are distributed across nodes.\n          break;\n        }\n\n      }\n      if (validBlocks.size() != 0) {\n        // This implies that the last few blocks (or all in case maxSize=0)\n        // were not part of a split. The node is complete.\n\n        // if there were any blocks left over and their combined size is\n        // larger than minSplitNode, then combine them into one split.\n        // Otherwise add them back to the unprocessed pool. It is likely\n        // that they will be combined with other blocks from the\n        // same rack later on.\n        // This condition also kicks in when max split size is not set. All\n        // blocks on a node will be grouped together into a single split.\n        if (minSizeNode != 0 && curSplitSize >= minSizeNode\n            && splitsPerNode.count(node) == 0) {\n          // haven't created any split on this machine. so its ok to add a\n          // smaller one for parallelism. Otherwise group it in the rack for\n          // balanced size create an input split and add it to the splits\n          // array\n          addCreatedSplit(splits, Collections.singleton(node), validBlocks);\n          totalLength -= curSplitSize;\n          splitsPerNode.add(node);\n          // Remove entries from blocksInNode so that we don't walk this again.\n          blocksInCurrentNode.removeAll(validBlocks);\n          // The node is done. This was the last set of blocks for this node.\n        } else {\n          // Put the unplaced blocks back into the pool for later rack-allocation.\n          for (OneBlockInfo oneblock : validBlocks) {\n            blockToNodes.put(oneblock, oneblock.hosts);\n          }\n        }\n        validBlocks.clear();\n        curSplitSize = 0;\n        completedNodes.add(node);\n      } else { // No in-flight blocks.\n        if (blocksInCurrentNode.size() == 0) {\n          // Node is done. All blocks were fit into node-local splits.\n          completedNodes.add(node);\n        } // else Run through the node again.\n      }\n    }\n\n    // Check if node-local assignments are complete.\n    if (completedNodes.size() == totalNodes || totalLength == 0) {\n      // All nodes have been walked over and marked as completed or all blocks\n      // have been assigned. The rest should be handled via rackLock assignment.\n      LOG.debug(\"Terminated node allocation with : CompletedNodes: {}, size left: {}\",\n          completedNodes.size(), totalLength);\n      break;\n    }\n  }\n\n  // if blocks in a rack are below the specified minimum size, then keep them\n  // in 'overflow'. After the processing of all racks is complete, these\n  // overflow blocks will be combined into splits.\n  ArrayList<OneBlockInfo> overflowBlocks = new ArrayList<OneBlockInfo>();\n  Set<String> racks = new HashSet<String>();\n\n  // Process all racks over and over again until there is no more work to do.\n  while (blockToNodes.size() > 0) {\n\n    // Create one split for this rack before moving over to the next rack.\n    // Come back to this rack after creating a single split for each of the\n    // remaining racks.\n    // Process one rack location at a time, Combine all possible blocks that\n    // reside on this rack as one split. (constrained by minimum and maximum\n    // split size).\n\n    // iterate over all racks\n    for (Iterator<Map.Entry<String, List<OneBlockInfo>>> iter =\n         rackToBlocks.entrySet().iterator(); iter.hasNext();) {\n\n      Map.Entry<String, List<OneBlockInfo>> one = iter.next();\n      racks.add(one.getKey());\n      List<OneBlockInfo> blocks = one.getValue();\n\n      // for each block, copy it into validBlocks. Delete it from\n      // blockToNodes so that the same block does not appear in\n      // two different splits.\n      boolean createdSplit = false;\n      for (OneBlockInfo oneblock : blocks) {\n        if (blockToNodes.containsKey(oneblock)) {\n          validBlocks.add(oneblock);\n          blockToNodes.remove(oneblock);\n          curSplitSize += oneblock.length;\n\n          // if the accumulated split size exceeds the maximum, then\n          // create this split.\n          if (maxSize != 0 && curSplitSize >= maxSize) {\n            // create an input split and add it to the splits array\n            addCreatedSplit(splits, getHosts(racks), validBlocks);\n            createdSplit = true;\n            break;\n          }\n        }\n      }\n\n      // if we created a split, then just go to the next rack\n      if (createdSplit) {\n        curSplitSize = 0;\n        validBlocks.clear();\n        racks.clear();\n        continue;\n      }\n\n      if (!validBlocks.isEmpty()) {\n        if (minSizeRack != 0 && curSplitSize >= minSizeRack) {\n          // if there is a minimum size specified, then create a single split\n          // otherwise, store these blocks into overflow data structure\n          addCreatedSplit(splits, getHosts(racks), validBlocks);\n        } else {\n          // There were a few blocks in this rack that\n      \t// remained to be processed. Keep them in 'overflow' block list.\n      \t// These will be combined later.\n          overflowBlocks.addAll(validBlocks);\n        }\n      }\n      curSplitSize = 0;\n      validBlocks.clear();\n      racks.clear();\n    }\n  }\n\n  assert blockToNodes.isEmpty();\n  assert curSplitSize == 0;\n  assert validBlocks.isEmpty();\n  assert racks.isEmpty();\n\n  // Process all overflow blocks\n  for (OneBlockInfo oneblock : overflowBlocks) {\n    validBlocks.add(oneblock);\n    curSplitSize += oneblock.length;\n\n    // This might cause an exiting rack location to be re-added,\n    // but it should be ok.\n    for (int i = 0; i < oneblock.racks.length; i++) {\n      racks.add(oneblock.racks[i]);\n    }\n\n    // if the accumulated split size exceeds the maximum, then\n    // create this split.\n    if (maxSize != 0 && curSplitSize >= maxSize) {\n      // create an input split and add it to the splits array\n      addCreatedSplit(splits, getHosts(racks), validBlocks);\n      curSplitSize = 0;\n      validBlocks.clear();\n      racks.clear();\n    }\n  }\n\n  // Process any remaining blocks, if any.\n  if (!validBlocks.isEmpty()) {\n    addCreatedSplit(splits, getHosts(racks), validBlocks);\n  }\n}",
        "reject_response": "@VisibleForTesting\nvoid createSplits(Map<String, Set<OneBlockInfo>> nodeToBlocks,\n                   Map<OneBlockInfo, String[]> blockToNodes,\n                   Map<String, List<OneBlockInfo>> rackToBlocks,\n                   long totLength,\n                   long maxSize,\n                   long minSizeNode,\n                   long minSizeRack,\n                   List<InputSplit> splits\n                  ) {\n  ArrayList<OneBlockInfo> validBlocks = new ArrayList<OneBlockInfo>();\n  long curSplitSize = 0;\n\n  int totalNodes = nodeToBlocks.size();\n  long totalLength = totLength;\n\n  Multiset<String> splitsPerNode = HashMultiset.create();\n  Set<String> completedNodes = new HashSet<String>();\n\n  while(true) {\n    for (Iterator<Map.Entry<String, Set<OneBlockInfo>>> iter = nodeToBlocks\n        .entrySet().iterator(); iter.hasNext();) {\n      Map.Entry<String, Set<OneBlockInfo>> one = iter.next();\n\n      String node = one.getKey();\n\n      // Skip the node if it has previously been marked as completed.\n      if (completedNodes.contains(node)) {\n        continue;\n      }\n\n      Set<OneBlockInfo> blocksInCurrentNode = one.getValue();\n\n      // for each block, copy it into validBlocks. Delete it from\n      // blockToNodes so that the same block does not appear in\n      // two different splits.\n      Iterator<OneBlockInfo> oneBlockIter = blocksInCurrentNode.iterator();\n      while (oneBlockIter.hasNext()) {\n        OneBlockInfo oneblock = oneBlockIter.next();\n\n        // Remove all blocks which may already have been assigned to other\n        // splits.\n        if(!blockToNodes.containsKey(oneblock)) {\n          oneBlockIter.remove();\n          continue;\n        }\n\n        validBlocks.add(oneblock);\n        blockToNodes.remove(oneblock);\n        curSplitSize += oneblock.length;\n\n        // if the accumulated split size exceeds the maximum, then\n        // create this split.\n        if (maxSize != 0 && curSplitSize >= maxSize) {\n          // create an input split and add it to the splits array\n          addCreatedSplit(splits, Collections.singleton(node), validBlocks);\n          totalLength -= curSplitSize;\n          curSplitSize = 0;\n\n          splitsPerNode.add(node);\n\n          // Remove entries from blocksInNode so that we don't walk these\n          // again.\n          blocksInCurrentNode.removeAll(validBlocks);\n          validBlocks.clear();\n\n          // Done creating a single split for this node. Move on to the next\n          // node so that splits are distributed across nodes.\n          break;\n        }\n\n      }\n      if (validBlocks.size() != 0) {\n        // This implies that the last few blocks (or all in case maxSize=0)\n        // were not part of a split. The node is complete.\n\n        // if there were any blocks left over and their combined size is\n        // larger than minSplitNode, then combine them into one split.\n        // Otherwise add them back to the unprocessed pool. It is likely\n        // that they will be combined with other blocks from the\n        // same rack later on.\n        // This condition also kicks in when max split size is not set. All\n        // blocks on a node will be grouped together into a single split.\n        if (minSizeNode != 0 && curSplitSize >= minSizeNode\n            && splitsPerNode.count(node) == 0) {\n          // haven't created any split on this machine. so its ok to add a\n          // smaller one for parallelism. Otherwise group it in the rack for\n          // balanced size create an input split and add it to the splits\n          // array\n          addCreatedSplit(splits, Collections.singleton(node), validBlocks);\n          totalLength -= curSplitSize;\n          splitsPerNode.add(node);\n          // Remove entries from blocksInNode so that we don't walk this again.\n          blocksInCurrentNode.removeAll(validBlocks);\n          // The node is done. This was the last set of blocks for this node.\n        } else {\n          // Put the unplaced blocks back into the pool for later rack-allocation.\n          for (OneBlockInfo oneblock : validBlocks) {\n            blockToNodes.put(oneblock, oneblock.hosts);\n          }\n        }\n        validBlocks.clear();\n        curSplitSize = 0;\n        completedNodes.add(node);\n      } else { // No in-flight blocks.\n        if (blocksInCurrentNode.size() == 0) {\n          // Node is done. All blocks were fit into node-local splits.\n          completedNodes.add(node);\n        } // else Run through the node again.\n      }\n    }\n\n    // Check if node-local assignments are complete.\n    if (completedNodes.size() == totalNodes || totalLength == 0) {\n      // All nodes have been walked over and marked as completed or all blocks\n      // have been assigned. The rest should be handled via rackLock assignment.\n      LOG.info(\"DEBUG: Terminated node allocation with : CompletedNodes: \"\n          + completedNodes.size() + \", size left: \" + totalLength);\n      break;\n    }\n  }\n\n  // if blocks in a rack are below the specified minimum size, then keep them\n  // in 'overflow'. After the processing of all racks is complete, these\n  // overflow blocks will be combined into splits.\n  ArrayList<OneBlockInfo> overflowBlocks = new ArrayList<OneBlockInfo>();\n  Set<String> racks = new HashSet<String>();\n\n  // Process all racks over and over again until there is no more work to do.\n  while (blockToNodes.size() > 0) {\n\n    // Create one split for this rack before moving over to the next rack.\n    // Come back to this rack after creating a single split for each of the\n    // remaining racks.\n    // Process one rack location at a time, Combine all possible blocks that\n    // reside on this rack as one split. (constrained by minimum and maximum\n    // split size).\n\n    // iterate over all racks\n    for (Iterator<Map.Entry<String, List<OneBlockInfo>>> iter =\n         rackToBlocks.entrySet().iterator(); iter.hasNext();) {\n\n      Map.Entry<String, List<OneBlockInfo>> one = iter.next();\n      racks.add(one.getKey());\n      List<OneBlockInfo> blocks = one.getValue();\n\n      // for each block, copy it into validBlocks. Delete it from\n      // blockToNodes so that the same block does not appear in\n      // two different splits.\n      boolean createdSplit = false;\n      for (OneBlockInfo oneblock : blocks) {\n        if (blockToNodes.containsKey(oneblock)) {\n          validBlocks.add(oneblock);\n          blockToNodes.remove(oneblock);\n          curSplitSize += oneblock.length;\n\n          // if the accumulated split size exceeds the maximum, then\n          // create this split.\n          if (maxSize != 0 && curSplitSize >= maxSize) {\n            // create an input split and add it to the splits array\n            addCreatedSplit(splits, getHosts(racks), validBlocks);\n            createdSplit = true;\n            break;\n          }\n        }\n      }\n\n      // if we created a split, then just go to the next rack\n      if (createdSplit) {\n        curSplitSize = 0;\n        validBlocks.clear();\n        racks.clear();\n        continue;\n      }\n\n      if (!validBlocks.isEmpty()) {\n        if (minSizeRack != 0 && curSplitSize >= minSizeRack) {\n          // if there is a minimum size specified, then create a single split\n          // otherwise, store these blocks into overflow data structure\n          addCreatedSplit(splits, getHosts(racks), validBlocks);\n        } else {\n          // There were a few blocks in this rack that\n      \t// remained to be processed. Keep them in 'overflow' block list.\n      \t// These will be combined later.\n          overflowBlocks.addAll(validBlocks);\n        }\n      }\n      curSplitSize = 0;\n      validBlocks.clear();\n      racks.clear();\n    }\n  }\n\n  assert blockToNodes.isEmpty();\n  assert curSplitSize == 0;\n  assert validBlocks.isEmpty();\n  assert racks.isEmpty();\n\n  // Process all overflow blocks\n  for (OneBlockInfo oneblock : overflowBlocks) {\n    validBlocks.add(oneblock);\n    curSplitSize += oneblock.length;\n\n    // This might cause an exiting rack location to be re-added,\n    // but it should be ok.\n    for (int i = 0; i < oneblock.racks.length; i++) {\n      racks.add(oneblock.racks[i]);\n    }\n\n    // if the accumulated split size exceeds the maximum, then\n    // create this split.\n    if (maxSize != 0 && curSplitSize >= maxSize) {\n      // create an input split and add it to the splits array\n      addCreatedSplit(splits, getHosts(racks), validBlocks);\n      curSplitSize = 0;\n      validBlocks.clear();\n      racks.clear();\n    }\n  }\n\n  // Process any remaining blocks, if any.\n  if (!validBlocks.isEmpty()) {\n    addCreatedSplit(splits, getHosts(racks), validBlocks);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2482,
        "instruction": "private void checkForFailures() throws IOException {\n  // Note that this function is never called by multiple threads and is the only place that\n  // we remove from failures, so this code is safe.\n  if (failures.isEmpty()) {\n    return;\n  }\n\n  StringBuilder logEntry = new StringBuilder();\n  int i = 0;\n  for (; i < 10 && !failures.isEmpty(); ++i) {\n    BigtableWriteException exc = failures.remove();\n    logEntry.append(\"\\n\").append(exc.getMessage());\n    if (exc.getCause() != null) {\n      logEntry.append(\": \").append(exc.getCause().getMessage());\n    }\n  }\n  String message =\n      String.format(\n          \"At least %d errors occurred writing to Bigtable. First %d errors: %s\",\n          i + failures.size(),\n          i,\n          logEntry.toString());\n  throw new IOException(message);\n}",
        "accept_response": "private void checkForFailures() throws IOException {\n  // Note that this function is never called by multiple threads and is the only place that\n  // we remove from failures, so this code is safe.\n  if (failures.isEmpty()) {\n    return;\n  }\n\n  StringBuilder logEntry = new StringBuilder();\n  int i = 0;\n  for (; i < 10 && !failures.isEmpty(); ++i) {\n    BigtableWriteException exc = failures.remove();\n    logEntry.append(\"\\n\").append(exc.getMessage());\n    if (exc.getCause() != null) {\n      logEntry.append(\": \").append(exc.getCause().getMessage());\n    }\n  }\n  String message =\n      String.format(\n          \"At least %d errors occurred writing to Bigtable. First %d errors: %s\",\n          i + failures.size(),\n          i,\n          logEntry.toString());\n  LOG.error(message);\n  throw new IOException(message);\n}",
        "reject_response": "private void checkForFailures() throws IOException {\n  // Note that this function is never called by multiple threads and is the only place that\n  // we remove from failures, so this code is safe.\n  if (failures.isEmpty()) {\n    return;\n  }\n\n  StringBuilder logEntry = new StringBuilder();\n  int i = 0;\n  for (; i < 10 && !failures.isEmpty(); ++i) {\n    BigtableWriteException exc = failures.remove();\n    logEntry.append(\"\\n\").append(exc.getMessage());\n    if (exc.getCause() != null) {\n      logEntry.append(\": \").append(exc.getCause().getMessage());\n    }\n  }\n  String message =\n      String.format(\n          \"At least %d errors occurred writing to Bigtable. First %d errors: %s\",\n          i + failures.size(),\n          i,\n          logEntry.toString());\n  logger.error(message);\n  throw new IOException(message);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3228,
        "instruction": "public void processPacket(ServerCnxn cnxn, ByteBuffer incomingBuffer) throws IOException {\n    // We have the request, now process and setup for next\n    InputStream bais = new ByteBufferInputStream(incomingBuffer);\n    BinaryInputArchive bia = BinaryInputArchive.getArchive(bais);\n    RequestHeader h = new RequestHeader();\n    h.deserialize(bia, \"header\");\n\n    // Need to increase the outstanding request count first, otherwise\n    // there might be a race condition that it enabled recv after\n    // processing request and then disabled when check throttling.\n    //\n    // Be aware that we're actually checking the global outstanding\n    // request before this request.\n    //\n    // It's fine if the IOException thrown before we decrease the count\n    // in cnxn, since it will close the cnxn anyway.\n    cnxn.incrOutstandingAndCheckThrottle(h);\n\n    // Through the magic of byte buffers, txn will not be\n    // pointing\n    // to the start of the txn\n    incomingBuffer = incomingBuffer.slice();\n    if (h.getType() == OpCode.auth) {\n        LOG.info(\"got auth packet \" + cnxn.getRemoteSocketAddress());\n        AuthPacket authPacket = new AuthPacket();\n        ByteBufferInputStream.byteBuffer2Record(incomingBuffer, authPacket);\n        String scheme = authPacket.getScheme();\n        ServerAuthenticationProvider ap = ProviderRegistry.getServerProvider(scheme);\n        Code authReturn = KeeperException.Code.AUTHFAILED;\n        if(ap != null) {\n            try {\n                // handleAuthentication may close the connection, to allow the client to choose\n                // a different server to connect to.\n                authReturn = ap.handleAuthentication(new ServerAuthenticationProvider.ServerObjs(this, cnxn), authPacket.getAuth());\n            } catch(RuntimeException e) {\n                LOG.warn(\"Caught runtime exception from AuthenticationProvider: \" + scheme + \" due to \" + e);\n                authReturn = KeeperException.Code.AUTHFAILED;\n            }\n        }\n        if (authReturn == KeeperException.Code.OK) {\n            LOG.info(\"auth success \" + cnxn.getRemoteSocketAddress());\n            ReplyHeader rh = new ReplyHeader(h.getXid(), 0,\n                    KeeperException.Code.OK.intValue());\n            cnxn.sendResponse(rh, null, null);\n        } else {\n            if (ap == null) {\n                LOG.warn(\"No authentication provider for scheme: \"\n                        + scheme + \" has \"\n                        + ProviderRegistry.listProviders());\n            } else {\n                LOG.warn(\"Authentication failed for scheme: \" + scheme);\n            }\n            // send a response...\n            ReplyHeader rh = new ReplyHeader(h.getXid(), 0,\n                    KeeperException.Code.AUTHFAILED.intValue());\n            cnxn.sendResponse(rh, null, null);\n            // ... and close connection\n            cnxn.sendBuffer(ServerCnxnFactory.closeConn);\n            cnxn.disableRecv();\n        }\n        return;\n    } else if (h.getType() == OpCode.sasl) {\n        processSasl(incomingBuffer,cnxn, h);\n    } else {\n      if (shouldRequireClientSaslAuth() && !hasCnxSASLAuthenticated(cnxn)) {\n        ReplyHeader replyHeader = new ReplyHeader(h.getXid(), 0,\n            Code.SESSIONCLOSEDREQUIRESASLAUTH.intValue());\n        cnxn.sendResponse(replyHeader, null, \"response\");\n        cnxn.sendCloseSession();\n        cnxn.disableRecv();\n      } else {\n        Request si = new Request(cnxn, cnxn.getSessionId(), h.getXid(),\n            h.getType(), incomingBuffer, cnxn.getAuthInfo());\n        si.setOwner(ServerCnxn.me);\n        // Always treat packet from the client as a possible\n        // local request.\n        setLocalSessionFlag(si);\n        submitRequest(si);\n      }\n    }\n}",
        "accept_response": "public void processPacket(ServerCnxn cnxn, ByteBuffer incomingBuffer) throws IOException {\n    // We have the request, now process and setup for next\n    InputStream bais = new ByteBufferInputStream(incomingBuffer);\n    BinaryInputArchive bia = BinaryInputArchive.getArchive(bais);\n    RequestHeader h = new RequestHeader();\n    h.deserialize(bia, \"header\");\n\n    // Need to increase the outstanding request count first, otherwise\n    // there might be a race condition that it enabled recv after\n    // processing request and then disabled when check throttling.\n    //\n    // Be aware that we're actually checking the global outstanding\n    // request before this request.\n    //\n    // It's fine if the IOException thrown before we decrease the count\n    // in cnxn, since it will close the cnxn anyway.\n    cnxn.incrOutstandingAndCheckThrottle(h);\n\n    // Through the magic of byte buffers, txn will not be\n    // pointing\n    // to the start of the txn\n    incomingBuffer = incomingBuffer.slice();\n    if (h.getType() == OpCode.auth) {\n        LOG.info(\"got auth packet \" + cnxn.getRemoteSocketAddress());\n        AuthPacket authPacket = new AuthPacket();\n        ByteBufferInputStream.byteBuffer2Record(incomingBuffer, authPacket);\n        String scheme = authPacket.getScheme();\n        ServerAuthenticationProvider ap = ProviderRegistry.getServerProvider(scheme);\n        Code authReturn = KeeperException.Code.AUTHFAILED;\n        if(ap != null) {\n            try {\n                // handleAuthentication may close the connection, to allow the client to choose\n                // a different server to connect to.\n                authReturn = ap.handleAuthentication(new ServerAuthenticationProvider.ServerObjs(this, cnxn), authPacket.getAuth());\n            } catch(RuntimeException e) {\n                LOG.warn(\"Caught runtime exception from AuthenticationProvider: \" + scheme + \" due to \" + e);\n                authReturn = KeeperException.Code.AUTHFAILED;\n            }\n        }\n        if (authReturn == KeeperException.Code.OK) {\n            LOG.debug(\"Authentication succeeded for scheme: {}\", scheme);\n            LOG.info(\"auth success \" + cnxn.getRemoteSocketAddress());\n            ReplyHeader rh = new ReplyHeader(h.getXid(), 0,\n                    KeeperException.Code.OK.intValue());\n            cnxn.sendResponse(rh, null, null);\n        } else {\n            if (ap == null) {\n                LOG.warn(\"No authentication provider for scheme: \"\n                        + scheme + \" has \"\n                        + ProviderRegistry.listProviders());\n            } else {\n                LOG.warn(\"Authentication failed for scheme: \" + scheme);\n            }\n            // send a response...\n            ReplyHeader rh = new ReplyHeader(h.getXid(), 0,\n                    KeeperException.Code.AUTHFAILED.intValue());\n            cnxn.sendResponse(rh, null, null);\n            // ... and close connection\n            cnxn.sendBuffer(ServerCnxnFactory.closeConn);\n            cnxn.disableRecv();\n        }\n        return;\n    } else if (h.getType() == OpCode.sasl) {\n        processSasl(incomingBuffer,cnxn, h);\n    } else {\n      if (shouldRequireClientSaslAuth() && !hasCnxSASLAuthenticated(cnxn)) {\n        ReplyHeader replyHeader = new ReplyHeader(h.getXid(), 0,\n            Code.SESSIONCLOSEDREQUIRESASLAUTH.intValue());\n        cnxn.sendResponse(replyHeader, null, \"response\");\n        cnxn.sendCloseSession();\n        cnxn.disableRecv();\n      } else {\n        Request si = new Request(cnxn, cnxn.getSessionId(), h.getXid(),\n            h.getType(), incomingBuffer, cnxn.getAuthInfo());\n        si.setOwner(ServerCnxn.me);\n        // Always treat packet from the client as a possible\n        // local request.\n        setLocalSessionFlag(si);\n        submitRequest(si);\n      }\n    }\n}",
        "reject_response": "public void processPacket(ServerCnxn cnxn, ByteBuffer incomingBuffer) throws IOException {\n    // We have the request, now process and setup for next\n    InputStream bais = new ByteBufferInputStream(incomingBuffer);\n    BinaryInputArchive bia = BinaryInputArchive.getArchive(bais);\n    RequestHeader h = new RequestHeader();\n    h.deserialize(bia, \"header\");\n\n    // Need to increase the outstanding request count first, otherwise\n    // there might be a race condition that it enabled recv after\n    // processing request and then disabled when check throttling.\n    //\n    // Be aware that we're actually checking the global outstanding\n    // request before this request.\n    //\n    // It's fine if the IOException thrown before we decrease the count\n    // in cnxn, since it will close the cnxn anyway.\n    cnxn.incrOutstandingAndCheckThrottle(h);\n\n    // Through the magic of byte buffers, txn will not be\n    // pointing\n    // to the start of the txn\n    incomingBuffer = incomingBuffer.slice();\n    if (h.getType() == OpCode.auth) {\n        LOG.info(\"got auth packet \" + cnxn.getRemoteSocketAddress());\n        AuthPacket authPacket = new AuthPacket();\n        ByteBufferInputStream.byteBuffer2Record(incomingBuffer, authPacket);\n        String scheme = authPacket.getScheme();\n        ServerAuthenticationProvider ap = ProviderRegistry.getServerProvider(scheme);\n        Code authReturn = KeeperException.Code.AUTHFAILED;\n        if(ap != null) {\n            try {\n                // handleAuthentication may close the connection, to allow the client to choose\n                // a different server to connect to.\n                authReturn = ap.handleAuthentication(new ServerAuthenticationProvider.ServerObjs(this, cnxn), authPacket.getAuth());\n            } catch(RuntimeException e) {\n                LOG.warn(\"Caught runtime exception from AuthenticationProvider: \" + scheme + \" due to \" + e);\n                authReturn = KeeperException.Code.AUTHFAILED;\n            }\n        }\n        if (authReturn == KeeperException.Code.OK) {\n            if (LOG.isDebugEnabled()) {\n                LOG.debug(\"Authentication succeeded for scheme: \" + scheme);\n            }\n            LOG.info(\"auth success \" + cnxn.getRemoteSocketAddress());\n            ReplyHeader rh = new ReplyHeader(h.getXid(), 0,\n                    KeeperException.Code.OK.intValue());\n            cnxn.sendResponse(rh, null, null);\n        } else {\n            if (ap == null) {\n                LOG.warn(\"No authentication provider for scheme: \"\n                        + scheme + \" has \"\n                        + ProviderRegistry.listProviders());\n            } else {\n                LOG.warn(\"Authentication failed for scheme: \" + scheme);\n            }\n            // send a response...\n            ReplyHeader rh = new ReplyHeader(h.getXid(), 0,\n                    KeeperException.Code.AUTHFAILED.intValue());\n            cnxn.sendResponse(rh, null, null);\n            // ... and close connection\n            cnxn.sendBuffer(ServerCnxnFactory.closeConn);\n            cnxn.disableRecv();\n        }\n        return;\n    } else if (h.getType() == OpCode.sasl) {\n        processSasl(incomingBuffer,cnxn, h);\n    } else {\n      if (shouldRequireClientSaslAuth() && !hasCnxSASLAuthenticated(cnxn)) {\n        ReplyHeader replyHeader = new ReplyHeader(h.getXid(), 0,\n            Code.SESSIONCLOSEDREQUIRESASLAUTH.intValue());\n        cnxn.sendResponse(replyHeader, null, \"response\");\n        cnxn.sendCloseSession();\n        cnxn.disableRecv();\n      } else {\n        Request si = new Request(cnxn, cnxn.getSessionId(), h.getXid(),\n            h.getType(), incomingBuffer, cnxn.getAuthInfo());\n        si.setOwner(ServerCnxn.me);\n        // Always treat packet from the client as a possible\n        // local request.\n        setLocalSessionFlag(si);\n        submitRequest(si);\n      }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2402,
        "instruction": "@Override\npublic void run() {\n  String zkConnectString = LogFeederUtil.getStringProperty(\"logfeeder.solr.zk_connect_string\");\n  String solrUrl = LogFeederUtil.getStringProperty(\"logfeeder.solr.url\");\n  if ((zkConnectString == null || zkConnectString.trim().length() == 0 )\n      && (solrUrl == null || solrUrl.trim().length() == 0)) {\n    return;\n  }\n  solrConfigInterval = LogFeederUtil.getIntProperty(\"logfeeder.solr.config.interval\", solrConfigInterval);\n  delay = 1000 * solrConfigInterval;\n  do {\n    logger.debug(\"Updating config from solr after every \" + solrConfigInterval + \" sec.\");\n    pullConfigFromSolr();\n    try {\n      Thread.sleep(delay);\n    } catch (InterruptedException e) {\n      logger.error(e.getLocalizedMessage(), e.getCause());\n    }\n  } while (true);\n}",
        "accept_response": "@Override\npublic void run() {\n  String zkConnectString = LogFeederUtil.getStringProperty(\"logfeeder.solr.zk_connect_string\");\n  String solrUrl = LogFeederUtil.getStringProperty(\"logfeeder.solr.url\");\n  if ((zkConnectString == null || zkConnectString.trim().length() == 0 )\n      && (solrUrl == null || solrUrl.trim().length() == 0)) {\n    logger.warn(\"Neither Solr ZK Connect String nor solr Uril for UserConfig/History is set.\" +\n        \"Won't look for level configuration from Solr.\");\n    return;\n  }\n  solrConfigInterval = LogFeederUtil.getIntProperty(\"logfeeder.solr.config.interval\", solrConfigInterval);\n  delay = 1000 * solrConfigInterval;\n  do {\n    logger.debug(\"Updating config from solr after every \" + solrConfigInterval + \" sec.\");\n    pullConfigFromSolr();\n    try {\n      Thread.sleep(delay);\n    } catch (InterruptedException e) {\n      logger.error(e.getLocalizedMessage(), e.getCause());\n    }\n  } while (true);\n}",
        "reject_response": "@Override\npublic void run() {\n  String zkConnectString = LogFeederUtil.getStringProperty(\"logfeeder.solr.zk_connect_string\");\n  String solrUrl = LogFeederUtil.getStringProperty(\"logfeeder.solr.url\");\n  if ((zkConnectString == null || zkConnectString.trim().length() == 0 )\n      && (solrUrl == null || solrUrl.trim().length() == 0)) {\n    logger\n        .warn(\"Solr ZKHosts or solrUrl for UserConfig/History is not set. Won't look for level configuration from Solr.\");\n    return;\n  }\n  solrConfigInterval = LogFeederUtil.getIntProperty(\"logfeeder.solr.config.interval\", solrConfigInterval);\n  delay = 1000 * solrConfigInterval;\n  do {\n    logger.debug(\"Updating config from solr after every \" + solrConfigInterval + \" sec.\");\n    pullConfigFromSolr();\n    try {\n      Thread.sleep(delay);\n    } catch (InterruptedException e) {\n      logger.error(e.getLocalizedMessage(), e.getCause());\n    }\n  } while (true);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3018,
        "instruction": "@Override\npublic List<T> receive(final List<? extends Identifier> order) throws NetworkException, InterruptedException {\n  LOG.entering(\"GatherReceiver\", \"receive\");\n  final Map<String, T> mapOfTaskIdToData = receiveMapOfTaskIdToData();\n\n  final List<T> retList = new LinkedList<>();\n  for (final Identifier key : order) {\n    final String keyString = key.toString();\n    if (mapOfTaskIdToData.containsKey(keyString)) {\n      retList.add(mapOfTaskIdToData.get(key.toString()));\n    } else {\n      LOG.warning(this + \" Received no data from \" + keyString + \". Adding null.\");\n      retList.add(null);\n    }\n  }\n\n  LOG.exiting(\"GatherReceiver\", \"receive\");\n  return retList;\n}",
        "accept_response": "@Override\npublic List<T> receive(final List<? extends Identifier> order) throws NetworkException, InterruptedException {\n  LOG.entering(\"GatherReceiver\", \"receive\");\n  final Map<String, T> mapOfTaskIdToData = receiveMapOfTaskIdToData();\n\n  LOG.log(Level.FINE, \"{0} Sorting data according to specified order of task identifiers.\", this);\n  final List<T> retList = new LinkedList<>();\n  for (final Identifier key : order) {\n    final String keyString = key.toString();\n    if (mapOfTaskIdToData.containsKey(keyString)) {\n      retList.add(mapOfTaskIdToData.get(key.toString()));\n    } else {\n      LOG.warning(this + \" Received no data from \" + keyString + \". Adding null.\");\n      retList.add(null);\n    }\n  }\n\n  LOG.exiting(\"GatherReceiver\", \"receive\");\n  return retList;\n}",
        "reject_response": "@Override\npublic List<T> receive(final List<? extends Identifier> order) throws NetworkException, InterruptedException {\n  LOG.entering(\"GatherReceiver\", \"receive\");\n  final Map<String, T> mapOfTaskIdToData = receiveMapOfTaskIdToData();\n\n  LOG.fine(this + \" Sorting data according to specified order of task identifiers.\");\n  final List<T> retList = new LinkedList<>();\n  for (final Identifier key : order) {\n    final String keyString = key.toString();\n    if (mapOfTaskIdToData.containsKey(keyString)) {\n      retList.add(mapOfTaskIdToData.get(key.toString()));\n    } else {\n      LOG.warning(this + \" Received no data from \" + keyString + \". Adding null.\");\n      retList.add(null);\n    }\n  }\n\n  LOG.exiting(\"GatherReceiver\", \"receive\");\n  return retList;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2990,
        "instruction": "@GET\n@Path(KMSRESTConstants.KEYS_NAMES_RESOURCE)\n@Produces(MediaType.APPLICATION_JSON)\npublic Response getKeyNames(@Context HttpServletRequest request) throws Exception {\n  try {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Entering getKeyNames method.\");\n    }\n    KMSWebApp.getAdminCallsMeter().mark();\n    UserGroupInformation user = HttpUserGroupInformation.get();\n    assertAccess(Type.GET_KEYS, user, KMSOp.GET_KEYS, request.getRemoteAddr());\n    List<String> json = user.doAs(new PrivilegedExceptionAction<List<String>>() {\n      @Override\n      public List<String> run() throws Exception {\n        return provider.getKeys();\n      }\n    });\n    kmsAudit.ok(user, KMSOp.GET_KEYS, \"\");\n    if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Exiting getKeyNames method.\");\n    }\n    return Response.ok().type(MediaType.APPLICATION_JSON).entity(json).build();\n  } catch (Exception e) {\n    throw e;\n  }\n}",
        "accept_response": "@GET\n@Path(KMSRESTConstants.KEYS_NAMES_RESOURCE)\n@Produces(MediaType.APPLICATION_JSON)\npublic Response getKeyNames(@Context HttpServletRequest request) throws Exception {\n  try {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Entering getKeyNames method.\");\n    }\n    KMSWebApp.getAdminCallsMeter().mark();\n    UserGroupInformation user = HttpUserGroupInformation.get();\n    assertAccess(Type.GET_KEYS, user, KMSOp.GET_KEYS, request.getRemoteAddr());\n    List<String> json = user.doAs(new PrivilegedExceptionAction<List<String>>() {\n      @Override\n      public List<String> run() throws Exception {\n        return provider.getKeys();\n      }\n    });\n    kmsAudit.ok(user, KMSOp.GET_KEYS, \"\");\n    if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Exiting getKeyNames method.\");\n    }\n    return Response.ok().type(MediaType.APPLICATION_JSON).entity(json).build();\n  } catch (Exception e) {\n    LOG.error(\"Exception in getkeyNames.\", e);\n    throw e;\n  }\n}",
        "reject_response": "@GET\n@Path(KMSRESTConstants.KEYS_NAMES_RESOURCE)\n@Produces(MediaType.APPLICATION_JSON)\npublic Response getKeyNames(@Context HttpServletRequest request) throws Exception {\n  try {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Entering getKeyNames method.\");\n    }\n    KMSWebApp.getAdminCallsMeter().mark();\n    UserGroupInformation user = HttpUserGroupInformation.get();\n    assertAccess(Type.GET_KEYS, user, KMSOp.GET_KEYS, request.getRemoteAddr());\n    List<String> json = user.doAs(new PrivilegedExceptionAction<List<String>>() {\n      @Override\n      public List<String> run() throws Exception {\n        return provider.getKeys();\n      }\n    });\n    kmsAudit.ok(user, KMSOp.GET_KEYS, \"\");\n    if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Exiting getKeyNames method.\");\n    }\n    return Response.ok().type(MediaType.APPLICATION_JSON).entity(json).build();\n  } catch (Exception e) {\n    LOG.debug(\"Exception in getkeyNames.\", e);\n    throw e;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2947,
        "instruction": "private void writeRecord(final Record record, final RecordSchema writeSchema, final JsonGenerator generator,\n    final GeneratorTask startTask, final GeneratorTask endTask, final boolean schemaAware) throws IOException {\n\n    final Optional<SerializedForm> serializedForm = record.getSerializedForm();\n    if (serializedForm.isPresent()) {\n        final SerializedForm form = serializedForm.get();\n        if (form.getMimeType().equals(getMimeType()) && record.getSchema().equals(writeSchema)) {\n            final Object serialized = form.getSerialized();\n            if (serialized instanceof String) {\n                generator.writeRawValue((String) serialized);\n                return;\n            }\n        }\n    }\n\n    try {\n        startTask.apply(generator);\n\n        if (schemaAware) {\n            for (final RecordField field : writeSchema.getFields()) {\n                final String fieldName = field.getFieldName();\n                final Object value = record.getValue(field);\n                if (value == null) {\n                    if (nullSuppression == NullSuppression.NEVER_SUPPRESS || (nullSuppression == NullSuppression.SUPPRESS_MISSING) && isFieldPresent(field, record)) {\n                        generator.writeNullField(fieldName);\n                    }\n\n                    continue;\n                }\n\n                generator.writeFieldName(fieldName);\n\n                final DataType dataType = writeSchema.getDataType(fieldName).get();\n                writeValue(generator, value, fieldName, dataType);\n            }\n        } else {\n            for (final String fieldName : record.getRawFieldNames()) {\n                final Object value = record.getValue(fieldName);\n                if (value == null) {\n                    if (nullSuppression == NullSuppression.NEVER_SUPPRESS || (nullSuppression == NullSuppression.SUPPRESS_MISSING) && record.getRawFieldNames().contains(fieldName)) {\n                        generator.writeNullField(fieldName);\n                    }\n\n                    continue;\n                }\n\n                generator.writeFieldName(fieldName);\n                writeRawValue(generator, value, fieldName);\n            }\n        }\n\n        endTask.apply(generator);\n    } catch (final Exception e) {\n        throw e;\n    }\n}",
        "accept_response": "private void writeRecord(final Record record, final RecordSchema writeSchema, final JsonGenerator generator,\n    final GeneratorTask startTask, final GeneratorTask endTask, final boolean schemaAware) throws IOException {\n\n    final Optional<SerializedForm> serializedForm = record.getSerializedForm();\n    if (serializedForm.isPresent()) {\n        final SerializedForm form = serializedForm.get();\n        if (form.getMimeType().equals(getMimeType()) && record.getSchema().equals(writeSchema)) {\n            final Object serialized = form.getSerialized();\n            if (serialized instanceof String) {\n                generator.writeRawValue((String) serialized);\n                return;\n            }\n        }\n    }\n\n    try {\n        startTask.apply(generator);\n\n        if (schemaAware) {\n            for (final RecordField field : writeSchema.getFields()) {\n                final String fieldName = field.getFieldName();\n                final Object value = record.getValue(field);\n                if (value == null) {\n                    if (nullSuppression == NullSuppression.NEVER_SUPPRESS || (nullSuppression == NullSuppression.SUPPRESS_MISSING) && isFieldPresent(field, record)) {\n                        generator.writeNullField(fieldName);\n                    }\n\n                    continue;\n                }\n\n                generator.writeFieldName(fieldName);\n\n                final DataType dataType = writeSchema.getDataType(fieldName).get();\n                writeValue(generator, value, fieldName, dataType);\n            }\n        } else {\n            for (final String fieldName : record.getRawFieldNames()) {\n                final Object value = record.getValue(fieldName);\n                if (value == null) {\n                    if (nullSuppression == NullSuppression.NEVER_SUPPRESS || (nullSuppression == NullSuppression.SUPPRESS_MISSING) && record.getRawFieldNames().contains(fieldName)) {\n                        generator.writeNullField(fieldName);\n                    }\n\n                    continue;\n                }\n\n                generator.writeFieldName(fieldName);\n                writeRawValue(generator, value, fieldName);\n            }\n        }\n\n        endTask.apply(generator);\n    } catch (final Exception e) {\n        logger.error(\"Failed to write {} with reader schema {} and writer schema {} as a JSON Object due to {}\", record, record.getSchema(), writeSchema, e.toString(), e);\n        throw e;\n    }\n}",
        "reject_response": "private void writeRecord(final Record record, final RecordSchema writeSchema, final JsonGenerator generator,\n    final GeneratorTask startTask, final GeneratorTask endTask, final boolean schemaAware) throws IOException {\n\n    final Optional<SerializedForm> serializedForm = record.getSerializedForm();\n    if (serializedForm.isPresent()) {\n        final SerializedForm form = serializedForm.get();\n        if (form.getMimeType().equals(getMimeType()) && record.getSchema().equals(writeSchema)) {\n            final Object serialized = form.getSerialized();\n            if (serialized instanceof String) {\n                generator.writeRawValue((String) serialized);\n                return;\n            }\n        }\n    }\n\n    try {\n        startTask.apply(generator);\n\n        if (schemaAware) {\n            for (final RecordField field : writeSchema.getFields()) {\n                final String fieldName = field.getFieldName();\n                final Object value = record.getValue(field);\n                if (value == null) {\n                    if (nullSuppression == NullSuppression.NEVER_SUPPRESS || (nullSuppression == NullSuppression.SUPPRESS_MISSING) && isFieldPresent(field, record)) {\n                        generator.writeNullField(fieldName);\n                    }\n\n                    continue;\n                }\n\n                generator.writeFieldName(fieldName);\n\n                final DataType dataType = writeSchema.getDataType(fieldName).get();\n                writeValue(generator, value, fieldName, dataType);\n            }\n        } else {\n            for (final String fieldName : record.getRawFieldNames()) {\n                final Object value = record.getValue(fieldName);\n                if (value == null) {\n                    if (nullSuppression == NullSuppression.NEVER_SUPPRESS || (nullSuppression == NullSuppression.SUPPRESS_MISSING) && record.getRawFieldNames().contains(fieldName)) {\n                        generator.writeNullField(fieldName);\n                    }\n\n                    continue;\n                }\n\n                generator.writeFieldName(fieldName);\n                writeRawValue(generator, value, fieldName);\n            }\n        }\n\n        endTask.apply(generator);\n    } catch (final Exception e) {\n        logger.error(\"Failed to write {} with schema {} as a JSON Object due to {}\", new Object[] {record, record.getSchema(), e.toString(), e});\n        throw e;\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2980,
        "instruction": "private void logAuthorizationConext(AuthorizationContext context) {\n\ttry {\n\t\t// Note: This method should be called with isDebugEnabled()\n\t\tString collections = \"\";\n\t\tint i = -1;\n\t\tfor (CollectionRequest collectionRequest : context\n\t\t\t\t.getCollectionRequests()) {\n\t\t\ti++;\n\t\t\tif (i > 0) {\n\t\t\t\tcollections += \",\";\n\t\t\t}\n\t\t\tcollections += collectionRequest.collectionName;\n\t\t}\n\n\t\tString headers = \"\";\n\t\ti = -1;\n\t\t@SuppressWarnings(\"unchecked\")\n\t\tEnumeration<String> eList = context.getHeaderNames();\n\t\twhile (eList.hasMoreElements()) {\n\t\t\ti++;\n\t\t\tif (i > 0) {\n\t\t\t\theaders += \",\";\n\t\t\t}\n\t\t\tString header = eList.nextElement();\n\t\t\tString value = context.getHttpHeader(header);\n\t\t\theaders += header + \"=\" + value;\n\t\t}\n\n\t\tString ipAddress = context.getHttpHeader(\"HTTP_X_FORWARDED_FOR\");\n\n\t\tif (ipAddress == null) {\n\t\t\tipAddress = context.getHttpHeader(\"REMOTE_HOST\");\n\t\t}\n\t\tif (ipAddress == null) {\n\t\t\tipAddress = context.getHttpHeader(\"REMOTE_ADDR\");\n\t\t}\n\t\tif (ipAddress == null) {\n\t\t\tipAddress = context.getRemoteAddr();\n\t\t}\n\n\t\tString userName = getUserName(context);\n\t\tSet<String> groups = getGroupsForUser(userName);\n\t\tString resource    = context.getResource();\n\t\tString solrParams  = \"\";\n\t\ttry {\n\t\t\tsolrParams = context.getParams().toQueryString();\n\t\t} catch (Throwable t) {\n\t\t\t//Exception ignored\n\t\t}\n\t\tRequestType requestType  = context.getRequestType();\n\t\tString \t\taccessType   = mapToRangerAccessType(context);\n\t\tPrincipal\tprincipal\t = context.getUserPrincipal();\n\n\t\tString contextString = new String(\"AuthorizationContext: \");\n\t\tcontextString  = contextString + \"context.getResource()= \" + ((resource != null ) ? resource : \"\");\n\t\tcontextString  = contextString + \", solarParams= \" + (( solrParams != null ) ? solrParams : \"\");\n\t\tcontextString  = contextString + \", requestType= \" + (( requestType != null ) ? requestType : \"\");\n\t\tcontextString  = contextString + \", ranger.requestType= \" + ((accessType != null ) ? accessType : \"\");\n\t\tcontextString  = contextString + \", userPrincipal= \" + ((principal != null ) ? principal : \"\");\n\t\tcontextString  = contextString + \", userName= \"  + userName;\n\t\tcontextString  = contextString + \", groups= \" + groups;\n\t\tcontextString  = contextString + \", ipAddress= \" + ipAddress;\n\t\tcontextString  = contextString + \", collections= \" + collections;\n\t\tcontextString  = contextString + \", headers= \" + headers;\n\n\t} catch (Throwable t) {\n\t\tlogger.error(\"Error getting request context!!!\", t);\n\t}\n\n}",
        "accept_response": "private void logAuthorizationConext(AuthorizationContext context) {\n\ttry {\n\t\t// Note: This method should be called with isDebugEnabled()\n\t\tString collections = \"\";\n\t\tint i = -1;\n\t\tfor (CollectionRequest collectionRequest : context\n\t\t\t\t.getCollectionRequests()) {\n\t\t\ti++;\n\t\t\tif (i > 0) {\n\t\t\t\tcollections += \",\";\n\t\t\t}\n\t\t\tcollections += collectionRequest.collectionName;\n\t\t}\n\n\t\tString headers = \"\";\n\t\ti = -1;\n\t\t@SuppressWarnings(\"unchecked\")\n\t\tEnumeration<String> eList = context.getHeaderNames();\n\t\twhile (eList.hasMoreElements()) {\n\t\t\ti++;\n\t\t\tif (i > 0) {\n\t\t\t\theaders += \",\";\n\t\t\t}\n\t\t\tString header = eList.nextElement();\n\t\t\tString value = context.getHttpHeader(header);\n\t\t\theaders += header + \"=\" + value;\n\t\t}\n\n\t\tString ipAddress = context.getHttpHeader(\"HTTP_X_FORWARDED_FOR\");\n\n\t\tif (ipAddress == null) {\n\t\t\tipAddress = context.getHttpHeader(\"REMOTE_HOST\");\n\t\t}\n\t\tif (ipAddress == null) {\n\t\t\tipAddress = context.getHttpHeader(\"REMOTE_ADDR\");\n\t\t}\n\t\tif (ipAddress == null) {\n\t\t\tipAddress = context.getRemoteAddr();\n\t\t}\n\n\t\tString userName = getUserName(context);\n\t\tSet<String> groups = getGroupsForUser(userName);\n\t\tString resource    = context.getResource();\n\t\tString solrParams  = \"\";\n\t\ttry {\n\t\t\tsolrParams = context.getParams().toQueryString();\n\t\t} catch (Throwable t) {\n\t\t\t//Exception ignored\n\t\t}\n\t\tRequestType requestType  = context.getRequestType();\n\t\tString \t\taccessType   = mapToRangerAccessType(context);\n\t\tPrincipal\tprincipal\t = context.getUserPrincipal();\n\n\t\tString contextString = new String(\"AuthorizationContext: \");\n\t\tcontextString  = contextString + \"context.getResource()= \" + ((resource != null ) ? resource : \"\");\n\t\tcontextString  = contextString + \", solarParams= \" + (( solrParams != null ) ? solrParams : \"\");\n\t\tcontextString  = contextString + \", requestType= \" + (( requestType != null ) ? requestType : \"\");\n\t\tcontextString  = contextString + \", ranger.requestType= \" + ((accessType != null ) ? accessType : \"\");\n\t\tcontextString  = contextString + \", userPrincipal= \" + ((principal != null ) ? principal : \"\");\n\t\tcontextString  = contextString + \", userName= \"  + userName;\n\t\tcontextString  = contextString + \", groups= \" + groups;\n\t\tcontextString  = contextString + \", ipAddress= \" + ipAddress;\n\t\tcontextString  = contextString + \", collections= \" + collections;\n\t\tcontextString  = contextString + \", headers= \" + headers;\n\n\t\tlogger.debug(contextString);\n\t} catch (Throwable t) {\n\t\tlogger.error(\"Error getting request context!!!\", t);\n\t}\n\n}",
        "reject_response": "private void logAuthorizationConext(AuthorizationContext context) {\n\ttry {\n\t\t// Note: This method should be called with isDebugEnabled()\n\t\tString collections = \"\";\n\t\tint i = -1;\n\t\tfor (CollectionRequest collectionRequest : context\n\t\t\t\t.getCollectionRequests()) {\n\t\t\ti++;\n\t\t\tif (i > 0) {\n\t\t\t\tcollections += \",\";\n\t\t\t}\n\t\t\tcollections += collectionRequest.collectionName;\n\t\t}\n\n\t\tString headers = \"\";\n\t\ti = -1;\n\t\t@SuppressWarnings(\"unchecked\")\n\t\tEnumeration<String> eList = context.getHeaderNames();\n\t\twhile (eList.hasMoreElements()) {\n\t\t\ti++;\n\t\t\tif (i > 0) {\n\t\t\t\theaders += \",\";\n\t\t\t}\n\t\t\tString header = eList.nextElement();\n\t\t\tString value = context.getHttpHeader(header);\n\t\t\theaders += header + \"=\" + value;\n\t\t}\n\n\t\tString ipAddress = context.getHttpHeader(\"HTTP_X_FORWARDED_FOR\");\n\n\t\tif (ipAddress == null) {\n\t\t\tipAddress = context.getHttpHeader(\"REMOTE_HOST\");\n\t\t}\n\t\tif (ipAddress == null) {\n\t\t\tipAddress = context.getHttpHeader(\"REMOTE_ADDR\");\n\t\t}\n\t\tif (ipAddress == null) {\n\t\t\tipAddress = context.getRemoteAddr();\n\t\t}\n\n\t\tString userName = getUserName(context);\n\t\tSet<String> groups = getGroupsForUser(userName);\n\t\tString resource    = context.getResource();\n\t\tString solrParams  = \"\";\n\t\ttry {\n\t\t\tsolrParams = context.getParams().toQueryString();\n\t\t} catch (Throwable t) {\n\t\t\t//Exception ignored\n\t\t}\n\t\tRequestType requestType  = context.getRequestType();\n\t\tString \t\taccessType   = mapToRangerAccessType(context);\n\t\tPrincipal\tprincipal\t = context.getUserPrincipal();\n\n\t\tString contextString = new String(\"AuthorizationContext: \");\n\t\tcontextString  = contextString + \"context.getResource()= \" + ((resource != null ) ? resource : \"\");\n\t\tcontextString  = contextString + \", solarParams= \" + (( solrParams != null ) ? solrParams : \"\");\n\t\tcontextString  = contextString + \", requestType= \" + (( requestType != null ) ? requestType : \"\");\n\t\tcontextString  = contextString + \", ranger.requestType= \" + ((accessType != null ) ? accessType : \"\");\n\t\tcontextString  = contextString + \", userPrincipal= \" + ((principal != null ) ? principal : \"\");\n\t\tcontextString  = contextString + \", userName= \"  + userName;\n\t\tcontextString  = contextString + \", groups= \" + groups;\n\t\tcontextString  = contextString + \", ipAddress= \" + ipAddress;\n\t\tcontextString  = contextString + \", collections= \" + collections;\n\t\tcontextString  = contextString + \", headers= \" + headers;\n\n\n\t\tlogger.info(\"AuthorizationContext: context.getResource()=\"\n\t\t\t\t+ context.getResource() + \", solarParams=\"\n\t\t\t\t+ context.getParams() + \", requestType=\"\n\t\t\t\t+ context.getRequestType() + \", ranger.requestType=\"\n\t\t\t\t+ mapToRangerAccessType(context) + \", userPrincipal=\"\n\t\t\t\t+ context.getUserPrincipal() + \", userName=\" + userName\n\t\t\t\t+ \", groups=\" + groups + \", ipAddress=\" + ipAddress\n\t\t\t\t+ \", collections=\" + collections + \", headers=\" + headers);\n\t} catch (Throwable t) {\n\t\tlogger.error(\"Error getting request context!!!\", t);\n\t}\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2478,
        "instruction": "public ApiSurface includingPackage(String packageName) throws IOException {\n  ClassPath classPath = ClassPath.from(ClassLoader.getSystemClassLoader());\n\n  Set<Class<?>> newRootClasses = Sets.newHashSet();\n  for (ClassInfo classInfo : classPath.getTopLevelClassesRecursive(packageName)) {\n    Class clazz = classInfo.load();\n    if (exposed(clazz.getModifiers())) {\n      newRootClasses.add(clazz);\n    }\n  }\n  newRootClasses.addAll(rootClasses);\n\n  return new ApiSurface(newRootClasses, patternsToPrune);\n}",
        "accept_response": "public ApiSurface includingPackage(String packageName) throws IOException {\n  ClassPath classPath = ClassPath.from(ClassLoader.getSystemClassLoader());\n\n  Set<Class<?>> newRootClasses = Sets.newHashSet();\n  for (ClassInfo classInfo : classPath.getTopLevelClassesRecursive(packageName)) {\n    Class clazz = classInfo.load();\n    if (exposed(clazz.getModifiers())) {\n      newRootClasses.add(clazz);\n    }\n  }\n  LOG.debug(\"Including package {} and subpackages: {}\", packageName, newRootClasses);\n  newRootClasses.addAll(rootClasses);\n\n  return new ApiSurface(newRootClasses, patternsToPrune);\n}",
        "reject_response": "public ApiSurface includingPackage(String packageName) throws IOException {\n  ClassPath classPath = ClassPath.from(ClassLoader.getSystemClassLoader());\n\n  Set<Class<?>> newRootClasses = Sets.newHashSet();\n  for (ClassInfo classInfo : classPath.getTopLevelClassesRecursive(packageName)) {\n    Class clazz = classInfo.load();\n    if (exposed(clazz.getModifiers())) {\n      newRootClasses.add(clazz);\n    }\n  }\n  logger.debug(\"Including package {} and subpackages: {}\", packageName, newRootClasses);\n  newRootClasses.addAll(rootClasses);\n\n  return new ApiSurface(newRootClasses, patternsToPrune);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3004,
        "instruction": "@Override\npublic void run() {\n\tif (LOG.isDebugEnabled()) {\n\t\tLOG.debug(\"==> ConsumerRunnable.run()\");\n\t}\n\n\twhile (true) {\n\n\t\ttry {\n\t\t\tList<AtlasKafkaMessage<EntityNotification>> newMessages = consumer.receive(MAX_WAIT_TIME_IN_MILLIS);\n\n\t\t\tif (newMessages.size() == 0) {\n\t\t\t\tif (CollectionUtils.isNotEmpty(atlasEntitiesWithTags)) {\n\t\t\t\t\tbuildAndUploadServiceTags();\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfor (AtlasKafkaMessage<EntityNotification> message : newMessages) {\n\t\t\t\t\tEntityNotification notification = message != null ? message.getMessage() : null;\n\n\t\t\t\t\tif (notification != null) {\n\t\t\t\t\t\tEntityNotificationWrapper notificationWrapper = null;\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tnotificationWrapper = new EntityNotificationWrapper(notification);\n\t\t\t\t\t\t} catch (Throwable e) {\n\t\t\t\t\t\t\tLOG.error(\"notification:[\" + notification + \"] has some issues..perhaps null entity??\", e);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif (notificationWrapper != null) {\n\t\t\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\t\t\tLOG.debug(\"Message-offset=\" + message.getOffset() + \", Notification=\" + getPrintableEntityNotification(notificationWrapper));\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tRangerAtlasEntityWithTags entityWithTags = new RangerAtlasEntityWithTags(notificationWrapper);\n\n\t\t\t\t\t\t\tif ((notificationWrapper.getIsEntityDeleteOp() && !isHandlingDeleteOps) || (!notificationWrapper.getIsEntityDeleteOp() && isHandlingDeleteOps)) {\n\t\t\t\t\t\t\t\tbuildAndUploadServiceTags();\n\t\t\t\t\t\t\t\tisHandlingDeleteOps = !isHandlingDeleteOps;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tatlasEntitiesWithTags.add(entityWithTags);\n\t\t\t\t\t\t\tmessages.add(message);\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\tLOG.error(\"Null entityNotification received from Kafka!! Ignoring..\");\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (CollectionUtils.isNotEmpty(atlasEntitiesWithTags) && atlasEntitiesWithTags.size() >= maxBatchSize) {\n\t\t\t\t\tbuildAndUploadServiceTags();\n\t\t\t\t}\n\t\t\t}\n\n\t\t} catch (Exception exception) {\n\t\t\tLOG.error(\"Caught exception..: \", exception);\n\t\t\t// If transient error, retry after short interval\n\t\t\ttry {\n\t\t\t\tThread.sleep(100);\n\t\t\t} catch (InterruptedException interrupted) {\n\t\t\t\tLOG.error(\"Interrupted: \", interrupted);\n\t\t\t\tLOG.error(\"Returning from thread. May cause process to be up but not processing events!!\");\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}\n}",
        "accept_response": "@Override\npublic void run() {\n\tif (LOG.isDebugEnabled()) {\n\t\tLOG.debug(\"==> ConsumerRunnable.run()\");\n\t}\n\n\twhile (true) {\n\n\t\ttry {\n\t\t\tList<AtlasKafkaMessage<EntityNotification>> newMessages = consumer.receive(MAX_WAIT_TIME_IN_MILLIS);\n\n\t\t\tif (newMessages.size() == 0) {\n\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\tLOG.debug(\"AtlasTagSource.ConsumerRunnable.run: no message from NotificationConsumer within \" + MAX_WAIT_TIME_IN_MILLIS + \" milliseconds\");\n\t\t\t\t}\n\t\t\t\tif (CollectionUtils.isNotEmpty(atlasEntitiesWithTags)) {\n\t\t\t\t\tbuildAndUploadServiceTags();\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfor (AtlasKafkaMessage<EntityNotification> message : newMessages) {\n\t\t\t\t\tEntityNotification notification = message != null ? message.getMessage() : null;\n\n\t\t\t\t\tif (notification != null) {\n\t\t\t\t\t\tEntityNotificationWrapper notificationWrapper = null;\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tnotificationWrapper = new EntityNotificationWrapper(notification);\n\t\t\t\t\t\t} catch (Throwable e) {\n\t\t\t\t\t\t\tLOG.error(\"notification:[\" + notification + \"] has some issues..perhaps null entity??\", e);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif (notificationWrapper != null) {\n\t\t\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\t\t\tLOG.debug(\"Message-offset=\" + message.getOffset() + \", Notification=\" + getPrintableEntityNotification(notificationWrapper));\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tRangerAtlasEntityWithTags entityWithTags = new RangerAtlasEntityWithTags(notificationWrapper);\n\n\t\t\t\t\t\t\tif ((notificationWrapper.getIsEntityDeleteOp() && !isHandlingDeleteOps) || (!notificationWrapper.getIsEntityDeleteOp() && isHandlingDeleteOps)) {\n\t\t\t\t\t\t\t\tbuildAndUploadServiceTags();\n\t\t\t\t\t\t\t\tisHandlingDeleteOps = !isHandlingDeleteOps;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tatlasEntitiesWithTags.add(entityWithTags);\n\t\t\t\t\t\t\tmessages.add(message);\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\tLOG.error(\"Null entityNotification received from Kafka!! Ignoring..\");\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (CollectionUtils.isNotEmpty(atlasEntitiesWithTags) && atlasEntitiesWithTags.size() >= maxBatchSize) {\n\t\t\t\t\tbuildAndUploadServiceTags();\n\t\t\t\t}\n\t\t\t}\n\n\t\t} catch (Exception exception) {\n\t\t\tLOG.error(\"Caught exception..: \", exception);\n\t\t\t// If transient error, retry after short interval\n\t\t\ttry {\n\t\t\t\tThread.sleep(100);\n\t\t\t} catch (InterruptedException interrupted) {\n\t\t\t\tLOG.error(\"Interrupted: \", interrupted);\n\t\t\t\tLOG.error(\"Returning from thread. May cause process to be up but not processing events!!\");\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}\n}",
        "reject_response": "@Override\npublic void run() {\n\tif (LOG.isDebugEnabled()) {\n\t\tLOG.debug(\"==> ConsumerRunnable.run()\");\n\t}\n\n\twhile (true) {\n\n\t\ttry {\n\t\t\tList<AtlasKafkaMessage<EntityNotification>> newMessages = consumer.receive(MAX_WAIT_TIME_IN_MILLIS);\n\n\t\t\tif (newMessages.size() == 0) {\n\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\tLOG.info(\"AtlasTagSource.ConsumerRunnable.run: no message from NotificationConsumer within \" + MAX_WAIT_TIME_IN_MILLIS + \" milliseconds\");\n\t\t\t\t}\n\t\t\t\tif (CollectionUtils.isNotEmpty(atlasEntitiesWithTags)) {\n\t\t\t\t\tbuildAndUploadServiceTags();\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfor (AtlasKafkaMessage<EntityNotification> message : newMessages) {\n\t\t\t\t\tEntityNotification notification = message != null ? message.getMessage() : null;\n\n\t\t\t\t\tif (notification != null) {\n\t\t\t\t\t\tEntityNotificationWrapper notificationWrapper = null;\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tnotificationWrapper = new EntityNotificationWrapper(notification);\n\t\t\t\t\t\t} catch (Throwable e) {\n\t\t\t\t\t\t\tLOG.error(\"notification:[\" + notification + \"] has some issues..perhaps null entity??\", e);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif (notificationWrapper != null) {\n\t\t\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\t\t\tLOG.debug(\"Message-offset=\" + message.getOffset() + \", Notification=\" + getPrintableEntityNotification(notificationWrapper));\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tRangerAtlasEntityWithTags entityWithTags = new RangerAtlasEntityWithTags(notificationWrapper);\n\n\t\t\t\t\t\t\tif ((notificationWrapper.getIsEntityDeleteOp() && !isHandlingDeleteOps) || (!notificationWrapper.getIsEntityDeleteOp() && isHandlingDeleteOps)) {\n\t\t\t\t\t\t\t\tbuildAndUploadServiceTags();\n\t\t\t\t\t\t\t\tisHandlingDeleteOps = !isHandlingDeleteOps;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tatlasEntitiesWithTags.add(entityWithTags);\n\t\t\t\t\t\t\tmessages.add(message);\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\tLOG.error(\"Null entityNotification received from Kafka!! Ignoring..\");\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (CollectionUtils.isNotEmpty(atlasEntitiesWithTags) && atlasEntitiesWithTags.size() >= maxBatchSize) {\n\t\t\t\t\tbuildAndUploadServiceTags();\n\t\t\t\t}\n\t\t\t}\n\n\t\t} catch (Exception exception) {\n\t\t\tLOG.error(\"Caught exception..: \", exception);\n\t\t\t// If transient error, retry after short interval\n\t\t\ttry {\n\t\t\t\tThread.sleep(100);\n\t\t\t} catch (InterruptedException interrupted) {\n\t\t\t\tLOG.error(\"Interrupted: \", interrupted);\n\t\t\t\tLOG.error(\"Returning from thread. May cause process to be up but not processing events!!\");\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2460,
        "instruction": "protected static Topology waitForTopology(final ActiveMQServer server,\n                                   final int liveNodes,\n                                   final int backupNodes,\n                                   final long timeout) throws Exception {\n\n   Set<ClusterConnection> ccs = server.getClusterManager().getClusterConnections();\n\n   if (ccs.size() != 1) {\n      throw new IllegalStateException(\"You need a single cluster connection on this version of waitForTopology on ServiceTestBase\");\n   }\n\n   Topology topology = server.getClusterManager().getDefaultConnection(null).getTopology();\n\n   return waitForTopology(topology, timeout, liveNodes, backupNodes);\n}",
        "accept_response": "protected static Topology waitForTopology(final ActiveMQServer server,\n                                   final int liveNodes,\n                                   final int backupNodes,\n                                   final long timeout) throws Exception {\n   logger.debug(\"waiting for {} on the topology for server = {}\", liveNodes, server);\n\n   Set<ClusterConnection> ccs = server.getClusterManager().getClusterConnections();\n\n   if (ccs.size() != 1) {\n      throw new IllegalStateException(\"You need a single cluster connection on this version of waitForTopology on ServiceTestBase\");\n   }\n\n   Topology topology = server.getClusterManager().getDefaultConnection(null).getTopology();\n\n   return waitForTopology(topology, timeout, liveNodes, backupNodes);\n}",
        "reject_response": "protected static Topology waitForTopology(final ActiveMQServer server,\n                                   final int liveNodes,\n                                   final int backupNodes,\n                                   final long timeout) throws Exception {\n   logger.debug(\"waiting for \" + liveNodes + \" on the topology for server = \" + server);\n\n   Set<ClusterConnection> ccs = server.getClusterManager().getClusterConnections();\n\n   if (ccs.size() != 1) {\n      throw new IllegalStateException(\"You need a single cluster connection on this version of waitForTopology on ServiceTestBase\");\n   }\n\n   Topology topology = server.getClusterManager().getDefaultConnection(null).getTopology();\n\n   return waitForTopology(topology, timeout, liveNodes, backupNodes);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2537,
        "instruction": "private void pushGiantRecord(int partitionId, byte[] buffer, int numBytes) throws IOException {\n  long pushStartTime = System.nanoTime();\n  int bytesWritten =\n      rssShuffleClient.pushData(\n          appId,\n          shuffleId,\n          mapId,\n          taskContext.attemptNumber(),\n          partitionId,\n          buffer,\n          0,\n          numBytes,\n          numMappers,\n          numPartitions);\n  mapStatusLengths[partitionId].add(bytesWritten);\n  writeMetrics.incBytesWritten(bytesWritten);\n  writeMetrics.incWriteTime(System.nanoTime() - pushStartTime);\n}",
        "accept_response": "private void pushGiantRecord(int partitionId, byte[] buffer, int numBytes) throws IOException {\n  logger.debug(\"Push giant record, size {}.\", Utils.bytesToString(numBytes));\n  long pushStartTime = System.nanoTime();\n  int bytesWritten =\n      rssShuffleClient.pushData(\n          appId,\n          shuffleId,\n          mapId,\n          taskContext.attemptNumber(),\n          partitionId,\n          buffer,\n          0,\n          numBytes,\n          numMappers,\n          numPartitions);\n  mapStatusLengths[partitionId].add(bytesWritten);\n  writeMetrics.incBytesWritten(bytesWritten);\n  writeMetrics.incWriteTime(System.nanoTime() - pushStartTime);\n}",
        "reject_response": "private void pushGiantRecord(int partitionId, byte[] buffer, int numBytes) throws IOException {\n  logger.debug(\"Push giant record, size {}.\", numBytes);\n  long pushStartTime = System.nanoTime();\n  int bytesWritten =\n      rssShuffleClient.pushData(\n          appId,\n          shuffleId,\n          mapId,\n          taskContext.attemptNumber(),\n          partitionId,\n          buffer,\n          0,\n          numBytes,\n          numMappers,\n          numPartitions);\n  mapStatusLengths[partitionId].add(bytesWritten);\n  writeMetrics.incBytesWritten(bytesWritten);\n  writeMetrics.incWriteTime(System.nanoTime() - pushStartTime);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2541,
        "instruction": "@SuppressWarnings(\"unchecked\")\n@Override\npublic void aroundWriteTo(WriterInterceptorContext c) throws IOException, WebApplicationException {\n\n    if (LOG.isLoggable(Level.FINE)) {\n        String writerClassName = writer == null ? \"null\" : writer.getClass().getName();\n    }\n\n    MultivaluedMap<String, Object> headers = c.getHeaders();\n    Object mtObject = headers.getFirst(HttpHeaders.CONTENT_TYPE);\n    MediaType entityMt = mtObject == null ? c.getMediaType() : JAXRSUtils.toMediaType(mtObject.toString());\n    m.put(Message.CONTENT_TYPE, entityMt.toString());\n\n    Class<?> entityCls = c.getType();\n    Type entityType = c.getGenericType();\n    Annotation[] entityAnns = c.getAnnotations();\n\n    if (writer == null\n        || m.get(ProviderFactory.PROVIDER_SELECTION_PROPERTY_CHANGED) == Boolean.TRUE\n        && !writer.isWriteable(entityCls, entityType, entityAnns, entityMt)) {\n\n        writer = (MessageBodyWriter<Object>)ProviderFactory.getInstance(m)\n            .createMessageBodyWriter(entityCls, entityType, entityAnns, entityMt, m);\n        if (writer == null) {\n            String errorMessage = JAXRSUtils.logMessageHandlerProblem(\"NO_MSG_WRITER\", entityCls, entityMt);\n            throw new ProcessingException(errorMessage);\n        }\n    }\n\n    HttpUtils.convertHeaderValuesToString(headers, true);\n\n    writer.writeTo(c.getEntity(),\n                   c.getType(),\n                   c.getGenericType(),\n                   c.getAnnotations(),\n                   entityMt,\n                   headers,\n                   c.getOutputStream());\n}",
        "accept_response": "@SuppressWarnings(\"unchecked\")\n@Override\npublic void aroundWriteTo(WriterInterceptorContext c) throws IOException, WebApplicationException {\n\n    if (LOG.isLoggable(Level.FINE)) {\n        String writerClassName = writer == null ? \"null\" : writer.getClass().getName();\n        LOG.fine(\"Response EntityProvider is: \" + writerClassName);\n    }\n\n    MultivaluedMap<String, Object> headers = c.getHeaders();\n    Object mtObject = headers.getFirst(HttpHeaders.CONTENT_TYPE);\n    MediaType entityMt = mtObject == null ? c.getMediaType() : JAXRSUtils.toMediaType(mtObject.toString());\n    m.put(Message.CONTENT_TYPE, entityMt.toString());\n\n    Class<?> entityCls = c.getType();\n    Type entityType = c.getGenericType();\n    Annotation[] entityAnns = c.getAnnotations();\n\n    if (writer == null\n        || m.get(ProviderFactory.PROVIDER_SELECTION_PROPERTY_CHANGED) == Boolean.TRUE\n        && !writer.isWriteable(entityCls, entityType, entityAnns, entityMt)) {\n\n        writer = (MessageBodyWriter<Object>)ProviderFactory.getInstance(m)\n            .createMessageBodyWriter(entityCls, entityType, entityAnns, entityMt, m);\n        if (writer == null) {\n            String errorMessage = JAXRSUtils.logMessageHandlerProblem(\"NO_MSG_WRITER\", entityCls, entityMt);\n            throw new ProcessingException(errorMessage);\n        }\n    }\n\n    HttpUtils.convertHeaderValuesToString(headers, true);\n\n    writer.writeTo(c.getEntity(),\n                   c.getType(),\n                   c.getGenericType(),\n                   c.getAnnotations(),\n                   entityMt,\n                   headers,\n                   c.getOutputStream());\n}",
        "reject_response": "@SuppressWarnings(\"unchecked\")\n@Override\npublic void aroundWriteTo(WriterInterceptorContext c) throws IOException, WebApplicationException {\n\n    if (LOG.isLoggable(Level.FINE)) {\n        String writerClassName = writer == null ? \"null\" : writer.getClass().getName();\n        LOG.fine(\"Response EntityProvider is: \" + writer.getClass().getName());\n    }\n\n    MultivaluedMap<String, Object> headers = c.getHeaders();\n    Object mtObject = headers.getFirst(HttpHeaders.CONTENT_TYPE);\n    MediaType entityMt = mtObject == null ? c.getMediaType() : JAXRSUtils.toMediaType(mtObject.toString());\n    m.put(Message.CONTENT_TYPE, entityMt.toString());\n\n    Class<?> entityCls = c.getType();\n    Type entityType = c.getGenericType();\n    Annotation[] entityAnns = c.getAnnotations();\n\n    if (writer == null\n        || m.get(ProviderFactory.PROVIDER_SELECTION_PROPERTY_CHANGED) == Boolean.TRUE\n        && !writer.isWriteable(entityCls, entityType, entityAnns, entityMt)) {\n\n        writer = (MessageBodyWriter<Object>)ProviderFactory.getInstance(m)\n            .createMessageBodyWriter(entityCls, entityType, entityAnns, entityMt, m);\n        if (writer == null) {\n            String errorMessage = JAXRSUtils.logMessageHandlerProblem(\"NO_MSG_WRITER\", entityCls, entityMt);\n            throw new ProcessingException(errorMessage);\n        }\n    }\n\n    HttpUtils.convertHeaderValuesToString(headers, true);\n\n    writer.writeTo(c.getEntity(),\n                   c.getType(),\n                   c.getGenericType(),\n                   c.getAnnotations(),\n                   entityMt,\n                   headers,\n                   c.getOutputStream());\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2969,
        "instruction": "private boolean compareAgainstBaseState(SegmentNodeState current, SegmentNodeState before, SegmentNodeBuilder builder) throws Exception {\n    while (true) {\n        try {\n            return current.compareAgainstBaseState(before, new StandbyDiff(builder, store, client, running));\n        } catch (SegmentNotFoundException e) {\n            copySegmentHierarchyFromPrimary(UUID.fromString(e.getSegmentId()));\n        }\n    }\n}",
        "accept_response": "private boolean compareAgainstBaseState(SegmentNodeState current, SegmentNodeState before, SegmentNodeBuilder builder) throws Exception {\n    while (true) {\n        try {\n            return current.compareAgainstBaseState(before, new StandbyDiff(builder, store, client, running));\n        } catch (SegmentNotFoundException e) {\n            log.debug(\"Found missing segment {}\", e.getSegmentId());\n            copySegmentHierarchyFromPrimary(UUID.fromString(e.getSegmentId()));\n        }\n    }\n}",
        "reject_response": "private boolean compareAgainstBaseState(SegmentNodeState current, SegmentNodeState before, SegmentNodeBuilder builder) throws Exception {\n    while (true) {\n        try {\n            return current.compareAgainstBaseState(before, new StandbyDiff(builder, store, client, running));\n        } catch (SegmentNotFoundException e) {\n            log.info(\"Found missing segment {}\", e.getSegmentId());\n            copySegmentHierarchyFromPrimary(UUID.fromString(e.getSegmentId()));\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2871,
        "instruction": "@Override\npublic final boolean hasNext()\n{\n    if ( finished )\n        // Even if aborted. Finished is finished.\n        return false ;\n\n    if ( requestingCancel && abortIterator )\n    {\n        // Try to close first to release resources (in case the user\n        // doesn't have a close() call in a finally block)\n        close() ;\n        throw new QueryCancelledException() ;\n    }\n\n    // Handles exceptions\n    boolean r = hasNextBinding() ;\n\n    if ( r == false )\n        try {\n            close() ;\n        } catch (QueryFatalException ex)\n        {\n            throw ex ;      // And pass on up the exception.\n        }\n    return r ;\n}",
        "accept_response": "@Override\npublic final boolean hasNext()\n{\n    if ( finished )\n        // Even if aborted. Finished is finished.\n        return false ;\n\n    if ( requestingCancel && abortIterator )\n    {\n        // Try to close first to release resources (in case the user\n        // doesn't have a close() call in a finally block)\n        close() ;\n        throw new QueryCancelledException() ;\n    }\n\n    // Handles exceptions\n    boolean r = hasNextBinding() ;\n\n    if ( r == false )\n        try {\n            close() ;\n        } catch (QueryFatalException ex)\n        {\n            Log.error(this, \"Fatal exception: \"+ex.getMessage() ) ;\n            throw ex ;      // And pass on up the exception.\n        }\n    return r ;\n}",
        "reject_response": "@Override\npublic final boolean hasNext()\n{\n    if ( finished )\n        // Even if aborted. Finished is finished.\n        return false ;\n\n    if ( requestingCancel && abortIterator )\n    {\n        // Try to close first to release resources (in case the user\n        // doesn't have a close() call in a finally block)\n        close() ;\n        throw new QueryCancelledException() ;\n    }\n\n    // Handles exceptions\n    boolean r = hasNextBinding() ;\n\n    if ( r == false )\n        try {\n            close() ;\n        } catch (QueryFatalException ex)\n        {\n            Log.fatal(this, \"Fatal exception: \"+ex.getMessage() ) ;\n            throw ex ;      // And pass on up the exception.\n        }\n    return r ;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3138,
        "instruction": "@Override\npublic void stopServices() {\n  Exception firstException = null;\n  List<AbstractService> stoppedServices = new ArrayList<>();\n  writeLock.lock();\n  try {\n    if (servicesInited.get()) {\n      for (AbstractService srvc : services) {\n        Exception ex = ServiceOperations.stopQuietly(srvc);\n        if (ex != null && firstException == null) {\n          LOG.warn(String.format(\n              \"Failed to stop service=(%s) for vertex name=(%s)\",\n              srvc.getName(), getName()), ex);\n          firstException = ex;\n        } else {\n          stoppedServices.add(srvc);\n        }\n      }\n      services.clear();\n    }\n    servicesInited.set(false);\n  } finally {\n    writeLock.unlock();\n  }\n  // wait for services to stop\n  for (AbstractService srvc : stoppedServices) {\n    srvc.waitForServiceToStop(60000L);\n  }\n  // After stopping all services, rethrow the first exception raised\n  if (firstException != null) {\n    throw ServiceStateException.convert(firstException);\n  }\n}",
        "accept_response": "@Override\npublic void stopServices() {\n  Exception firstException = null;\n  List<AbstractService> stoppedServices = new ArrayList<>();\n  writeLock.lock();\n  try {\n    if (servicesInited.get()) {\n      for (AbstractService srvc : services) {\n        LOG.debug(\"Stopping service : {}\", srvc);\n        Exception ex = ServiceOperations.stopQuietly(srvc);\n        if (ex != null && firstException == null) {\n          LOG.warn(String.format(\n              \"Failed to stop service=(%s) for vertex name=(%s)\",\n              srvc.getName(), getName()), ex);\n          firstException = ex;\n        } else {\n          stoppedServices.add(srvc);\n        }\n      }\n      services.clear();\n    }\n    servicesInited.set(false);\n  } finally {\n    writeLock.unlock();\n  }\n  // wait for services to stop\n  for (AbstractService srvc : stoppedServices) {\n    srvc.waitForServiceToStop(60000L);\n  }\n  // After stopping all services, rethrow the first exception raised\n  if (firstException != null) {\n    throw ServiceStateException.convert(firstException);\n  }\n}",
        "reject_response": "@Override\npublic void stopServices() {\n  Exception firstException = null;\n  List<AbstractService> stoppedServices = new ArrayList<>();\n  writeLock.lock();\n  try {\n    if (servicesInited.get()) {\n      for (AbstractService srvc : services) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Stopping service : \" + srvc);\n        }\n        Exception ex = ServiceOperations.stopQuietly(srvc);\n        if (ex != null && firstException == null) {\n          LOG.warn(String.format(\n              \"Failed to stop service=(%s) for vertex name=(%s)\",\n              srvc.getName(), getName()), ex);\n          firstException = ex;\n        } else {\n          stoppedServices.add(srvc);\n        }\n      }\n      services.clear();\n    }\n    servicesInited.set(false);\n  } finally {\n    writeLock.unlock();\n  }\n  // wait for services to stop\n  for (AbstractService srvc : stoppedServices) {\n    srvc.waitForServiceToStop(60000L);\n  }\n  // After stopping all services, rethrow the first exception raised\n  if (firstException != null) {\n    throw ServiceStateException.convert(firstException);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3143,
        "instruction": "boolean preemptIfNeeded() {\n  if (preemptionPercentage == 0) {\n    // turned off\n    return true;\n  }\n  ContainerId[] preemptedContainers = null;\n  int numPendingRequestsToService = 0;\n  synchronized (this) {\n    Resource freeResources = amRmClient.getAvailableResources();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(constructPreemptionPeriodicLog(freeResources));\n    } else {\n      if (numHeartbeats % 50 == 1) {\n        LOG.info(constructPreemptionPeriodicLog(freeResources));\n      }\n    }\n    assert freeResources.getMemory() >= 0;\n\n    CookieContainerRequest highestPriRequest = null;\n    int numHighestPriRequests = 0;\n    for(CookieContainerRequest request : taskRequests.values()) {\n      if(highestPriRequest == null) {\n        highestPriRequest = request;\n        numHighestPriRequests = 1;\n      } else if(isHigherPriority(request.getPriority(),\n                                   highestPriRequest.getPriority())){\n        highestPriRequest = request;\n        numHighestPriRequests = 1;\n      } else if (request.getPriority().equals(highestPriRequest.getPriority())) {\n        numHighestPriRequests++;\n      }\n    }\n\n    if (highestPriRequest == null) {\n      // nothing pending\n      resetHighestWaitingPriority(null);\n      return true;\n    }\n\n    // reset the wait time when waiting priority changes to prevent carry over of the value\n    if (highestWaitingRequestPriority == null ||\n        !highestPriRequest.getPriority().equals(highestWaitingRequestPriority)) {\n      resetHighestWaitingPriority(highestPriRequest.getPriority());\n    }\n\n    long currTime = System.currentTimeMillis();\n    if (highestWaitingRequestWaitStartTime == 0) {\n      highestWaitingRequestWaitStartTime = currTime;\n    }\n\n    boolean preemptionWaitDeadlineCrossed =\n        (currTime - highestWaitingRequestWaitStartTime) > preemptionMaxWaitTime ? true : false;\n\n    if(!preemptionWaitDeadlineCrossed &&\n        fitsIn(highestPriRequest.getCapability(), freeResources)) {\n      if (numHeartbeats % 50 == 1) {\n        LOG.info(highestPriRequest + \" fits in free resources\");\n      }\n      return true;\n    }\n\n    if (preemptionWaitDeadlineCrossed) {\n      // check if anything lower priority is running - priority inversion\n      // this check could have been done earlier but in the common case\n      // this would be unnecessary since there are usually requests pending\n      // in the normal case without priority inversion. So do this expensive\n      // iteration now\n      boolean lowerPriRunning = false;\n      for(Map.Entry<Object, Container> entry : taskAllocations.entrySet()) {\n        HeldContainer heldContainer = heldContainers.get(entry.getValue().getId());\n        CookieContainerRequest lastTaskInfo = heldContainer.getLastTaskInfo();\n        Priority taskPriority = lastTaskInfo.getPriority();\n        Object signature = lastTaskInfo.getCookie().getContainerSignature();\n        if(isHigherPriority(highestPriRequest.getPriority(), taskPriority)) {\n          // lower priority task is running\n          if (containerSignatureMatcher.isExactMatch(\n              highestPriRequest.getCookie().getContainerSignature(),\n              signature)) {\n            // exact match with different priorities\n            continue;\n          }\n          lowerPriRunning = true;\n          break;\n        }\n      }\n      if (!lowerPriRunning) {\n        // nothing lower priority running\n        // normal case of many pending request without priority inversion\n        resetHighestWaitingPriority(null);\n        return true;\n      }\n      LOG.info(\"Preemption deadline crossed at pri: \" + highestPriRequest.getPriority()\n          + \" numRequests: \" + numHighestPriRequests + \". \"\n          + constructPreemptionPeriodicLog(freeResources));\n    }\n\n    // highest priority request will not fit in existing free resources\n    // free up some more\n    // TODO this is subject to error wrt RM resource normalization\n\n    numPendingRequestsToService = scaleDownByPreemptionPercentage(numHighestPriRequests,\n        preemptionPercentage);\n\n    if (numPendingRequestsToService < 1) {\n      // nothing to preempt - reset preemption last heartbeat\n      return true;\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Trying to service \" + numPendingRequestsToService + \" out of total \"\n          + numHighestPriRequests + \" pending requests at pri: \"\n          + highestPriRequest.getPriority());\n    }\n\n    for (int i=0; i<numPendingRequestsToService; ++i) {\n      // This request must have been considered for matching with all existing\n      // containers when request was made.\n      Container lowestPriNewContainer = null;\n      // could not find anything to preempt. Check if we can release unused\n      // containers\n      for (HeldContainer heldContainer : delayedContainerManager.delayedContainers) {\n        if (!heldContainer.isNew()) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Reused container exists. Wait for assignment loop to release it. \"\n                + heldContainer.getContainer().getId());\n          }\n          return true;\n        }\n        if (heldContainer.geNumAssignmentAttempts() < 3) {\n          // we havent tried to assign this container at node/rack/ANY\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Brand new container. Wait for assignment loop to match it. \"\n                + heldContainer.getContainer().getId());\n          }\n          return true;\n        }\n        Container container = heldContainer.getContainer();\n        if (lowestPriNewContainer == null ||\n            isHigherPriority(lowestPriNewContainer.getPriority(), container.getPriority())){\n          // there is a lower priority new container\n          lowestPriNewContainer = container;\n        }\n      }\n\n      if (lowestPriNewContainer != null) {\n        LOG.info(\"Preempting new container: \" + lowestPriNewContainer.getId() +\n            \" with priority: \" + lowestPriNewContainer.getPriority() +\n            \" to free resource for request: \" + highestPriRequest +\n            \" . Current free resources: \" + freeResources);\n        numPendingRequestsToService--;\n        releaseUnassignedContainers(Collections.singletonList(lowestPriNewContainer));\n        // We are returning an unused resource back the RM. The RM thinks it\n        // has serviced our initial request and will not re-allocate this back\n        // to us anymore. So we need to ask for this again. If there is no\n        // outstanding request at that priority then its fine to not ask again.\n        // See TEZ-915 for more details\n        maybeRescheduleContainerAtPriority(lowestPriNewContainer.getPriority());\n\n        // come back and free more new containers if needed\n        continue;\n      }\n    }\n\n    if (numPendingRequestsToService < 1) {\n      return true;\n    }\n\n    // there are no reused or new containers to release. try to preempt running containers\n    // this assert will be a no-op in production but can help identify\n    // invalid assumptions during testing\n    assert delayedContainerManager.delayedContainers.isEmpty();\n    if (!delayedContainerManager.delayedContainers.isEmpty()) {\n      LOG.warn(\"Expected delayed containers to be empty. \"\n          + constructPreemptionPeriodicLog(freeResources));\n    }\n\n    Priority preemptedTaskPriority = null;\n    int numEntriesAtPreemptedPriority = 0;\n    for(Map.Entry<Object, Container> entry : taskAllocations.entrySet()) {\n      HeldContainer heldContainer = heldContainers.get(entry.getValue().getId());\n      CookieContainerRequest lastTaskInfo = heldContainer.getLastTaskInfo();\n      Priority taskPriority = lastTaskInfo.getPriority();\n      Object signature = lastTaskInfo.getCookie().getContainerSignature();\n      if(!isHigherPriority(highestPriRequest.getPriority(), taskPriority)) {\n        // higher or same priority\n        continue;\n      }\n      if (containerSignatureMatcher.isExactMatch(\n          highestPriRequest.getCookie().getContainerSignature(),\n          signature)) {\n        // exact match with different priorities\n        continue;\n      }\n      if (preemptedTaskPriority == null ||\n          !isHigherPriority(taskPriority, preemptedTaskPriority)) {\n        // keep the lower priority\n        if (taskPriority.equals(preemptedTaskPriority)) {\n          numEntriesAtPreemptedPriority++;\n        } else {\n          // this is at a lower priority than existing\n          numEntriesAtPreemptedPriority = 1;\n        }\n        preemptedTaskPriority = taskPriority;\n      }\n    }\n    if(preemptedTaskPriority != null) {\n      int newNumPendingRequestsToService = scaleDownByPreemptionPercentage(Math.min(\n          numEntriesAtPreemptedPriority, numHighestPriRequests), preemptionPercentage);\n      numPendingRequestsToService = Math.min(newNumPendingRequestsToService,\n          numPendingRequestsToService);\n      if (numPendingRequestsToService < 1) {\n        return true;\n      }\n      // wait for enough heartbeats since this request became active for preemption\n      if ((numHeartbeats - heartbeatAtLastPreemption) < numHeartbeatsBetweenPreemptions) {\n        // stop incrementing lastpreemption heartbeat count\n        return false;\n      }\n      LOG.info(\"Trying to service \" + numPendingRequestsToService + \" out of total \"\n          + numHighestPriRequests + \" pending requests at pri: \"\n          + highestPriRequest.getPriority() + \" by preempting from \"\n          + numEntriesAtPreemptedPriority + \" running tasks at priority: \" + preemptedTaskPriority);\n      // found something to preempt. get others of the same priority\n      preemptedContainers = new ContainerId[numPendingRequestsToService];\n      int currIndex = 0;\n      for (Map.Entry<Object, Container> entry : taskAllocations.entrySet()) {\n        HeldContainer heldContainer = heldContainers.get(entry.getValue().getId());\n        CookieContainerRequest lastTaskInfo = heldContainer.getLastTaskInfo();\n        Priority taskPriority = lastTaskInfo.getPriority();\n        Container container = entry.getValue();\n        if (preemptedTaskPriority.equals(taskPriority)) {\n          // taskAllocations map will iterate from oldest to newest assigned containers\n          // keep the N newest containersIds with the matching priority\n          preemptedContainers[currIndex++ % numPendingRequestsToService] = container.getId();\n        }\n      }\n      // app client will be notified when after container is killed\n      // and we get its completed container status\n    }\n  }\n\n  // upcall outside locks\n  if (preemptedContainers != null) {\n    for(int i=0; i<numPendingRequestsToService; ++i) {\n      ContainerId cId = preemptedContainers[i];\n      if (cId != null) {\n        LOG.info(\"Preempting container: \" + cId + \" currently allocated to a task.\");\n        getContext().preemptContainer(cId);\n      }\n    }\n  }\n  return true;\n}",
        "accept_response": "boolean preemptIfNeeded() {\n  if (preemptionPercentage == 0) {\n    // turned off\n    return true;\n  }\n  ContainerId[] preemptedContainers = null;\n  int numPendingRequestsToService = 0;\n  synchronized (this) {\n    Resource freeResources = amRmClient.getAvailableResources();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(constructPreemptionPeriodicLog(freeResources));\n    } else {\n      if (numHeartbeats % 50 == 1) {\n        LOG.info(constructPreemptionPeriodicLog(freeResources));\n      }\n    }\n    assert freeResources.getMemory() >= 0;\n\n    CookieContainerRequest highestPriRequest = null;\n    int numHighestPriRequests = 0;\n    for(CookieContainerRequest request : taskRequests.values()) {\n      if(highestPriRequest == null) {\n        highestPriRequest = request;\n        numHighestPriRequests = 1;\n      } else if(isHigherPriority(request.getPriority(),\n                                   highestPriRequest.getPriority())){\n        highestPriRequest = request;\n        numHighestPriRequests = 1;\n      } else if (request.getPriority().equals(highestPriRequest.getPriority())) {\n        numHighestPriRequests++;\n      }\n    }\n\n    if (highestPriRequest == null) {\n      // nothing pending\n      resetHighestWaitingPriority(null);\n      return true;\n    }\n\n    // reset the wait time when waiting priority changes to prevent carry over of the value\n    if (highestWaitingRequestPriority == null ||\n        !highestPriRequest.getPriority().equals(highestWaitingRequestPriority)) {\n      resetHighestWaitingPriority(highestPriRequest.getPriority());\n    }\n\n    long currTime = System.currentTimeMillis();\n    if (highestWaitingRequestWaitStartTime == 0) {\n      highestWaitingRequestWaitStartTime = currTime;\n    }\n\n    boolean preemptionWaitDeadlineCrossed =\n        (currTime - highestWaitingRequestWaitStartTime) > preemptionMaxWaitTime ? true : false;\n\n    if(!preemptionWaitDeadlineCrossed &&\n        fitsIn(highestPriRequest.getCapability(), freeResources)) {\n      LOG.debug(\"{} fits in free resources\", highestPriRequest);\n      if (numHeartbeats % 50 == 1) {\n        LOG.info(highestPriRequest + \" fits in free resources\");\n      }\n      return true;\n    }\n\n    if (preemptionWaitDeadlineCrossed) {\n      // check if anything lower priority is running - priority inversion\n      // this check could have been done earlier but in the common case\n      // this would be unnecessary since there are usually requests pending\n      // in the normal case without priority inversion. So do this expensive\n      // iteration now\n      boolean lowerPriRunning = false;\n      for(Map.Entry<Object, Container> entry : taskAllocations.entrySet()) {\n        HeldContainer heldContainer = heldContainers.get(entry.getValue().getId());\n        CookieContainerRequest lastTaskInfo = heldContainer.getLastTaskInfo();\n        Priority taskPriority = lastTaskInfo.getPriority();\n        Object signature = lastTaskInfo.getCookie().getContainerSignature();\n        if(isHigherPriority(highestPriRequest.getPriority(), taskPriority)) {\n          // lower priority task is running\n          if (containerSignatureMatcher.isExactMatch(\n              highestPriRequest.getCookie().getContainerSignature(),\n              signature)) {\n            // exact match with different priorities\n            continue;\n          }\n          lowerPriRunning = true;\n          break;\n        }\n      }\n      if (!lowerPriRunning) {\n        // nothing lower priority running\n        // normal case of many pending request without priority inversion\n        resetHighestWaitingPriority(null);\n        return true;\n      }\n      LOG.info(\"Preemption deadline crossed at pri: \" + highestPriRequest.getPriority()\n          + \" numRequests: \" + numHighestPriRequests + \". \"\n          + constructPreemptionPeriodicLog(freeResources));\n    }\n\n    // highest priority request will not fit in existing free resources\n    // free up some more\n    // TODO this is subject to error wrt RM resource normalization\n\n    numPendingRequestsToService = scaleDownByPreemptionPercentage(numHighestPriRequests,\n        preemptionPercentage);\n\n    if (numPendingRequestsToService < 1) {\n      // nothing to preempt - reset preemption last heartbeat\n      return true;\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Trying to service \" + numPendingRequestsToService + \" out of total \"\n          + numHighestPriRequests + \" pending requests at pri: \"\n          + highestPriRequest.getPriority());\n    }\n\n    for (int i=0; i<numPendingRequestsToService; ++i) {\n      // This request must have been considered for matching with all existing\n      // containers when request was made.\n      Container lowestPriNewContainer = null;\n      // could not find anything to preempt. Check if we can release unused\n      // containers\n      for (HeldContainer heldContainer : delayedContainerManager.delayedContainers) {\n        if (!heldContainer.isNew()) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Reused container exists. Wait for assignment loop to release it. \"\n                + heldContainer.getContainer().getId());\n          }\n          return true;\n        }\n        if (heldContainer.geNumAssignmentAttempts() < 3) {\n          // we havent tried to assign this container at node/rack/ANY\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Brand new container. Wait for assignment loop to match it. \"\n                + heldContainer.getContainer().getId());\n          }\n          return true;\n        }\n        Container container = heldContainer.getContainer();\n        if (lowestPriNewContainer == null ||\n            isHigherPriority(lowestPriNewContainer.getPriority(), container.getPriority())){\n          // there is a lower priority new container\n          lowestPriNewContainer = container;\n        }\n      }\n\n      if (lowestPriNewContainer != null) {\n        LOG.info(\"Preempting new container: \" + lowestPriNewContainer.getId() +\n            \" with priority: \" + lowestPriNewContainer.getPriority() +\n            \" to free resource for request: \" + highestPriRequest +\n            \" . Current free resources: \" + freeResources);\n        numPendingRequestsToService--;\n        releaseUnassignedContainers(Collections.singletonList(lowestPriNewContainer));\n        // We are returning an unused resource back the RM. The RM thinks it\n        // has serviced our initial request and will not re-allocate this back\n        // to us anymore. So we need to ask for this again. If there is no\n        // outstanding request at that priority then its fine to not ask again.\n        // See TEZ-915 for more details\n        maybeRescheduleContainerAtPriority(lowestPriNewContainer.getPriority());\n\n        // come back and free more new containers if needed\n        continue;\n      }\n    }\n\n    if (numPendingRequestsToService < 1) {\n      return true;\n    }\n\n    // there are no reused or new containers to release. try to preempt running containers\n    // this assert will be a no-op in production but can help identify\n    // invalid assumptions during testing\n    assert delayedContainerManager.delayedContainers.isEmpty();\n    if (!delayedContainerManager.delayedContainers.isEmpty()) {\n      LOG.warn(\"Expected delayed containers to be empty. \"\n          + constructPreemptionPeriodicLog(freeResources));\n    }\n\n    Priority preemptedTaskPriority = null;\n    int numEntriesAtPreemptedPriority = 0;\n    for(Map.Entry<Object, Container> entry : taskAllocations.entrySet()) {\n      HeldContainer heldContainer = heldContainers.get(entry.getValue().getId());\n      CookieContainerRequest lastTaskInfo = heldContainer.getLastTaskInfo();\n      Priority taskPriority = lastTaskInfo.getPriority();\n      Object signature = lastTaskInfo.getCookie().getContainerSignature();\n      if(!isHigherPriority(highestPriRequest.getPriority(), taskPriority)) {\n        // higher or same priority\n        continue;\n      }\n      if (containerSignatureMatcher.isExactMatch(\n          highestPriRequest.getCookie().getContainerSignature(),\n          signature)) {\n        // exact match with different priorities\n        continue;\n      }\n      if (preemptedTaskPriority == null ||\n          !isHigherPriority(taskPriority, preemptedTaskPriority)) {\n        // keep the lower priority\n        if (taskPriority.equals(preemptedTaskPriority)) {\n          numEntriesAtPreemptedPriority++;\n        } else {\n          // this is at a lower priority than existing\n          numEntriesAtPreemptedPriority = 1;\n        }\n        preemptedTaskPriority = taskPriority;\n      }\n    }\n    if(preemptedTaskPriority != null) {\n      int newNumPendingRequestsToService = scaleDownByPreemptionPercentage(Math.min(\n          numEntriesAtPreemptedPriority, numHighestPriRequests), preemptionPercentage);\n      numPendingRequestsToService = Math.min(newNumPendingRequestsToService,\n          numPendingRequestsToService);\n      if (numPendingRequestsToService < 1) {\n        return true;\n      }\n      // wait for enough heartbeats since this request became active for preemption\n      if ((numHeartbeats - heartbeatAtLastPreemption) < numHeartbeatsBetweenPreemptions) {\n        // stop incrementing lastpreemption heartbeat count\n        return false;\n      }\n      LOG.info(\"Trying to service \" + numPendingRequestsToService + \" out of total \"\n          + numHighestPriRequests + \" pending requests at pri: \"\n          + highestPriRequest.getPriority() + \" by preempting from \"\n          + numEntriesAtPreemptedPriority + \" running tasks at priority: \" + preemptedTaskPriority);\n      // found something to preempt. get others of the same priority\n      preemptedContainers = new ContainerId[numPendingRequestsToService];\n      int currIndex = 0;\n      for (Map.Entry<Object, Container> entry : taskAllocations.entrySet()) {\n        HeldContainer heldContainer = heldContainers.get(entry.getValue().getId());\n        CookieContainerRequest lastTaskInfo = heldContainer.getLastTaskInfo();\n        Priority taskPriority = lastTaskInfo.getPriority();\n        Container container = entry.getValue();\n        if (preemptedTaskPriority.equals(taskPriority)) {\n          // taskAllocations map will iterate from oldest to newest assigned containers\n          // keep the N newest containersIds with the matching priority\n          preemptedContainers[currIndex++ % numPendingRequestsToService] = container.getId();\n        }\n      }\n      // app client will be notified when after container is killed\n      // and we get its completed container status\n    }\n  }\n\n  // upcall outside locks\n  if (preemptedContainers != null) {\n    for(int i=0; i<numPendingRequestsToService; ++i) {\n      ContainerId cId = preemptedContainers[i];\n      if (cId != null) {\n        LOG.info(\"Preempting container: \" + cId + \" currently allocated to a task.\");\n        getContext().preemptContainer(cId);\n      }\n    }\n  }\n  return true;\n}",
        "reject_response": "boolean preemptIfNeeded() {\n  if (preemptionPercentage == 0) {\n    // turned off\n    return true;\n  }\n  ContainerId[] preemptedContainers = null;\n  int numPendingRequestsToService = 0;\n  synchronized (this) {\n    Resource freeResources = amRmClient.getAvailableResources();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(constructPreemptionPeriodicLog(freeResources));\n    } else {\n      if (numHeartbeats % 50 == 1) {\n        LOG.info(constructPreemptionPeriodicLog(freeResources));\n      }\n    }\n    assert freeResources.getMemory() >= 0;\n\n    CookieContainerRequest highestPriRequest = null;\n    int numHighestPriRequests = 0;\n    for(CookieContainerRequest request : taskRequests.values()) {\n      if(highestPriRequest == null) {\n        highestPriRequest = request;\n        numHighestPriRequests = 1;\n      } else if(isHigherPriority(request.getPriority(),\n                                   highestPriRequest.getPriority())){\n        highestPriRequest = request;\n        numHighestPriRequests = 1;\n      } else if (request.getPriority().equals(highestPriRequest.getPriority())) {\n        numHighestPriRequests++;\n      }\n    }\n\n    if (highestPriRequest == null) {\n      // nothing pending\n      resetHighestWaitingPriority(null);\n      return true;\n    }\n\n    // reset the wait time when waiting priority changes to prevent carry over of the value\n    if (highestWaitingRequestPriority == null ||\n        !highestPriRequest.getPriority().equals(highestWaitingRequestPriority)) {\n      resetHighestWaitingPriority(highestPriRequest.getPriority());\n    }\n\n    long currTime = System.currentTimeMillis();\n    if (highestWaitingRequestWaitStartTime == 0) {\n      highestWaitingRequestWaitStartTime = currTime;\n    }\n\n    boolean preemptionWaitDeadlineCrossed =\n        (currTime - highestWaitingRequestWaitStartTime) > preemptionMaxWaitTime ? true : false;\n\n    if(!preemptionWaitDeadlineCrossed &&\n        fitsIn(highestPriRequest.getCapability(), freeResources)) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(highestPriRequest + \" fits in free resources\");\n      } else {\n        if (numHeartbeats % 50 == 1) {\n          LOG.info(highestPriRequest + \" fits in free resources\");\n        }\n      if (numHeartbeats % 50 == 1) {\n        LOG.info(highestPriRequest + \" fits in free resources\");\n      }\n      return true;\n    }\n\n    if (preemptionWaitDeadlineCrossed) {\n      // check if anything lower priority is running - priority inversion\n      // this check could have been done earlier but in the common case\n      // this would be unnecessary since there are usually requests pending\n      // in the normal case without priority inversion. So do this expensive\n      // iteration now\n      boolean lowerPriRunning = false;\n      for(Map.Entry<Object, Container> entry : taskAllocations.entrySet()) {\n        HeldContainer heldContainer = heldContainers.get(entry.getValue().getId());\n        CookieContainerRequest lastTaskInfo = heldContainer.getLastTaskInfo();\n        Priority taskPriority = lastTaskInfo.getPriority();\n        Object signature = lastTaskInfo.getCookie().getContainerSignature();\n        if(isHigherPriority(highestPriRequest.getPriority(), taskPriority)) {\n          // lower priority task is running\n          if (containerSignatureMatcher.isExactMatch(\n              highestPriRequest.getCookie().getContainerSignature(),\n              signature)) {\n            // exact match with different priorities\n            continue;\n          }\n          lowerPriRunning = true;\n          break;\n        }\n      }\n      if (!lowerPriRunning) {\n        // nothing lower priority running\n        // normal case of many pending request without priority inversion\n        resetHighestWaitingPriority(null);\n        return true;\n      }\n      LOG.info(\"Preemption deadline crossed at pri: \" + highestPriRequest.getPriority()\n          + \" numRequests: \" + numHighestPriRequests + \". \"\n          + constructPreemptionPeriodicLog(freeResources));\n    }\n\n    // highest priority request will not fit in existing free resources\n    // free up some more\n    // TODO this is subject to error wrt RM resource normalization\n\n    numPendingRequestsToService = scaleDownByPreemptionPercentage(numHighestPriRequests,\n        preemptionPercentage);\n\n    if (numPendingRequestsToService < 1) {\n      // nothing to preempt - reset preemption last heartbeat\n      return true;\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Trying to service \" + numPendingRequestsToService + \" out of total \"\n          + numHighestPriRequests + \" pending requests at pri: \"\n          + highestPriRequest.getPriority());\n    }\n\n    for (int i=0; i<numPendingRequestsToService; ++i) {\n      // This request must have been considered for matching with all existing\n      // containers when request was made.\n      Container lowestPriNewContainer = null;\n      // could not find anything to preempt. Check if we can release unused\n      // containers\n      for (HeldContainer heldContainer : delayedContainerManager.delayedContainers) {\n        if (!heldContainer.isNew()) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Reused container exists. Wait for assignment loop to release it. \"\n                + heldContainer.getContainer().getId());\n          }\n          return true;\n        }\n        if (heldContainer.geNumAssignmentAttempts() < 3) {\n          // we havent tried to assign this container at node/rack/ANY\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Brand new container. Wait for assignment loop to match it. \"\n                + heldContainer.getContainer().getId());\n          }\n          return true;\n        }\n        Container container = heldContainer.getContainer();\n        if (lowestPriNewContainer == null ||\n            isHigherPriority(lowestPriNewContainer.getPriority(), container.getPriority())){\n          // there is a lower priority new container\n          lowestPriNewContainer = container;\n        }\n      }\n\n      if (lowestPriNewContainer != null) {\n        LOG.info(\"Preempting new container: \" + lowestPriNewContainer.getId() +\n            \" with priority: \" + lowestPriNewContainer.getPriority() +\n            \" to free resource for request: \" + highestPriRequest +\n            \" . Current free resources: \" + freeResources);\n        numPendingRequestsToService--;\n        releaseUnassignedContainers(Collections.singletonList(lowestPriNewContainer));\n        // We are returning an unused resource back the RM. The RM thinks it\n        // has serviced our initial request and will not re-allocate this back\n        // to us anymore. So we need to ask for this again. If there is no\n        // outstanding request at that priority then its fine to not ask again.\n        // See TEZ-915 for more details\n        maybeRescheduleContainerAtPriority(lowestPriNewContainer.getPriority());\n\n        // come back and free more new containers if needed\n        continue;\n      }\n    }\n\n    if (numPendingRequestsToService < 1) {\n      return true;\n    }\n\n    // there are no reused or new containers to release. try to preempt running containers\n    // this assert will be a no-op in production but can help identify\n    // invalid assumptions during testing\n    assert delayedContainerManager.delayedContainers.isEmpty();\n    if (!delayedContainerManager.delayedContainers.isEmpty()) {\n      LOG.warn(\"Expected delayed containers to be empty. \"\n          + constructPreemptionPeriodicLog(freeResources));\n    }\n\n    Priority preemptedTaskPriority = null;\n    int numEntriesAtPreemptedPriority = 0;\n    for(Map.Entry<Object, Container> entry : taskAllocations.entrySet()) {\n      HeldContainer heldContainer = heldContainers.get(entry.getValue().getId());\n      CookieContainerRequest lastTaskInfo = heldContainer.getLastTaskInfo();\n      Priority taskPriority = lastTaskInfo.getPriority();\n      Object signature = lastTaskInfo.getCookie().getContainerSignature();\n      if(!isHigherPriority(highestPriRequest.getPriority(), taskPriority)) {\n        // higher or same priority\n        continue;\n      }\n      if (containerSignatureMatcher.isExactMatch(\n          highestPriRequest.getCookie().getContainerSignature(),\n          signature)) {\n        // exact match with different priorities\n        continue;\n      }\n      if (preemptedTaskPriority == null ||\n          !isHigherPriority(taskPriority, preemptedTaskPriority)) {\n        // keep the lower priority\n        if (taskPriority.equals(preemptedTaskPriority)) {\n          numEntriesAtPreemptedPriority++;\n        } else {\n          // this is at a lower priority than existing\n          numEntriesAtPreemptedPriority = 1;\n        }\n        preemptedTaskPriority = taskPriority;\n      }\n    }\n    if(preemptedTaskPriority != null) {\n      int newNumPendingRequestsToService = scaleDownByPreemptionPercentage(Math.min(\n          numEntriesAtPreemptedPriority, numHighestPriRequests), preemptionPercentage);\n      numPendingRequestsToService = Math.min(newNumPendingRequestsToService,\n          numPendingRequestsToService);\n      if (numPendingRequestsToService < 1) {\n        return true;\n      }\n      // wait for enough heartbeats since this request became active for preemption\n      if ((numHeartbeats - heartbeatAtLastPreemption) < numHeartbeatsBetweenPreemptions) {\n        // stop incrementing lastpreemption heartbeat count\n        return false;\n      }\n      LOG.info(\"Trying to service \" + numPendingRequestsToService + \" out of total \"\n          + numHighestPriRequests + \" pending requests at pri: \"\n          + highestPriRequest.getPriority() + \" by preempting from \"\n          + numEntriesAtPreemptedPriority + \" running tasks at priority: \" + preemptedTaskPriority);\n      // found something to preempt. get others of the same priority\n      preemptedContainers = new ContainerId[numPendingRequestsToService];\n      int currIndex = 0;\n      for (Map.Entry<Object, Container> entry : taskAllocations.entrySet()) {\n        HeldContainer heldContainer = heldContainers.get(entry.getValue().getId());\n        CookieContainerRequest lastTaskInfo = heldContainer.getLastTaskInfo();\n        Priority taskPriority = lastTaskInfo.getPriority();\n        Container container = entry.getValue();\n        if (preemptedTaskPriority.equals(taskPriority)) {\n          // taskAllocations map will iterate from oldest to newest assigned containers\n          // keep the N newest containersIds with the matching priority\n          preemptedContainers[currIndex++ % numPendingRequestsToService] = container.getId();\n        }\n      }\n      // app client will be notified when after container is killed\n      // and we get its completed container status\n    }\n  }\n\n  // upcall outside locks\n  if (preemptedContainers != null) {\n    for(int i=0; i<numPendingRequestsToService; ++i) {\n      ContainerId cId = preemptedContainers[i];\n      if (cId != null) {\n        LOG.info(\"Preempting container: \" + cId + \" currently allocated to a task.\");\n        getContext().preemptContainer(cId);\n      }\n    }\n  }\n  return true;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2888,
        "instruction": "@SuppressWarnings(\"unused\")\npublic int size() {\n\n  int count=0;\n\n  for (ArpaNgram ngram : this) {\n    count++;\n  }\n  LOG.debug(\"Done counting n-grams in ARPA file\");\n\n  return count;\n}",
        "accept_response": "@SuppressWarnings(\"unused\")\npublic int size() {\n\n  LOG.debug(\"Counting n-grams in ARPA file\");\n  int count=0;\n\n  for (ArpaNgram ngram : this) {\n    count++;\n  }\n  LOG.debug(\"Done counting n-grams in ARPA file\");\n\n  return count;\n}",
        "reject_response": "@SuppressWarnings(\"unused\")\npublic int size() {\n\n  logger.fine(\"Counting n-grams in ARPA file\");\n  int count=0;\n\n  for (ArpaNgram ngram : this) {\n    count++;\n  }\n  LOG.debug(\"Done counting n-grams in ARPA file\");\n\n  return count;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2461,
        "instruction": "protected void waitForTopology(final ActiveMQServer server,\n                               String clusterConnectionName,\n                               final int nodes,\n                               final long timeout) throws Exception {\n\n   long start = System.currentTimeMillis();\n\n   ClusterConnection clusterConnection = server.getClusterManager().getClusterConnection(clusterConnectionName);\n   Topology topology = clusterConnection.getTopology();\n\n   do {\n      if (nodes == topology.getMembers().size()) {\n         return;\n      }\n\n      Thread.sleep(10);\n   }\n   while (System.currentTimeMillis() - start < timeout);\n\n   String msg = \"Timed out waiting for cluster topology of \" + nodes +\n      \" (received \" +\n      topology.getMembers().size() +\n      \") topology = \" +\n      topology +\n      \")\";\n\n   logger.error(msg);\n\n   throw new Exception(msg);\n}",
        "accept_response": "protected void waitForTopology(final ActiveMQServer server,\n                               String clusterConnectionName,\n                               final int nodes,\n                               final long timeout) throws Exception {\n   logger.debug(\"waiting for {} on the topology for server = {}\", nodes, server);\n\n   long start = System.currentTimeMillis();\n\n   ClusterConnection clusterConnection = server.getClusterManager().getClusterConnection(clusterConnectionName);\n   Topology topology = clusterConnection.getTopology();\n\n   do {\n      if (nodes == topology.getMembers().size()) {\n         return;\n      }\n\n      Thread.sleep(10);\n   }\n   while (System.currentTimeMillis() - start < timeout);\n\n   String msg = \"Timed out waiting for cluster topology of \" + nodes +\n      \" (received \" +\n      topology.getMembers().size() +\n      \") topology = \" +\n      topology +\n      \")\";\n\n   logger.error(msg);\n\n   throw new Exception(msg);\n}",
        "reject_response": "protected void waitForTopology(final ActiveMQServer server,\n                               String clusterConnectionName,\n                               final int nodes,\n                               final long timeout) throws Exception {\n   logger.debug(\"waiting for \" + nodes + \" on the topology for server = \" + server);\n\n   long start = System.currentTimeMillis();\n\n   ClusterConnection clusterConnection = server.getClusterManager().getClusterConnection(clusterConnectionName);\n   Topology topology = clusterConnection.getTopology();\n\n   do {\n      if (nodes == topology.getMembers().size()) {\n         return;\n      }\n\n      Thread.sleep(10);\n   }\n   while (System.currentTimeMillis() - start < timeout);\n\n   String msg = \"Timed out waiting for cluster topology of \" + nodes +\n      \" (received \" +\n      topology.getMembers().size() +\n      \") topology = \" +\n      topology +\n      \")\";\n\n   logger.error(msg);\n\n   throw new Exception(msg);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3001,
        "instruction": "public boolean addResourceIfReadable(String aResourceName) {\n\tboolean ret = false;\n\n\tif(LOG.isDebugEnabled()) {\n\t\tLOG.debug(\"==> addResourceIfReadable(\" + aResourceName + \")\");\n\t}\n\n\tURL fUrl = getFileLocation(aResourceName);\n\tif (fUrl != null) {\n\t\ttry {\n\t\t\taddResource(fUrl);\n\t\t\tret = true;\n\t\t} catch (Exception e) {\n\t\t\tLOG.error(\"Unable to load the resource name [\" + aResourceName + \"]. Ignoring the resource:\" + fUrl);\n\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\tLOG.debug(\"Resource loading failed for \" + fUrl, e);\n\t\t\t}\n\t\t}\n\t} else {\n\t\tLOG.error(\"addResourceIfReadable(\" + aResourceName + \"): couldn't find resource file location\");\n\t}\n\n\tif(LOG.isDebugEnabled()) {\n\t\tLOG.debug(\"<== addResourceIfReadable(\" + aResourceName + \"), result=\" + ret);\n\t}\n\treturn ret;\n}",
        "accept_response": "public boolean addResourceIfReadable(String aResourceName) {\n\tboolean ret = false;\n\n\tif(LOG.isDebugEnabled()) {\n\t\tLOG.debug(\"==> addResourceIfReadable(\" + aResourceName + \")\");\n\t}\n\n\tURL fUrl = getFileLocation(aResourceName);\n\tif (fUrl != null) {\n\t\tLOG.info(\"addResourceIfReadable(\" + aResourceName + \"): resource file is \" + fUrl);\n\t\ttry {\n\t\t\taddResource(fUrl);\n\t\t\tret = true;\n\t\t} catch (Exception e) {\n\t\t\tLOG.error(\"Unable to load the resource name [\" + aResourceName + \"]. Ignoring the resource:\" + fUrl);\n\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\tLOG.debug(\"Resource loading failed for \" + fUrl, e);\n\t\t\t}\n\t\t}\n\t} else {\n\t\tLOG.error(\"addResourceIfReadable(\" + aResourceName + \"): couldn't find resource file location\");\n\t}\n\n\tif(LOG.isDebugEnabled()) {\n\t\tLOG.debug(\"<== addResourceIfReadable(\" + aResourceName + \"), result=\" + ret);\n\t}\n\treturn ret;\n}",
        "reject_response": "public boolean addResourceIfReadable(String aResourceName) {\n\tboolean ret = false;\n\n\tif(LOG.isDebugEnabled()) {\n\t\tLOG.debug(\"==> addResourceIfReadable(\" + aResourceName + \")\");\n\t}\n\n\tURL fUrl = getFileLocation(aResourceName);\n\tif (fUrl != null) {\n\t\tif(LOG.isInfoEnabled()) {\n\t\t\tLOG.info(\"addResourceIfReadable(\" + aResourceName + \"): resource file is \" + fUrl);\n\t\t}\n\t\ttry {\n\t\t\taddResource(fUrl);\n\t\t\tret = true;\n\t\t} catch (Exception e) {\n\t\t\tLOG.error(\"Unable to load the resource name [\" + aResourceName + \"]. Ignoring the resource:\" + fUrl);\n\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\tLOG.debug(\"Resource loading failed for \" + fUrl, e);\n\t\t\t}\n\t\t}\n\t} else {\n\t\tLOG.error(\"addResourceIfReadable(\" + aResourceName + \"): couldn't find resource file location\");\n\t}\n\n\tif(LOG.isDebugEnabled()) {\n\t\tLOG.debug(\"<== addResourceIfReadable(\" + aResourceName + \"), result=\" + ret);\n\t}\n\treturn ret;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3022,
        "instruction": "public TopicConfig createTopicInSendMessageBackMethod(\n    final String topic,\n    final int clientDefaultTopicQueueNums,\n    final int perm,\n    final int topicSysFlag) {\n    TopicConfig topicConfig = this.topicConfigTable.get(topic);\n    if (topicConfig != null)\n        return topicConfig;\n\n    boolean createNew = false;\n\n    try {\n        if (this.lockTopicConfigTable.tryLock(LOCK_TIMEOUT_MILLIS, TimeUnit.MILLISECONDS)) {\n            try {\n                topicConfig = this.topicConfigTable.get(topic);\n                if (topicConfig != null)\n                    return topicConfig;\n\n                topicConfig = new TopicConfig(topic);\n                topicConfig.setReadQueueNums(clientDefaultTopicQueueNums);\n                topicConfig.setWriteQueueNums(clientDefaultTopicQueueNums);\n                topicConfig.setPerm(perm);\n                topicConfig.setTopicSysFlag(topicSysFlag);\n\n                this.topicConfigTable.put(topic, topicConfig);\n                createNew = true;\n                this.dataVersion.nextVersion();\n                this.persist();\n            } finally {\n                this.lockTopicConfigTable.unlock();\n            }\n        }\n    } catch (InterruptedException e) {\n        log.error(\"createTopicInSendMessageBackMethod exception\", e);\n    }\n\n    if (createNew) {\n        this.brokerController.registerBrokerAll(false, true);\n    }\n\n    return topicConfig;\n}",
        "accept_response": "public TopicConfig createTopicInSendMessageBackMethod(\n    final String topic,\n    final int clientDefaultTopicQueueNums,\n    final int perm,\n    final int topicSysFlag) {\n    TopicConfig topicConfig = this.topicConfigTable.get(topic);\n    if (topicConfig != null)\n        return topicConfig;\n\n    boolean createNew = false;\n\n    try {\n        if (this.lockTopicConfigTable.tryLock(LOCK_TIMEOUT_MILLIS, TimeUnit.MILLISECONDS)) {\n            try {\n                topicConfig = this.topicConfigTable.get(topic);\n                if (topicConfig != null)\n                    return topicConfig;\n\n                topicConfig = new TopicConfig(topic);\n                topicConfig.setReadQueueNums(clientDefaultTopicQueueNums);\n                topicConfig.setWriteQueueNums(clientDefaultTopicQueueNums);\n                topicConfig.setPerm(perm);\n                topicConfig.setTopicSysFlag(topicSysFlag);\n\n                log.info(\"create new topic {}\", topicConfig);\n                this.topicConfigTable.put(topic, topicConfig);\n                createNew = true;\n                this.dataVersion.nextVersion();\n                this.persist();\n            } finally {\n                this.lockTopicConfigTable.unlock();\n            }\n        }\n    } catch (InterruptedException e) {\n        log.error(\"createTopicInSendMessageBackMethod exception\", e);\n    }\n\n    if (createNew) {\n        this.brokerController.registerBrokerAll(false, true);\n    }\n\n    return topicConfig;\n}",
        "reject_response": "public TopicConfig createTopicInSendMessageBackMethod(\n    final String topic,\n    final int clientDefaultTopicQueueNums,\n    final int perm,\n    final int topicSysFlag) {\n    TopicConfig topicConfig = this.topicConfigTable.get(topic);\n    if (topicConfig != null)\n        return topicConfig;\n\n    boolean createNew = false;\n\n    try {\n        if (this.lockTopicConfigTable.tryLock(LOCK_TIMEOUT_MILLIS, TimeUnit.MILLISECONDS)) {\n            try {\n                topicConfig = this.topicConfigTable.get(topic);\n                if (topicConfig != null)\n                    return topicConfig;\n\n                topicConfig = new TopicConfig(topic);\n                topicConfig.setReadQueueNums(clientDefaultTopicQueueNums);\n                topicConfig.setWriteQueueNums(clientDefaultTopicQueueNums);\n                topicConfig.setPerm(perm);\n                topicConfig.setTopicSysFlag(topicSysFlag);\n\n                LOG.info(\"create new topic {}\", topicConfig);\n                this.topicConfigTable.put(topic, topicConfig);\n                createNew = true;\n                this.dataVersion.nextVersion();\n                this.persist();\n            } finally {\n                this.lockTopicConfigTable.unlock();\n            }\n        }\n    } catch (InterruptedException e) {\n        log.error(\"createTopicInSendMessageBackMethod exception\", e);\n    }\n\n    if (createNew) {\n        this.brokerController.registerBrokerAll(false, true);\n    }\n\n    return topicConfig;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2587,
        "instruction": "@Override\nprotected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException {\n\n    final StopWatch task = new StopWatch();\n    task.start();\n\n    try {\n\n        if (\"OPTIONS\".equalsIgnoreCase(request.getMethod())) {\n            // ignore to allow 'preflight' requests from AJAX applications\n            // in different origin (domain name)\n        } else {\n\n            String tenantIdentifier = request.getHeader(this.tenantRequestHeader);\n\n            if (org.apache.commons.lang.StringUtils.isBlank(tenantIdentifier)) {\n                tenantIdentifier = request.getParameter(\"tenantIdentifier\");\n            }\n\n            if (tenantIdentifier == null && this.exceptionIfHeaderMissing) { throw new InvalidTenantIdentiferException(\n                    \"No tenant identifier found: Add request header of '\" + this.tenantRequestHeader\n                            + \"' or add the parameter 'tenantIdentifier' to query string of request URL.\"); }\n\n            String pathInfo = request.getRequestURI();\n            boolean isReportRequest = false;\n            if (pathInfo != null && pathInfo.contains(\"report\")) {\n                isReportRequest = true;\n            }\n            final FineractPlatformTenant tenant = this.basicAuthTenantDetailsService.loadTenantById(tenantIdentifier, isReportRequest);\n\n            ThreadLocalContextUtil.setTenant(tenant);\n            String authToken = request.getHeader(\"Authorization\");\n\n            if (authToken != null && authToken.startsWith(\"Basic \")) {\n                ThreadLocalContextUtil.setAuthToken(authToken.replaceFirst(\"Basic \", \"\"));\n            }\n\n            if (!firstRequestProcessed) {\n                final String baseUrl = request.getRequestURL().toString().replace(request.getPathInfo(), \"/\");\n                System.setProperty(\"baseUrl\", baseUrl);\n\n                final boolean ehcacheEnabled = this.configurationDomainService.isEhcacheEnabled();\n                if (ehcacheEnabled) {\n                    this.cacheWritePlatformService.switchToCache(CacheType.SINGLE_NODE);\n                } else {\n                    this.cacheWritePlatformService.switchToCache(CacheType.NO_CACHE);\n                }\n                TenantAwareBasicAuthenticationFilter.firstRequestProcessed = true;\n            }\n        }\n\n        super.doFilterInternal(request, response, filterChain);\n    } catch (final InvalidTenantIdentiferException e) {\n        // deal with exception at low level\n        SecurityContextHolder.getContext().setAuthentication(null);\n\n        response.addHeader(\"WWW-Authenticate\", \"Basic realm=\\\"\" + \"Fineract Platform API\" + \"\\\"\");\n        response.sendError(HttpServletResponse.SC_BAD_REQUEST, e.getMessage());\n    } finally {\n        task.stop();\n        final PlatformRequestLog log = PlatformRequestLog.from(task, request);\n    }\n}",
        "accept_response": "@Override\nprotected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException {\n\n    final StopWatch task = new StopWatch();\n    task.start();\n\n    try {\n\n        if (\"OPTIONS\".equalsIgnoreCase(request.getMethod())) {\n            // ignore to allow 'preflight' requests from AJAX applications\n            // in different origin (domain name)\n        } else {\n\n            String tenantIdentifier = request.getHeader(this.tenantRequestHeader);\n\n            if (org.apache.commons.lang.StringUtils.isBlank(tenantIdentifier)) {\n                tenantIdentifier = request.getParameter(\"tenantIdentifier\");\n            }\n\n            if (tenantIdentifier == null && this.exceptionIfHeaderMissing) { throw new InvalidTenantIdentiferException(\n                    \"No tenant identifier found: Add request header of '\" + this.tenantRequestHeader\n                            + \"' or add the parameter 'tenantIdentifier' to query string of request URL.\"); }\n\n            String pathInfo = request.getRequestURI();\n            boolean isReportRequest = false;\n            if (pathInfo != null && pathInfo.contains(\"report\")) {\n                isReportRequest = true;\n            }\n            final FineractPlatformTenant tenant = this.basicAuthTenantDetailsService.loadTenantById(tenantIdentifier, isReportRequest);\n\n            ThreadLocalContextUtil.setTenant(tenant);\n            String authToken = request.getHeader(\"Authorization\");\n\n            if (authToken != null && authToken.startsWith(\"Basic \")) {\n                ThreadLocalContextUtil.setAuthToken(authToken.replaceFirst(\"Basic \", \"\"));\n            }\n\n            if (!firstRequestProcessed) {\n                final String baseUrl = request.getRequestURL().toString().replace(request.getPathInfo(), \"/\");\n                System.setProperty(\"baseUrl\", baseUrl);\n\n                final boolean ehcacheEnabled = this.configurationDomainService.isEhcacheEnabled();\n                if (ehcacheEnabled) {\n                    this.cacheWritePlatformService.switchToCache(CacheType.SINGLE_NODE);\n                } else {\n                    this.cacheWritePlatformService.switchToCache(CacheType.NO_CACHE);\n                }\n                TenantAwareBasicAuthenticationFilter.firstRequestProcessed = true;\n            }\n        }\n\n        super.doFilterInternal(request, response, filterChain);\n    } catch (final InvalidTenantIdentiferException e) {\n        // deal with exception at low level\n        SecurityContextHolder.getContext().setAuthentication(null);\n\n        response.addHeader(\"WWW-Authenticate\", \"Basic realm=\\\"\" + \"Fineract Platform API\" + \"\\\"\");\n        response.sendError(HttpServletResponse.SC_BAD_REQUEST, e.getMessage());\n    } finally {\n        task.stop();\n        final PlatformRequestLog log = PlatformRequestLog.from(task, request);\n        logger.debug(this.toApiJsonSerializer.serialize(log));\n    }\n}",
        "reject_response": "@Override\nprotected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException {\n\n    final StopWatch task = new StopWatch();\n    task.start();\n\n    try {\n\n        if (\"OPTIONS\".equalsIgnoreCase(request.getMethod())) {\n            // ignore to allow 'preflight' requests from AJAX applications\n            // in different origin (domain name)\n        } else {\n\n            String tenantIdentifier = request.getHeader(this.tenantRequestHeader);\n\n            if (org.apache.commons.lang.StringUtils.isBlank(tenantIdentifier)) {\n                tenantIdentifier = request.getParameter(\"tenantIdentifier\");\n            }\n\n            if (tenantIdentifier == null && this.exceptionIfHeaderMissing) { throw new InvalidTenantIdentiferException(\n                    \"No tenant identifier found: Add request header of '\" + this.tenantRequestHeader\n                            + \"' or add the parameter 'tenantIdentifier' to query string of request URL.\"); }\n\n            String pathInfo = request.getRequestURI();\n            boolean isReportRequest = false;\n            if (pathInfo != null && pathInfo.contains(\"report\")) {\n                isReportRequest = true;\n            }\n            final FineractPlatformTenant tenant = this.basicAuthTenantDetailsService.loadTenantById(tenantIdentifier, isReportRequest);\n\n            ThreadLocalContextUtil.setTenant(tenant);\n            String authToken = request.getHeader(\"Authorization\");\n\n            if (authToken != null && authToken.startsWith(\"Basic \")) {\n                ThreadLocalContextUtil.setAuthToken(authToken.replaceFirst(\"Basic \", \"\"));\n            }\n\n            if (!firstRequestProcessed) {\n                final String baseUrl = request.getRequestURL().toString().replace(request.getPathInfo(), \"/\");\n                System.setProperty(\"baseUrl\", baseUrl);\n\n                final boolean ehcacheEnabled = this.configurationDomainService.isEhcacheEnabled();\n                if (ehcacheEnabled) {\n                    this.cacheWritePlatformService.switchToCache(CacheType.SINGLE_NODE);\n                } else {\n                    this.cacheWritePlatformService.switchToCache(CacheType.NO_CACHE);\n                }\n                TenantAwareBasicAuthenticationFilter.firstRequestProcessed = true;\n            }\n        }\n\n        super.doFilterInternal(request, response, filterChain);\n    } catch (final InvalidTenantIdentiferException e) {\n        // deal with exception at low level\n        SecurityContextHolder.getContext().setAuthentication(null);\n\n        response.addHeader(\"WWW-Authenticate\", \"Basic realm=\\\"\" + \"Fineract Platform API\" + \"\\\"\");\n        response.sendError(HttpServletResponse.SC_BAD_REQUEST, e.getMessage());\n    } finally {\n        task.stop();\n        final PlatformRequestLog log = PlatformRequestLog.from(task, request);\n        logger.info(this.toApiJsonSerializer.serialize(log));\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2902,
        "instruction": "public SPF1Record parse(String spfRecord) throws PermErrorException,\n        NoneException, NeutralException {\n\n\n    SPF1Record result = new SPF1Record();\n\n    // check the version \"header\"\n    if (spfRecord.toLowerCase().startsWith(SPF1Constants.SPF_VERSION1 + \" \") || spfRecord.equalsIgnoreCase(SPF1Constants.SPF_VERSION1)) {\n        if (!spfRecord.toLowerCase().startsWith(SPF1Constants.SPF_VERSION1 + \" \")) throw new NeutralException(\"Empty SPF Record\");\n    } else {\n        throw new NoneException(\"No valid SPF Record: \" + spfRecord);\n    }\n\n    // extract terms\n    String[] terms = termsSeparatorPattern.split(spfRecord.replaceFirst(\n            SPF1Constants.SPF_VERSION1, \"\"));\n\n    // cycle terms\n    for (int i = 0; i < terms.length; i++) {\n        if (terms[i].length() > 0) {\n            Matcher termMatcher = termPattern.matcher(terms[i]);\n            if (!termMatcher.matches()) {\n                throw new PermErrorException(\"Term [\" + terms[i]\n                        + \"] is not syntactically valid: \"\n                        + termPattern.pattern());\n            }\n\n            // true if we matched a modifier, false if we matched a\n            // directive\n            String modifierString = termMatcher\n                    .group(TERM_STEP_REGEX_MODIFIER_POS);\n\n            if (modifierString != null) {\n                // MODIFIER\n                Modifier mod = (Modifier) lookupAndCreateTerm(termMatcher,\n                        TERM_STEP_REGEX_MODIFIER_POS);\n\n                if (mod.enforceSingleInstance()) {\n                    Iterator<Modifier> it = result.getModifiers().iterator();\n                    while (it.hasNext()) {\n                        if (it.next().getClass().equals(mod.getClass())) {\n                            throw new PermErrorException(\"More than one \"\n                                    + modifierString\n                                    + \" found in SPF-Record\");\n                        }\n                    }\n                }\n\n                result.getModifiers().add(mod);\n\n            } else {\n                // DIRECTIVE\n                String qualifier = termMatcher\n                        .group(TERM_STEP_REGEX_QUALIFIER_POS);\n\n                Object mech = lookupAndCreateTerm(termMatcher,\n                        TERM_STEP_REGEX_MECHANISM_POS);\n\n                result.getDirectives().add(\n                        new Directive(qualifier, (Mechanism) mech));\n\n            }\n\n        }\n    }\n\n    return result;\n}",
        "accept_response": "public SPF1Record parse(String spfRecord) throws PermErrorException,\n        NoneException, NeutralException {\n\n    LOGGER.debug(\"Start parsing SPF-Record: \" + spfRecord);\n\n    SPF1Record result = new SPF1Record();\n\n    // check the version \"header\"\n    if (spfRecord.toLowerCase().startsWith(SPF1Constants.SPF_VERSION1 + \" \") || spfRecord.equalsIgnoreCase(SPF1Constants.SPF_VERSION1)) {\n        if (!spfRecord.toLowerCase().startsWith(SPF1Constants.SPF_VERSION1 + \" \")) throw new NeutralException(\"Empty SPF Record\");\n    } else {\n        throw new NoneException(\"No valid SPF Record: \" + spfRecord);\n    }\n\n    // extract terms\n    String[] terms = termsSeparatorPattern.split(spfRecord.replaceFirst(\n            SPF1Constants.SPF_VERSION1, \"\"));\n\n    // cycle terms\n    for (int i = 0; i < terms.length; i++) {\n        if (terms[i].length() > 0) {\n            Matcher termMatcher = termPattern.matcher(terms[i]);\n            if (!termMatcher.matches()) {\n                throw new PermErrorException(\"Term [\" + terms[i]\n                        + \"] is not syntactically valid: \"\n                        + termPattern.pattern());\n            }\n\n            // true if we matched a modifier, false if we matched a\n            // directive\n            String modifierString = termMatcher\n                    .group(TERM_STEP_REGEX_MODIFIER_POS);\n\n            if (modifierString != null) {\n                // MODIFIER\n                Modifier mod = (Modifier) lookupAndCreateTerm(termMatcher,\n                        TERM_STEP_REGEX_MODIFIER_POS);\n\n                if (mod.enforceSingleInstance()) {\n                    Iterator<Modifier> it = result.getModifiers().iterator();\n                    while (it.hasNext()) {\n                        if (it.next().getClass().equals(mod.getClass())) {\n                            throw new PermErrorException(\"More than one \"\n                                    + modifierString\n                                    + \" found in SPF-Record\");\n                        }\n                    }\n                }\n\n                result.getModifiers().add(mod);\n\n            } else {\n                // DIRECTIVE\n                String qualifier = termMatcher\n                        .group(TERM_STEP_REGEX_QUALIFIER_POS);\n\n                Object mech = lookupAndCreateTerm(termMatcher,\n                        TERM_STEP_REGEX_MECHANISM_POS);\n\n                result.getDirectives().add(\n                        new Directive(qualifier, (Mechanism) mech));\n\n            }\n\n        }\n    }\n\n    return result;\n}",
        "reject_response": "public SPF1Record parse(String spfRecord) throws PermErrorException,\n        NoneException, NeutralException {\n\n    log.debug(\"Start parsing SPF-Record: \" + spfRecord);\n\n    SPF1Record result = new SPF1Record();\n\n    // check the version \"header\"\n    if (spfRecord.toLowerCase().startsWith(SPF1Constants.SPF_VERSION1 + \" \") || spfRecord.equalsIgnoreCase(SPF1Constants.SPF_VERSION1)) {\n        if (!spfRecord.toLowerCase().startsWith(SPF1Constants.SPF_VERSION1 + \" \")) throw new NeutralException(\"Empty SPF Record\");\n    } else {\n        throw new NoneException(\"No valid SPF Record: \" + spfRecord);\n    }\n\n    // extract terms\n    String[] terms = termsSeparatorPattern.split(spfRecord.replaceFirst(\n            SPF1Constants.SPF_VERSION1, \"\"));\n\n    // cycle terms\n    for (int i = 0; i < terms.length; i++) {\n        if (terms[i].length() > 0) {\n            Matcher termMatcher = termPattern.matcher(terms[i]);\n            if (!termMatcher.matches()) {\n                throw new PermErrorException(\"Term [\" + terms[i]\n                        + \"] is not syntactically valid: \"\n                        + termPattern.pattern());\n            }\n\n            // true if we matched a modifier, false if we matched a\n            // directive\n            String modifierString = termMatcher\n                    .group(TERM_STEP_REGEX_MODIFIER_POS);\n\n            if (modifierString != null) {\n                // MODIFIER\n                Modifier mod = (Modifier) lookupAndCreateTerm(termMatcher,\n                        TERM_STEP_REGEX_MODIFIER_POS);\n\n                if (mod.enforceSingleInstance()) {\n                    Iterator<Modifier> it = result.getModifiers().iterator();\n                    while (it.hasNext()) {\n                        if (it.next().getClass().equals(mod.getClass())) {\n                            throw new PermErrorException(\"More than one \"\n                                    + modifierString\n                                    + \" found in SPF-Record\");\n                        }\n                    }\n                }\n\n                result.getModifiers().add(mod);\n\n            } else {\n                // DIRECTIVE\n                String qualifier = termMatcher\n                        .group(TERM_STEP_REGEX_QUALIFIER_POS);\n\n                Object mech = lookupAndCreateTerm(termMatcher,\n                        TERM_STEP_REGEX_MECHANISM_POS);\n\n                result.getDirectives().add(\n                        new Directive(qualifier, (Mechanism) mech));\n\n            }\n\n        }\n    }\n\n    return result;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2429,
        "instruction": "protected void checkMemorySystemUsageLimits() throws Exception {\n    final SystemUsage usage = getSystemUsage();\n    long memLimit = usage.getMemoryUsage().getLimit();\n    long jvmLimit = Runtime.getRuntime().maxMemory();\n\n    if (memLimit > jvmLimit) {\n        final String message = \"Memory Usage for the Broker (\" + memLimit / (1024 * 1024)\n                + \"mb) is more than the maximum available for the JVM: \" + jvmLimit / (1024 * 1024);\n\n        if (adjustUsageLimits) {\n            usage.getMemoryUsage().setPercentOfJvmHeap(70);\n        } else {\n            LOG.error(message);\n            throw new ConfigurationException(message);\n        }\n    }\n}",
        "accept_response": "protected void checkMemorySystemUsageLimits() throws Exception {\n    final SystemUsage usage = getSystemUsage();\n    long memLimit = usage.getMemoryUsage().getLimit();\n    long jvmLimit = Runtime.getRuntime().maxMemory();\n\n    if (memLimit > jvmLimit) {\n        final String message = \"Memory Usage for the Broker (\" + memLimit / (1024 * 1024)\n                + \"mb) is more than the maximum available for the JVM: \" + jvmLimit / (1024 * 1024);\n\n        if (adjustUsageLimits) {\n            usage.getMemoryUsage().setPercentOfJvmHeap(70);\n            LOG.warn(\"{} mb - resetting to 70% of maximum available: {} mb\", message, (usage.getMemoryUsage().getLimit() / (1024 * 1024)));\n        } else {\n            LOG.error(message);\n            throw new ConfigurationException(message);\n        }\n    }\n}",
        "reject_response": "protected void checkMemorySystemUsageLimits() throws Exception {\n    final SystemUsage usage = getSystemUsage();\n    long memLimit = usage.getMemoryUsage().getLimit();\n    long jvmLimit = Runtime.getRuntime().maxMemory();\n\n    if (memLimit > jvmLimit) {\n        final String message = \"Memory Usage for the Broker (\" + memLimit / (1024 * 1024)\n                + \"mb) is more than the maximum available for the JVM: \" + jvmLimit / (1024 * 1024);\n\n        if (adjustUsageLimits) {\n            usage.getMemoryUsage().setPercentOfJvmHeap(70);\n            LOG.warn(message + \" mb - resetting to 70% of maximum available: \" + (usage.getMemoryUsage().getLimit() / (1024 * 1024)) + \" mb\");\n        } else {\n            LOG.error(message);\n            throw new ConfigurationException(message);\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2447,
        "instruction": "private boolean send(final Packet packet, final int reconnectID, final boolean flush, final boolean batch) {\n   if (invokeInterceptors(packet, interceptors, connection) != null) {\n      return false;\n   }\n\n   synchronized (sendLock) {\n      packet.setChannelID(id);\n\n      ActiveMQBuffer buffer = packet.encode(connection);\n\n      lock.lock();\n\n      try {\n         if (failingOver) {\n            waitForFailOver(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" timed-out waiting for fail-over condition on non-blocking send\");\n         }\n\n         // Sanity check\n         if (transferring) {\n            throw ActiveMQClientMessageBundle.BUNDLE.cannotSendPacketDuringFailover();\n         }\n\n         if (resendCache != null && packet.isRequiresConfirmations()) {\n            addResendPacket(packet);\n         }\n      } finally {\n         lock.unlock();\n      }\n\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" Writing buffer for channelID=\" + id);\n      }\n\n      checkReconnectID(reconnectID);\n\n      // The actual send must be outside the lock, or with OIO transport, the write can block if the tcp\n      // buffer is full, preventing any incoming buffers being handled and blocking failover\n      connection.getTransportConnection().write(buffer, flush, batch);\n      return true;\n   }\n}",
        "accept_response": "private boolean send(final Packet packet, final int reconnectID, final boolean flush, final boolean batch) {\n   if (invokeInterceptors(packet, interceptors, connection) != null) {\n      return false;\n   }\n\n   synchronized (sendLock) {\n      packet.setChannelID(id);\n\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" Sending packet nonblocking \" + packet + \" on channelID=\" + id);\n      }\n\n      ActiveMQBuffer buffer = packet.encode(connection);\n\n      lock.lock();\n\n      try {\n         if (failingOver) {\n            waitForFailOver(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" timed-out waiting for fail-over condition on non-blocking send\");\n         }\n\n         // Sanity check\n         if (transferring) {\n            throw ActiveMQClientMessageBundle.BUNDLE.cannotSendPacketDuringFailover();\n         }\n\n         if (resendCache != null && packet.isRequiresConfirmations()) {\n            addResendPacket(packet);\n         }\n      } finally {\n         lock.unlock();\n      }\n\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" Writing buffer for channelID=\" + id);\n      }\n\n      checkReconnectID(reconnectID);\n\n      // The actual send must be outside the lock, or with OIO transport, the write can block if the tcp\n      // buffer is full, preventing any incoming buffers being handled and blocking failover\n      connection.getTransportConnection().write(buffer, flush, batch);\n      return true;\n   }\n}",
        "reject_response": "private boolean send(final Packet packet, final int reconnectID, final boolean flush, final boolean batch) {\n   if (invokeInterceptors(packet, interceptors, connection) != null) {\n      return false;\n   }\n\n   synchronized (sendLock) {\n      packet.setChannelID(id);\n\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"Sending packet nonblocking \" + packet + \" on channelID=\" + id);\n      }\n\n      ActiveMQBuffer buffer = packet.encode(connection);\n\n      lock.lock();\n\n      try {\n         if (failingOver) {\n            waitForFailOver(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" timed-out waiting for fail-over condition on non-blocking send\");\n         }\n\n         // Sanity check\n         if (transferring) {\n            throw ActiveMQClientMessageBundle.BUNDLE.cannotSendPacketDuringFailover();\n         }\n\n         if (resendCache != null && packet.isRequiresConfirmations()) {\n            addResendPacket(packet);\n         }\n      } finally {\n         lock.unlock();\n      }\n\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" Writing buffer for channelID=\" + id);\n      }\n\n      checkReconnectID(reconnectID);\n\n      // The actual send must be outside the lock, or with OIO transport, the write can block if the tcp\n      // buffer is full, preventing any incoming buffers being handled and blocking failover\n      connection.getTransportConnection().write(buffer, flush, batch);\n      return true;\n   }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3020,
        "instruction": "public void updateTopicUnitFlag(final String topic, final boolean unit) {\n\n    TopicConfig topicConfig = this.topicConfigTable.get(topic);\n    if (topicConfig != null) {\n        int oldTopicSysFlag = topicConfig.getTopicSysFlag();\n        if (unit) {\n            topicConfig.setTopicSysFlag(TopicSysFlag.setUnitFlag(oldTopicSysFlag));\n        } else {\n            topicConfig.setTopicSysFlag(TopicSysFlag.clearUnitFlag(oldTopicSysFlag));\n        }\n\n\n        this.topicConfigTable.put(topic, topicConfig);\n\n        this.dataVersion.nextVersion();\n\n        this.persist();\n        this.brokerController.registerBrokerAll(false, true);\n    }\n}",
        "accept_response": "public void updateTopicUnitFlag(final String topic, final boolean unit) {\n\n    TopicConfig topicConfig = this.topicConfigTable.get(topic);\n    if (topicConfig != null) {\n        int oldTopicSysFlag = topicConfig.getTopicSysFlag();\n        if (unit) {\n            topicConfig.setTopicSysFlag(TopicSysFlag.setUnitFlag(oldTopicSysFlag));\n        } else {\n            topicConfig.setTopicSysFlag(TopicSysFlag.clearUnitFlag(oldTopicSysFlag));\n        }\n\n        log.info(\"update topic sys flag. oldTopicSysFlag={}, newTopicSysFlag\", oldTopicSysFlag,\n                topicConfig.getTopicSysFlag());\n\n        this.topicConfigTable.put(topic, topicConfig);\n\n        this.dataVersion.nextVersion();\n\n        this.persist();\n        this.brokerController.registerBrokerAll(false, true);\n    }\n}",
        "reject_response": "public void updateTopicUnitFlag(final String topic, final boolean unit) {\n\n    TopicConfig topicConfig = this.topicConfigTable.get(topic);\n    if (topicConfig != null) {\n        int oldTopicSysFlag = topicConfig.getTopicSysFlag();\n        if (unit) {\n            topicConfig.setTopicSysFlag(TopicSysFlag.setUnitFlag(oldTopicSysFlag));\n        } else {\n            topicConfig.setTopicSysFlag(TopicSysFlag.clearUnitFlag(oldTopicSysFlag));\n        }\n\n        LOG.info(\"update topic sys flag. oldTopicSysFlag={}, newTopicSysFlag\", oldTopicSysFlag,\n            topicConfig.getTopicSysFlag());\n\n        this.topicConfigTable.put(topic, topicConfig);\n\n        this.dataVersion.nextVersion();\n\n        this.persist();\n        this.brokerController.registerBrokerAll(false, true);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2758,
        "instruction": "public synchronized void close(boolean force) {\n  if (!force && this.numThreads > 0) {\n    // this is an erroneous case, but we have to close the connection\n    // anyway since there will be connection leak if we don't do so\n    // the connection has been moved out of the pool\n  }\n  this.closed = true;\n  Object proxy = this.client.getProxy();\n  // Nobody should be using this anymore, so it should close right away\n  RPC.stopProxy(proxy);\n}",
        "accept_response": "public synchronized void close(boolean force) {\n  if (!force && this.numThreads > 0) {\n    // this is an erroneous case, but we have to close the connection\n    // anyway since there will be connection leak if we don't do so\n    // the connection has been moved out of the pool\n    LOG.error(\"Active connection with {} handlers will be closed, ConnectionContext is {}\",\n        this.numThreads, this);\n  }\n  this.closed = true;\n  Object proxy = this.client.getProxy();\n  // Nobody should be using this anymore, so it should close right away\n  RPC.stopProxy(proxy);\n}",
        "reject_response": "public synchronized void close(boolean force) {\n  if (!force && this.numThreads > 0) {\n    // this is an erroneous case, but we have to close the connection\n    // anyway since there will be connection leak if we don't do so\n    // the connection has been moved out of the pool\n    LOG.error(\"Active connection with {} handlers will be closed\",\n        this.numThreads);\n  }\n  this.closed = true;\n  Object proxy = this.client.getProxy();\n  // Nobody should be using this anymore, so it should close right away\n  RPC.stopProxy(proxy);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2929,
        "instruction": "public void flush(final String namespace) {\n    numFlushes++;\n\n    final NamedCache cache = getCache(namespace);\n    if (cache == null) {\n        return;\n    }\n    cache.flush();\n\n    if (log.isTraceEnabled())\n}",
        "accept_response": "public void flush(final String namespace) {\n    numFlushes++;\n\n    final NamedCache cache = getCache(namespace);\n    if (cache == null) {\n        return;\n    }\n    cache.flush();\n\n    if (log.isTraceEnabled())\n        log.trace(\"{} Cache stats on flush: #puts={}, #gets={}, #evicts={}, #flushes={}\",\n                logPrefix, puts(), gets(), evicts(), flushes());\n}",
        "reject_response": "public void flush(final String namespace) {\n    numFlushes++;\n\n    final NamedCache cache = getCache(namespace);\n    if (cache == null) {\n        return;\n    }\n    cache.flush();\n\n    if (log.isTraceEnabled())\n    log.trace(\"Thread {} cache stats on flush: #puts={}, #gets={}, #evicts={}, #flushes={}\",\n              name, puts(), gets(), evicts(), flushes());\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3066,
        "instruction": "protected void handleMessage(\n    BlockTransferMessage msgObj,\n    TransportClient client,\n    RpcResponseCallback callback) {\n  if (msgObj instanceof OpenBlocks) {\n    OpenBlocks msg = (OpenBlocks) msgObj;\n    checkAuth(client, msg.appId);\n\n    List<ManagedBuffer> blocks = Lists.newArrayList();\n    for (String blockId : msg.blockIds) {\n      blocks.add(blockManager.getBlockData(msg.appId, msg.execId, blockId));\n    }\n    long streamId = streamManager.registerStream(client.getClientId(), blocks.iterator());\n    callback.onSuccess(new StreamHandle(streamId, msg.blockIds.length).toByteBuffer());\n\n  } else if (msgObj instanceof RegisterExecutor) {\n    RegisterExecutor msg = (RegisterExecutor) msgObj;\n    checkAuth(client, msg.appId);\n    blockManager.registerExecutor(msg.appId, msg.execId, msg.executorInfo);\n    callback.onSuccess(ByteBuffer.wrap(new byte[0]));\n\n  } else {\n    throw new UnsupportedOperationException(\"Unexpected message: \" + msgObj);\n  }\n}",
        "accept_response": "protected void handleMessage(\n    BlockTransferMessage msgObj,\n    TransportClient client,\n    RpcResponseCallback callback) {\n  if (msgObj instanceof OpenBlocks) {\n    OpenBlocks msg = (OpenBlocks) msgObj;\n    checkAuth(client, msg.appId);\n\n    List<ManagedBuffer> blocks = Lists.newArrayList();\n    for (String blockId : msg.blockIds) {\n      blocks.add(blockManager.getBlockData(msg.appId, msg.execId, blockId));\n    }\n    long streamId = streamManager.registerStream(client.getClientId(), blocks.iterator());\n    logger.trace(\"Registered streamId {} with {} buffers for client {} from host {}\", streamId,\n      msg.blockIds.length, client.getClientId(), NettyUtils.getRemoteAddress(client.getChannel()));\n    callback.onSuccess(new StreamHandle(streamId, msg.blockIds.length).toByteBuffer());\n\n  } else if (msgObj instanceof RegisterExecutor) {\n    RegisterExecutor msg = (RegisterExecutor) msgObj;\n    checkAuth(client, msg.appId);\n    blockManager.registerExecutor(msg.appId, msg.execId, msg.executorInfo);\n    callback.onSuccess(ByteBuffer.wrap(new byte[0]));\n\n  } else {\n    throw new UnsupportedOperationException(\"Unexpected message: \" + msgObj);\n  }\n}",
        "reject_response": "protected void handleMessage(\n    BlockTransferMessage msgObj,\n    TransportClient client,\n    RpcResponseCallback callback) {\n  if (msgObj instanceof OpenBlocks) {\n    OpenBlocks msg = (OpenBlocks) msgObj;\n    checkAuth(client, msg.appId);\n\n    List<ManagedBuffer> blocks = Lists.newArrayList();\n    for (String blockId : msg.blockIds) {\n      blocks.add(blockManager.getBlockData(msg.appId, msg.execId, blockId));\n    }\n    long streamId = streamManager.registerStream(client.getClientId(), blocks.iterator());\n    logger.trace(\"Registered streamId {} with {} buffers\", streamId, msg.blockIds.length);\n    callback.onSuccess(new StreamHandle(streamId, msg.blockIds.length).toByteBuffer());\n\n  } else if (msgObj instanceof RegisterExecutor) {\n    RegisterExecutor msg = (RegisterExecutor) msgObj;\n    checkAuth(client, msg.appId);\n    blockManager.registerExecutor(msg.appId, msg.execId, msg.executorInfo);\n    callback.onSuccess(ByteBuffer.wrap(new byte[0]));\n\n  } else {\n    throw new UnsupportedOperationException(\"Unexpected message: \" + msgObj);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3256,
        "instruction": "@Override\npublic boolean verify(final String host, final SSLSession session) {\n    try {\n        final Certificate[] certs = session.getPeerCertificates();\n        final X509Certificate x509 = (X509Certificate) certs[0];\n        verify(host, x509);\n        return true;\n    } catch (final SSLException ex) {\n        return false;\n    }\n}",
        "accept_response": "@Override\npublic boolean verify(final String host, final SSLSession session) {\n    try {\n        final Certificate[] certs = session.getPeerCertificates();\n        final X509Certificate x509 = (X509Certificate) certs[0];\n        verify(host, x509);\n        return true;\n    } catch (final SSLException ex) {\n        log.debug(\"Unexpected exception\", ex);\n        return false;\n    }\n}",
        "reject_response": "@Override\npublic boolean verify(final String host, final SSLSession session) {\n    try {\n        final Certificate[] certs = session.getPeerCertificates();\n        final X509Certificate x509 = (X509Certificate) certs[0];\n        verify(host, x509);\n        return true;\n    } catch (final SSLException ex) {\n        if (log.isDebugEnabled()) {\n            log.debug(ex.getMessage(), ex);\n        }\n        return false;\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2418,
        "instruction": "protected synchronized String findPreferredCollectHost() {\n  if (!isInitializedForHA) {\n    init();\n  }\n\n  shardExpired = false;\n  // Auto expire and re-calculate after 1 hour\n  if (targetCollectorHostSupplier != null) {\n    String targetCollector = targetCollectorHostSupplier.get();\n    if (targetCollector != null) {\n      return targetCollector;\n    }\n  }\n\n  // Reach out to all configured collectors before Zookeeper\n  Collection<String> collectorHosts = getConfiguredCollectorHosts();\n  refreshCollectorsFromConfigured(collectorHosts);\n\n  // Lookup Zookeeper for live hosts - max 10 seconds wait time\n  long currentTime = System.currentTimeMillis();\n  if (allKnownLiveCollectors.size() == 0 && getZookeeperQuorum() != null\n    && (currentTime - lastFailedZkRequestTime) > zookeeperBackoffTimeMillis) {\n\n    LOG.debug(\"No live collectors from configuration. Requesting zookeeper...\");\n    allKnownLiveCollectors.addAll(collectorHAHelper.findLiveCollectorHostsFromZNode());\n    boolean noNewCollectorFromZk = true;\n    for (String collectorHostFromZk : allKnownLiveCollectors) {\n      if (!collectorHosts.contains(collectorHostFromZk)) {\n        noNewCollectorFromZk = false;\n        break;\n      }\n    }\n    if (noNewCollectorFromZk) {\n      LOG.debug(\"No new collector was found from Zookeeper. Will not request zookeeper for \" + zookeeperBackoffTimeMillis + \" millis\");\n      lastFailedZkRequestTime = System.currentTimeMillis();\n    }\n  }\n\n  if (allKnownLiveCollectors.size() != 0) {\n    targetCollectorHostSupplier = Suppliers.memoizeWithExpiration(\n      new Supplier<String>() {\n        @Override\n        public String get() {\n          //shardExpired flag is used to determine if the Supplier.get() is invoked through the\n          // findPreferredCollectHost method (No need to refresh collector hosts\n          // OR\n          // through Expiry (Refresh needed to pick up dead collectors that might have not become alive).\n          if (shardExpired) {\n            refreshCollectorsFromConfigured(getConfiguredCollectorHosts());\n          }\n          return metricSinkWriteShardStrategy.findCollectorShard(new ArrayList<>(allKnownLiveCollectors));\n        }\n      },  // random.nextInt(max - min + 1) + min # (60 to 75 minutes)\n      rand.nextInt(COLLECTOR_HOST_CACHE_MAX_EXPIRATION_MINUTES\n        - COLLECTOR_HOST_CACHE_MIN_EXPIRATION_MINUTES + 1)\n        + COLLECTOR_HOST_CACHE_MIN_EXPIRATION_MINUTES,\n      TimeUnit.MINUTES\n    );\n\n    String collectorHost = targetCollectorHostSupplier.get();\n    shardExpired = true;\n    return collectorHost;\n  }\n  shardExpired = true;\n  return null;\n}",
        "accept_response": "protected synchronized String findPreferredCollectHost() {\n  if (!isInitializedForHA) {\n    init();\n  }\n\n  shardExpired = false;\n  // Auto expire and re-calculate after 1 hour\n  if (targetCollectorHostSupplier != null) {\n    String targetCollector = targetCollectorHostSupplier.get();\n    if (targetCollector != null) {\n      return targetCollector;\n    }\n  }\n\n  // Reach out to all configured collectors before Zookeeper\n  Collection<String> collectorHosts = getConfiguredCollectorHosts();\n  refreshCollectorsFromConfigured(collectorHosts);\n\n  // Lookup Zookeeper for live hosts - max 10 seconds wait time\n  long currentTime = System.currentTimeMillis();\n  if (allKnownLiveCollectors.size() == 0 && getZookeeperQuorum() != null\n    && (currentTime - lastFailedZkRequestTime) > zookeeperBackoffTimeMillis) {\n\n    LOG.debug(\"No live collectors from configuration. Requesting zookeeper...\");\n    allKnownLiveCollectors.addAll(collectorHAHelper.findLiveCollectorHostsFromZNode());\n    boolean noNewCollectorFromZk = true;\n    for (String collectorHostFromZk : allKnownLiveCollectors) {\n      if (!collectorHosts.contains(collectorHostFromZk)) {\n        noNewCollectorFromZk = false;\n        break;\n      }\n    }\n    if (noNewCollectorFromZk) {\n      LOG.debug(\"No new collector was found from Zookeeper. Will not request zookeeper for \" + zookeeperBackoffTimeMillis + \" millis\");\n      lastFailedZkRequestTime = System.currentTimeMillis();\n    }\n  }\n\n  if (allKnownLiveCollectors.size() != 0) {\n    targetCollectorHostSupplier = Suppliers.memoizeWithExpiration(\n      new Supplier<String>() {\n        @Override\n        public String get() {\n          //shardExpired flag is used to determine if the Supplier.get() is invoked through the\n          // findPreferredCollectHost method (No need to refresh collector hosts\n          // OR\n          // through Expiry (Refresh needed to pick up dead collectors that might have not become alive).\n          if (shardExpired) {\n            refreshCollectorsFromConfigured(getConfiguredCollectorHosts());\n          }\n          return metricSinkWriteShardStrategy.findCollectorShard(new ArrayList<>(allKnownLiveCollectors));\n        }\n      },  // random.nextInt(max - min + 1) + min # (60 to 75 minutes)\n      rand.nextInt(COLLECTOR_HOST_CACHE_MAX_EXPIRATION_MINUTES\n        - COLLECTOR_HOST_CACHE_MIN_EXPIRATION_MINUTES + 1)\n        + COLLECTOR_HOST_CACHE_MIN_EXPIRATION_MINUTES,\n      TimeUnit.MINUTES\n    );\n\n    String collectorHost = targetCollectorHostSupplier.get();\n    shardExpired = true;\n    return collectorHost;\n  }\n  LOG.debug(\"Couldn't find any live collectors. Returning null\");\n  shardExpired = true;\n  return null;\n}",
        "reject_response": "protected synchronized String findPreferredCollectHost() {\n  if (!isInitializedForHA) {\n    init();\n  }\n\n  shardExpired = false;\n  // Auto expire and re-calculate after 1 hour\n  if (targetCollectorHostSupplier != null) {\n    String targetCollector = targetCollectorHostSupplier.get();\n    if (targetCollector != null) {\n      return targetCollector;\n    }\n  }\n\n  // Reach out to all configured collectors before Zookeeper\n  Collection<String> collectorHosts = getConfiguredCollectorHosts();\n  refreshCollectorsFromConfigured(collectorHosts);\n\n  // Lookup Zookeeper for live hosts - max 10 seconds wait time\n  long currentTime = System.currentTimeMillis();\n  if (allKnownLiveCollectors.size() == 0 && getZookeeperQuorum() != null\n    && (currentTime - lastFailedZkRequestTime) > zookeeperBackoffTimeMillis) {\n\n    LOG.debug(\"No live collectors from configuration. Requesting zookeeper...\");\n    allKnownLiveCollectors.addAll(collectorHAHelper.findLiveCollectorHostsFromZNode());\n    boolean noNewCollectorFromZk = true;\n    for (String collectorHostFromZk : allKnownLiveCollectors) {\n      if (!collectorHosts.contains(collectorHostFromZk)) {\n        noNewCollectorFromZk = false;\n        break;\n      }\n    }\n    if (noNewCollectorFromZk) {\n      LOG.debug(\"No new collector was found from Zookeeper. Will not request zookeeper for \" + zookeeperBackoffTimeMillis + \" millis\");\n      lastFailedZkRequestTime = System.currentTimeMillis();\n    }\n  }\n\n  if (allKnownLiveCollectors.size() != 0) {\n    targetCollectorHostSupplier = Suppliers.memoizeWithExpiration(\n      new Supplier<String>() {\n        @Override\n        public String get() {\n          //shardExpired flag is used to determine if the Supplier.get() is invoked through the\n          // findPreferredCollectHost method (No need to refresh collector hosts\n          // OR\n          // through Expiry (Refresh needed to pick up dead collectors that might have not become alive).\n          if (shardExpired) {\n            refreshCollectorsFromConfigured(getConfiguredCollectorHosts());\n          }\n          return metricSinkWriteShardStrategy.findCollectorShard(new ArrayList<>(allKnownLiveCollectors));\n        }\n      },  // random.nextInt(max - min + 1) + min # (60 to 75 minutes)\n      rand.nextInt(COLLECTOR_HOST_CACHE_MAX_EXPIRATION_MINUTES\n        - COLLECTOR_HOST_CACHE_MIN_EXPIRATION_MINUTES + 1)\n        + COLLECTOR_HOST_CACHE_MIN_EXPIRATION_MINUTES,\n      TimeUnit.MINUTES\n    );\n\n    String collectorHost = targetCollectorHostSupplier.get();\n    shardExpired = true;\n    return collectorHost;\n  }\n  LOG.warn(\"Couldn't find any live collectors. Returning null\");\n  shardExpired = true;\n  return null;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2735,
        "instruction": "private void unwrapPacketAndProcessRpcs(byte[] inBuf)\n    throws IOException, InterruptedException {\n  inBuf = saslServer.unwrap(inBuf, 0, inBuf.length);\n  ReadableByteChannel ch = Channels.newChannel(new ByteArrayInputStream(\n      inBuf));\n  // Read all RPCs contained in the inBuf, even partial ones\n  while (!shouldClose()) { // stop if a fatal response has been sent.\n    int count = -1;\n    if (unwrappedDataLengthBuffer.remaining() > 0) {\n      count = channelRead(ch, unwrappedDataLengthBuffer);\n      if (count <= 0 || unwrappedDataLengthBuffer.remaining() > 0)\n        return;\n    }\n\n    if (unwrappedData == null) {\n      unwrappedDataLengthBuffer.flip();\n      int unwrappedDataLength = unwrappedDataLengthBuffer.getInt();\n      unwrappedData = ByteBuffer.allocate(unwrappedDataLength);\n    }\n\n    count = channelRead(ch, unwrappedData);\n    if (count <= 0 || unwrappedData.remaining() > 0)\n      return;\n\n    if (unwrappedData.remaining() == 0) {\n      unwrappedDataLengthBuffer.clear();\n      unwrappedData.flip();\n      ByteBuffer requestData = unwrappedData;\n      unwrappedData = null; // null out in case processOneRpc throws.\n      processOneRpc(requestData);\n    }\n  }\n}",
        "accept_response": "private void unwrapPacketAndProcessRpcs(byte[] inBuf)\n    throws IOException, InterruptedException {\n  LOG.debug(\"Have read input token of size {} for processing by saslServer.unwrap()\",\n      inBuf.length);\n  inBuf = saslServer.unwrap(inBuf, 0, inBuf.length);\n  ReadableByteChannel ch = Channels.newChannel(new ByteArrayInputStream(\n      inBuf));\n  // Read all RPCs contained in the inBuf, even partial ones\n  while (!shouldClose()) { // stop if a fatal response has been sent.\n    int count = -1;\n    if (unwrappedDataLengthBuffer.remaining() > 0) {\n      count = channelRead(ch, unwrappedDataLengthBuffer);\n      if (count <= 0 || unwrappedDataLengthBuffer.remaining() > 0)\n        return;\n    }\n\n    if (unwrappedData == null) {\n      unwrappedDataLengthBuffer.flip();\n      int unwrappedDataLength = unwrappedDataLengthBuffer.getInt();\n      unwrappedData = ByteBuffer.allocate(unwrappedDataLength);\n    }\n\n    count = channelRead(ch, unwrappedData);\n    if (count <= 0 || unwrappedData.remaining() > 0)\n      return;\n\n    if (unwrappedData.remaining() == 0) {\n      unwrappedDataLengthBuffer.clear();\n      unwrappedData.flip();\n      ByteBuffer requestData = unwrappedData;\n      unwrappedData = null; // null out in case processOneRpc throws.\n      processOneRpc(requestData);\n    }\n  }\n}",
        "reject_response": "private void unwrapPacketAndProcessRpcs(byte[] inBuf)\n    throws IOException, InterruptedException {\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"Have read input token of size \" + inBuf.length\n        + \" for processing by saslServer.unwrap()\");\n  }\n  inBuf = saslServer.unwrap(inBuf, 0, inBuf.length);\n  ReadableByteChannel ch = Channels.newChannel(new ByteArrayInputStream(\n      inBuf));\n  // Read all RPCs contained in the inBuf, even partial ones\n  while (!shouldClose()) { // stop if a fatal response has been sent.\n    int count = -1;\n    if (unwrappedDataLengthBuffer.remaining() > 0) {\n      count = channelRead(ch, unwrappedDataLengthBuffer);\n      if (count <= 0 || unwrappedDataLengthBuffer.remaining() > 0)\n        return;\n    }\n\n    if (unwrappedData == null) {\n      unwrappedDataLengthBuffer.flip();\n      int unwrappedDataLength = unwrappedDataLengthBuffer.getInt();\n      unwrappedData = ByteBuffer.allocate(unwrappedDataLength);\n    }\n\n    count = channelRead(ch, unwrappedData);\n    if (count <= 0 || unwrappedData.remaining() > 0)\n      return;\n\n    if (unwrappedData.remaining() == 0) {\n      unwrappedDataLengthBuffer.clear();\n      unwrappedData.flip();\n      ByteBuffer requestData = unwrappedData;\n      unwrappedData = null; // null out in case processOneRpc throws.\n      processOneRpc(requestData);\n    }\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2699,
        "instruction": "Response readResponse(final Bridge bridge, ProtocolData protData,\n                      final boolean threadSafe) {\n    final int fragment = protData.getDataFragment();\n    final String dataDir = protData.getDataSource();\n\n    // Creating an internal streaming class\n    // which will iterate the records and put them on the\n    // output stream\n    final StreamingOutput streaming = new StreamingOutput() {\n        @Override\n        public void write(final OutputStream out) throws IOException,\n                WebApplicationException {\n            long recordCount = 0;\n\n            if (!threadSafe) {\n                lock(dataDir);\n            }\n            try {\n\n                if (!bridge.beginIteration()) {\n                    return;\n                }\n\n                Writable record;\n                DataOutputStream dos = new DataOutputStream(out);\n                LOG.debug(\"Starting streaming fragment \" + fragment\n                        + \" of resource \" + dataDir);\n                while ((record = bridge.getNext()) != null) {\n                    record.write(dos);\n                    ++recordCount;\n                }\n                LOG.debug(\"Finished streaming fragment \" + fragment\n                        + \" of resource \" + dataDir + \", \" + recordCount\n                        + \" records.\");\n            } catch (ClientAbortException e) {\n                // Occurs whenever client (HAWQ) decides the end the\n                // connection\n                LOG.error(\"Remote connection closed by HAWQ\", e);\n            } catch (Exception e) {\n                LOG.error(\"Exception thrown when streaming\", e);\n                throw new IOException(e.getMessage());\n            } finally {\n                if (!threadSafe) {\n                    unlock(dataDir);\n                }\n            }\n        }\n    };\n\n    return Response.ok(streaming, MediaType.APPLICATION_OCTET_STREAM).build();\n}",
        "accept_response": "Response readResponse(final Bridge bridge, ProtocolData protData,\n                      final boolean threadSafe) {\n    final int fragment = protData.getDataFragment();\n    final String dataDir = protData.getDataSource();\n\n    // Creating an internal streaming class\n    // which will iterate the records and put them on the\n    // output stream\n    final StreamingOutput streaming = new StreamingOutput() {\n        @Override\n        public void write(final OutputStream out) throws IOException,\n                WebApplicationException {\n            long recordCount = 0;\n\n            if (!threadSafe) {\n                lock(dataDir);\n            }\n            try {\n\n                if (!bridge.beginIteration()) {\n                    return;\n                }\n\n                Writable record;\n                DataOutputStream dos = new DataOutputStream(out);\n                LOG.debug(\"Starting streaming fragment \" + fragment\n                        + \" of resource \" + dataDir);\n                while ((record = bridge.getNext()) != null) {\n                    record.write(dos);\n                    ++recordCount;\n                }\n                LOG.debug(\"Finished streaming fragment \" + fragment\n                        + \" of resource \" + dataDir + \", \" + recordCount\n                        + \" records.\");\n            } catch (ClientAbortException e) {\n                // Occurs whenever client (HAWQ) decides the end the\n                // connection\n                LOG.error(\"Remote connection closed by HAWQ\", e);\n            } catch (Exception e) {\n                LOG.error(\"Exception thrown when streaming\", e);\n                throw new IOException(e.getMessage());\n            } finally {\n                LOG.debug(\"Stopped streaming fragment \" + fragment\n                        + \" of resource \" + dataDir + \", \" + recordCount\n                        + \" records.\");\n                if (!threadSafe) {\n                    unlock(dataDir);\n                }\n            }\n        }\n    };\n\n    return Response.ok(streaming, MediaType.APPLICATION_OCTET_STREAM).build();\n}",
        "reject_response": "Response readResponse(final Bridge bridge, ProtocolData protData,\n                      final boolean threadSafe) {\n    final int fragment = protData.getDataFragment();\n    final String dataDir = protData.getDataSource();\n\n    // Creating an internal streaming class\n    // which will iterate the records and put them on the\n    // output stream\n    final StreamingOutput streaming = new StreamingOutput() {\n        @Override\n        public void write(final OutputStream out) throws IOException,\n                WebApplicationException {\n            long recordCount = 0;\n\n            if (!threadSafe) {\n                lock(dataDir);\n            }\n            try {\n\n                if (!bridge.beginIteration()) {\n                    return;\n                }\n\n                Writable record;\n                DataOutputStream dos = new DataOutputStream(out);\n                LOG.debug(\"Starting streaming fragment \" + fragment\n                        + \" of resource \" + dataDir);\n                while ((record = bridge.getNext()) != null) {\n                    record.write(dos);\n                    ++recordCount;\n                }\n                LOG.debug(\"Finished streaming fragment \" + fragment\n                        + \" of resource \" + dataDir + \", \" + recordCount\n                        + \" records.\");\n            } catch (ClientAbortException e) {\n                // Occurs whenever client (HAWQ) decides the end the\n                // connection\n                LOG.error(\"Remote connection closed by HAWQ\", e);\n            } catch (Exception e) {\n                LOG.error(\"Exception thrown when streaming\", e);\n                throw new IOException(e.getMessage());\n            } finally {\n                Log.debug(\"Stopped streaming fragment \" + fragment\n                if (!threadSafe) {\n                    unlock(dataDir);\n                }\n            }\n        }\n    };\n\n    return Response.ok(streaming, MediaType.APPLICATION_OCTET_STREAM).build();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2600,
        "instruction": "private void handleFullyFailedRequest(\n        Throwable err, List<Record> requestEntries, Consumer<List<Record>> requestResult) {\n    numRecordsOutErrorsCounter.inc(requestEntries.size());\n    numRecordsSendErrorsCounter.inc(requestEntries.size());\n\n    if (isRetryable(err)) {\n        requestResult.accept(requestEntries);\n    }\n}",
        "accept_response": "private void handleFullyFailedRequest(\n        Throwable err, List<Record> requestEntries, Consumer<List<Record>> requestResult) {\n    LOG.debug(\n            \"KDF Sink failed to write and will retry {} entries to KDF first request was {}\",\n            requestEntries.size(),\n            requestEntries.get(0).toString(),\n            err);\n    numRecordsOutErrorsCounter.inc(requestEntries.size());\n    numRecordsSendErrorsCounter.inc(requestEntries.size());\n\n    if (isRetryable(err)) {\n        requestResult.accept(requestEntries);\n    }\n}",
        "reject_response": "private void handleFullyFailedRequest(\n        Throwable err, List<Record> requestEntries, Consumer<List<Record>> requestResult) {\n    LOG.warn(\n            \"KDF Sink failed to persist {} entries to KDF first request was {}\",\n    numRecordsOutErrorsCounter.inc(requestEntries.size());\n    numRecordsSendErrorsCounter.inc(requestEntries.size());\n\n    if (isRetryable(err)) {\n        requestResult.accept(requestEntries);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2899,
        "instruction": "public List<String> getLocalDomainNames() {\n    List<String> names = new ArrayList<String>();\n\n    LOGGER.debug(\"Start Local ipaddress lookup\");\n    try {\n        InetAddress ia[] = InetAddress.getAllByName(InetAddress\n                .getLocalHost().getHostName());\n\n        for (int i = 0; i < ia.length; i++) {\n            String host = ia[i].getHostName();\n            names.add(host);\n\n        }\n    } catch (UnknownHostException e) {\n        // just ignore this..\n    }\n    return names;\n\n}",
        "accept_response": "public List<String> getLocalDomainNames() {\n    List<String> names = new ArrayList<String>();\n\n    LOGGER.debug(\"Start Local ipaddress lookup\");\n    try {\n        InetAddress ia[] = InetAddress.getAllByName(InetAddress\n                .getLocalHost().getHostName());\n\n        for (int i = 0; i < ia.length; i++) {\n            String host = ia[i].getHostName();\n            names.add(host);\n\n            LOGGER.debug(\"Add hostname {} to list\", host);\n        }\n    } catch (UnknownHostException e) {\n        // just ignore this..\n    }\n    return names;\n\n}",
        "reject_response": "public List<String> getLocalDomainNames() {\n    List<String> names = new ArrayList<String>();\n\n    LOGGER.debug(\"Start Local ipaddress lookup\");\n    try {\n        InetAddress ia[] = InetAddress.getAllByName(InetAddress\n                .getLocalHost().getHostName());\n\n        for (int i = 0; i < ia.length; i++) {\n            String host = ia[i].getHostName();\n            names.add(host);\n\n            log.debug(\"Add hostname \" + host + \" to list\");\n        }\n    } catch (UnknownHostException e) {\n        // just ignore this..\n    }\n    return names;\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2552,
        "instruction": "@Override\npublic void update() {\n  // Get sizing information for the batch.\n  setRecordBatchSizer(new RecordBatchSizer(incoming));\n\n  final TypedFieldId typedFieldId = incoming.getValueVectorId(popConfig.getColumn());\n  final MaterializedField field = incoming.getSchema().getColumn(typedFieldId.getFieldIds()[0]);\n\n  // Get column size of flatten column.\n  RecordBatchSizer.ColumnSize columnSize = getRecordBatchSizer().getColumn(field.getName());\n\n  // Average rowWidth of flatten column\n  final int avgRowWidthFlattenColumn = columnSize.getNetSizePerEntry();\n\n  // Average rowWidth excluding the flatten column.\n  final int avgRowWidthWithOutFlattenColumn = getRecordBatchSizer().netRowWidth() - avgRowWidthFlattenColumn;\n\n  // Average rowWidth of single element in the flatten list.\n  // subtract the offset vector size from column data size.\n  final int avgRowWidthSingleFlattenEntry =\n    RecordBatchSizer.safeDivide(columnSize.getTotalNetSize() - (getOffsetVectorWidth() * columnSize.getValueCount()),\n      columnSize.getElementCount());\n\n  // Average rowWidth of outgoing batch.\n  final int avgOutgoingRowWidth = avgRowWidthWithOutFlattenColumn + avgRowWidthSingleFlattenEntry;\n\n  final int outputBatchSize = getOutputBatchSize();\n  // Number of rows in outgoing batch\n  setOutputRowCount(outputBatchSize, avgOutgoingRowWidth);\n\n  setOutgoingRowWidth(avgOutgoingRowWidth);\n\n  // Limit to lower bound of total number of rows possible for this batch\n  // i.e. all rows fit within memory budget.\n  setOutputRowCount(Math.min(columnSize.getElementCount(), getOutputRowCount()));\n\n\n  updateIncomingStats();\n}",
        "accept_response": "@Override\npublic void update() {\n  // Get sizing information for the batch.\n  setRecordBatchSizer(new RecordBatchSizer(incoming));\n\n  final TypedFieldId typedFieldId = incoming.getValueVectorId(popConfig.getColumn());\n  final MaterializedField field = incoming.getSchema().getColumn(typedFieldId.getFieldIds()[0]);\n\n  // Get column size of flatten column.\n  RecordBatchSizer.ColumnSize columnSize = getRecordBatchSizer().getColumn(field.getName());\n\n  // Average rowWidth of flatten column\n  final int avgRowWidthFlattenColumn = columnSize.getNetSizePerEntry();\n\n  // Average rowWidth excluding the flatten column.\n  final int avgRowWidthWithOutFlattenColumn = getRecordBatchSizer().netRowWidth() - avgRowWidthFlattenColumn;\n\n  // Average rowWidth of single element in the flatten list.\n  // subtract the offset vector size from column data size.\n  final int avgRowWidthSingleFlattenEntry =\n    RecordBatchSizer.safeDivide(columnSize.getTotalNetSize() - (getOffsetVectorWidth() * columnSize.getValueCount()),\n      columnSize.getElementCount());\n\n  // Average rowWidth of outgoing batch.\n  final int avgOutgoingRowWidth = avgRowWidthWithOutFlattenColumn + avgRowWidthSingleFlattenEntry;\n\n  final int outputBatchSize = getOutputBatchSize();\n  // Number of rows in outgoing batch\n  setOutputRowCount(outputBatchSize, avgOutgoingRowWidth);\n\n  setOutgoingRowWidth(avgOutgoingRowWidth);\n\n  // Limit to lower bound of total number of rows possible for this batch\n  // i.e. all rows fit within memory budget.\n  setOutputRowCount(Math.min(columnSize.getElementCount(), getOutputRowCount()));\n\n  logger.debug(\"BATCH_STATS, incoming: {}\", getRecordBatchSizer());\n\n  updateIncomingStats();\n}",
        "reject_response": "@Override\npublic void update() {\n  // Get sizing information for the batch.\n  setRecordBatchSizer(new RecordBatchSizer(incoming));\n\n  final TypedFieldId typedFieldId = incoming.getValueVectorId(popConfig.getColumn());\n  final MaterializedField field = incoming.getSchema().getColumn(typedFieldId.getFieldIds()[0]);\n\n  // Get column size of flatten column.\n  RecordBatchSizer.ColumnSize columnSize = getRecordBatchSizer().getColumn(field.getName());\n\n  // Average rowWidth of flatten column\n  final int avgRowWidthFlattenColumn = columnSize.getNetSizePerEntry();\n\n  // Average rowWidth excluding the flatten column.\n  final int avgRowWidthWithOutFlattenColumn = getRecordBatchSizer().netRowWidth() - avgRowWidthFlattenColumn;\n\n  // Average rowWidth of single element in the flatten list.\n  // subtract the offset vector size from column data size.\n  final int avgRowWidthSingleFlattenEntry =\n    RecordBatchSizer.safeDivide(columnSize.getTotalNetSize() - (getOffsetVectorWidth() * columnSize.getValueCount()),\n      columnSize.getElementCount());\n\n  // Average rowWidth of outgoing batch.\n  final int avgOutgoingRowWidth = avgRowWidthWithOutFlattenColumn + avgRowWidthSingleFlattenEntry;\n\n  final int outputBatchSize = getOutputBatchSize();\n  // Number of rows in outgoing batch\n  setOutputRowCount(outputBatchSize, avgOutgoingRowWidth);\n\n  setOutgoingRowWidth(avgOutgoingRowWidth);\n\n  // Limit to lower bound of total number of rows possible for this batch\n  // i.e. all rows fit within memory budget.\n  setOutputRowCount(Math.min(columnSize.getElementCount(), getOutputRowCount()));\n\n  if (logger.isDebugEnabled()) {\n    logger.debug(\"BATCH_STATS, incoming:\\n {}\", getRecordBatchSizer());\n  }\n\n  updateIncomingStats();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2769,
        "instruction": "public boolean isServableFromCache(final HttpRequest request) {\n    final String method = request.getMethod();\n\n    final ProtocolVersion pv = request.getVersion() != null ? request.getVersion() : HttpVersion.DEFAULT;\n    if (HttpVersion.HTTP_1_1.compareToVersion(pv) != 0) {\n        log.debug(\"non-HTTP/1.1 request is not serveable from cache\");\n        return false;\n    }\n\n    if (!method.equals(HeaderConstants.GET_METHOD) && !method.equals(HeaderConstants.HEAD_METHOD)) {\n        return false;\n    }\n\n    if (request.countHeaders(HeaderConstants.PRAGMA) > 0) {\n        log.debug(\"request with Pragma header is not serveable from cache\");\n        return false;\n    }\n\n    final Iterator<HeaderElement> it = MessageSupport.iterate(request, HeaderConstants.CACHE_CONTROL);\n    while (it.hasNext()) {\n        final HeaderElement cacheControlElement = it.next();\n        if (HeaderConstants.CACHE_CONTROL_NO_STORE.equalsIgnoreCase(cacheControlElement.getName())) {\n            log.debug(\"Request with no-store is not serveable from cache\");\n            return false;\n        }\n        if (HeaderConstants.CACHE_CONTROL_NO_CACHE.equalsIgnoreCase(cacheControlElement.getName())) {\n            log.debug(\"Request with no-cache is not serveable from cache\");\n            return false;\n        }\n    }\n\n    log.debug(\"Request is serveable from cache\");\n    return true;\n}",
        "accept_response": "public boolean isServableFromCache(final HttpRequest request) {\n    final String method = request.getMethod();\n\n    final ProtocolVersion pv = request.getVersion() != null ? request.getVersion() : HttpVersion.DEFAULT;\n    if (HttpVersion.HTTP_1_1.compareToVersion(pv) != 0) {\n        log.debug(\"non-HTTP/1.1 request is not serveable from cache\");\n        return false;\n    }\n\n    if (!method.equals(HeaderConstants.GET_METHOD) && !method.equals(HeaderConstants.HEAD_METHOD)) {\n        if (log.isDebugEnabled()) {\n            log.debug(\"{} request is not serveable from cache\", method);\n        }\n        return false;\n    }\n\n    if (request.countHeaders(HeaderConstants.PRAGMA) > 0) {\n        log.debug(\"request with Pragma header is not serveable from cache\");\n        return false;\n    }\n\n    final Iterator<HeaderElement> it = MessageSupport.iterate(request, HeaderConstants.CACHE_CONTROL);\n    while (it.hasNext()) {\n        final HeaderElement cacheControlElement = it.next();\n        if (HeaderConstants.CACHE_CONTROL_NO_STORE.equalsIgnoreCase(cacheControlElement.getName())) {\n            log.debug(\"Request with no-store is not serveable from cache\");\n            return false;\n        }\n        if (HeaderConstants.CACHE_CONTROL_NO_CACHE.equalsIgnoreCase(cacheControlElement.getName())) {\n            log.debug(\"Request with no-cache is not serveable from cache\");\n            return false;\n        }\n    }\n\n    log.debug(\"Request is serveable from cache\");\n    return true;\n}",
        "reject_response": "public boolean isServableFromCache(final HttpRequest request) {\n    final String method = request.getMethod();\n\n    final ProtocolVersion pv = request.getVersion() != null ? request.getVersion() : HttpVersion.DEFAULT;\n    if (HttpVersion.HTTP_1_1.compareToVersion(pv) != 0) {\n        log.debug(\"non-HTTP/1.1 request is not serveable from cache\");\n        return false;\n    }\n\n    if (!method.equals(HeaderConstants.GET_METHOD) && !method.equals(HeaderConstants.HEAD_METHOD)) {\n        if (log.isDebugEnabled()) {\n            log.debug(method + \" request is not serveable from cache\");\n        }\n        return false;\n    }\n\n    if (request.countHeaders(HeaderConstants.PRAGMA) > 0) {\n        log.debug(\"request with Pragma header is not serveable from cache\");\n        return false;\n    }\n\n    final Iterator<HeaderElement> it = MessageSupport.iterate(request, HeaderConstants.CACHE_CONTROL);\n    while (it.hasNext()) {\n        final HeaderElement cacheControlElement = it.next();\n        if (HeaderConstants.CACHE_CONTROL_NO_STORE.equalsIgnoreCase(cacheControlElement.getName())) {\n            log.debug(\"Request with no-store is not serveable from cache\");\n            return false;\n        }\n        if (HeaderConstants.CACHE_CONTROL_NO_CACHE.equalsIgnoreCase(cacheControlElement.getName())) {\n            log.debug(\"Request with no-cache is not serveable from cache\");\n            return false;\n        }\n    }\n\n    log.debug(\"Request is serveable from cache\");\n    return true;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3134,
        "instruction": "@Override\nprotected void startRpcServer() {\n  try {\n    this.address = new InetSocketAddress(InetAddress.getLocalHost(), 0);\n  } catch (UnknownHostException e) {\n    throw new TezUncheckedException(e);\n  }\n}",
        "accept_response": "@Override\nprotected void startRpcServer() {\n  try {\n    this.address = new InetSocketAddress(InetAddress.getLocalHost(), 0);\n  } catch (UnknownHostException e) {\n    throw new TezUncheckedException(e);\n  }\n  LOG.debug(\"Not starting TaskAttemptListener RPC in LocalMode\");\n}",
        "reject_response": "@Override\nprotected void startRpcServer() {\n  try {\n    this.address = new InetSocketAddress(InetAddress.getLocalHost(), 0);\n  } catch (UnknownHostException e) {\n    throw new TezUncheckedException(e);\n  }\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"Not starting TaskAttemptListener RPC in LocalMode\");\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3236,
        "instruction": "public void testInitiateConnection(long sid) throws Exception {\n    Socket sock = new Socket();\n    setSockOpts(sock);\n    sock.connect(self.getVotingView().get(sid).electionAddr, cnxTO);\n    initiateConnection(sock, sid);\n}",
        "accept_response": "public void testInitiateConnection(long sid) throws Exception {\n    LOG.debug(\"Opening channel to server {}\", sid);\n    Socket sock = new Socket();\n    setSockOpts(sock);\n    sock.connect(self.getVotingView().get(sid).electionAddr, cnxTO);\n    initiateConnection(sock, sid);\n}",
        "reject_response": "public void testInitiateConnection(long sid) throws Exception {\n    LOG.debug(\"Opening channel to server \" + sid);\n    Socket sock = new Socket();\n    setSockOpts(sock);\n    sock.connect(self.getVotingView().get(sid).electionAddr, cnxTO);\n    initiateConnection(sock, sid);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2879,
        "instruction": "@Override\npublic Block allocWrite(int bytesSpace)\n{\n    //log.info(\"AW(\"+bytesSpace+\"):\"+state()) ;\n    if ( inAllocWrite )\n\n    // Include space for length.\n    int spaceRequired = bytesSpace + SizeOfInt ;\n\n    // Find space.\n    if (  writeBuffer != null && spaceRequired > writeBuffer.remaining() )\n        flushOutputBuffer() ;\n\n    if ( writeBuffer == null || spaceRequired > writeBuffer.remaining() )\n    {\n        // Too big. Have flushed buffering if buffering.\n        inAllocWrite = true ;\n        ByteBuffer bb = ByteBuffer.allocate(bytesSpace) ;\n        allocBlock = new Block(filesize, bb) ;\n        allocLocation = -1 ;\n        //log.info(\"AW:\"+state()+\"-> ----\") ;\n        return allocBlock ;\n    }\n\n    // Will fit.\n    inAllocWrite = true ;\n    int start = writeBuffer.position() ;\n    // Old values for restoration\n    oldBufferPosn = start ;\n    oldBufferLimit = writeBuffer.limit() ;\n\n    // id (but don't tell the caller yet).\n    allocLocation = filesize+start ;\n\n    // Slice it.\n    writeBuffer.putInt(bytesSpace) ;\n    writeBuffer.position(start + SizeOfInt) ;\n    writeBuffer.limit(start+spaceRequired) ;\n    ByteBuffer bb = writeBuffer.slice() ;\n\n    allocBlock = new Block(allocLocation, bb) ;\n\n    if ( logging )\n        log(\"AW: %s->0x%X\", state(), allocLocation) ;\n    return allocBlock ;\n}",
        "accept_response": "@Override\npublic Block allocWrite(int bytesSpace)\n{\n    //log.info(\"AW(\"+bytesSpace+\"):\"+state()) ;\n    if ( inAllocWrite )\n        Log.error(this, \"In the middle of an alloc-write\") ;\n\n    // Include space for length.\n    int spaceRequired = bytesSpace + SizeOfInt ;\n\n    // Find space.\n    if (  writeBuffer != null && spaceRequired > writeBuffer.remaining() )\n        flushOutputBuffer() ;\n\n    if ( writeBuffer == null || spaceRequired > writeBuffer.remaining() )\n    {\n        // Too big. Have flushed buffering if buffering.\n        inAllocWrite = true ;\n        ByteBuffer bb = ByteBuffer.allocate(bytesSpace) ;\n        allocBlock = new Block(filesize, bb) ;\n        allocLocation = -1 ;\n        //log.info(\"AW:\"+state()+\"-> ----\") ;\n        return allocBlock ;\n    }\n\n    // Will fit.\n    inAllocWrite = true ;\n    int start = writeBuffer.position() ;\n    // Old values for restoration\n    oldBufferPosn = start ;\n    oldBufferLimit = writeBuffer.limit() ;\n\n    // id (but don't tell the caller yet).\n    allocLocation = filesize+start ;\n\n    // Slice it.\n    writeBuffer.putInt(bytesSpace) ;\n    writeBuffer.position(start + SizeOfInt) ;\n    writeBuffer.limit(start+spaceRequired) ;\n    ByteBuffer bb = writeBuffer.slice() ;\n\n    allocBlock = new Block(allocLocation, bb) ;\n\n    if ( logging )\n        log(\"AW: %s->0x%X\", state(), allocLocation) ;\n    return allocBlock ;\n}",
        "reject_response": "@Override\npublic Block allocWrite(int bytesSpace)\n{\n    //log.info(\"AW(\"+bytesSpace+\"):\"+state()) ;\n    if ( inAllocWrite )\n        Log.fatal(this, \"In the middle of an alloc-write\") ;\n\n    // Include space for length.\n    int spaceRequired = bytesSpace + SizeOfInt ;\n\n    // Find space.\n    if (  writeBuffer != null && spaceRequired > writeBuffer.remaining() )\n        flushOutputBuffer() ;\n\n    if ( writeBuffer == null || spaceRequired > writeBuffer.remaining() )\n    {\n        // Too big. Have flushed buffering if buffering.\n        inAllocWrite = true ;\n        ByteBuffer bb = ByteBuffer.allocate(bytesSpace) ;\n        allocBlock = new Block(filesize, bb) ;\n        allocLocation = -1 ;\n        //log.info(\"AW:\"+state()+\"-> ----\") ;\n        return allocBlock ;\n    }\n\n    // Will fit.\n    inAllocWrite = true ;\n    int start = writeBuffer.position() ;\n    // Old values for restoration\n    oldBufferPosn = start ;\n    oldBufferLimit = writeBuffer.limit() ;\n\n    // id (but don't tell the caller yet).\n    allocLocation = filesize+start ;\n\n    // Slice it.\n    writeBuffer.putInt(bytesSpace) ;\n    writeBuffer.position(start + SizeOfInt) ;\n    writeBuffer.limit(start+spaceRequired) ;\n    ByteBuffer bb = writeBuffer.slice() ;\n\n    allocBlock = new Block(allocLocation, bb) ;\n\n    if ( logging )\n        log(\"AW: %s->0x%X\", state(), allocLocation) ;\n    return allocBlock ;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2475,
        "instruction": "private AtlasGraph retry() throws RepositoryException {\n    int retryCounter = 0;\n\n    while (retryCounter < MAX_RETRY_COUNT) {\n        try {\n            // Retry after 30 sec to get graph instance\n            Thread.sleep(RETRY_SLEEP_TIME_MS);\n\n            return getGraphInstance();\n        } catch (Exception ex) {\n            retryCounter++;\n\n\n            if (retryCounter >= MAX_RETRY_COUNT) {\n                LOG.info(\"Max retries exceeded.\");\n                break;\n            }\n        }\n    }\n\n    throw new RepositoryException(\"Max retries exceeded. Failed to obtain graph instance after \" + MAX_RETRY_COUNT + \" retries\");\n}",
        "accept_response": "private AtlasGraph retry() throws RepositoryException {\n    int retryCounter = 0;\n\n    while (retryCounter < MAX_RETRY_COUNT) {\n        try {\n            // Retry after 30 sec to get graph instance\n            Thread.sleep(RETRY_SLEEP_TIME_MS);\n\n            return getGraphInstance();\n        } catch (Exception ex) {\n            retryCounter++;\n\n            LOG.warn(\"Failed to obtain graph instance on attempt \" + retryCounter + \" of \" + MAX_RETRY_COUNT, ex);\n\n            if (retryCounter >= MAX_RETRY_COUNT) {\n                LOG.info(\"Max retries exceeded.\");\n                break;\n            }\n        }\n    }\n\n    throw new RepositoryException(\"Max retries exceeded. Failed to obtain graph instance after \" + MAX_RETRY_COUNT + \" retries\");\n}",
        "reject_response": "private AtlasGraph retry() throws RepositoryException {\n    int retryCounter = 0;\n\n    while (retryCounter < MAX_RETRY_COUNT) {\n        try {\n            // Retry after 30 sec to get graph instance\n            Thread.sleep(RETRY_SLEEP_TIME_MS);\n\n            return getGraphInstance();\n        } catch (Exception ex) {\n            retryCounter++;\n\n            LOG.info(\"Failed to obtain graph instance on retry \" + retryCounter + \" of \" + MAX_RETRY_COUNT + \" error: \" + ex);\n\n            if (retryCounter >= MAX_RETRY_COUNT) {\n                LOG.info(\"Max retries exceeded.\");\n                break;\n            }\n        }\n    }\n\n    throw new RepositoryException(\"Max retries exceeded. Failed to obtain graph instance after \" + MAX_RETRY_COUNT + \" retries\");\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3239,
        "instruction": "@Override\npublic void run() {\n    threadCnt.incrementAndGet();\n    try {\n        /**\n         * If there is nothing in the queue to send, then we\n         * send the lastMessage to ensure that the last message\n         * was received by the peer. The message could be dropped\n         * in case self or the peer shutdown their connection\n         * (and exit the thread) prior to reading/processing\n         * the last message. Duplicate messages are handled correctly\n         * by the peer.\n         *\n         * If the send queue is non-empty, then we have a recent\n         * message than that stored in lastMessage. To avoid sending\n         * stale message, we should send the message in the send queue.\n         */\n        ArrayBlockingQueue<ByteBuffer> bq = queueSendMap.get(sid);\n        if (bq == null || isSendQueueEmpty(bq)) {\n           ByteBuffer b = lastMessageSent.get(sid);\n           if (b != null) {\n               send(b);\n           }\n        }\n    } catch (IOException e) {\n        LOG.error(\"Failed to send last message. Shutting down thread.\", e);\n        this.finish();\n    }\n\n    try {\n        while (running && !shutdown && sock != null) {\n\n            ByteBuffer b = null;\n            try {\n                ArrayBlockingQueue<ByteBuffer> bq = queueSendMap\n                        .get(sid);\n                if (bq != null) {\n                    b = pollSendQueue(bq, 1000, TimeUnit.MILLISECONDS);\n                } else {\n                    LOG.error(\"No queue of incoming messages for \" +\n                              \"server \" + sid);\n                    break;\n                }\n\n                if(b != null){\n                    lastMessageSent.put(sid, b);\n                    send(b);\n                }\n            } catch (InterruptedException e) {\n                LOG.warn(\"Interrupted while waiting for message on queue\",\n                        e);\n            }\n        }\n    } catch (Exception e) {\n        LOG.warn(\"Exception when using channel: for id \" + sid\n                 + \" my id = \" + QuorumCnxManager.this.mySid\n                 + \" error = \" + e);\n    }\n    this.finish();\n    LOG.warn(\"Send worker leaving thread \" + \" id \" + sid + \" my id = \" + self.getId());\n}",
        "accept_response": "@Override\npublic void run() {\n    threadCnt.incrementAndGet();\n    try {\n        /**\n         * If there is nothing in the queue to send, then we\n         * send the lastMessage to ensure that the last message\n         * was received by the peer. The message could be dropped\n         * in case self or the peer shutdown their connection\n         * (and exit the thread) prior to reading/processing\n         * the last message. Duplicate messages are handled correctly\n         * by the peer.\n         *\n         * If the send queue is non-empty, then we have a recent\n         * message than that stored in lastMessage. To avoid sending\n         * stale message, we should send the message in the send queue.\n         */\n        ArrayBlockingQueue<ByteBuffer> bq = queueSendMap.get(sid);\n        if (bq == null || isSendQueueEmpty(bq)) {\n           ByteBuffer b = lastMessageSent.get(sid);\n           if (b != null) {\n               LOG.debug(\"Attempting to send lastMessage to sid={}\", sid);\n               send(b);\n           }\n        }\n    } catch (IOException e) {\n        LOG.error(\"Failed to send last message. Shutting down thread.\", e);\n        this.finish();\n    }\n\n    try {\n        while (running && !shutdown && sock != null) {\n\n            ByteBuffer b = null;\n            try {\n                ArrayBlockingQueue<ByteBuffer> bq = queueSendMap\n                        .get(sid);\n                if (bq != null) {\n                    b = pollSendQueue(bq, 1000, TimeUnit.MILLISECONDS);\n                } else {\n                    LOG.error(\"No queue of incoming messages for \" +\n                              \"server \" + sid);\n                    break;\n                }\n\n                if(b != null){\n                    lastMessageSent.put(sid, b);\n                    send(b);\n                }\n            } catch (InterruptedException e) {\n                LOG.warn(\"Interrupted while waiting for message on queue\",\n                        e);\n            }\n        }\n    } catch (Exception e) {\n        LOG.warn(\"Exception when using channel: for id \" + sid\n                 + \" my id = \" + QuorumCnxManager.this.mySid\n                 + \" error = \" + e);\n    }\n    this.finish();\n    LOG.warn(\"Send worker leaving thread \" + \" id \" + sid + \" my id = \" + self.getId());\n}",
        "reject_response": "@Override\npublic void run() {\n    threadCnt.incrementAndGet();\n    try {\n        /**\n         * If there is nothing in the queue to send, then we\n         * send the lastMessage to ensure that the last message\n         * was received by the peer. The message could be dropped\n         * in case self or the peer shutdown their connection\n         * (and exit the thread) prior to reading/processing\n         * the last message. Duplicate messages are handled correctly\n         * by the peer.\n         *\n         * If the send queue is non-empty, then we have a recent\n         * message than that stored in lastMessage. To avoid sending\n         * stale message, we should send the message in the send queue.\n         */\n        ArrayBlockingQueue<ByteBuffer> bq = queueSendMap.get(sid);\n        if (bq == null || isSendQueueEmpty(bq)) {\n           ByteBuffer b = lastMessageSent.get(sid);\n           if (b != null) {\n               LOG.debug(\"Attempting to send lastMessage to sid=\" + sid);\n               send(b);\n           }\n        }\n    } catch (IOException e) {\n        LOG.error(\"Failed to send last message. Shutting down thread.\", e);\n        this.finish();\n    }\n\n    try {\n        while (running && !shutdown && sock != null) {\n\n            ByteBuffer b = null;\n            try {\n                ArrayBlockingQueue<ByteBuffer> bq = queueSendMap\n                        .get(sid);\n                if (bq != null) {\n                    b = pollSendQueue(bq, 1000, TimeUnit.MILLISECONDS);\n                } else {\n                    LOG.error(\"No queue of incoming messages for \" +\n                              \"server \" + sid);\n                    break;\n                }\n\n                if(b != null){\n                    lastMessageSent.put(sid, b);\n                    send(b);\n                }\n            } catch (InterruptedException e) {\n                LOG.warn(\"Interrupted while waiting for message on queue\",\n                        e);\n            }\n        }\n    } catch (Exception e) {\n        LOG.warn(\"Exception when using channel: for id \" + sid\n                 + \" my id = \" + QuorumCnxManager.this.mySid\n                 + \" error = \" + e);\n    }\n    this.finish();\n    LOG.warn(\"Send worker leaving thread \" + \" id \" + sid + \" my id = \" + self.getId());\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2628,
        "instruction": "@RequestMapping(value = \"/dataBrowserExport\", method = RequestMethod.GET)\npublic void dataBrowserExport(HttpServletRequest request, HttpServletResponse response)\n    throws IOException {\n  // get query string\n  String query = request.getParameter(\"query\");\n  String members = request.getParameter(\"members\");\n  int limit = 0;\n\n  try {\n    limit = Integer.valueOf(request.getParameter(\"limit\"));\n  } catch (NumberFormatException e) {\n    limit = 0;\n  }\n\n  ObjectNode queryResult = mapper.createObjectNode();\n  try {\n\n    if (StringUtils.isNotBlank(query)) {\n      // get cluster object\n      Cluster cluster = Repository.get().getCluster();\n      String userName = request.getUserPrincipal().getName();\n\n      // Call execute query method\n      queryResult = cluster.executeQuery(query, members, limit);\n\n      // Add query in history if query is executed successfully\n      if (!queryResult.has(\"error\")) {\n        // Add html escaped query to history\n        String escapedQuery = StringEscapeUtils.escapeHtml(query);\n        cluster.addQueryInHistory(escapedQuery, userName);\n      }\n    }\n  } catch (Exception e) {\n    logger.debug(\"Exception Occurred : \", e);\n  }\n\n  response.setContentType(\"application/json\");\n  response.setHeader(\"Content-Disposition\", \"attachment; filename=results.json\");\n  response.getOutputStream().write(queryResult.toString().getBytes());\n}",
        "accept_response": "@RequestMapping(value = \"/dataBrowserExport\", method = RequestMethod.GET)\npublic void dataBrowserExport(HttpServletRequest request, HttpServletResponse response)\n    throws IOException {\n  // get query string\n  String query = request.getParameter(\"query\");\n  String members = request.getParameter(\"members\");\n  int limit = 0;\n\n  try {\n    limit = Integer.valueOf(request.getParameter(\"limit\"));\n  } catch (NumberFormatException e) {\n    limit = 0;\n    logger.debug(e);\n  }\n\n  ObjectNode queryResult = mapper.createObjectNode();\n  try {\n\n    if (StringUtils.isNotBlank(query)) {\n      // get cluster object\n      Cluster cluster = Repository.get().getCluster();\n      String userName = request.getUserPrincipal().getName();\n\n      // Call execute query method\n      queryResult = cluster.executeQuery(query, members, limit);\n\n      // Add query in history if query is executed successfully\n      if (!queryResult.has(\"error\")) {\n        // Add html escaped query to history\n        String escapedQuery = StringEscapeUtils.escapeHtml(query);\n        cluster.addQueryInHistory(escapedQuery, userName);\n      }\n    }\n  } catch (Exception e) {\n    logger.debug(\"Exception Occurred : \", e);\n  }\n\n  response.setContentType(\"application/json\");\n  response.setHeader(\"Content-Disposition\", \"attachment; filename=results.json\");\n  response.getOutputStream().write(queryResult.toString().getBytes());\n}",
        "reject_response": "@RequestMapping(value = \"/dataBrowserExport\", method = RequestMethod.GET)\npublic void dataBrowserExport(HttpServletRequest request, HttpServletResponse response)\n    throws IOException {\n  // get query string\n  String query = request.getParameter(\"query\");\n  String members = request.getParameter(\"members\");\n  int limit = 0;\n\n  try {\n    limit = Integer.valueOf(request.getParameter(\"limit\"));\n  } catch (NumberFormatException e) {\n    limit = 0;\n    if (LOGGER.finerEnabled()) {\n      LOGGER.finer(e.getMessage());\n    }\n  }\n\n  ObjectNode queryResult = mapper.createObjectNode();\n  try {\n\n    if (StringUtils.isNotBlank(query)) {\n      // get cluster object\n      Cluster cluster = Repository.get().getCluster();\n      String userName = request.getUserPrincipal().getName();\n\n      // Call execute query method\n      queryResult = cluster.executeQuery(query, members, limit);\n\n      // Add query in history if query is executed successfully\n      if (!queryResult.has(\"error\")) {\n        // Add html escaped query to history\n        String escapedQuery = StringEscapeUtils.escapeHtml(query);\n        cluster.addQueryInHistory(escapedQuery, userName);\n      }\n    }\n  } catch (Exception e) {\n    logger.debug(\"Exception Occurred : \", e);\n  }\n\n  response.setContentType(\"application/json\");\n  response.setHeader(\"Content-Disposition\", \"attachment; filename=results.json\");\n  response.getOutputStream().write(queryResult.toString().getBytes());\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3049,
        "instruction": "public static void linkConfSet(SolrZkClient zkClient, String collection, String confSetName) throws KeeperException, InterruptedException {\n  String path = ZkStateReader.COLLECTIONS_ZKNODE + \"/\" + collection;\n  byte[] data;\n  try {\n    data = zkClient.getData(path, null, null, true);\n  } catch (NoNodeException e) {\n    // if there is no node, we will try and create it\n    // first try to make in case we are pre configuring\n    ZkNodeProps props = new ZkNodeProps(CONFIGNAME_PROP, confSetName);\n    try {\n\n      zkClient.makePath(path, Utils.toJSON(props),\n          CreateMode.PERSISTENT, null, true);\n    } catch (KeeperException e2) {\n      // it's okay if the node already exists\n      if (e2.code() != KeeperException.Code.NODEEXISTS) {\n        throw e;\n      }\n      // if we fail creating, setdata\n      // TODO: we should consider using version\n      zkClient.setData(path, Utils.toJSON(props), true);\n    }\n    return;\n  }\n  // we found existing data, let's update it\n  ZkNodeProps props = null;\n  if (data != null) {\n    props = ZkNodeProps.load(data);\n    Map<String, Object> newProps = new HashMap<>();\n    newProps.putAll(props.getProperties());\n    newProps.put(CONFIGNAME_PROP, confSetName);\n    props = new ZkNodeProps(newProps);\n  } else {\n    props = new ZkNodeProps(CONFIGNAME_PROP, confSetName);\n  }\n\n  // TODO: we should consider using version\n  zkClient.setData(path, Utils.toJSON(props), true);\n\n}",
        "accept_response": "public static void linkConfSet(SolrZkClient zkClient, String collection, String confSetName) throws KeeperException, InterruptedException {\n  String path = ZkStateReader.COLLECTIONS_ZKNODE + \"/\" + collection;\n  log.debug(\"Load collection config from: {}\", path);\n  byte[] data;\n  try {\n    data = zkClient.getData(path, null, null, true);\n  } catch (NoNodeException e) {\n    // if there is no node, we will try and create it\n    // first try to make in case we are pre configuring\n    ZkNodeProps props = new ZkNodeProps(CONFIGNAME_PROP, confSetName);\n    try {\n\n      zkClient.makePath(path, Utils.toJSON(props),\n          CreateMode.PERSISTENT, null, true);\n    } catch (KeeperException e2) {\n      // it's okay if the node already exists\n      if (e2.code() != KeeperException.Code.NODEEXISTS) {\n        throw e;\n      }\n      // if we fail creating, setdata\n      // TODO: we should consider using version\n      zkClient.setData(path, Utils.toJSON(props), true);\n    }\n    return;\n  }\n  // we found existing data, let's update it\n  ZkNodeProps props = null;\n  if (data != null) {\n    props = ZkNodeProps.load(data);\n    Map<String, Object> newProps = new HashMap<>();\n    newProps.putAll(props.getProperties());\n    newProps.put(CONFIGNAME_PROP, confSetName);\n    props = new ZkNodeProps(newProps);\n  } else {\n    props = new ZkNodeProps(CONFIGNAME_PROP, confSetName);\n  }\n\n  // TODO: we should consider using version\n  zkClient.setData(path, Utils.toJSON(props), true);\n\n}",
        "reject_response": "public static void linkConfSet(SolrZkClient zkClient, String collection, String confSetName) throws KeeperException, InterruptedException {\n  String path = ZkStateReader.COLLECTIONS_ZKNODE + \"/\" + collection;\n  log.debug(\"Load collection config from:\" + path);\n  byte[] data;\n  try {\n    data = zkClient.getData(path, null, null, true);\n  } catch (NoNodeException e) {\n    // if there is no node, we will try and create it\n    // first try to make in case we are pre configuring\n    ZkNodeProps props = new ZkNodeProps(CONFIGNAME_PROP, confSetName);\n    try {\n\n      zkClient.makePath(path, Utils.toJSON(props),\n          CreateMode.PERSISTENT, null, true);\n    } catch (KeeperException e2) {\n      // it's okay if the node already exists\n      if (e2.code() != KeeperException.Code.NODEEXISTS) {\n        throw e;\n      }\n      // if we fail creating, setdata\n      // TODO: we should consider using version\n      zkClient.setData(path, Utils.toJSON(props), true);\n    }\n    return;\n  }\n  // we found existing data, let's update it\n  ZkNodeProps props = null;\n  if (data != null) {\n    props = ZkNodeProps.load(data);\n    Map<String, Object> newProps = new HashMap<>();\n    newProps.putAll(props.getProperties());\n    newProps.put(CONFIGNAME_PROP, confSetName);\n    props = new ZkNodeProps(newProps);\n  } else {\n    props = new ZkNodeProps(CONFIGNAME_PROP, confSetName);\n  }\n\n  // TODO: we should consider using version\n  zkClient.setData(path, Utils.toJSON(props), true);\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2950,
        "instruction": "@GET\n@Consumes(MediaType.WILDCARD)\n@Produces(SAML_METADATA_MEDIA_TYPE)\n@Path(SAMLEndpoints.SERVICE_PROVIDER_METADATA_RELATIVE)\n@ApiOperation(\n        value = \"Retrieves the service provider metadata.\",\n        notes = NON_GUARANTEED_ENDPOINT\n)\npublic Response samlMetadata(@Context HttpServletRequest httpServletRequest, @Context HttpServletResponse httpServletResponse) throws Exception {\n    // only consider user specific access over https\n    if (!httpServletRequest.isSecure()) {\n        throw new AuthenticationNotSupportedException(AUTHENTICATION_NOT_ENABLED_MSG);\n    }\n\n    // ensure saml is enabled\n    if (!samlService.isSamlEnabled()) {\n        return Response.status(Response.Status.CONFLICT).entity(SAMLService.SAML_SUPPORT_IS_NOT_CONFIGURED).build();\n    }\n\n    // ensure saml service provider is initialized\n    initializeSamlServiceProvider();\n\n    final String metadataXml = samlService.getServiceProviderMetadata();\n    return Response.ok(metadataXml, SAML_METADATA_MEDIA_TYPE).build();\n}",
        "accept_response": "@GET\n@Consumes(MediaType.WILDCARD)\n@Produces(SAML_METADATA_MEDIA_TYPE)\n@Path(SAMLEndpoints.SERVICE_PROVIDER_METADATA_RELATIVE)\n@ApiOperation(\n        value = \"Retrieves the service provider metadata.\",\n        notes = NON_GUARANTEED_ENDPOINT\n)\npublic Response samlMetadata(@Context HttpServletRequest httpServletRequest, @Context HttpServletResponse httpServletResponse) throws Exception {\n    // only consider user specific access over https\n    if (!httpServletRequest.isSecure()) {\n        throw new AuthenticationNotSupportedException(AUTHENTICATION_NOT_ENABLED_MSG);\n    }\n\n    // ensure saml is enabled\n    if (!samlService.isSamlEnabled()) {\n        logger.debug(SAMLService.SAML_SUPPORT_IS_NOT_CONFIGURED);\n        return Response.status(Response.Status.CONFLICT).entity(SAMLService.SAML_SUPPORT_IS_NOT_CONFIGURED).build();\n    }\n\n    // ensure saml service provider is initialized\n    initializeSamlServiceProvider();\n\n    final String metadataXml = samlService.getServiceProviderMetadata();\n    return Response.ok(metadataXml, SAML_METADATA_MEDIA_TYPE).build();\n}",
        "reject_response": "@GET\n@Consumes(MediaType.WILDCARD)\n@Produces(SAML_METADATA_MEDIA_TYPE)\n@Path(SAMLEndpoints.SERVICE_PROVIDER_METADATA_RELATIVE)\n@ApiOperation(\n        value = \"Retrieves the service provider metadata.\",\n        notes = NON_GUARANTEED_ENDPOINT\n)\npublic Response samlMetadata(@Context HttpServletRequest httpServletRequest, @Context HttpServletResponse httpServletResponse) throws Exception {\n    // only consider user specific access over https\n    if (!httpServletRequest.isSecure()) {\n        throw new AuthenticationNotSupportedException(AUTHENTICATION_NOT_ENABLED_MSG);\n    }\n\n    // ensure saml is enabled\n    if (!samlService.isSamlEnabled()) {\n        logger.warn(SAMLService.SAML_SUPPORT_IS_NOT_CONFIGURED);\n        return Response.status(Response.Status.CONFLICT).entity(SAMLService.SAML_SUPPORT_IS_NOT_CONFIGURED).build();\n    }\n\n    // ensure saml service provider is initialized\n    initializeSamlServiceProvider();\n\n    final String metadataXml = samlService.getServiceProviderMetadata();\n    return Response.ok(metadataXml, SAML_METADATA_MEDIA_TYPE).build();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3168,
        "instruction": "protected final void renderComponentTag(ComponentTag tag)\n{\n\tif (needToRenderTag(tag))\n\t{\n\t\t// apply behaviors that are attached to the component tag.\n\t\tif (tag.hasBehaviors())\n\t\t{\n\t\t\tIterator<? extends Behavior> tagBehaviors = tag.getBehaviors();\n\t\t\twhile (tagBehaviors.hasNext())\n\t\t\t{\n\t\t\t\tfinal Behavior behavior = tagBehaviors.next();\n\t\t\t\tif (behavior.isEnabled(this))\n\t\t\t\t{\n\t\t\t\t\tbehavior.onComponentTag(this, tag);\n\t\t\t\t}\n\t\t\t\tbehavior.detach(this);\n\t\t\t}\n\t\t}\n\n\t\t// Apply behavior modifiers\n\t\tList<? extends Behavior> behaviors = getBehaviors();\n\t\tif ((behaviors != null) && !behaviors.isEmpty() && !tag.isClose() &&\n\t\t\t(isIgnoreAttributeModifier() == false))\n\t\t{\n\t\t\ttag = tag.mutable();\n\t\t\tfor (Behavior behavior : behaviors)\n\t\t\t{\n\t\t\t\t// Components may reject some behavior components\n\t\t\t\tif (isBehaviorAccepted(behavior))\n\t\t\t\t{\n\t\t\t\t\tbehavior.onComponentTag(this, tag);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif ((tag instanceof WicketTag) && !tag.isClose() &&\n\t\t\t!getFlag(FLAG_IGNORE_ATTRIBUTE_MODIFIER))\n\t\t{\n\t\t\tExceptionSettings.NotRenderableErrorStrategy notRenderableErrorStrategy = ExceptionSettings.NotRenderableErrorStrategy.LOG_WARNING;\n\t\t\tif (Application.exists())\n\t\t\t{\n\t\t\t\tnotRenderableErrorStrategy = getApplication().getExceptionSettings().getNotRenderableErrorStrategy();\n\t\t\t}\n\n\t\t\tString tagName = tag.getNamespace() + \":\" + tag.getName();\n\t\t\tString componentId = getId();\n\t\t\tif (getFlag(FLAG_OUTPUT_MARKUP_ID))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Markup id set on a component that is usually not rendered into markup. \" +\n\t\t\t\t                               \"Markup id: %s, component id: %s, component tag: %s.\",\n\t\t\t\t                               getMarkupId(), componentId, tagName);\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t\tif (getFlag(FLAG_PLACEHOLDER))\n\t\t\t{\n\t\t\t\tString message = String.format(\n\t\t\t\t\t\t\"Placeholder tag set on a component that is usually not rendered into markup. \" +\n\t\t\t\t\t\t\"Component id: %s, component tag: %s.\", componentId, tagName);\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Write the tag\n\t\ttag.writeOutput(getResponse(), !needToRenderTag(null),\n\t\t\tgetMarkup().getMarkupResourceStream().getWicketNamespace());\n\t}\n}",
        "accept_response": "protected final void renderComponentTag(ComponentTag tag)\n{\n\tif (needToRenderTag(tag))\n\t{\n\t\t// apply behaviors that are attached to the component tag.\n\t\tif (tag.hasBehaviors())\n\t\t{\n\t\t\tIterator<? extends Behavior> tagBehaviors = tag.getBehaviors();\n\t\t\twhile (tagBehaviors.hasNext())\n\t\t\t{\n\t\t\t\tfinal Behavior behavior = tagBehaviors.next();\n\t\t\t\tif (behavior.isEnabled(this))\n\t\t\t\t{\n\t\t\t\t\tbehavior.onComponentTag(this, tag);\n\t\t\t\t}\n\t\t\t\tbehavior.detach(this);\n\t\t\t}\n\t\t}\n\n\t\t// Apply behavior modifiers\n\t\tList<? extends Behavior> behaviors = getBehaviors();\n\t\tif ((behaviors != null) && !behaviors.isEmpty() && !tag.isClose() &&\n\t\t\t(isIgnoreAttributeModifier() == false))\n\t\t{\n\t\t\ttag = tag.mutable();\n\t\t\tfor (Behavior behavior : behaviors)\n\t\t\t{\n\t\t\t\t// Components may reject some behavior components\n\t\t\t\tif (isBehaviorAccepted(behavior))\n\t\t\t\t{\n\t\t\t\t\tbehavior.onComponentTag(this, tag);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif ((tag instanceof WicketTag) && !tag.isClose() &&\n\t\t\t!getFlag(FLAG_IGNORE_ATTRIBUTE_MODIFIER))\n\t\t{\n\t\t\tExceptionSettings.NotRenderableErrorStrategy notRenderableErrorStrategy = ExceptionSettings.NotRenderableErrorStrategy.LOG_WARNING;\n\t\t\tif (Application.exists())\n\t\t\t{\n\t\t\t\tnotRenderableErrorStrategy = getApplication().getExceptionSettings().getNotRenderableErrorStrategy();\n\t\t\t}\n\n\t\t\tString tagName = tag.getNamespace() + \":\" + tag.getName();\n\t\t\tString componentId = getId();\n\t\t\tif (getFlag(FLAG_OUTPUT_MARKUP_ID))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Markup id set on a component that is usually not rendered into markup. \" +\n\t\t\t\t                               \"Markup id: %s, component id: %s, component tag: %s.\",\n\t\t\t\t                               getMarkupId(), componentId, tagName);\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t\tif (getFlag(FLAG_PLACEHOLDER))\n\t\t\t{\n\t\t\t\tString message = String.format(\n\t\t\t\t\t\t\"Placeholder tag set on a component that is usually not rendered into markup. \" +\n\t\t\t\t\t\t\"Component id: %s, component tag: %s.\", componentId, tagName);\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t}\n\n\t\t// Write the tag\n\t\ttag.writeOutput(getResponse(), !needToRenderTag(null),\n\t\t\tgetMarkup().getMarkupResourceStream().getWicketNamespace());\n\t}\n}",
        "reject_response": "protected final void renderComponentTag(ComponentTag tag)\n{\n\tif (needToRenderTag(tag))\n\t{\n\t\t// apply behaviors that are attached to the component tag.\n\t\tif (tag.hasBehaviors())\n\t\t{\n\t\t\tIterator<? extends Behavior> tagBehaviors = tag.getBehaviors();\n\t\t\twhile (tagBehaviors.hasNext())\n\t\t\t{\n\t\t\t\tfinal Behavior behavior = tagBehaviors.next();\n\t\t\t\tif (behavior.isEnabled(this))\n\t\t\t\t{\n\t\t\t\t\tbehavior.onComponentTag(this, tag);\n\t\t\t\t}\n\t\t\t\tbehavior.detach(this);\n\t\t\t}\n\t\t}\n\n\t\t// Apply behavior modifiers\n\t\tList<? extends Behavior> behaviors = getBehaviors();\n\t\tif ((behaviors != null) && !behaviors.isEmpty() && !tag.isClose() &&\n\t\t\t(isIgnoreAttributeModifier() == false))\n\t\t{\n\t\t\ttag = tag.mutable();\n\t\t\tfor (Behavior behavior : behaviors)\n\t\t\t{\n\t\t\t\t// Components may reject some behavior components\n\t\t\t\tif (isBehaviorAccepted(behavior))\n\t\t\t\t{\n\t\t\t\t\tbehavior.onComponentTag(this, tag);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif ((tag instanceof WicketTag) && !tag.isClose() &&\n\t\t\t!getFlag(FLAG_IGNORE_ATTRIBUTE_MODIFIER))\n\t\t{\n\t\t\tExceptionSettings.NotRenderableErrorStrategy notRenderableErrorStrategy = ExceptionSettings.NotRenderableErrorStrategy.LOG_WARNING;\n\t\t\tif (Application.exists())\n\t\t\t{\n\t\t\t\tnotRenderableErrorStrategy = getApplication().getExceptionSettings().getNotRenderableErrorStrategy();\n\t\t\t}\n\n\t\t\tString tagName = tag.getNamespace() + \":\" + tag.getName();\n\t\t\tString componentId = getId();\n\t\t\tif (getFlag(FLAG_OUTPUT_MARKUP_ID))\n\t\t\t{\n\t\t\t\tString message = String.format(\"Markup id set on a component that is usually not rendered into markup. \" +\n\t\t\t\t                               \"Markup id: %s, component id: %s, component tag: %s.\",\n\t\t\t\t                               getMarkupId(), componentId, tagName);\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(message);\n\t\t\t}\n\t\t\tif (getFlag(FLAG_PLACEHOLDER))\n\t\t\t{\n\t\t\t\tString message = String.format(\n\t\t\t\t\t\t\"Placeholder tag set on a component that is usually not rendered into markup. \" +\n\t\t\t\t\t\t\"Component id: %s, component tag: %s.\", componentId, tagName);\n\t\t\t\tif (notRenderableErrorStrategy == ExceptionSettings.NotRenderableErrorStrategy.THROW_EXCEPTION)\n\t\t\t\t{\n\t\t\t\t\tthrow new IllegalStateException(message);\n\t\t\t\t}\n\t\t\t\tlog.warn(String.format(\n\t\t\t\t\t\"Placeholder tag set on a component that is usually not rendered into markup. \"\n\t\t\t\t\t\t+ \"Component id: %s, component tag: %s.\", getId(), tag.getName()));\n\t\t\t}\n\t\t}\n\n\t\t// Write the tag\n\t\ttag.writeOutput(getResponse(), !needToRenderTag(null),\n\t\t\tgetMarkup().getMarkupResourceStream().getWicketNamespace());\n\t}\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2657,
        "instruction": "public static InternalLogWriter createLogWriterLogger(final boolean isLoner,\n    final boolean isSecure, final LogConfig config, final boolean logConfig) {\n\n  // if isSecurity then use \"org.apache.geode.security\" else use \"org.apache.geode\"\n  String name = null;\n  if (isSecure) {\n    name = LogService.SECURITY_LOGGER_NAME;\n  } else {\n    name = LogService.MAIN_LOGGER_NAME;\n  }\n\n  // create the LogWriterLogger\n  final LogWriterLogger logger =\n      LogService.createLogWriterLogger(name, config.getName(), isSecure);\n\n  if (isSecure) {\n    logger.setLogWriterLevel(((DistributionConfig) config).getSecurityLogLevel());\n  } else {\n    boolean defaultSource = false;\n    if (config instanceof DistributionConfig) {\n      ConfigSource source = ((DistributionConfig) config).getConfigSource(LOG_LEVEL);\n      if (source == null) {\n        defaultSource = true;\n      }\n    }\n    if (!defaultSource) {\n      // LOG: fix bug #51709 by not setting if log-level was not specified\n      // LOG: let log4j2.xml specify log level which defaults to INFO\n      logger.setLogWriterLevel(config.getLogLevel());\n    }\n  }\n\n  // log the banner\n  if (!Boolean.getBoolean(InternalLocator.INHIBIT_DM_BANNER)) {\n    if (InternalDistributedSystem.getReconnectAttemptCounter() == 0 // avoid filling up logs\n                                                                    // during auto-reconnect\n        && !isSecure // && !isLoner /* do this on a loner to fix bug 35602 */\n    ) {\n      // LOG:CONFIG:\n      logger.info(LogMarker.CONFIG, Banner.getString(null));\n    }\n    System.setProperty(InternalLocator.INHIBIT_DM_BANNER, \"true\"); // Ensure no more banners will\n                                                                   // be logged\n  } else {\n    logger.debug(\"skipping banner - \" + InternalLocator.INHIBIT_DM_BANNER + \" is set to true\");\n  }\n\n  // log the config\n  if (logConfig && !isLoner) {\n    // LOG:CONFIG: changed from config to info\n  }\n\n  return logger;\n}",
        "accept_response": "public static InternalLogWriter createLogWriterLogger(final boolean isLoner,\n    final boolean isSecure, final LogConfig config, final boolean logConfig) {\n\n  // if isSecurity then use \"org.apache.geode.security\" else use \"org.apache.geode\"\n  String name = null;\n  if (isSecure) {\n    name = LogService.SECURITY_LOGGER_NAME;\n  } else {\n    name = LogService.MAIN_LOGGER_NAME;\n  }\n\n  // create the LogWriterLogger\n  final LogWriterLogger logger =\n      LogService.createLogWriterLogger(name, config.getName(), isSecure);\n\n  if (isSecure) {\n    logger.setLogWriterLevel(((DistributionConfig) config).getSecurityLogLevel());\n  } else {\n    boolean defaultSource = false;\n    if (config instanceof DistributionConfig) {\n      ConfigSource source = ((DistributionConfig) config).getConfigSource(LOG_LEVEL);\n      if (source == null) {\n        defaultSource = true;\n      }\n    }\n    if (!defaultSource) {\n      // LOG: fix bug #51709 by not setting if log-level was not specified\n      // LOG: let log4j2.xml specify log level which defaults to INFO\n      logger.setLogWriterLevel(config.getLogLevel());\n    }\n  }\n\n  // log the banner\n  if (!Boolean.getBoolean(InternalLocator.INHIBIT_DM_BANNER)) {\n    if (InternalDistributedSystem.getReconnectAttemptCounter() == 0 // avoid filling up logs\n                                                                    // during auto-reconnect\n        && !isSecure // && !isLoner /* do this on a loner to fix bug 35602 */\n    ) {\n      // LOG:CONFIG:\n      logger.info(LogMarker.CONFIG, Banner.getString(null));\n    }\n    System.setProperty(InternalLocator.INHIBIT_DM_BANNER, \"true\"); // Ensure no more banners will\n                                                                   // be logged\n  } else {\n    logger.debug(\"skipping banner - \" + InternalLocator.INHIBIT_DM_BANNER + \" is set to true\");\n  }\n\n  // log the config\n  if (logConfig && !isLoner) {\n    // LOG:CONFIG: changed from config to info\n    logger.info(LogMarker.CONFIG,\n        LocalizedMessage.create(\n            LocalizedStrings.InternalDistributedSystem_STARTUP_CONFIGURATIONN_0,\n            config.toLoggerString()));\n  }\n\n  return logger;\n}",
        "reject_response": "public static InternalLogWriter createLogWriterLogger(final boolean isLoner,\n    final boolean isSecure, final LogConfig config, final boolean logConfig) {\n\n  // if isSecurity then use \"org.apache.geode.security\" else use \"org.apache.geode\"\n  String name = null;\n  if (isSecure) {\n    name = LogService.SECURITY_LOGGER_NAME;\n  } else {\n    name = LogService.MAIN_LOGGER_NAME;\n  }\n\n  // create the LogWriterLogger\n  final LogWriterLogger logger =\n      LogService.createLogWriterLogger(name, config.getName(), isSecure);\n\n  if (isSecure) {\n    logger.setLogWriterLevel(((DistributionConfig) config).getSecurityLogLevel());\n  } else {\n    boolean defaultSource = false;\n    if (config instanceof DistributionConfig) {\n      ConfigSource source = ((DistributionConfig) config).getConfigSource(LOG_LEVEL);\n      if (source == null) {\n        defaultSource = true;\n      }\n    }\n    if (!defaultSource) {\n      // LOG: fix bug #51709 by not setting if log-level was not specified\n      // LOG: let log4j2.xml specify log level which defaults to INFO\n      logger.setLogWriterLevel(config.getLogLevel());\n    }\n  }\n\n  // log the banner\n  if (!Boolean.getBoolean(InternalLocator.INHIBIT_DM_BANNER)) {\n    if (InternalDistributedSystem.getReconnectAttemptCounter() == 0 // avoid filling up logs\n                                                                    // during auto-reconnect\n        && !isSecure // && !isLoner /* do this on a loner to fix bug 35602 */\n    ) {\n      // LOG:CONFIG:\n      logger.info(LogMarker.CONFIG, Banner.getString(null));\n    }\n    System.setProperty(InternalLocator.INHIBIT_DM_BANNER, \"true\"); // Ensure no more banners will\n                                                                   // be logged\n  } else {\n    logger.debug(\"skipping banner - \" + InternalLocator.INHIBIT_DM_BANNER + \" is set to true\");\n  }\n\n  // log the config\n  if (logConfig && !isLoner) {\n    // LOG:CONFIG: changed from config to info\n  if (logConfig) {\n    if (!isLoner) {\n      // LOG:CONFIG: changed from config to info\n      logger.info(LogMarker.CONFIG,\n          LocalizedMessage.create(\n              LocalizedStrings.InternalDistributedSystem_STARTUP_CONFIGURATIONN_0,\n              config.toLoggerString()));\n    }\n  }\n\n  return logger;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2681,
        "instruction": "public Writable convertKeyValue(Object key) {\n    if (converter == null) {\n        converter = initializeConverter(key);\n    }\n\n    return converter.get(key);\n}",
        "accept_response": "public Writable convertKeyValue(Object key) {\n    if (converter == null) {\n        converter = initializeConverter(key);\n        LOG.debug(\"converter initialized for type \" + key.getClass()\n                + \" (key value: \" + key + \")\");\n    }\n\n    return converter.get(key);\n}",
        "reject_response": "public Writable convertKeyValue(Object key) {\n    if (converter == null) {\n        converter = initializeConverter(key);\n        Log.debug(\"converter initialized for type \" + key.getClass() +\n                \" (key value: \" + key + \")\");\n    }\n\n    return converter.get(key);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2680,
        "instruction": "@Override\npublic boolean openForWrite() throws Exception {\n    FileSystem fs;\n    Path parent;\n    String fileName = inputData.getDataSource();\n    conf = new Configuration();\n\n    getCompressionCodec(inputData);\n    fileName = updateFileExtension(fileName, codec);\n\n    // construct the output stream\n    file = new Path(fileName);\n    fs = file.getFileSystem(conf);\n    fc = FileContext.getFileContext();\n    defaultKey = new LongWritable(inputData.getSegmentId());\n\n    if (fs.exists(file)) {\n        throw new IOException(\"file \" + file\n                + \" already exists, can't write data\");\n    }\n    parent = file.getParent();\n    if (!fs.exists(parent)) {\n        fs.mkdirs(parent);\n    }\n\n    writer = null;\n    return true;\n}",
        "accept_response": "@Override\npublic boolean openForWrite() throws Exception {\n    FileSystem fs;\n    Path parent;\n    String fileName = inputData.getDataSource();\n    conf = new Configuration();\n\n    getCompressionCodec(inputData);\n    fileName = updateFileExtension(fileName, codec);\n\n    // construct the output stream\n    file = new Path(fileName);\n    fs = file.getFileSystem(conf);\n    fc = FileContext.getFileContext();\n    defaultKey = new LongWritable(inputData.getSegmentId());\n\n    if (fs.exists(file)) {\n        throw new IOException(\"file \" + file\n                + \" already exists, can't write data\");\n    }\n    parent = file.getParent();\n    if (!fs.exists(parent)) {\n        fs.mkdirs(parent);\n        LOG.debug(\"Created new dir \" + parent);\n    }\n\n    writer = null;\n    return true;\n}",
        "reject_response": "@Override\npublic boolean openForWrite() throws Exception {\n    FileSystem fs;\n    Path parent;\n    String fileName = inputData.getDataSource();\n    conf = new Configuration();\n\n    getCompressionCodec(inputData);\n    fileName = updateFileExtension(fileName, codec);\n\n    // construct the output stream\n    file = new Path(fileName);\n    fs = file.getFileSystem(conf);\n    fc = FileContext.getFileContext();\n    defaultKey = new LongWritable(inputData.getSegmentId());\n\n    if (fs.exists(file)) {\n        throw new IOException(\"file \" + file\n                + \" already exists, can't write data\");\n    }\n    parent = file.getParent();\n    if (!fs.exists(parent)) {\n        fs.mkdirs(parent);\n        Log.debug(\"Created new dir \" + parent);\n    }\n\n    writer = null;\n    return true;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2444,
        "instruction": "private ArrowRecordBatch readRecordBatch(SeekableReadChannel in,\n                                         ArrowBlock block,\n                                         BufferAllocator allocator) throws IOException {\n  in.setPosition(block.getOffset());\n  ArrowRecordBatch batch = MessageSerializer.deserializeRecordBatch(in, block, allocator);\n  if (batch == null) {\n    throw new IOException(\"Invalid file. No batch at offset: \" + block.getOffset());\n  }\n  return batch;\n}",
        "accept_response": "private ArrowRecordBatch readRecordBatch(SeekableReadChannel in,\n                                         ArrowBlock block,\n                                         BufferAllocator allocator) throws IOException {\n  LOGGER.debug(\"RecordBatch at {}, metadata: {}, body: {}\",\n      block.getOffset(), block.getMetadataLength(),\n      block.getBodyLength());\n  in.setPosition(block.getOffset());\n  ArrowRecordBatch batch = MessageSerializer.deserializeRecordBatch(in, block, allocator);\n  if (batch == null) {\n    throw new IOException(\"Invalid file. No batch at offset: \" + block.getOffset());\n  }\n  return batch;\n}",
        "reject_response": "private ArrowRecordBatch readRecordBatch(SeekableReadChannel in,\n                                         ArrowBlock block,\n                                         BufferAllocator allocator) throws IOException {\n  LOGGER.debug(String.format(\"RecordBatch at %d, metadata: %d, body: %d\",\n  in.setPosition(block.getOffset());\n  ArrowRecordBatch batch = MessageSerializer.deserializeRecordBatch(in, block, allocator);\n  if (batch == null) {\n    throw new IOException(\"Invalid file. No batch at offset: \" + block.getOffset());\n  }\n  return batch;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3200,
        "instruction": "@VisibleForTesting\nprotected Future removeApplicationCollector(final ContainerId containerId) {\n  final ApplicationId appId =\n      containerId.getApplicationAttemptId().getApplicationId();\n  return scheduler.schedule(new Runnable() {\n    public void run() {\n      boolean shouldRemoveApplication = false;\n      synchronized (appIdToContainerId) {\n        Set<ContainerId> masterContainers = appIdToContainerId.get(appId);\n        if (masterContainers == null) {\n          return;\n        }\n        masterContainers.remove(containerId);\n        if (masterContainers.size() == 0) {\n          // remove only if it is last master container\n          shouldRemoveApplication = true;\n          appIdToContainerId.remove(appId);\n        }\n      }\n\n      if (shouldRemoveApplication) {\n        removeApplication(appId);\n      }\n    }\n  }, collectorLingerPeriod, TimeUnit.MILLISECONDS);\n}",
        "accept_response": "@VisibleForTesting\nprotected Future removeApplicationCollector(final ContainerId containerId) {\n  final ApplicationId appId =\n      containerId.getApplicationAttemptId().getApplicationId();\n  return scheduler.schedule(new Runnable() {\n    public void run() {\n      boolean shouldRemoveApplication = false;\n      synchronized (appIdToContainerId) {\n        Set<ContainerId> masterContainers = appIdToContainerId.get(appId);\n        if (masterContainers == null) {\n          LOG.info(\"Stop container for {}\"\n              + \" is called before initializing container.\", containerId);\n          return;\n        }\n        masterContainers.remove(containerId);\n        if (masterContainers.size() == 0) {\n          // remove only if it is last master container\n          shouldRemoveApplication = true;\n          appIdToContainerId.remove(appId);\n        }\n      }\n\n      if (shouldRemoveApplication) {\n        removeApplication(appId);\n      }\n    }\n  }, collectorLingerPeriod, TimeUnit.MILLISECONDS);\n}",
        "reject_response": "@VisibleForTesting\nprotected Future removeApplicationCollector(final ContainerId containerId) {\n  final ApplicationId appId =\n      containerId.getApplicationAttemptId().getApplicationId();\n  return scheduler.schedule(new Runnable() {\n    public void run() {\n      boolean shouldRemoveApplication = false;\n      synchronized (appIdToContainerId) {\n        Set<ContainerId> masterContainers = appIdToContainerId.get(appId);\n        if (masterContainers == null) {\n          LOG.info(\"Stop container for \" + containerId\n              + \" is called before initializing container.\");\n          return;\n        }\n        masterContainers.remove(containerId);\n        if (masterContainers.size() == 0) {\n          // remove only if it is last master container\n          shouldRemoveApplication = true;\n          appIdToContainerId.remove(appId);\n        }\n      }\n\n      if (shouldRemoveApplication) {\n        removeApplication(appId);\n      }\n    }\n  }, collectorLingerPeriod, TimeUnit.MILLISECONDS);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3197,
        "instruction": "@PUT\n@Path(\"/domain\")\n@Consumes({ MediaType.APPLICATION_JSON /* , MediaType.APPLICATION_XML */ })\npublic Response putDomain(\n    @Context HttpServletRequest req,\n    @Context HttpServletResponse res,\n    @QueryParam(\"appid\") String appId,\n    TimelineDomain domain) {\n  init(res);\n  UserGroupInformation callerUgi = getUser(req);\n  if (callerUgi == null) {\n    String msg = \"The owner of the posted timeline entities is not set\";\n    LOG.error(msg);\n    throw new ForbiddenException(msg);\n  }\n\n  try {\n    ApplicationId appID = parseApplicationId(appId);\n    if (appID == null) {\n      return Response.status(Response.Status.BAD_REQUEST).build();\n    }\n    NodeTimelineCollectorManager collectorManager =\n        (NodeTimelineCollectorManager) context.getAttribute(\n            NodeTimelineCollectorManager.COLLECTOR_MANAGER_ATTR_KEY);\n    TimelineCollector collector = collectorManager.get(appID);\n    if (collector == null) {\n      throw new NotFoundException(\"Application: \" + appId + \" is not found\");\n    }\n\n    domain.setOwner(callerUgi.getShortUserName());\n    collector.putDomain(domain, callerUgi);\n\n    return Response.ok().build();\n  } catch (NotFoundException e) {\n    throw new WebApplicationException(e,\n        Response.Status.INTERNAL_SERVER_ERROR);\n  } catch (IOException e) {\n    LOG.error(\"Error putting entities\", e);\n    throw new WebApplicationException(e,\n        Response.Status.INTERNAL_SERVER_ERROR);\n  }\n}",
        "accept_response": "@PUT\n@Path(\"/domain\")\n@Consumes({ MediaType.APPLICATION_JSON /* , MediaType.APPLICATION_XML */ })\npublic Response putDomain(\n    @Context HttpServletRequest req,\n    @Context HttpServletResponse res,\n    @QueryParam(\"appid\") String appId,\n    TimelineDomain domain) {\n  init(res);\n  UserGroupInformation callerUgi = getUser(req);\n  if (callerUgi == null) {\n    String msg = \"The owner of the posted timeline entities is not set\";\n    LOG.error(msg);\n    throw new ForbiddenException(msg);\n  }\n\n  try {\n    ApplicationId appID = parseApplicationId(appId);\n    if (appID == null) {\n      return Response.status(Response.Status.BAD_REQUEST).build();\n    }\n    NodeTimelineCollectorManager collectorManager =\n        (NodeTimelineCollectorManager) context.getAttribute(\n            NodeTimelineCollectorManager.COLLECTOR_MANAGER_ATTR_KEY);\n    TimelineCollector collector = collectorManager.get(appID);\n    if (collector == null) {\n      LOG.error(\"Application: {} is not found\", appId);\n      throw new NotFoundException(\"Application: \" + appId + \" is not found\");\n    }\n\n    domain.setOwner(callerUgi.getShortUserName());\n    collector.putDomain(domain, callerUgi);\n\n    return Response.ok().build();\n  } catch (NotFoundException e) {\n    throw new WebApplicationException(e,\n        Response.Status.INTERNAL_SERVER_ERROR);\n  } catch (IOException e) {\n    LOG.error(\"Error putting entities\", e);\n    throw new WebApplicationException(e,\n        Response.Status.INTERNAL_SERVER_ERROR);\n  }\n}",
        "reject_response": "@PUT\n@Path(\"/domain\")\n@Consumes({ MediaType.APPLICATION_JSON /* , MediaType.APPLICATION_XML */ })\npublic Response putDomain(\n    @Context HttpServletRequest req,\n    @Context HttpServletResponse res,\n    @QueryParam(\"appid\") String appId,\n    TimelineDomain domain) {\n  init(res);\n  UserGroupInformation callerUgi = getUser(req);\n  if (callerUgi == null) {\n    String msg = \"The owner of the posted timeline entities is not set\";\n    LOG.error(msg);\n    throw new ForbiddenException(msg);\n  }\n\n  try {\n    ApplicationId appID = parseApplicationId(appId);\n    if (appID == null) {\n      return Response.status(Response.Status.BAD_REQUEST).build();\n    }\n    NodeTimelineCollectorManager collectorManager =\n        (NodeTimelineCollectorManager) context.getAttribute(\n            NodeTimelineCollectorManager.COLLECTOR_MANAGER_ATTR_KEY);\n    TimelineCollector collector = collectorManager.get(appID);\n    if (collector == null) {\n      LOG.error(\"Application: \" + appId + \" is not found\");\n      throw new NotFoundException(\"Application: \" + appId + \" is not found\");\n    }\n\n    domain.setOwner(callerUgi.getShortUserName());\n    collector.putDomain(domain, callerUgi);\n\n    return Response.ok().build();\n  } catch (NotFoundException e) {\n    throw new WebApplicationException(e,\n        Response.Status.INTERNAL_SERVER_ERROR);\n  } catch (IOException e) {\n    LOG.error(\"Error putting entities\", e);\n    throw new WebApplicationException(e,\n        Response.Status.INTERNAL_SERVER_ERROR);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3051,
        "instruction": "private NamedList<Integer> getTermCounts(String field, Integer mincount, ParsedParams parsed) throws IOException {\n  final SolrParams params = parsed.params;\n  final DocSet docs = parsed.docs;\n  final int threads = parsed.threads;\n  int offset = params.getFieldInt(field, FacetParams.FACET_OFFSET, 0);\n  int limit = params.getFieldInt(field, FacetParams.FACET_LIMIT, 100);\n  if (limit == 0) return new NamedList<>();\n  if (mincount==null) {\n    Boolean zeros = params.getFieldBool(field, FacetParams.FACET_ZEROS);\n    // mincount = (zeros!=null && zeros) ? 0 : 1;\n    mincount = (zeros!=null && !zeros) ? 1 : 0;\n    // current default is to include zeros.\n  }\n  boolean missing = params.getFieldBool(field, FacetParams.FACET_MISSING, false);\n  // default to sorting if there is a limit.\n  String sort = params.getFieldParam(field, FacetParams.FACET_SORT, limit>0 ? FacetParams.FACET_SORT_COUNT : FacetParams.FACET_SORT_INDEX);\n  String prefix = params.getFieldParam(field, FacetParams.FACET_PREFIX);\n\n  final Predicate<BytesRef> termFilter = newBytesRefFilter(field, params);\n\n  boolean exists = params.getFieldBool(field, FacetParams.FACET_EXISTS, false);\n\n  NamedList<Integer> counts;\n  SchemaField sf = searcher.getSchema().getField(field);\n  if (sf.getType().isPointField() && !sf.hasDocValues()) {\n    throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n        \"Can't facet on a PointField without docValues\");\n  }\n  FieldType ft = sf.getType();\n\n  // determine what type of faceting method to use\n  final String methodStr = params.getFieldParam(field, FacetParams.FACET_METHOD);\n  final FacetMethod requestedMethod;\n  if (FacetParams.FACET_METHOD_enum.equals(methodStr)) {\n    requestedMethod = FacetMethod.ENUM;\n  } else if (FacetParams.FACET_METHOD_fcs.equals(methodStr)) {\n    requestedMethod = FacetMethod.FCS;\n  } else if (FacetParams.FACET_METHOD_fc.equals(methodStr)) {\n    requestedMethod = FacetMethod.FC;\n  } else if(FacetParams.FACET_METHOD_uif.equals(methodStr)) {\n    requestedMethod = FacetMethod.UIF;\n  } else {\n    requestedMethod=null;\n  }\n\n  final boolean multiToken = sf.multiValued() || ft.multiValuedFieldCache();\n\n  FacetMethod appliedFacetMethod = selectFacetMethod(field,\n                              sf, requestedMethod, mincount,\n                              exists);\n\n  RTimer timer = null;\n  if (fdebug != null) {\n     fdebug.putInfoItem(\"requestedMethod\", requestedMethod==null?\"not specified\":requestedMethod.name());\n     fdebug.putInfoItem(\"appliedMethod\", appliedFacetMethod.name());\n     fdebug.putInfoItem(\"inputDocSetSize\", docs.size());\n     fdebug.putInfoItem(\"field\", field);\n     timer = new RTimer();\n  }\n\n  if (params.getFieldBool(field, GroupParams.GROUP_FACET, false)) {\n    counts = getGroupedCounts(searcher, docs, field, multiToken, offset,limit, mincount, missing, sort, prefix, termFilter);\n  } else {\n    assert appliedFacetMethod != null;\n    switch (appliedFacetMethod) {\n      case ENUM:\n        assert TrieField.getMainValuePrefix(ft) == null;\n        counts = getFacetTermEnumCounts(searcher, docs, field, offset, limit, mincount,missing,sort,prefix, termFilter, exists);\n        break;\n      case FCS:\n        assert ft.isPointField() || !multiToken;\n        if (ft.isPointField() || (ft.getNumberType() != null && !sf.multiValued())) {\n          if (prefix != null) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, FacetParams.FACET_PREFIX + \" is not supported on numeric types\");\n          }\n          if (termFilter != null) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"BytesRef term filters (\"\n                    + FacetParams.FACET_CONTAINS + \", \"\n                    + FacetParams.FACET_EXCLUDETERMS + \") are not supported on numeric types\");\n          }\n          if (ft.isPointField() && mincount <= 0) { // default is mincount=0.  See SOLR-10033 & SOLR-11174.\n            String warningMessage\n                = \"Raising facet.mincount from \" + mincount + \" to 1, because field \" + field + \" is Points-based.\";\n            List<String> warnings = (List<String>)rb.rsp.getResponseHeader().get(\"warnings\");\n            if (null == warnings) {\n              warnings = new ArrayList<>();\n              rb.rsp.getResponseHeader().add(\"warnings\", warnings);\n            }\n            warnings.add(warningMessage);\n\n            mincount = 1;\n          }\n          counts = NumericFacets.getCounts(searcher, docs, field, offset, limit, mincount, missing, sort);\n        } else {\n          PerSegmentSingleValuedFaceting ps = new PerSegmentSingleValuedFaceting(searcher, docs, field, offset, limit, mincount, missing, sort, prefix, termFilter);\n          Executor executor = threads == 0 ? directExecutor : facetExecutor;\n          ps.setNumThreads(threads);\n          counts = ps.getFacetCounts(executor);\n        }\n        break;\n      case UIF:\n          //Emulate the JSON Faceting structure so we can use the same parsing classes\n          Map<String, Object> jsonFacet = new HashMap<>(13);\n          jsonFacet.put(\"type\", \"terms\");\n          jsonFacet.put(\"field\", field);\n          jsonFacet.put(\"offset\", offset);\n          jsonFacet.put(\"limit\", limit);\n          jsonFacet.put(\"mincount\", mincount);\n          jsonFacet.put(\"missing\", missing);\n          jsonFacet.put(\"prefix\", prefix);\n          jsonFacet.put(\"numBuckets\", params.getFieldBool(field, \"numBuckets\", false));\n          jsonFacet.put(\"allBuckets\", params.getFieldBool(field, \"allBuckets\", false));\n          jsonFacet.put(\"method\", \"uif\");\n          jsonFacet.put(\"cacheDf\", 0);\n          jsonFacet.put(\"perSeg\", false);\n\n          final String sortVal;\n          switch(sort){\n            case FacetParams.FACET_SORT_COUNT_LEGACY:\n              sortVal = FacetParams.FACET_SORT_COUNT;\n            break;\n            case FacetParams.FACET_SORT_INDEX_LEGACY:\n              sortVal = FacetParams.FACET_SORT_INDEX;\n            break;\n            default:\n              sortVal = sort;\n          }\n          jsonFacet.put(SORT, sortVal );\n\n          Map<String, Object> topLevel = new HashMap<>();\n          topLevel.put(field, jsonFacet);\n\n          topLevel.put(\"processEmpty\", true);\n\n          FacetProcessor fproc = FacetProcessor.createProcessor(rb.req, topLevel, // rb.getResults().docSet\n                                                                  docs );\n          //TODO do we handle debug?  Should probably already be handled by the legacy code\n          fproc.process();\n\n          //Go through the response to build the expected output for SimpleFacets\n          Object res = fproc.getResponse();\n          counts = new NamedList<Integer>();\n          if(res != null) {\n            SimpleOrderedMap<Object> som = (SimpleOrderedMap<Object>)res;\n            SimpleOrderedMap<Object> asdf = (SimpleOrderedMap<Object>) som.get(field);\n\n            List<SimpleOrderedMap<Object>> buckets = (List<SimpleOrderedMap<Object>>)asdf.get(\"buckets\");\n            for(SimpleOrderedMap<Object> b : buckets) {\n              counts.add(b.get(\"val\").toString(), (Integer)b.get(\"count\"));\n            }\n            if(missing) {\n              SimpleOrderedMap<Object> missingCounts = (SimpleOrderedMap<Object>) asdf.get(\"missing\");\n              counts.add(null, (Integer)missingCounts.get(\"count\"));\n            }\n          }\n        break;\n      case FC:\n        counts = DocValuesFacets.getCounts(searcher, docs, field, offset,limit, mincount, missing, sort, prefix, termFilter, fdebug);\n        break;\n      default:\n        throw new AssertionError();\n    }\n  }\n\n  if (fdebug != null) {\n    long timeElapsed = (long) timer.getTime();\n    fdebug.setElapse(timeElapsed);\n  }\n\n  return counts;\n}",
        "accept_response": "private NamedList<Integer> getTermCounts(String field, Integer mincount, ParsedParams parsed) throws IOException {\n  final SolrParams params = parsed.params;\n  final DocSet docs = parsed.docs;\n  final int threads = parsed.threads;\n  int offset = params.getFieldInt(field, FacetParams.FACET_OFFSET, 0);\n  int limit = params.getFieldInt(field, FacetParams.FACET_LIMIT, 100);\n  if (limit == 0) return new NamedList<>();\n  if (mincount==null) {\n    Boolean zeros = params.getFieldBool(field, FacetParams.FACET_ZEROS);\n    // mincount = (zeros!=null && zeros) ? 0 : 1;\n    mincount = (zeros!=null && !zeros) ? 1 : 0;\n    // current default is to include zeros.\n  }\n  boolean missing = params.getFieldBool(field, FacetParams.FACET_MISSING, false);\n  // default to sorting if there is a limit.\n  String sort = params.getFieldParam(field, FacetParams.FACET_SORT, limit>0 ? FacetParams.FACET_SORT_COUNT : FacetParams.FACET_SORT_INDEX);\n  String prefix = params.getFieldParam(field, FacetParams.FACET_PREFIX);\n\n  final Predicate<BytesRef> termFilter = newBytesRefFilter(field, params);\n\n  boolean exists = params.getFieldBool(field, FacetParams.FACET_EXISTS, false);\n\n  NamedList<Integer> counts;\n  SchemaField sf = searcher.getSchema().getField(field);\n  if (sf.getType().isPointField() && !sf.hasDocValues()) {\n    throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n        \"Can't facet on a PointField without docValues\");\n  }\n  FieldType ft = sf.getType();\n\n  // determine what type of faceting method to use\n  final String methodStr = params.getFieldParam(field, FacetParams.FACET_METHOD);\n  final FacetMethod requestedMethod;\n  if (FacetParams.FACET_METHOD_enum.equals(methodStr)) {\n    requestedMethod = FacetMethod.ENUM;\n  } else if (FacetParams.FACET_METHOD_fcs.equals(methodStr)) {\n    requestedMethod = FacetMethod.FCS;\n  } else if (FacetParams.FACET_METHOD_fc.equals(methodStr)) {\n    requestedMethod = FacetMethod.FC;\n  } else if(FacetParams.FACET_METHOD_uif.equals(methodStr)) {\n    requestedMethod = FacetMethod.UIF;\n  } else {\n    requestedMethod=null;\n  }\n\n  final boolean multiToken = sf.multiValued() || ft.multiValuedFieldCache();\n\n  FacetMethod appliedFacetMethod = selectFacetMethod(field,\n                              sf, requestedMethod, mincount,\n                              exists);\n\n  RTimer timer = null;\n  if (fdebug != null) {\n     fdebug.putInfoItem(\"requestedMethod\", requestedMethod==null?\"not specified\":requestedMethod.name());\n     fdebug.putInfoItem(\"appliedMethod\", appliedFacetMethod.name());\n     fdebug.putInfoItem(\"inputDocSetSize\", docs.size());\n     fdebug.putInfoItem(\"field\", field);\n     timer = new RTimer();\n  }\n\n  if (params.getFieldBool(field, GroupParams.GROUP_FACET, false)) {\n    counts = getGroupedCounts(searcher, docs, field, multiToken, offset,limit, mincount, missing, sort, prefix, termFilter);\n  } else {\n    assert appliedFacetMethod != null;\n    switch (appliedFacetMethod) {\n      case ENUM:\n        assert TrieField.getMainValuePrefix(ft) == null;\n        counts = getFacetTermEnumCounts(searcher, docs, field, offset, limit, mincount,missing,sort,prefix, termFilter, exists);\n        break;\n      case FCS:\n        assert ft.isPointField() || !multiToken;\n        if (ft.isPointField() || (ft.getNumberType() != null && !sf.multiValued())) {\n          if (prefix != null) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, FacetParams.FACET_PREFIX + \" is not supported on numeric types\");\n          }\n          if (termFilter != null) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"BytesRef term filters (\"\n                    + FacetParams.FACET_CONTAINS + \", \"\n                    + FacetParams.FACET_EXCLUDETERMS + \") are not supported on numeric types\");\n          }\n          if (ft.isPointField() && mincount <= 0) { // default is mincount=0.  See SOLR-10033 & SOLR-11174.\n            String warningMessage\n                = \"Raising facet.mincount from \" + mincount + \" to 1, because field \" + field + \" is Points-based.\";\n            LOG.warn(warningMessage);\n            List<String> warnings = (List<String>)rb.rsp.getResponseHeader().get(\"warnings\");\n            if (null == warnings) {\n              warnings = new ArrayList<>();\n              rb.rsp.getResponseHeader().add(\"warnings\", warnings);\n            }\n            warnings.add(warningMessage);\n\n            mincount = 1;\n          }\n          counts = NumericFacets.getCounts(searcher, docs, field, offset, limit, mincount, missing, sort);\n        } else {\n          PerSegmentSingleValuedFaceting ps = new PerSegmentSingleValuedFaceting(searcher, docs, field, offset, limit, mincount, missing, sort, prefix, termFilter);\n          Executor executor = threads == 0 ? directExecutor : facetExecutor;\n          ps.setNumThreads(threads);\n          counts = ps.getFacetCounts(executor);\n        }\n        break;\n      case UIF:\n          //Emulate the JSON Faceting structure so we can use the same parsing classes\n          Map<String, Object> jsonFacet = new HashMap<>(13);\n          jsonFacet.put(\"type\", \"terms\");\n          jsonFacet.put(\"field\", field);\n          jsonFacet.put(\"offset\", offset);\n          jsonFacet.put(\"limit\", limit);\n          jsonFacet.put(\"mincount\", mincount);\n          jsonFacet.put(\"missing\", missing);\n          jsonFacet.put(\"prefix\", prefix);\n          jsonFacet.put(\"numBuckets\", params.getFieldBool(field, \"numBuckets\", false));\n          jsonFacet.put(\"allBuckets\", params.getFieldBool(field, \"allBuckets\", false));\n          jsonFacet.put(\"method\", \"uif\");\n          jsonFacet.put(\"cacheDf\", 0);\n          jsonFacet.put(\"perSeg\", false);\n\n          final String sortVal;\n          switch(sort){\n            case FacetParams.FACET_SORT_COUNT_LEGACY:\n              sortVal = FacetParams.FACET_SORT_COUNT;\n            break;\n            case FacetParams.FACET_SORT_INDEX_LEGACY:\n              sortVal = FacetParams.FACET_SORT_INDEX;\n            break;\n            default:\n              sortVal = sort;\n          }\n          jsonFacet.put(SORT, sortVal );\n\n          Map<String, Object> topLevel = new HashMap<>();\n          topLevel.put(field, jsonFacet);\n\n          topLevel.put(\"processEmpty\", true);\n\n          FacetProcessor fproc = FacetProcessor.createProcessor(rb.req, topLevel, // rb.getResults().docSet\n                                                                  docs );\n          //TODO do we handle debug?  Should probably already be handled by the legacy code\n          fproc.process();\n\n          //Go through the response to build the expected output for SimpleFacets\n          Object res = fproc.getResponse();\n          counts = new NamedList<Integer>();\n          if(res != null) {\n            SimpleOrderedMap<Object> som = (SimpleOrderedMap<Object>)res;\n            SimpleOrderedMap<Object> asdf = (SimpleOrderedMap<Object>) som.get(field);\n\n            List<SimpleOrderedMap<Object>> buckets = (List<SimpleOrderedMap<Object>>)asdf.get(\"buckets\");\n            for(SimpleOrderedMap<Object> b : buckets) {\n              counts.add(b.get(\"val\").toString(), (Integer)b.get(\"count\"));\n            }\n            if(missing) {\n              SimpleOrderedMap<Object> missingCounts = (SimpleOrderedMap<Object>) asdf.get(\"missing\");\n              counts.add(null, (Integer)missingCounts.get(\"count\"));\n            }\n          }\n        break;\n      case FC:\n        counts = DocValuesFacets.getCounts(searcher, docs, field, offset,limit, mincount, missing, sort, prefix, termFilter, fdebug);\n        break;\n      default:\n        throw new AssertionError();\n    }\n  }\n\n  if (fdebug != null) {\n    long timeElapsed = (long) timer.getTime();\n    fdebug.setElapse(timeElapsed);\n  }\n\n  return counts;\n}",
        "reject_response": "private NamedList<Integer> getTermCounts(String field, Integer mincount, ParsedParams parsed) throws IOException {\n  final SolrParams params = parsed.params;\n  final DocSet docs = parsed.docs;\n  final int threads = parsed.threads;\n  int offset = params.getFieldInt(field, FacetParams.FACET_OFFSET, 0);\n  int limit = params.getFieldInt(field, FacetParams.FACET_LIMIT, 100);\n  if (limit == 0) return new NamedList<>();\n  if (mincount==null) {\n    Boolean zeros = params.getFieldBool(field, FacetParams.FACET_ZEROS);\n    // mincount = (zeros!=null && zeros) ? 0 : 1;\n    mincount = (zeros!=null && !zeros) ? 1 : 0;\n    // current default is to include zeros.\n  }\n  boolean missing = params.getFieldBool(field, FacetParams.FACET_MISSING, false);\n  // default to sorting if there is a limit.\n  String sort = params.getFieldParam(field, FacetParams.FACET_SORT, limit>0 ? FacetParams.FACET_SORT_COUNT : FacetParams.FACET_SORT_INDEX);\n  String prefix = params.getFieldParam(field, FacetParams.FACET_PREFIX);\n\n  final Predicate<BytesRef> termFilter = newBytesRefFilter(field, params);\n\n  boolean exists = params.getFieldBool(field, FacetParams.FACET_EXISTS, false);\n\n  NamedList<Integer> counts;\n  SchemaField sf = searcher.getSchema().getField(field);\n  if (sf.getType().isPointField() && !sf.hasDocValues()) {\n    throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n        \"Can't facet on a PointField without docValues\");\n  }\n  FieldType ft = sf.getType();\n\n  // determine what type of faceting method to use\n  final String methodStr = params.getFieldParam(field, FacetParams.FACET_METHOD);\n  final FacetMethod requestedMethod;\n  if (FacetParams.FACET_METHOD_enum.equals(methodStr)) {\n    requestedMethod = FacetMethod.ENUM;\n  } else if (FacetParams.FACET_METHOD_fcs.equals(methodStr)) {\n    requestedMethod = FacetMethod.FCS;\n  } else if (FacetParams.FACET_METHOD_fc.equals(methodStr)) {\n    requestedMethod = FacetMethod.FC;\n  } else if(FacetParams.FACET_METHOD_uif.equals(methodStr)) {\n    requestedMethod = FacetMethod.UIF;\n  } else {\n    requestedMethod=null;\n  }\n\n  final boolean multiToken = sf.multiValued() || ft.multiValuedFieldCache();\n\n  FacetMethod appliedFacetMethod = selectFacetMethod(field,\n                              sf, requestedMethod, mincount,\n                              exists);\n\n  RTimer timer = null;\n  if (fdebug != null) {\n     fdebug.putInfoItem(\"requestedMethod\", requestedMethod==null?\"not specified\":requestedMethod.name());\n     fdebug.putInfoItem(\"appliedMethod\", appliedFacetMethod.name());\n     fdebug.putInfoItem(\"inputDocSetSize\", docs.size());\n     fdebug.putInfoItem(\"field\", field);\n     timer = new RTimer();\n  }\n\n  if (params.getFieldBool(field, GroupParams.GROUP_FACET, false)) {\n    counts = getGroupedCounts(searcher, docs, field, multiToken, offset,limit, mincount, missing, sort, prefix, termFilter);\n  } else {\n    assert appliedFacetMethod != null;\n    switch (appliedFacetMethod) {\n      case ENUM:\n        assert TrieField.getMainValuePrefix(ft) == null;\n        counts = getFacetTermEnumCounts(searcher, docs, field, offset, limit, mincount,missing,sort,prefix, termFilter, exists);\n        break;\n      case FCS:\n        assert ft.isPointField() || !multiToken;\n        if (ft.isPointField() || (ft.getNumberType() != null && !sf.multiValued())) {\n          if (prefix != null) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, FacetParams.FACET_PREFIX + \" is not supported on numeric types\");\n          }\n          if (termFilter != null) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"BytesRef term filters (\"\n                    + FacetParams.FACET_CONTAINS + \", \"\n                    + FacetParams.FACET_EXCLUDETERMS + \") are not supported on numeric types\");\n          }\n          if (ft.isPointField() && mincount <= 0) { // default is mincount=0.  See SOLR-10033 & SOLR-11174.\n            String warningMessage\n                = \"Raising facet.mincount from \" + mincount + \" to 1, because field \" + field + \" is Points-based.\";\n            LOG.warn(\"Raising facet.mincount from \" + mincount + \" to 1, because field \" + field + \" is Points-based.\");\n            List<String> warnings = (List<String>)rb.rsp.getResponseHeader().get(\"warnings\");\n            if (null == warnings) {\n              warnings = new ArrayList<>();\n              rb.rsp.getResponseHeader().add(\"warnings\", warnings);\n            }\n            warnings.add(warningMessage);\n\n            mincount = 1;\n          }\n          counts = NumericFacets.getCounts(searcher, docs, field, offset, limit, mincount, missing, sort);\n        } else {\n          PerSegmentSingleValuedFaceting ps = new PerSegmentSingleValuedFaceting(searcher, docs, field, offset, limit, mincount, missing, sort, prefix, termFilter);\n          Executor executor = threads == 0 ? directExecutor : facetExecutor;\n          ps.setNumThreads(threads);\n          counts = ps.getFacetCounts(executor);\n        }\n        break;\n      case UIF:\n          //Emulate the JSON Faceting structure so we can use the same parsing classes\n          Map<String, Object> jsonFacet = new HashMap<>(13);\n          jsonFacet.put(\"type\", \"terms\");\n          jsonFacet.put(\"field\", field);\n          jsonFacet.put(\"offset\", offset);\n          jsonFacet.put(\"limit\", limit);\n          jsonFacet.put(\"mincount\", mincount);\n          jsonFacet.put(\"missing\", missing);\n          jsonFacet.put(\"prefix\", prefix);\n          jsonFacet.put(\"numBuckets\", params.getFieldBool(field, \"numBuckets\", false));\n          jsonFacet.put(\"allBuckets\", params.getFieldBool(field, \"allBuckets\", false));\n          jsonFacet.put(\"method\", \"uif\");\n          jsonFacet.put(\"cacheDf\", 0);\n          jsonFacet.put(\"perSeg\", false);\n\n          final String sortVal;\n          switch(sort){\n            case FacetParams.FACET_SORT_COUNT_LEGACY:\n              sortVal = FacetParams.FACET_SORT_COUNT;\n            break;\n            case FacetParams.FACET_SORT_INDEX_LEGACY:\n              sortVal = FacetParams.FACET_SORT_INDEX;\n            break;\n            default:\n              sortVal = sort;\n          }\n          jsonFacet.put(SORT, sortVal );\n\n          Map<String, Object> topLevel = new HashMap<>();\n          topLevel.put(field, jsonFacet);\n\n          topLevel.put(\"processEmpty\", true);\n\n          FacetProcessor fproc = FacetProcessor.createProcessor(rb.req, topLevel, // rb.getResults().docSet\n                                                                  docs );\n          //TODO do we handle debug?  Should probably already be handled by the legacy code\n          fproc.process();\n\n          //Go through the response to build the expected output for SimpleFacets\n          Object res = fproc.getResponse();\n          counts = new NamedList<Integer>();\n          if(res != null) {\n            SimpleOrderedMap<Object> som = (SimpleOrderedMap<Object>)res;\n            SimpleOrderedMap<Object> asdf = (SimpleOrderedMap<Object>) som.get(field);\n\n            List<SimpleOrderedMap<Object>> buckets = (List<SimpleOrderedMap<Object>>)asdf.get(\"buckets\");\n            for(SimpleOrderedMap<Object> b : buckets) {\n              counts.add(b.get(\"val\").toString(), (Integer)b.get(\"count\"));\n            }\n            if(missing) {\n              SimpleOrderedMap<Object> missingCounts = (SimpleOrderedMap<Object>) asdf.get(\"missing\");\n              counts.add(null, (Integer)missingCounts.get(\"count\"));\n            }\n          }\n        break;\n      case FC:\n        counts = DocValuesFacets.getCounts(searcher, docs, field, offset,limit, mincount, missing, sort, prefix, termFilter, fdebug);\n        break;\n      default:\n        throw new AssertionError();\n    }\n  }\n\n  if (fdebug != null) {\n    long timeElapsed = (long) timer.getTime();\n    fdebug.setElapse(timeElapsed);\n  }\n\n  return counts;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2818,
        "instruction": "private void registerStripedExecutorMBean(StripedExecutor stripedExecSvc) throws IgniteCheckedException {\n    if (stripedExecSvc == null || U.IGNITE_MBEANS_DISABLED)\n        return;\n\n    String name = \"StripedExecutor\";\n\n    try {\n        stripedExecSvcMBean = U.registerMBean(\n            cfg.getMBeanServer(),\n            cfg.getIgniteInstanceName(),\n            \"Thread Pools\",\n            name,\n            new StripedExecutorMXBeanAdapter(stripedExecSvc),\n            StripedExecutorMXBean.class);\n\n        if (log.isDebugEnabled())\n    }\n    catch (JMException e) {\n        throw new IgniteCheckedException(\"Failed to register executor service MBean [name=\"\n            + name + \", exec=\" + stripedExecSvc + ']', e);\n    }\n}",
        "accept_response": "private void registerStripedExecutorMBean(StripedExecutor stripedExecSvc) throws IgniteCheckedException {\n    if (stripedExecSvc == null || U.IGNITE_MBEANS_DISABLED)\n        return;\n\n    String name = \"StripedExecutor\";\n\n    try {\n        stripedExecSvcMBean = U.registerMBean(\n            cfg.getMBeanServer(),\n            cfg.getIgniteInstanceName(),\n            \"Thread Pools\",\n            name,\n            new StripedExecutorMXBeanAdapter(stripedExecSvc),\n            StripedExecutorMXBean.class);\n\n        if (log.isDebugEnabled())\n            log.debug(\"Registered executor service MBean: \" + stripedExecSvcMBean);\n    }\n    catch (JMException e) {\n        throw new IgniteCheckedException(\"Failed to register executor service MBean [name=\"\n            + name + \", exec=\" + stripedExecSvc + ']', e);\n    }\n}",
        "reject_response": "private void registerStripedExecutorMBean(StripedExecutor stripedExecSvc) throws IgniteCheckedException {\n    if (stripedExecSvc == null || U.IGNITE_MBEANS_DISABLED)\n        return;\n\n    String name = \"StripedExecutor\";\n\n    try {\n        stripedExecSvcMBean = U.registerMBean(\n            cfg.getMBeanServer(),\n            cfg.getIgniteInstanceName(),\n            \"Thread Pools\",\n            name,\n            new StripedExecutorMXBeanAdapter(stripedExecSvc),\n            StripedExecutorMXBean.class);\n\n        if (log.isDebugEnabled())\n            if (log.isDebugEnabled())\n                log.debug(\"Registered executor service MBean: \" + stripedExecSvcMBean);\n        } catch (JMException e) {\n            throw new IgniteCheckedException(\"Failed to register executor service MBean [name=\"\n                + name + \", exec=\" + stripedExecSvc + ']', e);\n        }\n    }\n    catch (JMException e) {\n        throw new IgniteCheckedException(\"Failed to register executor service MBean [name=\"\n            + name + \", exec=\" + stripedExecSvc + ']', e);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2908,
        "instruction": "protected void runTest() throws Throwable {\n\n    LOGGER.info(\"Running test: \"+getName()+\" ...\");\n\n    if (parser == null) {\n        /* PREVIOUS SLOW WAY\n        enabledServices = new WiringServiceTable();\n        enabledServices.put(LogEnabled.class, log);\n        */\n        parser = new RFC4408SPF1Parser(new DefaultTermsFactory(new WiringService() {\n            public void wire(Object component) {\n                if (component instanceof MacroExpandEnabled) {\n                    ((MacroExpandEnabled) component).enableMacroExpand(macroExpand);\n                }\n                if (component instanceof DNSServiceEnabled) {\n                    ((DNSServiceEnabled) component).enableDNSService(dns);\n                }\n                if (component instanceof SPFCheckEnabled) {\n                    ((SPFCheckEnabled) component).enableSPFChecking(spf);\n                }\n            }\n        }));\n    }\n    if (this.data != AbstractYamlTest.prevData) {\n        dns = new LoggingDNSService(getDNSService());\n        AbstractYamlTest.prevData = this.data;\n    }\n    macroExpand = new MacroExpand(dns);\n    if (getSpfExecutorType() == SYNCHRONOUS_EXECUTOR) {  // synchronous\n        executor = new SynchronousSPFExecutor(dns);\n    } else if (getSpfExecutorType() == STAGED_EXECUTOR || getSpfExecutorType() == STAGED_EXECUTOR_MULTITHREADED){\n        executor = new StagedMultipleSPFExecutor(new DNSServiceAsynchSimulator(dns, getSpfExecutorType() == STAGED_EXECUTOR_MULTITHREADED));\n    } else if (getSpfExecutorType() == STAGED_EXECUTOR_DNSJNIO) {\n\n        // reset cache between usages of the asynchronous lookuper\n        LookupAsynch.setDefaultCache(new Cache(), DClass.IN);\n        // reset cache between usages of the asynchronous lookuper\n        LookupAsynch.getDefaultCache(DClass.IN).clearCache();\n\n        try {\n            ExtendedNonblockingResolver resolver;\n\n            if (getDnsServiceMockStyle() == FAKE_SERVER) {\n                NonblockingResolver nonblockingResolver = new NonblockingResolver(\"127.0.0.1\");\n                resolver = ExtendedNonblockingResolver.newInstance(new NonblockingResolver[] {nonblockingResolver});\n                nonblockingResolver.setPort(FAKE_SERVER_PORT);\n                nonblockingResolver.setTCP(false);\n            } else if (getDnsServiceMockStyle() == REAL_SERVER) {\n                resolver = ExtendedNonblockingResolver.newInstance();\n                Resolver[] resolvers = resolver.getResolvers();\n                for (int i = 0; i < resolvers.length; i++) {\n                    resolvers[i].setTCP(false);\n                }\n            } else {\n                throw new IllegalStateException(\"DnsServiceMockStyle \"+getDnsServiceMockStyle()+\" is not supported when STAGED_EXECUTOR_DNSJNIO executor style is used\");\n            }\n\n            DNSJnioAsynchService jnioAsynchService = new DNSJnioAsynchService(resolver);\n            jnioAsynchService.setTimeout(TIMEOUT);\n            executor = new StagedMultipleSPFExecutor(jnioAsynchService);\n\n        } catch (UnknownHostException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        }\n\n    } else {\n        throw new UnsupportedOperationException(\"Unknown executor type\");\n    }\n    spf = new SPF(dns, parser, macroExpand, executor);\n\n    if (test != null) {\n        String next = test;\n        SPFResult res = runSingleTest(next);\n        verifyResult(next, res);\n    } else {\n        Map<String,SPFResult> queries = new HashMap<String,SPFResult>();\n        for (Iterator<String> i = data.getTests().keySet().iterator(); i.hasNext(); ) {\n            String next = i.next();\n            SPFResult res = runSingleTest(next);\n            queries.put(next, res);\n        }\n        AssertionFailedError firstError = null;\n        for (Iterator<String> i = queries.keySet().iterator(); i.hasNext(); ) {\n            String next = i.next();\n            try {\n                verifyResult(next, queries.get(next));\n            } catch (AssertionFailedError e) {\n                if (firstError == null) firstError = e;\n            }\n        }\n        if (firstError != null) throw firstError;\n    }\n\n}",
        "accept_response": "protected void runTest() throws Throwable {\n\n    LOGGER.info(\"Running test: \"+getName()+\" ...\");\n\n    if (parser == null) {\n        /* PREVIOUS SLOW WAY\n        enabledServices = new WiringServiceTable();\n        enabledServices.put(LogEnabled.class, log);\n        */\n        parser = new RFC4408SPF1Parser(new DefaultTermsFactory(new WiringService() {\n            public void wire(Object component) {\n                if (component instanceof MacroExpandEnabled) {\n                    ((MacroExpandEnabled) component).enableMacroExpand(macroExpand);\n                }\n                if (component instanceof DNSServiceEnabled) {\n                    ((DNSServiceEnabled) component).enableDNSService(dns);\n                }\n                if (component instanceof SPFCheckEnabled) {\n                    ((SPFCheckEnabled) component).enableSPFChecking(spf);\n                }\n            }\n        }));\n    }\n    if (this.data != AbstractYamlTest.prevData) {\n        dns = new LoggingDNSService(getDNSService());\n        AbstractYamlTest.prevData = this.data;\n    }\n    macroExpand = new MacroExpand(dns);\n    if (getSpfExecutorType() == SYNCHRONOUS_EXECUTOR) {  // synchronous\n        executor = new SynchronousSPFExecutor(dns);\n    } else if (getSpfExecutorType() == STAGED_EXECUTOR || getSpfExecutorType() == STAGED_EXECUTOR_MULTITHREADED){\n        executor = new StagedMultipleSPFExecutor(new DNSServiceAsynchSimulator(dns, getSpfExecutorType() == STAGED_EXECUTOR_MULTITHREADED));\n    } else if (getSpfExecutorType() == STAGED_EXECUTOR_DNSJNIO) {\n\n        // reset cache between usages of the asynchronous lookuper\n        LookupAsynch.setDefaultCache(new Cache(), DClass.IN);\n        // reset cache between usages of the asynchronous lookuper\n        LookupAsynch.getDefaultCache(DClass.IN).clearCache();\n\n        try {\n            ExtendedNonblockingResolver resolver;\n\n            if (getDnsServiceMockStyle() == FAKE_SERVER) {\n                NonblockingResolver nonblockingResolver = new NonblockingResolver(\"127.0.0.1\");\n                resolver = ExtendedNonblockingResolver.newInstance(new NonblockingResolver[] {nonblockingResolver});\n                nonblockingResolver.setPort(FAKE_SERVER_PORT);\n                nonblockingResolver.setTCP(false);\n            } else if (getDnsServiceMockStyle() == REAL_SERVER) {\n                resolver = ExtendedNonblockingResolver.newInstance();\n                Resolver[] resolvers = resolver.getResolvers();\n                for (int i = 0; i < resolvers.length; i++) {\n                    resolvers[i].setTCP(false);\n                }\n            } else {\n                throw new IllegalStateException(\"DnsServiceMockStyle \"+getDnsServiceMockStyle()+\" is not supported when STAGED_EXECUTOR_DNSJNIO executor style is used\");\n            }\n\n            DNSJnioAsynchService jnioAsynchService = new DNSJnioAsynchService(resolver);\n            jnioAsynchService.setTimeout(TIMEOUT);\n            executor = new StagedMultipleSPFExecutor(jnioAsynchService);\n\n        } catch (UnknownHostException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        }\n\n    } else {\n        throw new UnsupportedOperationException(\"Unknown executor type\");\n    }\n    spf = new SPF(dns, parser, macroExpand, executor);\n\n    if (test != null) {\n        String next = test;\n        SPFResult res = runSingleTest(next);\n        verifyResult(next, res);\n    } else {\n        Map<String,SPFResult> queries = new HashMap<String,SPFResult>();\n        for (Iterator<String> i = data.getTests().keySet().iterator(); i.hasNext(); ) {\n            String next = i.next();\n            SPFResult res = runSingleTest(next);\n            queries.put(next, res);\n        }\n        AssertionFailedError firstError = null;\n        for (Iterator<String> i = queries.keySet().iterator(); i.hasNext(); ) {\n            String next = i.next();\n            try {\n                verifyResult(next, queries.get(next));\n            } catch (AssertionFailedError e) {\n                LOGGER.info(\"FAILED. {} ({})\", e.getMessage(), getName(), e);\n                if (firstError == null) firstError = e;\n            }\n        }\n        if (firstError != null) throw firstError;\n    }\n\n}",
        "reject_response": "protected void runTest() throws Throwable {\n\n    LOGGER.info(\"Running test: \"+getName()+\" ...\");\n\n    if (parser == null) {\n        /* PREVIOUS SLOW WAY\n        enabledServices = new WiringServiceTable();\n        enabledServices.put(LogEnabled.class, log);\n        */\n        parser = new RFC4408SPF1Parser(new DefaultTermsFactory(new WiringService() {\n            public void wire(Object component) {\n                if (component instanceof MacroExpandEnabled) {\n                    ((MacroExpandEnabled) component).enableMacroExpand(macroExpand);\n                }\n                if (component instanceof DNSServiceEnabled) {\n                    ((DNSServiceEnabled) component).enableDNSService(dns);\n                }\n                if (component instanceof SPFCheckEnabled) {\n                    ((SPFCheckEnabled) component).enableSPFChecking(spf);\n                }\n            }\n        }));\n    }\n    if (this.data != AbstractYamlTest.prevData) {\n        dns = new LoggingDNSService(getDNSService());\n        AbstractYamlTest.prevData = this.data;\n    }\n    macroExpand = new MacroExpand(dns);\n    if (getSpfExecutorType() == SYNCHRONOUS_EXECUTOR) {  // synchronous\n        executor = new SynchronousSPFExecutor(dns);\n    } else if (getSpfExecutorType() == STAGED_EXECUTOR || getSpfExecutorType() == STAGED_EXECUTOR_MULTITHREADED){\n        executor = new StagedMultipleSPFExecutor(new DNSServiceAsynchSimulator(dns, getSpfExecutorType() == STAGED_EXECUTOR_MULTITHREADED));\n    } else if (getSpfExecutorType() == STAGED_EXECUTOR_DNSJNIO) {\n\n        // reset cache between usages of the asynchronous lookuper\n        LookupAsynch.setDefaultCache(new Cache(), DClass.IN);\n        // reset cache between usages of the asynchronous lookuper\n        LookupAsynch.getDefaultCache(DClass.IN).clearCache();\n\n        try {\n            ExtendedNonblockingResolver resolver;\n\n            if (getDnsServiceMockStyle() == FAKE_SERVER) {\n                NonblockingResolver nonblockingResolver = new NonblockingResolver(\"127.0.0.1\");\n                resolver = ExtendedNonblockingResolver.newInstance(new NonblockingResolver[] {nonblockingResolver});\n                nonblockingResolver.setPort(FAKE_SERVER_PORT);\n                nonblockingResolver.setTCP(false);\n            } else if (getDnsServiceMockStyle() == REAL_SERVER) {\n                resolver = ExtendedNonblockingResolver.newInstance();\n                Resolver[] resolvers = resolver.getResolvers();\n                for (int i = 0; i < resolvers.length; i++) {\n                    resolvers[i].setTCP(false);\n                }\n            } else {\n                throw new IllegalStateException(\"DnsServiceMockStyle \"+getDnsServiceMockStyle()+\" is not supported when STAGED_EXECUTOR_DNSJNIO executor style is used\");\n            }\n\n            DNSJnioAsynchService jnioAsynchService = new DNSJnioAsynchService(resolver);\n            jnioAsynchService.setTimeout(TIMEOUT);\n            executor = new StagedMultipleSPFExecutor(jnioAsynchService);\n\n        } catch (UnknownHostException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        }\n\n    } else {\n        throw new UnsupportedOperationException(\"Unknown executor type\");\n    }\n    spf = new SPF(dns, parser, macroExpand, executor);\n\n    if (test != null) {\n        String next = test;\n        SPFResult res = runSingleTest(next);\n        verifyResult(next, res);\n    } else {\n        Map<String,SPFResult> queries = new HashMap<String,SPFResult>();\n        for (Iterator<String> i = data.getTests().keySet().iterator(); i.hasNext(); ) {\n            String next = i.next();\n            SPFResult res = runSingleTest(next);\n            queries.put(next, res);\n        }\n        AssertionFailedError firstError = null;\n        for (Iterator<String> i = queries.keySet().iterator(); i.hasNext(); ) {\n            String next = i.next();\n            try {\n                verifyResult(next, queries.get(next));\n            } catch (AssertionFailedError e) {\n                log.getChildLogger(next).info(\"FAILED. \"+e.getMessage()+\" (\"+getName()+\")\", e.getMessage()==null ? e : null);\n                if (firstError == null) firstError = e;\n            }\n        }\n        if (firstError != null) throw firstError;\n    }\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2865,
        "instruction": "public static SPARQLResult result(String filenameOrURI, ResultsFormat format) {\n    if (format == null)\n        format = ResultsFormat.guessSyntax(filenameOrURI);\n\n    if (format == null) {\n        Log.warn(ResultSet.class, \"Null format - defaulting to XML\");\n        format = ResultsFormat.FMT_RS_XML;\n    }\n\n    if (format.equals(ResultsFormat.FMT_TEXT)) {\n        throw new ResultSetException(\"Can't read a text result set\");\n    }\n\n    if (format.equals(ResultsFormat.FMT_RS_XML) || format.equals(ResultsFormat.FMT_RS_JSON)\n        || format.equals(ResultsFormat.FMT_RS_TSV) || format.equals(ResultsFormat.FMT_RS_CSV)) {\n        InputStream in = null;\n        try {\n            in = FileManager.get().open(filenameOrURI);\n            if (in == null)\n                throw new NotFoundException(filenameOrURI);\n        } catch (NotFoundException ex) {\n            throw new NotFoundException(\"File not found: \" + filenameOrURI);\n        }\n\n        SPARQLResult x = null;\n\n        if (format.equals(ResultsFormat.FMT_RS_JSON))\n            return JSONInput.make(in, GraphFactory.makeDefaultModel());\n        else if (format.equals(ResultsFormat.FMT_RS_XML))\n            return XMLInput.make(in, GraphFactory.makeDefaultModel());\n        else if (format.equals(ResultsFormat.FMT_RS_TSV)) {\n            ResultSet rs = TSVInput.fromTSV(in);\n            return new SPARQLResult(rs);\n        } else if (format.equals(ResultsFormat.FMT_RS_CSV)) {\n            ResultSet rs = CSVInput.fromCSV(in);\n            return new SPARQLResult(rs);\n        } else if (format.equals(ResultsFormat.FMT_RS_BIO)) {\n            ResultSet rs = BIOInput.fromBIO(in);\n            return new SPARQLResult(rs);\n        }\n    }\n\n    if (ResultsFormat.isRDFGraphSyntax(format)) {\n        Model model = FileManager.get().loadModel(filenameOrURI);\n        return new SPARQLResult(model);\n    }\n\n    Log.error(ResultSet.class, \"Unknown result set syntax: \" + format);\n    return null;\n}",
        "accept_response": "public static SPARQLResult result(String filenameOrURI, ResultsFormat format) {\n    if (format == null)\n        format = ResultsFormat.guessSyntax(filenameOrURI);\n\n    if (format == null) {\n        Log.warn(ResultSet.class, \"Null format - defaulting to XML\");\n        format = ResultsFormat.FMT_RS_XML;\n    }\n\n    if (format.equals(ResultsFormat.FMT_TEXT)) {\n        Log.error(ResultSet.class, \"Can't read a text result set\");\n        throw new ResultSetException(\"Can't read a text result set\");\n    }\n\n    if (format.equals(ResultsFormat.FMT_RS_XML) || format.equals(ResultsFormat.FMT_RS_JSON)\n        || format.equals(ResultsFormat.FMT_RS_TSV) || format.equals(ResultsFormat.FMT_RS_CSV)) {\n        InputStream in = null;\n        try {\n            in = FileManager.get().open(filenameOrURI);\n            if (in == null)\n                throw new NotFoundException(filenameOrURI);\n        } catch (NotFoundException ex) {\n            throw new NotFoundException(\"File not found: \" + filenameOrURI);\n        }\n\n        SPARQLResult x = null;\n\n        if (format.equals(ResultsFormat.FMT_RS_JSON))\n            return JSONInput.make(in, GraphFactory.makeDefaultModel());\n        else if (format.equals(ResultsFormat.FMT_RS_XML))\n            return XMLInput.make(in, GraphFactory.makeDefaultModel());\n        else if (format.equals(ResultsFormat.FMT_RS_TSV)) {\n            ResultSet rs = TSVInput.fromTSV(in);\n            return new SPARQLResult(rs);\n        } else if (format.equals(ResultsFormat.FMT_RS_CSV)) {\n            ResultSet rs = CSVInput.fromCSV(in);\n            return new SPARQLResult(rs);\n        } else if (format.equals(ResultsFormat.FMT_RS_BIO)) {\n            ResultSet rs = BIOInput.fromBIO(in);\n            return new SPARQLResult(rs);\n        }\n    }\n\n    if (ResultsFormat.isRDFGraphSyntax(format)) {\n        Model model = FileManager.get().loadModel(filenameOrURI);\n        return new SPARQLResult(model);\n    }\n\n    Log.error(ResultSet.class, \"Unknown result set syntax: \" + format);\n    return null;\n}",
        "reject_response": "public static SPARQLResult result(String filenameOrURI, ResultsFormat format) {\n    if (format == null)\n        format = ResultsFormat.guessSyntax(filenameOrURI);\n\n    if (format == null) {\n        Log.warn(ResultSet.class, \"Null format - defaulting to XML\");\n        format = ResultsFormat.FMT_RS_XML;\n    }\n\n    if (format.equals(ResultsFormat.FMT_TEXT)) {\n        Log.fatal(ResultSet.class, \"Can't read a text result set\");\n        throw new ResultSetException(\"Can't read a text result set\");\n    }\n\n    if (format.equals(ResultsFormat.FMT_RS_XML) || format.equals(ResultsFormat.FMT_RS_JSON)\n        || format.equals(ResultsFormat.FMT_RS_TSV) || format.equals(ResultsFormat.FMT_RS_CSV)) {\n        InputStream in = null;\n        try {\n            in = FileManager.get().open(filenameOrURI);\n            if (in == null)\n                throw new NotFoundException(filenameOrURI);\n        } catch (NotFoundException ex) {\n            throw new NotFoundException(\"File not found: \" + filenameOrURI);\n        }\n\n        SPARQLResult x = null;\n\n        if (format.equals(ResultsFormat.FMT_RS_JSON))\n            return JSONInput.make(in, GraphFactory.makeDefaultModel());\n        else if (format.equals(ResultsFormat.FMT_RS_XML))\n            return XMLInput.make(in, GraphFactory.makeDefaultModel());\n        else if (format.equals(ResultsFormat.FMT_RS_TSV)) {\n            ResultSet rs = TSVInput.fromTSV(in);\n            return new SPARQLResult(rs);\n        } else if (format.equals(ResultsFormat.FMT_RS_CSV)) {\n            ResultSet rs = CSVInput.fromCSV(in);\n            return new SPARQLResult(rs);\n        } else if (format.equals(ResultsFormat.FMT_RS_BIO)) {\n            ResultSet rs = BIOInput.fromBIO(in);\n            return new SPARQLResult(rs);\n        }\n    }\n\n    if (ResultsFormat.isRDFGraphSyntax(format)) {\n        Model model = FileManager.get().loadModel(filenameOrURI);\n        return new SPARQLResult(model);\n    }\n\n    Log.error(ResultSet.class, \"Unknown result set syntax: \" + format);\n    return null;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2641,
        "instruction": "private Boolean getBooleanAttribute(Object object, String name) {\n  if (object == null) {\n    return Boolean.FALSE;\n  }\n\n  try {\n    if (!(object.getClass().equals(Boolean.class))) {\n      return Boolean.FALSE;\n    } else {\n      return (Boolean) object;\n    }\n  } catch (Exception e) {\n    logger.info(\"Exception Occured: \", e);\n    return Boolean.FALSE;\n  }\n}",
        "accept_response": "private Boolean getBooleanAttribute(Object object, String name) {\n  if (object == null) {\n    return Boolean.FALSE;\n  }\n\n  try {\n    if (!(object.getClass().equals(Boolean.class))) {\n      logger.info(\n          \"************************Unexpected type for attribute: {}; Expected type: {}; Received type: {}************************\",\n          name, Boolean.class.getName(), object.getClass().getName());\n      return Boolean.FALSE;\n    } else {\n      return (Boolean) object;\n    }\n  } catch (Exception e) {\n    logger.info(\"Exception Occured: \", e);\n    return Boolean.FALSE;\n  }\n}",
        "reject_response": "private Boolean getBooleanAttribute(Object object, String name) {\n  if (object == null) {\n    return Boolean.FALSE;\n  }\n\n  try {\n    if (!(object.getClass().equals(Boolean.class))) {\n      if (LOGGER.infoEnabled()) {\n        LOGGER.info(\"************************Unexpected type for attribute: \" + name\n            + \" Expected type: \" + Boolean.class.getName() + \" Received type: \"\n            + object.getClass().getName() + \"************************\");\n      }\n      return Boolean.FALSE;\n    } else {\n      return (Boolean) object;\n    }\n  } catch (Exception e) {\n    logger.info(\"Exception Occured: \", e);\n    return Boolean.FALSE;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2988,
        "instruction": "@GET\n@Path(KMSRESTConstants.KEYS_NAMES_RESOURCE)\n@Produces(MediaType.APPLICATION_JSON)\npublic Response getKeyNames(@Context HttpServletRequest request) throws Exception {\n  try {\n    KMSWebApp.getAdminCallsMeter().mark();\n    UserGroupInformation user = HttpUserGroupInformation.get();\n    assertAccess(Type.GET_KEYS, user, KMSOp.GET_KEYS, request.getRemoteAddr());\n    List<String> json = user.doAs(new PrivilegedExceptionAction<List<String>>() {\n      @Override\n      public List<String> run() throws Exception {\n        return provider.getKeys();\n      }\n    });\n    kmsAudit.ok(user, KMSOp.GET_KEYS, \"\");\n    if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Exiting getKeyNames method.\");\n    }\n    return Response.ok().type(MediaType.APPLICATION_JSON).entity(json).build();\n  } catch (Exception e) {\n    LOG.error(\"Exception in getkeyNames.\", e);\n    throw e;\n  }\n}",
        "accept_response": "@GET\n@Path(KMSRESTConstants.KEYS_NAMES_RESOURCE)\n@Produces(MediaType.APPLICATION_JSON)\npublic Response getKeyNames(@Context HttpServletRequest request) throws Exception {\n  try {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Entering getKeyNames method.\");\n    }\n    KMSWebApp.getAdminCallsMeter().mark();\n    UserGroupInformation user = HttpUserGroupInformation.get();\n    assertAccess(Type.GET_KEYS, user, KMSOp.GET_KEYS, request.getRemoteAddr());\n    List<String> json = user.doAs(new PrivilegedExceptionAction<List<String>>() {\n      @Override\n      public List<String> run() throws Exception {\n        return provider.getKeys();\n      }\n    });\n    kmsAudit.ok(user, KMSOp.GET_KEYS, \"\");\n    if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Exiting getKeyNames method.\");\n    }\n    return Response.ok().type(MediaType.APPLICATION_JSON).entity(json).build();\n  } catch (Exception e) {\n    LOG.error(\"Exception in getkeyNames.\", e);\n    throw e;\n  }\n}",
        "reject_response": "@GET\n@Path(KMSRESTConstants.KEYS_NAMES_RESOURCE)\n@Produces(MediaType.APPLICATION_JSON)\npublic Response getKeyNames(@Context HttpServletRequest request) throws Exception {\n  try {\n    if (LOG.isDebugEnabled()) {\n    LOG.info(\"Entering getKeyNames method.\");\n    }\n    KMSWebApp.getAdminCallsMeter().mark();\n    UserGroupInformation user = HttpUserGroupInformation.get();\n    assertAccess(Type.GET_KEYS, user, KMSOp.GET_KEYS, request.getRemoteAddr());\n    List<String> json = user.doAs(new PrivilegedExceptionAction<List<String>>() {\n      @Override\n      public List<String> run() throws Exception {\n        return provider.getKeys();\n      }\n    });\n    kmsAudit.ok(user, KMSOp.GET_KEYS, \"\");\n    if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Exiting getKeyNames method.\");\n    }\n    return Response.ok().type(MediaType.APPLICATION_JSON).entity(json).build();\n  } catch (Exception e) {\n    LOG.error(\"Exception in getkeyNames.\", e);\n    throw e;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2844,
        "instruction": "private void sendLocalPartitions(ClusterNode node, @Nullable GridDhtPartitionExchangeId id) {\n    GridDhtPartitionsSingleMessage m = createPartitionsSingleMessage(id,\n        cctx.kernalContext().clientNode(),\n        false,\n        false,\n        null);\n\n    if (log.isTraceEnabled())\n\n    try {\n        cctx.io().sendNoRetry(node, m, SYSTEM_POOL);\n    }\n    catch (ClusterTopologyCheckedException ignore) {\n        if (log.isDebugEnabled())\n            log.debug(\"Failed to send partition update to node because it left grid (will ignore) [node=\" +\n                node.id() + \", msg=\" + m + ']');\n    }\n    catch (IgniteCheckedException e) {\n        U.error(log, \"Failed to send local partition map to node [node=\" + node + \", exchId=\" + id + ']', e);\n    }\n}",
        "accept_response": "private void sendLocalPartitions(ClusterNode node, @Nullable GridDhtPartitionExchangeId id) {\n    GridDhtPartitionsSingleMessage m = createPartitionsSingleMessage(id,\n        cctx.kernalContext().clientNode(),\n        false,\n        false,\n        null);\n\n    if (log.isTraceEnabled())\n        log.trace(\"Sending local partitions [nodeId=\" + node.id() + \", msg=\" + m + ']');\n\n    try {\n        cctx.io().sendNoRetry(node, m, SYSTEM_POOL);\n    }\n    catch (ClusterTopologyCheckedException ignore) {\n        if (log.isDebugEnabled())\n            log.debug(\"Failed to send partition update to node because it left grid (will ignore) [node=\" +\n                node.id() + \", msg=\" + m + ']');\n    }\n    catch (IgniteCheckedException e) {\n        U.error(log, \"Failed to send local partition map to node [node=\" + node + \", exchId=\" + id + ']', e);\n    }\n}",
        "reject_response": "private void sendLocalPartitions(ClusterNode node, @Nullable GridDhtPartitionExchangeId id) {\n    GridDhtPartitionsSingleMessage m = createPartitionsSingleMessage(id,\n        cctx.kernalContext().clientNode(),\n        false,\n        false,\n        null);\n\n    if (log.isTraceEnabled())\n    if (log.isDebugEnabled())\n        log.debug(\"Sending local partitions [nodeId=\" + node.id() + \", msg=\" + m + ']');\n\n    try {\n        cctx.io().sendNoRetry(node, m, SYSTEM_POOL);\n    }\n    catch (ClusterTopologyCheckedException ignore) {\n        if (log.isDebugEnabled())\n            log.debug(\"Failed to send partition update to node because it left grid (will ignore) [node=\" +\n                node.id() + \", msg=\" + m + ']');\n    }\n    catch (IgniteCheckedException e) {\n        U.error(log, \"Failed to send local partition map to node [node=\" + node + \", exchId=\" + id + ']', e);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3083,
        "instruction": "@Override\npublic void execute(Tuple tuple) {\n    boolean flush = false;\n    try {\n        if (TupleUtils.isTick(tuple)) {\n            flush = true;\n        } else {\n            byte[] rowKey = this.mapper.rowKey(tuple);\n            ColumnList cols = this.mapper.columns(tuple);\n            List<Mutation> mutations = hBaseClient.constructMutationReq(rowKey, cols, writeToWAL? Durability.SYNC_WAL : Durability.SKIP_WAL);\n            batchMutations.addAll(mutations);\n            tupleBatch.add(tuple);\n            if (tupleBatch.size() >= batchSize) {\n                flush = true;\n            }\n        }\n\n        if (flush && !tupleBatch.isEmpty()) {\n            this.hBaseClient.batchMutate(batchMutations);\n            LOG.debug(\"acknowledging tuples after batchMutate\");\n            for(Tuple t : tupleBatch) {\n                collector.ack(t);\n            }\n            tupleBatch.clear();\n            batchMutations.clear();\n        }\n    } catch(Exception e){\n        this.collector.reportError(e);\n        for (Tuple t : tupleBatch) {\n            collector.fail(t);\n        }\n        tupleBatch.clear();\n        batchMutations.clear();\n    }\n}",
        "accept_response": "@Override\npublic void execute(Tuple tuple) {\n    boolean flush = false;\n    try {\n        if (TupleUtils.isTick(tuple)) {\n            LOG.debug(\"TICK received! current batch status [{}/{}]\", tupleBatch.size(), batchSize);\n            flush = true;\n        } else {\n            byte[] rowKey = this.mapper.rowKey(tuple);\n            ColumnList cols = this.mapper.columns(tuple);\n            List<Mutation> mutations = hBaseClient.constructMutationReq(rowKey, cols, writeToWAL? Durability.SYNC_WAL : Durability.SKIP_WAL);\n            batchMutations.addAll(mutations);\n            tupleBatch.add(tuple);\n            if (tupleBatch.size() >= batchSize) {\n                flush = true;\n            }\n        }\n\n        if (flush && !tupleBatch.isEmpty()) {\n            this.hBaseClient.batchMutate(batchMutations);\n            LOG.debug(\"acknowledging tuples after batchMutate\");\n            for(Tuple t : tupleBatch) {\n                collector.ack(t);\n            }\n            tupleBatch.clear();\n            batchMutations.clear();\n        }\n    } catch(Exception e){\n        this.collector.reportError(e);\n        for (Tuple t : tupleBatch) {\n            collector.fail(t);\n        }\n        tupleBatch.clear();\n        batchMutations.clear();\n    }\n}",
        "reject_response": "@Override\npublic void execute(Tuple tuple) {\n    boolean flush = false;\n    try {\n        if (TupleUtils.isTick(tuple)) {\n            LOG.debug(\"TICK received! current batch status [\" + tupleBatch.size() + \"/\" + batchSize + \"]\");\n            flush = true;\n        } else {\n            byte[] rowKey = this.mapper.rowKey(tuple);\n            ColumnList cols = this.mapper.columns(tuple);\n            List<Mutation> mutations = hBaseClient.constructMutationReq(rowKey, cols, writeToWAL? Durability.SYNC_WAL : Durability.SKIP_WAL);\n            batchMutations.addAll(mutations);\n            tupleBatch.add(tuple);\n            if (tupleBatch.size() >= batchSize) {\n                flush = true;\n            }\n        }\n\n        if (flush && !tupleBatch.isEmpty()) {\n            this.hBaseClient.batchMutate(batchMutations);\n            LOG.debug(\"acknowledging tuples after batchMutate\");\n            for(Tuple t : tupleBatch) {\n                collector.ack(t);\n            }\n            tupleBatch.clear();\n            batchMutations.clear();\n        }\n    } catch(Exception e){\n        this.collector.reportError(e);\n        for (Tuple t : tupleBatch) {\n            collector.fail(t);\n        }\n        tupleBatch.clear();\n        batchMutations.clear();\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2617,
        "instruction": "@Override\npublic void contextDestroyed(ServletContextEvent event) {\n\n  // Stop all running threads those are created in Pulse\n  // Stop cluster threads\n  Repository.get().removeAllClusters();\n\n}",
        "accept_response": "@Override\npublic void contextDestroyed(ServletContextEvent event) {\n\n  // Stop all running threads those are created in Pulse\n  // Stop cluster threads\n  Repository.get().removeAllClusters();\n\n  logger.info(\"{}{}\", resourceBundle.getString(\"LOG_MSG_CONTEXT_DESTROYED\"),\n      event.getServletContext().getContextPath());\n}",
        "reject_response": "@Override\npublic void contextDestroyed(ServletContextEvent event) {\n\n  // Stop all running threads those are created in Pulse\n  // Stop cluster threads\n  Repository.get().removeAllClusters();\n\n  if (LOGGER.infoEnabled()) {\n    LOGGER.info(resourceBundle.getString(\"LOG_MSG_CONTEXT_DESTROYED\")\n        + event.getServletContext().getContextPath());\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2663,
        "instruction": "protected void updateLocatorInLocatorList(HostAddress locator) {\n  if (locator.getSocketInetAddressNoLookup().getHostName() != null && !locator.isIpString()) {\n    LocatorList locatorList = locators.get();\n    List<HostAddress> newLocatorsList = new ArrayList<>();\n\n    for (HostAddress tloc : locatorList.getLocatorAddresses()) {\n      if (tloc.equals(locator)) {\n        InetSocketAddress changeLoc = new InetSocketAddress(locator.getHostName(),\n            locator.getSocketInetAddressNoLookup().getPort());\n        HostAddress hostAddress = new HostAddress(changeLoc, locator.getHostName());\n        newLocatorsList.add(hostAddress);\n      } else {\n        newLocatorsList.add(tloc);\n      }\n    }\n\n\n    LocatorList newLocatorList = new LocatorList(newLocatorsList);\n    locators.set(newLocatorList);\n  }\n}",
        "accept_response": "protected void updateLocatorInLocatorList(HostAddress locator) {\n  if (locator.getSocketInetAddressNoLookup().getHostName() != null && !locator.isIpString()) {\n    LocatorList locatorList = locators.get();\n    List<HostAddress> newLocatorsList = new ArrayList<>();\n\n    for (HostAddress tloc : locatorList.getLocatorAddresses()) {\n      if (tloc.equals(locator)) {\n        InetSocketAddress changeLoc = new InetSocketAddress(locator.getHostName(),\n            locator.getSocketInetAddressNoLookup().getPort());\n        HostAddress hostAddress = new HostAddress(changeLoc, locator.getHostName());\n        newLocatorsList.add(hostAddress);\n      } else {\n        newLocatorsList.add(tloc);\n      }\n    }\n\n    logger.info(\"updateLocatorInLocatorList locator list from: {} to {}\",\n        locatorList.getLocators(), newLocatorsList);\n\n    LocatorList newLocatorList = new LocatorList(newLocatorsList);\n    locators.set(newLocatorList);\n  }\n}",
        "reject_response": "protected void updateLocatorInLocatorList(HostAddress locator) {\n  if (locator.getSocketInetAddressNoLookup().getHostName() != null && !locator.isIpString()) {\n    LocatorList locatorList = locators.get();\n    List<HostAddress> newLocatorsList = new ArrayList<>();\n\n    for (HostAddress tloc : locatorList.getLocatorAddresses()) {\n      if (tloc.equals(locator)) {\n        InetSocketAddress changeLoc = new InetSocketAddress(locator.getHostName(),\n            locator.getSocketInetAddressNoLookup().getPort());\n        HostAddress hostAddress = new HostAddress(changeLoc, locator.getHostName());\n        newLocatorsList.add(hostAddress);\n      } else {\n        newLocatorsList.add(tloc);\n      }\n    }\n\n    logger.info(\"updateLocatorInLocatorList locator list from:\" + locatorList.getLocators()\n        + \" to: \" + newLocatorsList);\n\n    LocatorList newLocatorList = new LocatorList(newLocatorsList);\n    locators.set(newLocatorList);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2572,
        "instruction": "private void handleDataIntegrityIssues(final JsonCommand command, final Throwable realCause, final Exception dve) {\n    if (realCause.getMessage().contains(AccountNumberFormatConstants.ACCOUNT_TYPE_UNIQUE_CONSTRAINT_NAME)) {\n\n        final Integer accountTypeId = command.integerValueSansLocaleOfParameterNamed(AccountNumberFormatConstants.accountTypeParamName);\n        final EntityAccountType entityAccountType = EntityAccountType.fromInt(accountTypeId);\n        throw new PlatformDataIntegrityException(AccountNumberFormatConstants.EXCEPTION_DUPLICATE_ACCOUNT_TYPE,\n                \"Account Format preferences for Account type `\" + entityAccountType.getCode() + \"` already exists\", \"externalId\",\n                entityAccountType.getValue(), entityAccountType.getCode());\n    }\n    throw new PlatformDataIntegrityException(\"error.msg.account.number.format.unknown.data.integrity.issue\",\n            \"Unknown data integrity issue with resource.\");\n}",
        "accept_response": "private void handleDataIntegrityIssues(final JsonCommand command, final Throwable realCause, final Exception dve) {\n    if (realCause.getMessage().contains(AccountNumberFormatConstants.ACCOUNT_TYPE_UNIQUE_CONSTRAINT_NAME)) {\n\n        final Integer accountTypeId = command.integerValueSansLocaleOfParameterNamed(AccountNumberFormatConstants.accountTypeParamName);\n        final EntityAccountType entityAccountType = EntityAccountType.fromInt(accountTypeId);\n        throw new PlatformDataIntegrityException(AccountNumberFormatConstants.EXCEPTION_DUPLICATE_ACCOUNT_TYPE,\n                \"Account Format preferences for Account type `\" + entityAccountType.getCode() + \"` already exists\", \"externalId\",\n                entityAccountType.getValue(), entityAccountType.getCode());\n    }\n    logger.error(\"Error occured.\", dve);\n    throw new PlatformDataIntegrityException(\"error.msg.account.number.format.unknown.data.integrity.issue\",\n            \"Unknown data integrity issue with resource.\");\n}",
        "reject_response": "private void handleDataIntegrityIssues(final JsonCommand command, final Throwable realCause, final Exception dve) {\n    if (realCause.getMessage().contains(AccountNumberFormatConstants.ACCOUNT_TYPE_UNIQUE_CONSTRAINT_NAME)) {\n\n        final Integer accountTypeId = command.integerValueSansLocaleOfParameterNamed(AccountNumberFormatConstants.accountTypeParamName);\n        final EntityAccountType entityAccountType = EntityAccountType.fromInt(accountTypeId);\n        throw new PlatformDataIntegrityException(AccountNumberFormatConstants.EXCEPTION_DUPLICATE_ACCOUNT_TYPE,\n                \"Account Format preferences for Account type `\" + entityAccountType.getCode() + \"` already exists\", \"externalId\",\n                entityAccountType.getValue(), entityAccountType.getCode());\n    }\n    logger.error(dve.getMessage(), dve);\n    throw new PlatformDataIntegrityException(\"error.msg.account.number.format.unknown.data.integrity.issue\",\n            \"Unknown data integrity issue with resource.\");\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2455,
        "instruction": "private void clearUpTo(final int lastReceivedCommandID) {\n   final int numberToClear = 1 + lastReceivedCommandID - firstStoredCommandID;\n\n   for (int i = 0; i < numberToClear; i++) {\n      final Packet packet = resendCache.poll();\n\n      if (packet == null) {\n         ActiveMQClientLogger.LOGGER.cannotFindPacketToClear(lastReceivedCommandID, firstStoredCommandID);\n         firstStoredCommandID = lastReceivedCommandID + 1;\n         return;\n      }\n\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"RemotingConnectionID=\" + connection.getID() + \" ChannelImpl::clearUpTo confirming \" + packet + \" towards \" + commandConfirmationHandler);\n      }\n      if (commandConfirmationHandler != null) {\n         commandConfirmationHandler.commandConfirmed(packet);\n      }\n   }\n\n   firstStoredCommandID += numberToClear;\n}",
        "accept_response": "private void clearUpTo(final int lastReceivedCommandID) {\n   final int numberToClear = 1 + lastReceivedCommandID - firstStoredCommandID;\n\n   if (logger.isTraceEnabled()) {\n      logger.trace(\"RemotingConnectionID=\" + (connection == null ? \"NULL\" : connection.getID()) + \" ChannelImpl::clearUpTo lastReceived commandID=\" + lastReceivedCommandID + \" first commandID=\" + firstStoredCommandID + \" number to clear \" + numberToClear);\n   }\n\n   for (int i = 0; i < numberToClear; i++) {\n      final Packet packet = resendCache.poll();\n\n      if (packet == null) {\n         ActiveMQClientLogger.LOGGER.cannotFindPacketToClear(lastReceivedCommandID, firstStoredCommandID);\n         firstStoredCommandID = lastReceivedCommandID + 1;\n         return;\n      }\n\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"RemotingConnectionID=\" + connection.getID() + \" ChannelImpl::clearUpTo confirming \" + packet + \" towards \" + commandConfirmationHandler);\n      }\n      if (commandConfirmationHandler != null) {\n         commandConfirmationHandler.commandConfirmed(packet);\n      }\n   }\n\n   firstStoredCommandID += numberToClear;\n}",
        "reject_response": "private void clearUpTo(final int lastReceivedCommandID) {\n   final int numberToClear = 1 + lastReceivedCommandID - firstStoredCommandID;\n\n   if (logger.isTraceEnabled()) {\n      logger.trace(\"ChannelImpl::clearUpTo lastReceived commandID=\" + lastReceivedCommandID +\n                      \" first commandID=\" + firstStoredCommandID +\n                      \" number to clear \" + numberToClear);\n   }\n\n   for (int i = 0; i < numberToClear; i++) {\n      final Packet packet = resendCache.poll();\n\n      if (packet == null) {\n         ActiveMQClientLogger.LOGGER.cannotFindPacketToClear(lastReceivedCommandID, firstStoredCommandID);\n         firstStoredCommandID = lastReceivedCommandID + 1;\n         return;\n      }\n\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"RemotingConnectionID=\" + connection.getID() + \" ChannelImpl::clearUpTo confirming \" + packet + \" towards \" + commandConfirmationHandler);\n      }\n      if (commandConfirmationHandler != null) {\n         commandConfirmationHandler.commandConfirmed(packet);\n      }\n   }\n\n   firstStoredCommandID += numberToClear;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3192,
        "instruction": "private void startWebApp() {\n  Configuration conf = getConfig();\n  String initializers = conf.get(\"hadoop.http.filter.initializers\", \"\");\n  Set<String> defaultInitializers = new LinkedHashSet<String>();\n  TimelineServerUtils.addTimelineAuthFilter(\n      initializers, defaultInitializers, tokenMgrService);\n  TimelineServerUtils.setTimelineFilters(\n      conf, initializers, defaultInitializers);\n\n  String bindAddress = null;\n  String host =\n      conf.getTrimmed(YarnConfiguration.TIMELINE_SERVICE_COLLECTOR_BIND_HOST);\n  Configuration.IntegerRanges portRanges = conf.getRange(\n      YarnConfiguration.TIMELINE_SERVICE_COLLECTOR_BIND_PORT_RANGES, \"\");\n  int startPort = 0;\n  if (portRanges != null && !portRanges.isEmpty()) {\n    startPort = portRanges.getRangeStart();\n  }\n  if (host == null || host.isEmpty()) {\n    // if collector bind-host is not set, fall back to\n    // timeline-service.bind-host to maintain compatibility\n    bindAddress =\n        conf.get(YarnConfiguration.DEFAULT_TIMELINE_SERVICE_BIND_HOST,\n            YarnConfiguration.DEFAULT_TIMELINE_SERVICE_BIND_HOST)\n            + \":\" + startPort;\n  } else {\n    bindAddress = host + \":\" + startPort;\n  }\n\n  try {\n    HttpServer2.Builder builder = new HttpServer2.Builder()\n        .setName(\"timeline\")\n        .setConf(conf)\n        .addEndpoint(URI.create(\n            (YarnConfiguration.useHttps(conf) ? \"https://\" : \"http://\") +\n                bindAddress));\n    if (portRanges != null && !portRanges.isEmpty()) {\n      builder.setPortRanges(portRanges);\n    }\n    if (YarnConfiguration.useHttps(conf)) {\n      builder = WebAppUtils.loadSslConfiguration(builder, conf);\n    }\n    timelineRestServer = builder.build();\n\n    timelineRestServer.addJerseyResourcePackage(\n        TimelineCollectorWebService.class.getPackage().getName() + \";\"\n            + GenericExceptionHandler.class.getPackage().getName() + \";\"\n            + YarnJacksonJaxbJsonProvider.class.getPackage().getName(),\n        \"/*\");\n    timelineRestServer.setAttribute(COLLECTOR_MANAGER_ATTR_KEY, this);\n    timelineRestServer.start();\n  } catch (Exception e) {\n    String msg = \"The per-node collector webapp failed to start.\";\n    LOG.error(msg, e);\n    throw new YarnRuntimeException(msg, e);\n  }\n  //TODO: We need to think of the case of multiple interfaces\n  this.timelineRestServerBindAddress = WebAppUtils.getResolvedAddress(\n      timelineRestServer.getConnectorAddress(0));\n}",
        "accept_response": "private void startWebApp() {\n  Configuration conf = getConfig();\n  String initializers = conf.get(\"hadoop.http.filter.initializers\", \"\");\n  Set<String> defaultInitializers = new LinkedHashSet<String>();\n  TimelineServerUtils.addTimelineAuthFilter(\n      initializers, defaultInitializers, tokenMgrService);\n  TimelineServerUtils.setTimelineFilters(\n      conf, initializers, defaultInitializers);\n\n  String bindAddress = null;\n  String host =\n      conf.getTrimmed(YarnConfiguration.TIMELINE_SERVICE_COLLECTOR_BIND_HOST);\n  Configuration.IntegerRanges portRanges = conf.getRange(\n      YarnConfiguration.TIMELINE_SERVICE_COLLECTOR_BIND_PORT_RANGES, \"\");\n  int startPort = 0;\n  if (portRanges != null && !portRanges.isEmpty()) {\n    startPort = portRanges.getRangeStart();\n  }\n  if (host == null || host.isEmpty()) {\n    // if collector bind-host is not set, fall back to\n    // timeline-service.bind-host to maintain compatibility\n    bindAddress =\n        conf.get(YarnConfiguration.DEFAULT_TIMELINE_SERVICE_BIND_HOST,\n            YarnConfiguration.DEFAULT_TIMELINE_SERVICE_BIND_HOST)\n            + \":\" + startPort;\n  } else {\n    bindAddress = host + \":\" + startPort;\n  }\n\n  try {\n    HttpServer2.Builder builder = new HttpServer2.Builder()\n        .setName(\"timeline\")\n        .setConf(conf)\n        .addEndpoint(URI.create(\n            (YarnConfiguration.useHttps(conf) ? \"https://\" : \"http://\") +\n                bindAddress));\n    if (portRanges != null && !portRanges.isEmpty()) {\n      builder.setPortRanges(portRanges);\n    }\n    if (YarnConfiguration.useHttps(conf)) {\n      builder = WebAppUtils.loadSslConfiguration(builder, conf);\n    }\n    timelineRestServer = builder.build();\n\n    timelineRestServer.addJerseyResourcePackage(\n        TimelineCollectorWebService.class.getPackage().getName() + \";\"\n            + GenericExceptionHandler.class.getPackage().getName() + \";\"\n            + YarnJacksonJaxbJsonProvider.class.getPackage().getName(),\n        \"/*\");\n    timelineRestServer.setAttribute(COLLECTOR_MANAGER_ATTR_KEY, this);\n    timelineRestServer.start();\n  } catch (Exception e) {\n    String msg = \"The per-node collector webapp failed to start.\";\n    LOG.error(msg, e);\n    throw new YarnRuntimeException(msg, e);\n  }\n  //TODO: We need to think of the case of multiple interfaces\n  this.timelineRestServerBindAddress = WebAppUtils.getResolvedAddress(\n      timelineRestServer.getConnectorAddress(0));\n  LOG.info(\"Instantiated the per-node collector webapp at {}\",\n      timelineRestServerBindAddress);\n}",
        "reject_response": "private void startWebApp() {\n  Configuration conf = getConfig();\n  String initializers = conf.get(\"hadoop.http.filter.initializers\", \"\");\n  Set<String> defaultInitializers = new LinkedHashSet<String>();\n  TimelineServerUtils.addTimelineAuthFilter(\n      initializers, defaultInitializers, tokenMgrService);\n  TimelineServerUtils.setTimelineFilters(\n      conf, initializers, defaultInitializers);\n\n  String bindAddress = null;\n  String host =\n      conf.getTrimmed(YarnConfiguration.TIMELINE_SERVICE_COLLECTOR_BIND_HOST);\n  Configuration.IntegerRanges portRanges = conf.getRange(\n      YarnConfiguration.TIMELINE_SERVICE_COLLECTOR_BIND_PORT_RANGES, \"\");\n  int startPort = 0;\n  if (portRanges != null && !portRanges.isEmpty()) {\n    startPort = portRanges.getRangeStart();\n  }\n  if (host == null || host.isEmpty()) {\n    // if collector bind-host is not set, fall back to\n    // timeline-service.bind-host to maintain compatibility\n    bindAddress =\n        conf.get(YarnConfiguration.DEFAULT_TIMELINE_SERVICE_BIND_HOST,\n            YarnConfiguration.DEFAULT_TIMELINE_SERVICE_BIND_HOST)\n            + \":\" + startPort;\n  } else {\n    bindAddress = host + \":\" + startPort;\n  }\n\n  try {\n    HttpServer2.Builder builder = new HttpServer2.Builder()\n        .setName(\"timeline\")\n        .setConf(conf)\n        .addEndpoint(URI.create(\n            (YarnConfiguration.useHttps(conf) ? \"https://\" : \"http://\") +\n                bindAddress));\n    if (portRanges != null && !portRanges.isEmpty()) {\n      builder.setPortRanges(portRanges);\n    }\n    if (YarnConfiguration.useHttps(conf)) {\n      builder = WebAppUtils.loadSslConfiguration(builder, conf);\n    }\n    timelineRestServer = builder.build();\n\n    timelineRestServer.addJerseyResourcePackage(\n        TimelineCollectorWebService.class.getPackage().getName() + \";\"\n            + GenericExceptionHandler.class.getPackage().getName() + \";\"\n            + YarnJacksonJaxbJsonProvider.class.getPackage().getName(),\n        \"/*\");\n    timelineRestServer.setAttribute(COLLECTOR_MANAGER_ATTR_KEY, this);\n    timelineRestServer.start();\n  } catch (Exception e) {\n    String msg = \"The per-node collector webapp failed to start.\";\n    LOG.error(msg, e);\n    throw new YarnRuntimeException(msg, e);\n  }\n  //TODO: We need to think of the case of multiple interfaces\n  this.timelineRestServerBindAddress = WebAppUtils.getResolvedAddress(\n      timelineRestServer.getConnectorAddress(0));\n  LOG.info(\"Instantiated the per-node collector webapp at \" +\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2730,
        "instruction": "void load(File file) throws IOException {\n  long start = Time.monotonicNow();\n  imgDigest = MD5FileUtils.computeMd5ForFile(file);\n  RandomAccessFile raFile = new RandomAccessFile(file, \"r\");\n  FileInputStream fin = new FileInputStream(file);\n  try {\n    loadInternal(raFile, fin);\n    long end = Time.monotonicNow();\n  } finally {\n    fin.close();\n    raFile.close();\n  }\n}",
        "accept_response": "void load(File file) throws IOException {\n  long start = Time.monotonicNow();\n  imgDigest = MD5FileUtils.computeMd5ForFile(file);\n  RandomAccessFile raFile = new RandomAccessFile(file, \"r\");\n  FileInputStream fin = new FileInputStream(file);\n  try {\n    loadInternal(raFile, fin);\n    long end = Time.monotonicNow();\n    LOG.info(\"Loaded FSImage in {} seconds.\", (end - start) / 1000);\n  } finally {\n    fin.close();\n    raFile.close();\n  }\n}",
        "reject_response": "void load(File file) throws IOException {\n  long start = Time.monotonicNow();\n  imgDigest = MD5FileUtils.computeMd5ForFile(file);\n  RandomAccessFile raFile = new RandomAccessFile(file, \"r\");\n  FileInputStream fin = new FileInputStream(file);\n  try {\n    loadInternal(raFile, fin);\n    long end = Time.monotonicNow();\n    LOG.info(\"Loaded FSImage in \" + (end - start) / 1000 + \" seconds.\");\n  } finally {\n    fin.close();\n    raFile.close();\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3173,
        "instruction": "@Override\nprotected void serviceInit(Configuration conf) throws Exception {\n  this.maxThreadPoolSize = conf.getInt(\n      YarnConfiguration.NM_CLIENT_ASYNC_THREAD_POOL_MAX_SIZE,\n      YarnConfiguration.DEFAULT_NM_CLIENT_ASYNC_THREAD_POOL_MAX_SIZE);\n\n  client.init(conf);\n  super.serviceInit(conf);\n}",
        "accept_response": "@Override\nprotected void serviceInit(Configuration conf) throws Exception {\n  this.maxThreadPoolSize = conf.getInt(\n      YarnConfiguration.NM_CLIENT_ASYNC_THREAD_POOL_MAX_SIZE,\n      YarnConfiguration.DEFAULT_NM_CLIENT_ASYNC_THREAD_POOL_MAX_SIZE);\n  LOG.info(\"Upper bound of the thread pool size is {}.\", maxThreadPoolSize);\n\n  client.init(conf);\n  super.serviceInit(conf);\n}",
        "reject_response": "@Override\nprotected void serviceInit(Configuration conf) throws Exception {\n  this.maxThreadPoolSize = conf.getInt(\n      YarnConfiguration.NM_CLIENT_ASYNC_THREAD_POOL_MAX_SIZE,\n      YarnConfiguration.DEFAULT_NM_CLIENT_ASYNC_THREAD_POOL_MAX_SIZE);\n  LOG.info(\"Upper bound of the thread pool size is \" + maxThreadPoolSize);\n\n  client.init(conf);\n  super.serviceInit(conf);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2820,
        "instruction": "@SuppressWarnings(\"unchecked\")\nprotected void runQuery(GridCacheQueryInfo qryInfo) {\n    assert qryInfo != null;\n    assert qryInfo.query().type() != SCAN || !qryInfo.local() : qryInfo;\n\n    if (!enterBusy()) {\n        if (cctx.localNodeId().equals(qryInfo.senderId()))\n            throw new IllegalStateException(\"Failed to process query request (grid is stopping).\");\n\n        return; // Ignore remote requests when when node is stopping.\n    }\n\n    try {\n        boolean loc = qryInfo.local();\n\n        QueryResult<K, V> res = null;\n\n        if (log.isDebugEnabled())\n            log.debug(\"Running query: \" + qryInfo);\n\n        boolean rmvIter = true;\n\n        try {\n            // Preparing query closures.\n            IgniteClosure<Cache.Entry<K, V>, Object> trans =\n                (IgniteClosure<Cache.Entry<K, V>, Object>)qryInfo.transformer();\n\n            IgniteReducer<Cache.Entry<K, V>, Object> rdc = (IgniteReducer<Cache.Entry<K, V>, Object>)qryInfo.reducer();\n\n            injectResources(trans);\n            injectResources(rdc);\n\n            GridCacheQueryAdapter<?> qry = qryInfo.query();\n\n            int pageSize = qry.pageSize();\n\n            boolean incBackups = qry.includeBackups();\n\n            String taskName = cctx.kernalContext().task().resolveTaskName(qry.taskHash());\n\n            IgniteSpiCloseableIterator<IgniteBiTuple<K, V>> iter;\n            GridCacheQueryType type;\n\n            res = loc ?\n                executeQuery(qry, qryInfo.arguments(), loc, qry.subjectId(), taskName,\n                    recipient(qryInfo.senderId(), qryInfo.requestId())) :\n                queryResult(qryInfo, taskName);\n\n            if (res == null)\n                return;\n\n            iter = res.iterator(recipient(qryInfo.senderId(), qryInfo.requestId()));\n            type = res.type();\n\n            final GridCacheAdapter<K, V> cache = cctx.cache();\n\n            if (log.isDebugEnabled())\n                log.debug(\"Received index iterator [iterHasNext=\" + iter.hasNext() +\n                    \", cacheSize=\" + cache.size() + ']');\n\n            int cnt = 0;\n\n            boolean stop = false;\n            boolean pageSent = false;\n\n            Collection<Object> data = new ArrayList<>(pageSize);\n\n            AffinityTopologyVersion topVer = cctx.affinity().affinityTopologyVersion();\n\n            final boolean statsEnabled = cctx.config().isStatisticsEnabled();\n\n            final boolean readEvt = cctx.gridEvents().isRecordable(EVT_CACHE_QUERY_OBJECT_READ);\n\n            while (!Thread.currentThread().isInterrupted() && iter.hasNext()) {\n                long start = statsEnabled ? System.nanoTime() : 0L;\n\n                IgniteBiTuple<K, V> row = iter.next();\n\n                // Query is cancelled.\n                if (row == null) {\n                    onPageReady(loc, qryInfo, null, true, null);\n\n                    break;\n                }\n\n                final K key = row.getKey();\n\n                // Filter backups for SCAN queries, if it isn't partition scan.\n                // Other types are filtered in indexing manager.\n                if (!cctx.isReplicated() && qry.type() == SCAN && qry.partition() == null &&\n                    cctx.config().getCacheMode() != LOCAL && !incBackups &&\n                    !cctx.affinity().primary(cctx.localNode(), key, topVer)) {\n                    if (log.isDebugEnabled())\n                        log.debug(\"Ignoring backup element [row=\" + row +\n                            \", cacheMode=\" + cctx.config().getCacheMode() + \", incBackups=\" + incBackups +\n                            \", primary=\" + cctx.affinity().primary(cctx.localNode(), key, topVer) + ']');\n\n                    continue;\n                }\n\n                V val = row.getValue();\n\n                if (log.isDebugEnabled()) {\n                    ClusterNode primaryNode = CU.primaryNode(cctx, key);\n\n                    log.debug(S.toString(\"Record\",\n                        \"key\", key, true,\n                        \"val\", val, true,\n                        \"incBackups\", incBackups, false,\n                        \"priNode\", primaryNode != null ? U.id8(primaryNode.id()) : null, false,\n                        \"node\", U.id8(cctx.localNode().id()), false));\n                }\n\n                if (val == null) {\n                    if (log.isDebugEnabled())\n\n                    continue;\n                }\n\n                if (statsEnabled) {\n                    CacheMetricsImpl metrics = cctx.cache().metrics0();\n\n                    metrics.onRead(true);\n\n                    metrics.addGetTimeNanos(System.nanoTime() - start);\n                }\n\n                K key0 = null;\n                V val0 = null;\n\n                if (readEvt) {\n                    key0 = (K)cctx.unwrapBinaryIfNeeded(key, qry.keepBinary());\n                    val0 = (V)cctx.unwrapBinaryIfNeeded(val, qry.keepBinary());\n\n                    switch (type) {\n                        case SQL:\n                            cctx.gridEvents().record(new CacheQueryReadEvent<>(\n                                cctx.localNode(),\n                                \"SQL query entry read.\",\n                                EVT_CACHE_QUERY_OBJECT_READ,\n                                CacheQueryType.SQL.name(),\n                                cctx.namex(),\n                                qry.queryClassName(),\n                                qry.clause(),\n                                null,\n                                null,\n                                qryInfo.arguments(),\n                                qry.subjectId(),\n                                taskName,\n                                key0,\n                                val0,\n                                null,\n                                null));\n\n                            break;\n\n                        case TEXT:\n                            cctx.gridEvents().record(new CacheQueryReadEvent<>(\n                                cctx.localNode(),\n                                \"Full text query entry read.\",\n                                EVT_CACHE_QUERY_OBJECT_READ,\n                                CacheQueryType.FULL_TEXT.name(),\n                                cctx.namex(),\n                                qry.queryClassName(),\n                                qry.clause(),\n                                null,\n                                null,\n                                null,\n                                qry.subjectId(),\n                                taskName,\n                                key0,\n                                val0,\n                                null,\n                                null));\n\n                            break;\n\n                        case SCAN:\n                            cctx.gridEvents().record(new CacheQueryReadEvent<>(\n                                cctx.localNode(),\n                                \"Scan query entry read.\",\n                                EVT_CACHE_QUERY_OBJECT_READ,\n                                CacheQueryType.SCAN.name(),\n                                cctx.namex(),\n                                null,\n                                null,\n                                qry.scanFilter(),\n                                null,\n                                null,\n                                qry.subjectId(),\n                                taskName,\n                                key0,\n                                val0,\n                                null,\n                                null));\n\n                            break;\n                    }\n                }\n\n                if (rdc != null || trans != null) {\n                    if (key0 == null)\n                        key0 = (K)cctx.unwrapBinaryIfNeeded(key, qry.keepBinary());\n                    if (val0 == null)\n                        val0 = (V)cctx.unwrapBinaryIfNeeded(val, qry.keepBinary());\n\n                    Cache.Entry<K, V> entry = new CacheEntryImpl(key0, val0);\n\n                    // Reduce.\n                    if (rdc != null) {\n                        if (!rdc.collect(entry) || !iter.hasNext()) {\n                            onPageReady(loc, qryInfo, Collections.singletonList(rdc.reduce()), true, null);\n\n                            pageSent = true;\n\n                            break;\n                        }\n                        else\n                            continue;\n                    }\n\n                    data.add(trans != null ? trans.apply(entry) :\n                        !loc ? new GridCacheQueryResponseEntry<>(key, val) : F.t(key, val));\n                }\n                else\n                    data.add(!loc ? new GridCacheQueryResponseEntry<>(key, val) : F.t(key, val));\n\n                if (!loc) {\n                    if (++cnt == pageSize || !iter.hasNext()) {\n                        boolean finished = !iter.hasNext();\n\n                        onPageReady(loc, qryInfo, data, finished, null);\n\n                        pageSent = true;\n\n                        if (!finished)\n                            rmvIter = false;\n\n                        if (!qryInfo.allPages())\n                            return;\n\n                        data = new ArrayList<>(pageSize);\n\n                        if (stop)\n                            break; // while\n                    }\n                }\n            }\n\n            if (!pageSent) {\n                if (rdc == null)\n                    onPageReady(loc, qryInfo, data, true, null);\n                else\n                    onPageReady(loc, qryInfo, Collections.singletonList(rdc.reduce()), true, null);\n            }\n        }\n        catch (Throwable e) {\n            if (!X.hasCause(e, GridDhtUnreservedPartitionException.class))\n                U.error(log, \"Failed to run query [qry=\" + qryInfo + \", node=\" + cctx.nodeId() + \"]\", e);\n\n            onPageReady(loc, qryInfo, null, true, e);\n\n            if (e instanceof Error)\n                throw (Error)e;\n        }\n        finally {\n            if (loc) {\n                // Local iterators are always removed.\n                if (res != null) {\n                    try {\n                        res.closeIfNotShared(recipient(qryInfo.senderId(), qryInfo.requestId()));\n                    }\n                    catch (IgniteCheckedException e) {\n                        if (!X.hasCause(e, GridDhtUnreservedPartitionException.class))\n                            U.error(log, \"Failed to close local iterator [qry=\" + qryInfo + \", node=\" +\n                                cctx.nodeId() + \"]\", e);\n                    }\n                }\n            }\n            else if (rmvIter)\n                removeQueryResult(qryInfo.senderId(), qryInfo.requestId());\n        }\n    }\n    finally {\n        leaveBusy();\n    }\n}",
        "accept_response": "@SuppressWarnings(\"unchecked\")\nprotected void runQuery(GridCacheQueryInfo qryInfo) {\n    assert qryInfo != null;\n    assert qryInfo.query().type() != SCAN || !qryInfo.local() : qryInfo;\n\n    if (!enterBusy()) {\n        if (cctx.localNodeId().equals(qryInfo.senderId()))\n            throw new IllegalStateException(\"Failed to process query request (grid is stopping).\");\n\n        return; // Ignore remote requests when when node is stopping.\n    }\n\n    try {\n        boolean loc = qryInfo.local();\n\n        QueryResult<K, V> res = null;\n\n        if (log.isDebugEnabled())\n            log.debug(\"Running query: \" + qryInfo);\n\n        boolean rmvIter = true;\n\n        try {\n            // Preparing query closures.\n            IgniteClosure<Cache.Entry<K, V>, Object> trans =\n                (IgniteClosure<Cache.Entry<K, V>, Object>)qryInfo.transformer();\n\n            IgniteReducer<Cache.Entry<K, V>, Object> rdc = (IgniteReducer<Cache.Entry<K, V>, Object>)qryInfo.reducer();\n\n            injectResources(trans);\n            injectResources(rdc);\n\n            GridCacheQueryAdapter<?> qry = qryInfo.query();\n\n            int pageSize = qry.pageSize();\n\n            boolean incBackups = qry.includeBackups();\n\n            String taskName = cctx.kernalContext().task().resolveTaskName(qry.taskHash());\n\n            IgniteSpiCloseableIterator<IgniteBiTuple<K, V>> iter;\n            GridCacheQueryType type;\n\n            res = loc ?\n                executeQuery(qry, qryInfo.arguments(), loc, qry.subjectId(), taskName,\n                    recipient(qryInfo.senderId(), qryInfo.requestId())) :\n                queryResult(qryInfo, taskName);\n\n            if (res == null)\n                return;\n\n            iter = res.iterator(recipient(qryInfo.senderId(), qryInfo.requestId()));\n            type = res.type();\n\n            final GridCacheAdapter<K, V> cache = cctx.cache();\n\n            if (log.isDebugEnabled())\n                log.debug(\"Received index iterator [iterHasNext=\" + iter.hasNext() +\n                    \", cacheSize=\" + cache.size() + ']');\n\n            int cnt = 0;\n\n            boolean stop = false;\n            boolean pageSent = false;\n\n            Collection<Object> data = new ArrayList<>(pageSize);\n\n            AffinityTopologyVersion topVer = cctx.affinity().affinityTopologyVersion();\n\n            final boolean statsEnabled = cctx.config().isStatisticsEnabled();\n\n            final boolean readEvt = cctx.gridEvents().isRecordable(EVT_CACHE_QUERY_OBJECT_READ);\n\n            while (!Thread.currentThread().isInterrupted() && iter.hasNext()) {\n                long start = statsEnabled ? System.nanoTime() : 0L;\n\n                IgniteBiTuple<K, V> row = iter.next();\n\n                // Query is cancelled.\n                if (row == null) {\n                    onPageReady(loc, qryInfo, null, true, null);\n\n                    break;\n                }\n\n                final K key = row.getKey();\n\n                // Filter backups for SCAN queries, if it isn't partition scan.\n                // Other types are filtered in indexing manager.\n                if (!cctx.isReplicated() && qry.type() == SCAN && qry.partition() == null &&\n                    cctx.config().getCacheMode() != LOCAL && !incBackups &&\n                    !cctx.affinity().primary(cctx.localNode(), key, topVer)) {\n                    if (log.isDebugEnabled())\n                        log.debug(\"Ignoring backup element [row=\" + row +\n                            \", cacheMode=\" + cctx.config().getCacheMode() + \", incBackups=\" + incBackups +\n                            \", primary=\" + cctx.affinity().primary(cctx.localNode(), key, topVer) + ']');\n\n                    continue;\n                }\n\n                V val = row.getValue();\n\n                if (log.isDebugEnabled()) {\n                    ClusterNode primaryNode = CU.primaryNode(cctx, key);\n\n                    log.debug(S.toString(\"Record\",\n                        \"key\", key, true,\n                        \"val\", val, true,\n                        \"incBackups\", incBackups, false,\n                        \"priNode\", primaryNode != null ? U.id8(primaryNode.id()) : null, false,\n                        \"node\", U.id8(cctx.localNode().id()), false));\n                }\n\n                if (val == null) {\n                    if (log.isDebugEnabled())\n                        log.debug(S.toString(\"Unsuitable record value\", \"val\", val, true));\n\n                    continue;\n                }\n\n                if (statsEnabled) {\n                    CacheMetricsImpl metrics = cctx.cache().metrics0();\n\n                    metrics.onRead(true);\n\n                    metrics.addGetTimeNanos(System.nanoTime() - start);\n                }\n\n                K key0 = null;\n                V val0 = null;\n\n                if (readEvt) {\n                    key0 = (K)cctx.unwrapBinaryIfNeeded(key, qry.keepBinary());\n                    val0 = (V)cctx.unwrapBinaryIfNeeded(val, qry.keepBinary());\n\n                    switch (type) {\n                        case SQL:\n                            cctx.gridEvents().record(new CacheQueryReadEvent<>(\n                                cctx.localNode(),\n                                \"SQL query entry read.\",\n                                EVT_CACHE_QUERY_OBJECT_READ,\n                                CacheQueryType.SQL.name(),\n                                cctx.namex(),\n                                qry.queryClassName(),\n                                qry.clause(),\n                                null,\n                                null,\n                                qryInfo.arguments(),\n                                qry.subjectId(),\n                                taskName,\n                                key0,\n                                val0,\n                                null,\n                                null));\n\n                            break;\n\n                        case TEXT:\n                            cctx.gridEvents().record(new CacheQueryReadEvent<>(\n                                cctx.localNode(),\n                                \"Full text query entry read.\",\n                                EVT_CACHE_QUERY_OBJECT_READ,\n                                CacheQueryType.FULL_TEXT.name(),\n                                cctx.namex(),\n                                qry.queryClassName(),\n                                qry.clause(),\n                                null,\n                                null,\n                                null,\n                                qry.subjectId(),\n                                taskName,\n                                key0,\n                                val0,\n                                null,\n                                null));\n\n                            break;\n\n                        case SCAN:\n                            cctx.gridEvents().record(new CacheQueryReadEvent<>(\n                                cctx.localNode(),\n                                \"Scan query entry read.\",\n                                EVT_CACHE_QUERY_OBJECT_READ,\n                                CacheQueryType.SCAN.name(),\n                                cctx.namex(),\n                                null,\n                                null,\n                                qry.scanFilter(),\n                                null,\n                                null,\n                                qry.subjectId(),\n                                taskName,\n                                key0,\n                                val0,\n                                null,\n                                null));\n\n                            break;\n                    }\n                }\n\n                if (rdc != null || trans != null) {\n                    if (key0 == null)\n                        key0 = (K)cctx.unwrapBinaryIfNeeded(key, qry.keepBinary());\n                    if (val0 == null)\n                        val0 = (V)cctx.unwrapBinaryIfNeeded(val, qry.keepBinary());\n\n                    Cache.Entry<K, V> entry = new CacheEntryImpl(key0, val0);\n\n                    // Reduce.\n                    if (rdc != null) {\n                        if (!rdc.collect(entry) || !iter.hasNext()) {\n                            onPageReady(loc, qryInfo, Collections.singletonList(rdc.reduce()), true, null);\n\n                            pageSent = true;\n\n                            break;\n                        }\n                        else\n                            continue;\n                    }\n\n                    data.add(trans != null ? trans.apply(entry) :\n                        !loc ? new GridCacheQueryResponseEntry<>(key, val) : F.t(key, val));\n                }\n                else\n                    data.add(!loc ? new GridCacheQueryResponseEntry<>(key, val) : F.t(key, val));\n\n                if (!loc) {\n                    if (++cnt == pageSize || !iter.hasNext()) {\n                        boolean finished = !iter.hasNext();\n\n                        onPageReady(loc, qryInfo, data, finished, null);\n\n                        pageSent = true;\n\n                        if (!finished)\n                            rmvIter = false;\n\n                        if (!qryInfo.allPages())\n                            return;\n\n                        data = new ArrayList<>(pageSize);\n\n                        if (stop)\n                            break; // while\n                    }\n                }\n            }\n\n            if (!pageSent) {\n                if (rdc == null)\n                    onPageReady(loc, qryInfo, data, true, null);\n                else\n                    onPageReady(loc, qryInfo, Collections.singletonList(rdc.reduce()), true, null);\n            }\n        }\n        catch (Throwable e) {\n            if (!X.hasCause(e, GridDhtUnreservedPartitionException.class))\n                U.error(log, \"Failed to run query [qry=\" + qryInfo + \", node=\" + cctx.nodeId() + \"]\", e);\n\n            onPageReady(loc, qryInfo, null, true, e);\n\n            if (e instanceof Error)\n                throw (Error)e;\n        }\n        finally {\n            if (loc) {\n                // Local iterators are always removed.\n                if (res != null) {\n                    try {\n                        res.closeIfNotShared(recipient(qryInfo.senderId(), qryInfo.requestId()));\n                    }\n                    catch (IgniteCheckedException e) {\n                        if (!X.hasCause(e, GridDhtUnreservedPartitionException.class))\n                            U.error(log, \"Failed to close local iterator [qry=\" + qryInfo + \", node=\" +\n                                cctx.nodeId() + \"]\", e);\n                    }\n                }\n            }\n            else if (rmvIter)\n                removeQueryResult(qryInfo.senderId(), qryInfo.requestId());\n        }\n    }\n    finally {\n        leaveBusy();\n    }\n}",
        "reject_response": "@SuppressWarnings(\"unchecked\")\nprotected void runQuery(GridCacheQueryInfo qryInfo) {\n    assert qryInfo != null;\n    assert qryInfo.query().type() != SCAN || !qryInfo.local() : qryInfo;\n\n    if (!enterBusy()) {\n        if (cctx.localNodeId().equals(qryInfo.senderId()))\n            throw new IllegalStateException(\"Failed to process query request (grid is stopping).\");\n\n        return; // Ignore remote requests when when node is stopping.\n    }\n\n    try {\n        boolean loc = qryInfo.local();\n\n        QueryResult<K, V> res = null;\n\n        if (log.isDebugEnabled())\n            log.debug(\"Running query: \" + qryInfo);\n\n        boolean rmvIter = true;\n\n        try {\n            // Preparing query closures.\n            IgniteClosure<Cache.Entry<K, V>, Object> trans =\n                (IgniteClosure<Cache.Entry<K, V>, Object>)qryInfo.transformer();\n\n            IgniteReducer<Cache.Entry<K, V>, Object> rdc = (IgniteReducer<Cache.Entry<K, V>, Object>)qryInfo.reducer();\n\n            injectResources(trans);\n            injectResources(rdc);\n\n            GridCacheQueryAdapter<?> qry = qryInfo.query();\n\n            int pageSize = qry.pageSize();\n\n            boolean incBackups = qry.includeBackups();\n\n            String taskName = cctx.kernalContext().task().resolveTaskName(qry.taskHash());\n\n            IgniteSpiCloseableIterator<IgniteBiTuple<K, V>> iter;\n            GridCacheQueryType type;\n\n            res = loc ?\n                executeQuery(qry, qryInfo.arguments(), loc, qry.subjectId(), taskName,\n                    recipient(qryInfo.senderId(), qryInfo.requestId())) :\n                queryResult(qryInfo, taskName);\n\n            if (res == null)\n                return;\n\n            iter = res.iterator(recipient(qryInfo.senderId(), qryInfo.requestId()));\n            type = res.type();\n\n            final GridCacheAdapter<K, V> cache = cctx.cache();\n\n            if (log.isDebugEnabled())\n                log.debug(\"Received index iterator [iterHasNext=\" + iter.hasNext() +\n                    \", cacheSize=\" + cache.size() + ']');\n\n            int cnt = 0;\n\n            boolean stop = false;\n            boolean pageSent = false;\n\n            Collection<Object> data = new ArrayList<>(pageSize);\n\n            AffinityTopologyVersion topVer = cctx.affinity().affinityTopologyVersion();\n\n            final boolean statsEnabled = cctx.config().isStatisticsEnabled();\n\n            final boolean readEvt = cctx.gridEvents().isRecordable(EVT_CACHE_QUERY_OBJECT_READ);\n\n            while (!Thread.currentThread().isInterrupted() && iter.hasNext()) {\n                long start = statsEnabled ? System.nanoTime() : 0L;\n\n                IgniteBiTuple<K, V> row = iter.next();\n\n                // Query is cancelled.\n                if (row == null) {\n                    onPageReady(loc, qryInfo, null, true, null);\n\n                    break;\n                }\n\n                final K key = row.getKey();\n\n                // Filter backups for SCAN queries, if it isn't partition scan.\n                // Other types are filtered in indexing manager.\n                if (!cctx.isReplicated() && qry.type() == SCAN && qry.partition() == null &&\n                    cctx.config().getCacheMode() != LOCAL && !incBackups &&\n                    !cctx.affinity().primary(cctx.localNode(), key, topVer)) {\n                    if (log.isDebugEnabled())\n                        log.debug(\"Ignoring backup element [row=\" + row +\n                            \", cacheMode=\" + cctx.config().getCacheMode() + \", incBackups=\" + incBackups +\n                            \", primary=\" + cctx.affinity().primary(cctx.localNode(), key, topVer) + ']');\n\n                    continue;\n                }\n\n                V val = row.getValue();\n\n                if (log.isDebugEnabled()) {\n                    ClusterNode primaryNode = CU.primaryNode(cctx, key);\n\n                    log.debug(S.toString(\"Record\",\n                        \"key\", key, true,\n                        \"val\", val, true,\n                        \"incBackups\", incBackups, false,\n                        \"priNode\", primaryNode != null ? U.id8(primaryNode.id()) : null, false,\n                        \"node\", U.id8(cctx.localNode().id()), false));\n                }\n\n                if (val == null) {\n                    if (log.isDebugEnabled())\n                        log.debug(\"Unsuitable record value: \" + val);\n\n                    continue;\n                }\n\n                if (statsEnabled) {\n                    CacheMetricsImpl metrics = cctx.cache().metrics0();\n\n                    metrics.onRead(true);\n\n                    metrics.addGetTimeNanos(System.nanoTime() - start);\n                }\n\n                K key0 = null;\n                V val0 = null;\n\n                if (readEvt) {\n                    key0 = (K)cctx.unwrapBinaryIfNeeded(key, qry.keepBinary());\n                    val0 = (V)cctx.unwrapBinaryIfNeeded(val, qry.keepBinary());\n\n                    switch (type) {\n                        case SQL:\n                            cctx.gridEvents().record(new CacheQueryReadEvent<>(\n                                cctx.localNode(),\n                                \"SQL query entry read.\",\n                                EVT_CACHE_QUERY_OBJECT_READ,\n                                CacheQueryType.SQL.name(),\n                                cctx.namex(),\n                                qry.queryClassName(),\n                                qry.clause(),\n                                null,\n                                null,\n                                qryInfo.arguments(),\n                                qry.subjectId(),\n                                taskName,\n                                key0,\n                                val0,\n                                null,\n                                null));\n\n                            break;\n\n                        case TEXT:\n                            cctx.gridEvents().record(new CacheQueryReadEvent<>(\n                                cctx.localNode(),\n                                \"Full text query entry read.\",\n                                EVT_CACHE_QUERY_OBJECT_READ,\n                                CacheQueryType.FULL_TEXT.name(),\n                                cctx.namex(),\n                                qry.queryClassName(),\n                                qry.clause(),\n                                null,\n                                null,\n                                null,\n                                qry.subjectId(),\n                                taskName,\n                                key0,\n                                val0,\n                                null,\n                                null));\n\n                            break;\n\n                        case SCAN:\n                            cctx.gridEvents().record(new CacheQueryReadEvent<>(\n                                cctx.localNode(),\n                                \"Scan query entry read.\",\n                                EVT_CACHE_QUERY_OBJECT_READ,\n                                CacheQueryType.SCAN.name(),\n                                cctx.namex(),\n                                null,\n                                null,\n                                qry.scanFilter(),\n                                null,\n                                null,\n                                qry.subjectId(),\n                                taskName,\n                                key0,\n                                val0,\n                                null,\n                                null));\n\n                            break;\n                    }\n                }\n\n                if (rdc != null || trans != null) {\n                    if (key0 == null)\n                        key0 = (K)cctx.unwrapBinaryIfNeeded(key, qry.keepBinary());\n                    if (val0 == null)\n                        val0 = (V)cctx.unwrapBinaryIfNeeded(val, qry.keepBinary());\n\n                    Cache.Entry<K, V> entry = new CacheEntryImpl(key0, val0);\n\n                    // Reduce.\n                    if (rdc != null) {\n                        if (!rdc.collect(entry) || !iter.hasNext()) {\n                            onPageReady(loc, qryInfo, Collections.singletonList(rdc.reduce()), true, null);\n\n                            pageSent = true;\n\n                            break;\n                        }\n                        else\n                            continue;\n                    }\n\n                    data.add(trans != null ? trans.apply(entry) :\n                        !loc ? new GridCacheQueryResponseEntry<>(key, val) : F.t(key, val));\n                }\n                else\n                    data.add(!loc ? new GridCacheQueryResponseEntry<>(key, val) : F.t(key, val));\n\n                if (!loc) {\n                    if (++cnt == pageSize || !iter.hasNext()) {\n                        boolean finished = !iter.hasNext();\n\n                        onPageReady(loc, qryInfo, data, finished, null);\n\n                        pageSent = true;\n\n                        if (!finished)\n                            rmvIter = false;\n\n                        if (!qryInfo.allPages())\n                            return;\n\n                        data = new ArrayList<>(pageSize);\n\n                        if (stop)\n                            break; // while\n                    }\n                }\n            }\n\n            if (!pageSent) {\n                if (rdc == null)\n                    onPageReady(loc, qryInfo, data, true, null);\n                else\n                    onPageReady(loc, qryInfo, Collections.singletonList(rdc.reduce()), true, null);\n            }\n        }\n        catch (Throwable e) {\n            if (!X.hasCause(e, GridDhtUnreservedPartitionException.class))\n                U.error(log, \"Failed to run query [qry=\" + qryInfo + \", node=\" + cctx.nodeId() + \"]\", e);\n\n            onPageReady(loc, qryInfo, null, true, e);\n\n            if (e instanceof Error)\n                throw (Error)e;\n        }\n        finally {\n            if (loc) {\n                // Local iterators are always removed.\n                if (res != null) {\n                    try {\n                        res.closeIfNotShared(recipient(qryInfo.senderId(), qryInfo.requestId()));\n                    }\n                    catch (IgniteCheckedException e) {\n                        if (!X.hasCause(e, GridDhtUnreservedPartitionException.class))\n                            U.error(log, \"Failed to close local iterator [qry=\" + qryInfo + \", node=\" +\n                                cctx.nodeId() + \"]\", e);\n                    }\n                }\n            }\n            else if (rmvIter)\n                removeQueryResult(qryInfo.senderId(), qryInfo.requestId());\n        }\n    }\n    finally {\n        leaveBusy();\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2376,
        "instruction": "private void getZooLock(HostAndPort addr) throws KeeperException, InterruptedException {\n  log.info(\"Attempting to acquire Garbage Collector Lock\");\n  String path = ZooUtil.getRoot(getInstance()) + Constants.ZGC_LOCK;\n\n  LockWatcher lockWatcher = new LockWatcher() {\n    @Override\n    public void lostLock(LockLossReason reason) {\n      Halt.halt(\"GC lock in zookeeper lost (reason = \" + reason + \"), exiting!\", 1);\n    }\n\n    @Override\n    public void unableToMonitorLockNode(final Throwable e) {\n      // ACCUMULO-3651 Level changed to error and FATAL added to message for slf4j compatibility\n      Halt.halt(-1, new Runnable() {\n\n        @Override\n        public void run() {\n          log.error(\"FATAL: No longer able to monitor lock node \", e);\n        }\n      });\n\n    }\n  };\n\n  while (true) {\n    lock = new ZooLock(path);\n    if (lock.tryLock(lockWatcher, new ServerServices(addr.toString(), Service.GC_CLIENT).toString().getBytes())) {\n      log.info(\"Acquired Garbage Collector Lock\");\n      return;\n    }\n    sleepUninterruptibly(1, TimeUnit.SECONDS);\n  }\n}",
        "accept_response": "private void getZooLock(HostAndPort addr) throws KeeperException, InterruptedException {\n  log.info(\"Attempting to acquire Garbage Collector Lock\");\n  String path = ZooUtil.getRoot(getInstance()) + Constants.ZGC_LOCK;\n\n  LockWatcher lockWatcher = new LockWatcher() {\n    @Override\n    public void lostLock(LockLossReason reason) {\n      Halt.halt(\"GC lock in zookeeper lost (reason = \" + reason + \"), exiting!\", 1);\n    }\n\n    @Override\n    public void unableToMonitorLockNode(final Throwable e) {\n      // ACCUMULO-3651 Level changed to error and FATAL added to message for slf4j compatibility\n      Halt.halt(-1, new Runnable() {\n\n        @Override\n        public void run() {\n          log.error(\"FATAL: No longer able to monitor lock node \", e);\n        }\n      });\n\n    }\n  };\n\n  while (true) {\n    lock = new ZooLock(path);\n    if (lock.tryLock(lockWatcher, new ServerServices(addr.toString(), Service.GC_CLIENT).toString().getBytes())) {\n      log.info(\"Acquired Garbage Collector Lock\");\n      return;\n    }\n    log.debug(\"Failed to acquire GC ZooKeeper lock, will retry\");\n    sleepUninterruptibly(1, TimeUnit.SECONDS);\n  }\n}",
        "reject_response": "private void getZooLock(HostAndPort addr) throws KeeperException, InterruptedException {\n  log.info(\"Attempting to acquire Garbage Collector Lock\");\n  String path = ZooUtil.getRoot(getInstance()) + Constants.ZGC_LOCK;\n\n  LockWatcher lockWatcher = new LockWatcher() {\n    @Override\n    public void lostLock(LockLossReason reason) {\n      Halt.halt(\"GC lock in zookeeper lost (reason = \" + reason + \"), exiting!\", 1);\n    }\n\n    @Override\n    public void unableToMonitorLockNode(final Throwable e) {\n      // ACCUMULO-3651 Level changed to error and FATAL added to message for slf4j compatibility\n      Halt.halt(-1, new Runnable() {\n\n        @Override\n        public void run() {\n          log.error(\"FATAL: No longer able to monitor lock node \", e);\n        }\n      });\n\n    }\n  };\n\n  while (true) {\n    lock = new ZooLock(path);\n    if (lock.tryLock(lockWatcher, new ServerServices(addr.toString(), Service.GC_CLIENT).toString().getBytes())) {\n      log.info(\"Acquired Garbage Collector Lock\");\n      return;\n    }\n    log.debug(\"Failed to get GC ZooKeeper lock, will retry\");\n    sleepUninterruptibly(1, TimeUnit.SECONDS);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2549,
        "instruction": "private void finalizeOutputContainer() {\n  VectorAccessibleUtilities.setValueCount(container, outputIndex);\n\n  // Set the record count in the container\n  container.setRecordCount(outputIndex);\n  container.buildSchema(BatchSchema.SelectionVectorMode.NONE);\n\n  batchMemoryManager.updateOutgoingStats(outputIndex);\n  if (logger.isDebugEnabled()) {\n    logger.debug(\"Number of records emitted: \" + outputIndex);\n  }\n\n  // Update the output index for next output batch to zero\n  outputIndex = 0;\n}",
        "accept_response": "private void finalizeOutputContainer() {\n  VectorAccessibleUtilities.setValueCount(container, outputIndex);\n\n  // Set the record count in the container\n  container.setRecordCount(outputIndex);\n  container.buildSchema(BatchSchema.SelectionVectorMode.NONE);\n\n  batchMemoryManager.updateOutgoingStats(outputIndex);\n  if (logger.isDebugEnabled()) {\n    logger.debug(\"BATCH_STATS, outgoing:\\n {}\", new RecordBatchSizer(this));\n    logger.debug(\"Number of records emitted: \" + outputIndex);\n  }\n\n  // Update the output index for next output batch to zero\n  outputIndex = 0;\n}",
        "reject_response": "private void finalizeOutputContainer() {\n  VectorAccessibleUtilities.setValueCount(container, outputIndex);\n\n  // Set the record count in the container\n  container.setRecordCount(outputIndex);\n  container.buildSchema(BatchSchema.SelectionVectorMode.NONE);\n\n  batchMemoryManager.updateOutgoingStats(outputIndex);\n  if (logger.isDebugEnabled()) {\n  logger.debug(\"Number of records emitted: \" + outputIndex);\n    logger.debug(\"Number of records emitted: \" + outputIndex);\n  }\n\n  // Update the output index for next output batch to zero\n  outputIndex = 0;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2439,
        "instruction": "protected ProtocolInfo detectProtocol(byte[] buffer) throws IOException {\n    TcpTransportFactory detectedTransportFactory = transportFactory;\n    WireFormatFactory detectedWireFormatFactory = wireFormatFactory;\n\n    boolean found = false;\n    for (String scheme : protocolVerifiers.keySet()) {\n        if (protocolVerifiers.get(scheme).isProtocol(buffer)) {\n            detectedWireFormatFactory = findWireFormatFactory(scheme, wireFormatOptions);\n\n            if (scheme.equals(\"default\")) {\n                scheme = \"\";\n            }\n\n            detectedTransportFactory = (TcpTransportFactory) findTransportFactory(scheme, transportOptions);\n            found = true;\n            break;\n        }\n    }\n\n    if (!found) {\n        throw new IllegalStateException(\"Could not detect the wire format\");\n    }\n\n    return new ProtocolInfo(detectedTransportFactory, detectedWireFormatFactory);\n\n}",
        "accept_response": "protected ProtocolInfo detectProtocol(byte[] buffer) throws IOException {\n    TcpTransportFactory detectedTransportFactory = transportFactory;\n    WireFormatFactory detectedWireFormatFactory = wireFormatFactory;\n\n    boolean found = false;\n    for (String scheme : protocolVerifiers.keySet()) {\n        if (protocolVerifiers.get(scheme).isProtocol(buffer)) {\n            LOG.debug(\"Detected protocol {}\", scheme);\n            detectedWireFormatFactory = findWireFormatFactory(scheme, wireFormatOptions);\n\n            if (scheme.equals(\"default\")) {\n                scheme = \"\";\n            }\n\n            detectedTransportFactory = (TcpTransportFactory) findTransportFactory(scheme, transportOptions);\n            found = true;\n            break;\n        }\n    }\n\n    if (!found) {\n        throw new IllegalStateException(\"Could not detect the wire format\");\n    }\n\n    return new ProtocolInfo(detectedTransportFactory, detectedWireFormatFactory);\n\n}",
        "reject_response": "protected ProtocolInfo detectProtocol(byte[] buffer) throws IOException {\n    TcpTransportFactory detectedTransportFactory = transportFactory;\n    WireFormatFactory detectedWireFormatFactory = wireFormatFactory;\n\n    boolean found = false;\n    for (String scheme : protocolVerifiers.keySet()) {\n        if (protocolVerifiers.get(scheme).isProtocol(buffer)) {\n            LOG.debug(\"Detected protocol \" + scheme);\n            detectedWireFormatFactory = findWireFormatFactory(scheme, wireFormatOptions);\n\n            if (scheme.equals(\"default\")) {\n                scheme = \"\";\n            }\n\n            detectedTransportFactory = (TcpTransportFactory) findTransportFactory(scheme, transportOptions);\n            found = true;\n            break;\n        }\n    }\n\n    if (!found) {\n        throw new IllegalStateException(\"Could not detect the wire format\");\n    }\n\n    return new ProtocolInfo(detectedTransportFactory, detectedWireFormatFactory);\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3075,
        "instruction": "@Override\npublic void stop() {\n  super.stop();\n\n  // Shutdown embedded server\n  try {\n    if(started) {\n      server.shutdown();\n    }\n  } catch (Exception e) {\n    String message = \"Can't shut down embedded Derby server\";\n    throw new RuntimeException(message, e);\n  }\n}",
        "accept_response": "@Override\npublic void stop() {\n  super.stop();\n\n  // Shutdown embedded server\n  try {\n    if(started) {\n      server.shutdown();\n    }\n  } catch (Exception e) {\n    String message = \"Can't shut down embedded Derby server\";\n    LOG.fatal(message, e);\n    throw new RuntimeException(message, e);\n  }\n}",
        "reject_response": "@Override\npublic void stop() {\n  super.stop();\n\n  // Shutdown embedded server\n  try {\n    if(started) {\n      server.shutdown();\n    }\n  } catch (Exception e) {\n    String message = \"Can't shut down embedded Derby server\";\n    LOG.info(\"Can't shut down embedded server\", e);\n    throw new RuntimeException(message, e);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2966,
        "instruction": "public synchronized static Signature getSignature(Configuration conf) {\n  String clazz = conf.get(\"db.signature.class\", MD5Signature.class.getName());\n  ObjectCache objectCache = ObjectCache.get(conf);\n  Signature impl = (Signature) objectCache.getObject(clazz);\n  if (impl == null) {\n    try {\n      Class<?> implClass = Class.forName(clazz);\n      impl = (Signature) implClass.getConstructor().newInstance();\n      impl.setConf(conf);\n      objectCache.setObject(clazz, impl);\n    } catch (Exception e) {\n      throw new RuntimeException(\"Couldn't create \" + clazz, e);\n    }\n  }\n  return impl;\n}",
        "accept_response": "public synchronized static Signature getSignature(Configuration conf) {\n  String clazz = conf.get(\"db.signature.class\", MD5Signature.class.getName());\n  ObjectCache objectCache = ObjectCache.get(conf);\n  Signature impl = (Signature) objectCache.getObject(clazz);\n  if (impl == null) {\n    try {\n      LOG.info(\"Using Signature impl: {}\", clazz);\n      Class<?> implClass = Class.forName(clazz);\n      impl = (Signature) implClass.getConstructor().newInstance();\n      impl.setConf(conf);\n      objectCache.setObject(clazz, impl);\n    } catch (Exception e) {\n      throw new RuntimeException(\"Couldn't create \" + clazz, e);\n    }\n  }\n  return impl;\n}",
        "reject_response": "public synchronized static Signature getSignature(Configuration conf) {\n  String clazz = conf.get(\"db.signature.class\", MD5Signature.class.getName());\n  ObjectCache objectCache = ObjectCache.get(conf);\n  Signature impl = (Signature) objectCache.getObject(clazz);\n  if (impl == null) {\n    try {\n      if (LOG.isInfoEnabled()) {\n        LOG.info(\"Using Signature impl: \" + clazz);\n      }\n      Class<?> implClass = Class.forName(clazz);\n      impl = (Signature) implClass.getConstructor().newInstance();\n      impl.setConf(conf);\n      objectCache.setObject(clazz, impl);\n    } catch (Exception e) {\n      throw new RuntimeException(\"Couldn't create \" + clazz, e);\n    }\n  }\n  return impl;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3227,
        "instruction": "public boolean clientTunneledAuthenticationInProgress() {\n\tif (!isSASLConfigured) {\n\t    return false;\n    }\n    // TODO: Rather than checking a disjunction here, should be a single member\n    // variable or method in this class to determine whether the client is\n    // configured to use SASL. (see also ZOOKEEPER-1455).\n    try {\n        if ((clientConfig.getJaasConfKey() != null)\n                || ((Configuration.getConfiguration() != null) && (Configuration.getConfiguration()\n                        .getAppConfigurationEntry(clientConfig.getProperty(ZKClientConfig.LOGIN_CONTEXT_NAME_KEY,\n                                ZKClientConfig.LOGIN_CONTEXT_NAME_KEY_DEFAULT)) != null))) {\n            // Client is configured to use a valid login Configuration, so\n            // authentication is either in progress, successful, or failed.\n\n            // 1. Authentication hasn't finished yet: we must wait for it to do so.\n            if (!isComplete() && !isFailed()) {\n                return true;\n            }\n\n            // 2. SASL authentication has succeeded or failed..\n            if (!gotLastPacket) {\n                // ..but still in progress, because there is a final SASL\n                // message from server which must be received.\n                return true;\n            }\n        }\n        // Either client is not configured to use a tunnelled authentication\n        // scheme, or tunnelled authentication has completed (successfully or\n        // not), and all server SASL messages have been received.\n        return false;\n    } catch (SecurityException e) {\n        // Thrown if the caller does not have permission to retrieve the Configuration.\n        // In this case, simply returning false is correct.\n\n        return false;\n    }\n}",
        "accept_response": "public boolean clientTunneledAuthenticationInProgress() {\n\tif (!isSASLConfigured) {\n\t    return false;\n    }\n    // TODO: Rather than checking a disjunction here, should be a single member\n    // variable or method in this class to determine whether the client is\n    // configured to use SASL. (see also ZOOKEEPER-1455).\n    try {\n        if ((clientConfig.getJaasConfKey() != null)\n                || ((Configuration.getConfiguration() != null) && (Configuration.getConfiguration()\n                        .getAppConfigurationEntry(clientConfig.getProperty(ZKClientConfig.LOGIN_CONTEXT_NAME_KEY,\n                                ZKClientConfig.LOGIN_CONTEXT_NAME_KEY_DEFAULT)) != null))) {\n            // Client is configured to use a valid login Configuration, so\n            // authentication is either in progress, successful, or failed.\n\n            // 1. Authentication hasn't finished yet: we must wait for it to do so.\n            if (!isComplete() && !isFailed()) {\n                return true;\n            }\n\n            // 2. SASL authentication has succeeded or failed..\n            if (!gotLastPacket) {\n                // ..but still in progress, because there is a final SASL\n                // message from server which must be received.\n                return true;\n            }\n        }\n        // Either client is not configured to use a tunnelled authentication\n        // scheme, or tunnelled authentication has completed (successfully or\n        // not), and all server SASL messages have been received.\n        return false;\n    } catch (SecurityException e) {\n        // Thrown if the caller does not have permission to retrieve the Configuration.\n        // In this case, simply returning false is correct.\n        LOG.debug(\"Could not retrieve login configuration\", e);\n\n        return false;\n    }\n}",
        "reject_response": "public boolean clientTunneledAuthenticationInProgress() {\n\tif (!isSASLConfigured) {\n\t    return false;\n    }\n    // TODO: Rather than checking a disjunction here, should be a single member\n    // variable or method in this class to determine whether the client is\n    // configured to use SASL. (see also ZOOKEEPER-1455).\n    try {\n        if ((clientConfig.getJaasConfKey() != null)\n                || ((Configuration.getConfiguration() != null) && (Configuration.getConfiguration()\n                        .getAppConfigurationEntry(clientConfig.getProperty(ZKClientConfig.LOGIN_CONTEXT_NAME_KEY,\n                                ZKClientConfig.LOGIN_CONTEXT_NAME_KEY_DEFAULT)) != null))) {\n            // Client is configured to use a valid login Configuration, so\n            // authentication is either in progress, successful, or failed.\n\n            // 1. Authentication hasn't finished yet: we must wait for it to do so.\n            if (!isComplete() && !isFailed()) {\n                return true;\n            }\n\n            // 2. SASL authentication has succeeded or failed..\n            if (!gotLastPacket) {\n                // ..but still in progress, because there is a final SASL\n                // message from server which must be received.\n                return true;\n            }\n        }\n        // Either client is not configured to use a tunnelled authentication\n        // scheme, or tunnelled authentication has completed (successfully or\n        // not), and all server SASL messages have been received.\n        return false;\n    } catch (SecurityException e) {\n        // Thrown if the caller does not have permission to retrieve the Configuration.\n        // In this case, simply returning false is correct.\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Could not retrieve login configuration: \" + e);\n        }\n\n        return false;\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3176,
        "instruction": "@Deprecated\npublic void increaseContainerResourceAsync(Container container) {\n  if (!(callbackHandler instanceof AbstractCallbackHandler)) {\n    LOG.error(\"Callback handler does not implement container resource \"\n            + \"increase callback methods\");\n    return;\n  }\n  AbstractCallbackHandler handler = (AbstractCallbackHandler) callbackHandler;\n  if (containers.get(container.getId()) == null) {\n    handler.onIncreaseContainerResourceError(\n        container.getId(),\n        RPCUtil.getRemoteException(\n            \"Container \" + container.getId() +\n                \" is neither started nor scheduled to start\"));\n  }\n  try {\n    events.put(new UpdateContainerResourceEvent(container, true));\n  } catch (InterruptedException e) {\n    handler.onIncreaseContainerResourceError(container.getId(), e);\n  }\n}",
        "accept_response": "@Deprecated\npublic void increaseContainerResourceAsync(Container container) {\n  if (!(callbackHandler instanceof AbstractCallbackHandler)) {\n    LOG.error(\"Callback handler does not implement container resource \"\n            + \"increase callback methods\");\n    return;\n  }\n  AbstractCallbackHandler handler = (AbstractCallbackHandler) callbackHandler;\n  if (containers.get(container.getId()) == null) {\n    handler.onIncreaseContainerResourceError(\n        container.getId(),\n        RPCUtil.getRemoteException(\n            \"Container \" + container.getId() +\n                \" is neither started nor scheduled to start\"));\n  }\n  try {\n    events.put(new UpdateContainerResourceEvent(container, true));\n  } catch (InterruptedException e) {\n    LOG.warn(\"Exception when scheduling the event of increasing \" +\n            \"resource of Container {}\", container.getId());\n    handler.onIncreaseContainerResourceError(container.getId(), e);\n  }\n}",
        "reject_response": "@Deprecated\npublic void increaseContainerResourceAsync(Container container) {\n  if (!(callbackHandler instanceof AbstractCallbackHandler)) {\n    LOG.error(\"Callback handler does not implement container resource \"\n            + \"increase callback methods\");\n    return;\n  }\n  AbstractCallbackHandler handler = (AbstractCallbackHandler) callbackHandler;\n  if (containers.get(container.getId()) == null) {\n    handler.onIncreaseContainerResourceError(\n        container.getId(),\n        RPCUtil.getRemoteException(\n            \"Container \" + container.getId() +\n                \" is neither started nor scheduled to start\"));\n  }\n  try {\n    events.put(new UpdateContainerResourceEvent(container, true));\n  } catch (InterruptedException e) {\n    LOG.warn(\"Exception when scheduling the event of increasing resource of \"\n        + \"Container \" + container.getId());\n    handler.onIncreaseContainerResourceError(container.getId(), e);\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2507,
        "instruction": "protected void doCreateDeployment(Exchange exchange, String operation) throws Exception {\n    Deployment deployment = null;\n    String deploymentName = exchange.getIn().getHeader(KubernetesConstants.KUBERNETES_DEPLOYMENT_NAME, String.class);\n    String namespaceName = exchange.getIn().getHeader(KubernetesConstants.KUBERNETES_NAMESPACE_NAME, String.class);\n    DeploymentSpec deSpec = exchange.getIn().getHeader(KubernetesConstants.KUBERNETES_DEPLOYMENT_SPEC, DeploymentSpec.class);\n    if (ObjectHelper.isEmpty(deploymentName)) {\n        LOG.error(\"Create a specific Deployment require specify a Deployment name\");\n        throw new IllegalArgumentException(\"Create a specific Deployment require specify a pod name\");\n    }\n    if (ObjectHelper.isEmpty(namespaceName)) {\n        throw new IllegalArgumentException(\"Create a specific Deployment require specify a namespace name\");\n    }\n    if (ObjectHelper.isEmpty(deSpec)) {\n        LOG.error(\"Create a specific Deployment require specify a Deployment spec bean\");\n        throw new IllegalArgumentException(\"Create a specific Deployment require specify a Deployment spec bean\");\n    }\n    Map<String, String> labels = exchange.getIn().getHeader(KubernetesConstants.KUBERNETES_DEPLOYMENTS_LABELS, Map.class);\n    Deployment deploymentCreating = new DeploymentBuilder().withNewMetadata().withName(deploymentName).withLabels(labels).endMetadata().withSpec(deSpec).build();\n    deployment = getEndpoint().getKubernetesClient().apps().deployments().inNamespace(namespaceName).create(deploymentCreating);\n\n    MessageHelper.copyHeaders(exchange.getIn(), exchange.getOut(), true);\n    exchange.getOut().setBody(deployment);\n}",
        "accept_response": "protected void doCreateDeployment(Exchange exchange, String operation) throws Exception {\n    Deployment deployment = null;\n    String deploymentName = exchange.getIn().getHeader(KubernetesConstants.KUBERNETES_DEPLOYMENT_NAME, String.class);\n    String namespaceName = exchange.getIn().getHeader(KubernetesConstants.KUBERNETES_NAMESPACE_NAME, String.class);\n    DeploymentSpec deSpec = exchange.getIn().getHeader(KubernetesConstants.KUBERNETES_DEPLOYMENT_SPEC, DeploymentSpec.class);\n    if (ObjectHelper.isEmpty(deploymentName)) {\n        LOG.error(\"Create a specific Deployment require specify a Deployment name\");\n        throw new IllegalArgumentException(\"Create a specific Deployment require specify a pod name\");\n    }\n    if (ObjectHelper.isEmpty(namespaceName)) {\n        LOG.error(\"Create a specific Deployment require specify a namespace name\");\n        throw new IllegalArgumentException(\"Create a specific Deployment require specify a namespace name\");\n    }\n    if (ObjectHelper.isEmpty(deSpec)) {\n        LOG.error(\"Create a specific Deployment require specify a Deployment spec bean\");\n        throw new IllegalArgumentException(\"Create a specific Deployment require specify a Deployment spec bean\");\n    }\n    Map<String, String> labels = exchange.getIn().getHeader(KubernetesConstants.KUBERNETES_DEPLOYMENTS_LABELS, Map.class);\n    Deployment deploymentCreating = new DeploymentBuilder().withNewMetadata().withName(deploymentName).withLabels(labels).endMetadata().withSpec(deSpec).build();\n    deployment = getEndpoint().getKubernetesClient().apps().deployments().inNamespace(namespaceName).create(deploymentCreating);\n\n    MessageHelper.copyHeaders(exchange.getIn(), exchange.getOut(), true);\n    exchange.getOut().setBody(deployment);\n}",
        "reject_response": "protected void doCreateDeployment(Exchange exchange, String operation) throws Exception {\n    Deployment deployment = null;\n    String deploymentName = exchange.getIn().getHeader(KubernetesConstants.KUBERNETES_DEPLOYMENT_NAME, String.class);\n    String namespaceName = exchange.getIn().getHeader(KubernetesConstants.KUBERNETES_NAMESPACE_NAME, String.class);\n    DeploymentSpec deSpec = exchange.getIn().getHeader(KubernetesConstants.KUBERNETES_DEPLOYMENT_SPEC, DeploymentSpec.class);\n    if (ObjectHelper.isEmpty(deploymentName)) {\n        LOG.error(\"Create a specific Deployment require specify a Deployment name\");\n        throw new IllegalArgumentException(\"Create a specific Deployment require specify a pod name\");\n    }\n    if (ObjectHelper.isEmpty(namespaceName)) {\n        LOG.error(\"Create a specific pod require specify a namespace name\");\n        throw new IllegalArgumentException(\"Create a specific pod require specify a namespace name\");\n        throw new IllegalArgumentException(\"Create a specific Deployment require specify a namespace name\");\n    }\n    if (ObjectHelper.isEmpty(deSpec)) {\n        LOG.error(\"Create a specific Deployment require specify a Deployment spec bean\");\n        throw new IllegalArgumentException(\"Create a specific Deployment require specify a Deployment spec bean\");\n    }\n    Map<String, String> labels = exchange.getIn().getHeader(KubernetesConstants.KUBERNETES_DEPLOYMENTS_LABELS, Map.class);\n    Deployment deploymentCreating = new DeploymentBuilder().withNewMetadata().withName(deploymentName).withLabels(labels).endMetadata().withSpec(deSpec).build();\n    deployment = getEndpoint().getKubernetesClient().apps().deployments().inNamespace(namespaceName).create(deploymentCreating);\n\n    MessageHelper.copyHeaders(exchange.getIn(), exchange.getOut(), true);\n    exchange.getOut().setBody(deployment);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3084,
        "instruction": "@Override\npublic void execute(Tuple tuple) {\n    try {\n        boolean forceFlush = false;\n        if (TupleUtils.isTick(tuple)) {\n            forceFlush = true;\n        }\n        else {\n            List<String> partitionVals = options.getMapper().mapPartitions(tuple);\n            HiveEndPoint endPoint = HiveUtils.makeEndPoint(partitionVals, options);\n            HiveWriter writer = getOrCreateWriter(endPoint);\n            if (timeToSendHeartBeat.compareAndSet(true, false)) {\n                enableHeartBeatOnAllWriters();\n            }\n            writer.write(options.getMapper().mapRecord(tuple));\n            tupleBatch.add(tuple);\n            if (tupleBatch.size() >= options.getBatchSize())\n                forceFlush = true;\n        }\n        if(forceFlush && !tupleBatch.isEmpty()) {\n            flushAllWriters(true);\n            LOG.info(\"acknowledging tuples after writers flushed \");\n            for(Tuple t : tupleBatch)\n                collector.ack(t);\n            tupleBatch.clear();\n        }\n    } catch(Exception e) {\n        this.collector.reportError(e);\n        collector.fail(tuple);\n        try {\n            flushAndCloseWriters();\n            LOG.info(\"acknowledging tuples after writers flushed and closed\");\n            for (Tuple t : tupleBatch)\n                collector.ack(t);\n            tupleBatch.clear();\n        } catch (Exception e1) {\n            //If flushAndClose fails assume tuples are lost, do not ack\n            LOG.warn(\"Error while flushing and closing writers, tuples will NOT be acknowledged\");\n            for (Tuple t : tupleBatch)\n                collector.fail(t);\n            tupleBatch.clear();\n        }\n    }\n}",
        "accept_response": "@Override\npublic void execute(Tuple tuple) {\n    try {\n        boolean forceFlush = false;\n        if (TupleUtils.isTick(tuple)) {\n            LOG.debug(\"TICK received! current batch status [{}/{}]\", tupleBatch.size(), options.getBatchSize());\n            forceFlush = true;\n        }\n        else {\n            List<String> partitionVals = options.getMapper().mapPartitions(tuple);\n            HiveEndPoint endPoint = HiveUtils.makeEndPoint(partitionVals, options);\n            HiveWriter writer = getOrCreateWriter(endPoint);\n            if (timeToSendHeartBeat.compareAndSet(true, false)) {\n                enableHeartBeatOnAllWriters();\n            }\n            writer.write(options.getMapper().mapRecord(tuple));\n            tupleBatch.add(tuple);\n            if (tupleBatch.size() >= options.getBatchSize())\n                forceFlush = true;\n        }\n        if(forceFlush && !tupleBatch.isEmpty()) {\n            flushAllWriters(true);\n            LOG.info(\"acknowledging tuples after writers flushed \");\n            for(Tuple t : tupleBatch)\n                collector.ack(t);\n            tupleBatch.clear();\n        }\n    } catch(Exception e) {\n        this.collector.reportError(e);\n        collector.fail(tuple);\n        try {\n            flushAndCloseWriters();\n            LOG.info(\"acknowledging tuples after writers flushed and closed\");\n            for (Tuple t : tupleBatch)\n                collector.ack(t);\n            tupleBatch.clear();\n        } catch (Exception e1) {\n            //If flushAndClose fails assume tuples are lost, do not ack\n            LOG.warn(\"Error while flushing and closing writers, tuples will NOT be acknowledged\");\n            for (Tuple t : tupleBatch)\n                collector.fail(t);\n            tupleBatch.clear();\n        }\n    }\n}",
        "reject_response": "@Override\npublic void execute(Tuple tuple) {\n    try {\n        boolean forceFlush = false;\n        if (TupleUtils.isTick(tuple)) {\n            LOG.debug(\"TICK received! current batch status [\" + tupleBatch.size() + \"/\" + options.getBatchSize() + \"]\");\n            forceFlush = true;\n        }\n        else {\n            List<String> partitionVals = options.getMapper().mapPartitions(tuple);\n            HiveEndPoint endPoint = HiveUtils.makeEndPoint(partitionVals, options);\n            HiveWriter writer = getOrCreateWriter(endPoint);\n            if (timeToSendHeartBeat.compareAndSet(true, false)) {\n                enableHeartBeatOnAllWriters();\n            }\n            writer.write(options.getMapper().mapRecord(tuple));\n            tupleBatch.add(tuple);\n            if (tupleBatch.size() >= options.getBatchSize())\n                forceFlush = true;\n        }\n        if(forceFlush && !tupleBatch.isEmpty()) {\n            flushAllWriters(true);\n            LOG.info(\"acknowledging tuples after writers flushed \");\n            for(Tuple t : tupleBatch)\n                collector.ack(t);\n            tupleBatch.clear();\n        }\n    } catch(Exception e) {\n        this.collector.reportError(e);\n        collector.fail(tuple);\n        try {\n            flushAndCloseWriters();\n            LOG.info(\"acknowledging tuples after writers flushed and closed\");\n            for (Tuple t : tupleBatch)\n                collector.ack(t);\n            tupleBatch.clear();\n        } catch (Exception e1) {\n            //If flushAndClose fails assume tuples are lost, do not ack\n            LOG.warn(\"Error while flushing and closing writers, tuples will NOT be acknowledged\");\n            for (Tuple t : tupleBatch)\n                collector.fail(t);\n            tupleBatch.clear();\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3072,
        "instruction": "@Override\npublic void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception {\n  requestHandler.exceptionCaught(cause);\n  responseHandler.exceptionCaught(cause);\n  ctx.close();\n}",
        "accept_response": "@Override\npublic void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception {\n  logger.warn(\"Exception in connection from \" + getRemoteAddress(ctx.channel()),\n    cause);\n  requestHandler.exceptionCaught(cause);\n  responseHandler.exceptionCaught(cause);\n  ctx.close();\n}",
        "reject_response": "@Override\npublic void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception {\n  logger.warn(\"Exception in connection from \" + NettyUtils.getRemoteAddress(ctx.channel()),\n  requestHandler.exceptionCaught(cause);\n  responseHandler.exceptionCaught(cause);\n  ctx.close();\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2848,
        "instruction": "@Override public void apply(UUID nodeId, M msg) {\n    ClusterNode node = cctx.node(nodeId);\n\n    if (node == null) {\n        if (log.isTraceEnabled())\n            log.trace(\"Received message from failed node [node=\" + nodeId + \", msg=\" + msg + ']');\n\n        return;\n    }\n\n    if (log.isTraceEnabled())\n\n    onMessage(node, msg);\n}",
        "accept_response": "@Override public void apply(UUID nodeId, M msg) {\n    ClusterNode node = cctx.node(nodeId);\n\n    if (node == null) {\n        if (log.isTraceEnabled())\n            log.trace(\"Received message from failed node [node=\" + nodeId + \", msg=\" + msg + ']');\n\n        return;\n    }\n\n    if (log.isTraceEnabled())\n        log.trace(\"Received message from node [node=\" + nodeId + \", msg=\" + msg + ']');\n\n    onMessage(node, msg);\n}",
        "reject_response": "@Override public void apply(UUID nodeId, M msg) {\n    ClusterNode node = cctx.node(nodeId);\n\n    if (node == null) {\n        if (log.isTraceEnabled())\n            log.trace(\"Received message from failed node [node=\" + nodeId + \", msg=\" + msg + ']');\n\n        return;\n    }\n\n    if (log.isTraceEnabled())\n    if (log.isDebugEnabled())\n        log.debug(\"Received message from node [node=\" + nodeId + \", msg=\" + msg + ']');\n\n    onMessage(node, msg);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3101,
        "instruction": "private void checkRead() throws IOException {\n  if (current == totalCountLoadedSoFar) {\n    if (current != 0) {\n      long timeAssembling = System.currentTimeMillis() - startedAssemblingCurrentBlockAt;\n      totalTimeSpentProcessingRecords += timeAssembling;\n      if (DEBUG) LOG.debug(\"Assembled and processed \" + totalCountLoadedSoFar + \" records from \" + columnCount + \" columns in \" + totalTimeSpentProcessingRecords + \" ms: \" + ((float) totalCountLoadedSoFar / totalTimeSpentProcessingRecords) + \" rec/ms, \" + ((float) totalCountLoadedSoFar * columnCount / totalTimeSpentProcessingRecords) + \" cell/ms\");\n      long totalTime = totalTimeSpentProcessingRecords + totalTimeSpentReadingBytes;\n      long percentReading = 100 * totalTimeSpentReadingBytes / totalTime;\n      long percentProcessing = 100 * totalTimeSpentProcessingRecords / totalTime;\n      if (DEBUG) LOG.debug(\"time spent so far \" + percentReading + \"% reading (\"+totalTimeSpentReadingBytes+\" ms) and \" + percentProcessing + \"% processing (\"+totalTimeSpentProcessingRecords+\" ms)\");\n    }\n\n    long t0 = System.currentTimeMillis();\n    PageReadStore pages = reader.readNextRowGroup();\n    if (pages == null) {\n      throw new IOException(\"expecting more rows but reached last block. Read \" + current + \" out of \" + total);\n    }\n    long timeSpentReading = System.currentTimeMillis() - t0;\n    totalTimeSpentReadingBytes += timeSpentReading;\n    BenchmarkCounter.incrementTime(timeSpentReading);\n    if (DEBUG) {\n      LOG.debug(\"block read in memory in \" + timeSpentReading + \" ms. row count = \" + pages.getRowCount());\n      LOG.debug(\"initializing Record assembly with requested schema \" + requestedSchema);\n    }\n    MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema);\n    recordReader = columnIO.getRecordReader(pages, recordConverter, recordFilter);\n    startedAssemblingCurrentBlockAt = System.currentTimeMillis();\n    totalCountLoadedSoFar += pages.getRowCount();\n    ++ currentBlock;\n  }\n}",
        "accept_response": "private void checkRead() throws IOException {\n  if (current == totalCountLoadedSoFar) {\n    if (current != 0) {\n      long timeAssembling = System.currentTimeMillis() - startedAssemblingCurrentBlockAt;\n      totalTimeSpentProcessingRecords += timeAssembling;\n      if (DEBUG) LOG.debug(\"Assembled and processed \" + totalCountLoadedSoFar + \" records from \" + columnCount + \" columns in \" + totalTimeSpentProcessingRecords + \" ms: \" + ((float) totalCountLoadedSoFar / totalTimeSpentProcessingRecords) + \" rec/ms, \" + ((float) totalCountLoadedSoFar * columnCount / totalTimeSpentProcessingRecords) + \" cell/ms\");\n      long totalTime = totalTimeSpentProcessingRecords + totalTimeSpentReadingBytes;\n      long percentReading = 100 * totalTimeSpentReadingBytes / totalTime;\n      long percentProcessing = 100 * totalTimeSpentProcessingRecords / totalTime;\n      if (DEBUG) LOG.debug(\"time spent so far \" + percentReading + \"% reading (\"+totalTimeSpentReadingBytes+\" ms) and \" + percentProcessing + \"% processing (\"+totalTimeSpentProcessingRecords+\" ms)\");\n    }\n\n    if (DEBUG) LOG.debug(\"at row \" + current + \". reading next block\");\n    long t0 = System.currentTimeMillis();\n    PageReadStore pages = reader.readNextRowGroup();\n    if (pages == null) {\n      throw new IOException(\"expecting more rows but reached last block. Read \" + current + \" out of \" + total);\n    }\n    long timeSpentReading = System.currentTimeMillis() - t0;\n    totalTimeSpentReadingBytes += timeSpentReading;\n    BenchmarkCounter.incrementTime(timeSpentReading);\n    if (DEBUG) {\n      LOG.debug(\"block read in memory in \" + timeSpentReading + \" ms. row count = \" + pages.getRowCount());\n      LOG.debug(\"initializing Record assembly with requested schema \" + requestedSchema);\n    }\n    MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema);\n    recordReader = columnIO.getRecordReader(pages, recordConverter, recordFilter);\n    startedAssemblingCurrentBlockAt = System.currentTimeMillis();\n    totalCountLoadedSoFar += pages.getRowCount();\n    ++ currentBlock;\n  }\n}",
        "reject_response": "private void checkRead() throws IOException {\n  if (current == totalCountLoadedSoFar) {\n    if (current != 0) {\n      long timeAssembling = System.currentTimeMillis() - startedAssemblingCurrentBlockAt;\n      totalTimeSpentProcessingRecords += timeAssembling;\n      if (DEBUG) LOG.debug(\"Assembled and processed \" + totalCountLoadedSoFar + \" records from \" + columnCount + \" columns in \" + totalTimeSpentProcessingRecords + \" ms: \" + ((float) totalCountLoadedSoFar / totalTimeSpentProcessingRecords) + \" rec/ms, \" + ((float) totalCountLoadedSoFar * columnCount / totalTimeSpentProcessingRecords) + \" cell/ms\");\n      long totalTime = totalTimeSpentProcessingRecords + totalTimeSpentReadingBytes;\n      long percentReading = 100 * totalTimeSpentReadingBytes / totalTime;\n      long percentProcessing = 100 * totalTimeSpentProcessingRecords / totalTime;\n      if (DEBUG) LOG.debug(\"time spent so far \" + percentReading + \"% reading (\"+totalTimeSpentReadingBytes+\" ms) and \" + percentProcessing + \"% processing (\"+totalTimeSpentProcessingRecords+\" ms)\");\n    }\n\n    LOG.info(\"at row \" + current + \". reading next block\");\n    long t0 = System.currentTimeMillis();\n    PageReadStore pages = reader.readNextRowGroup();\n    if (pages == null) {\n      throw new IOException(\"expecting more rows but reached last block. Read \" + current + \" out of \" + total);\n    }\n    long timeSpentReading = System.currentTimeMillis() - t0;\n    totalTimeSpentReadingBytes += timeSpentReading;\n    BenchmarkCounter.incrementTime(timeSpentReading);\n    if (DEBUG) {\n      LOG.debug(\"block read in memory in \" + timeSpentReading + \" ms. row count = \" + pages.getRowCount());\n      LOG.debug(\"initializing Record assembly with requested schema \" + requestedSchema);\n    }\n    MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema);\n    recordReader = columnIO.getRecordReader(pages, recordConverter, recordFilter);\n    startedAssemblingCurrentBlockAt = System.currentTimeMillis();\n    totalCountLoadedSoFar += pages.getRowCount();\n    ++ currentBlock;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3097,
        "instruction": "@Override\nprotected ResourceResponse newResourceResponse(final IResource.Attributes attributes) {\n    ResourceResponse response = new ResourceResponse();\n\n    try {\n        final CookieUtils sessionCookieUtils = SyncopeEnduserSession.get().getCookieUtils();\n        // set XSRF_TOKEN cookie\n        if (!SyncopeEnduserSession.get().isXsrfTokenGenerated() && (sessionCookieUtils.getCookie(\n                SyncopeEnduserConstants.XSRF_COOKIE) == null || StringUtils.isBlank(\n                        sessionCookieUtils.getCookie(SyncopeEnduserConstants.XSRF_COOKIE).getValue()))) {\n            SyncopeEnduserSession.get().setXsrfTokenGenerated(true);\n            sessionCookieUtils.save(SyncopeEnduserConstants.XSRF_COOKIE, SaltGenerator.generate(\n                    SyncopeEnduserSession.get().getId()));\n        }\n        response.setWriteCallback(new WriteCallback() {\n\n            @Override\n            public void writeData(final IResource.Attributes attributes) throws IOException {\n                attributes.getResponse().write(\n                        MAPPER.writeValueAsString(\n                                PlatformInfoAdapter.toPlatformInfoRequest(\n                                        SyncopeEnduserSession.get().getPlatformInfo())));\n            }\n        });\n        response.setStatusCode(Response.Status.OK.getStatusCode());\n    } catch (Exception e) {\n        LOG.error(\"Error retrieving syncope info\", e);\n        response.setError(Response.Status.INTERNAL_SERVER_ERROR.getStatusCode(), new StringBuilder()\n                .append(\"ErrorMessage{{ \")\n                .append(e.getMessage())\n                .append(\" }}\")\n                .toString());\n    }\n\n    return response;\n}",
        "accept_response": "@Override\nprotected ResourceResponse newResourceResponse(final IResource.Attributes attributes) {\n    ResourceResponse response = new ResourceResponse();\n\n    try {\n        final CookieUtils sessionCookieUtils = SyncopeEnduserSession.get().getCookieUtils();\n        // set XSRF_TOKEN cookie\n        if (!SyncopeEnduserSession.get().isXsrfTokenGenerated() && (sessionCookieUtils.getCookie(\n                SyncopeEnduserConstants.XSRF_COOKIE) == null || StringUtils.isBlank(\n                        sessionCookieUtils.getCookie(SyncopeEnduserConstants.XSRF_COOKIE).getValue()))) {\n            LOG.debug(\"Set XSRF-TOKEN cookie\");\n            SyncopeEnduserSession.get().setXsrfTokenGenerated(true);\n            sessionCookieUtils.save(SyncopeEnduserConstants.XSRF_COOKIE, SaltGenerator.generate(\n                    SyncopeEnduserSession.get().getId()));\n        }\n        response.setWriteCallback(new WriteCallback() {\n\n            @Override\n            public void writeData(final IResource.Attributes attributes) throws IOException {\n                attributes.getResponse().write(\n                        MAPPER.writeValueAsString(\n                                PlatformInfoAdapter.toPlatformInfoRequest(\n                                        SyncopeEnduserSession.get().getPlatformInfo())));\n            }\n        });\n        response.setStatusCode(Response.Status.OK.getStatusCode());\n    } catch (Exception e) {\n        LOG.error(\"Error retrieving syncope info\", e);\n        response.setError(Response.Status.INTERNAL_SERVER_ERROR.getStatusCode(), new StringBuilder()\n                .append(\"ErrorMessage{{ \")\n                .append(e.getMessage())\n                .append(\" }}\")\n                .toString());\n    }\n\n    return response;\n}",
        "reject_response": "@Override\nprotected ResourceResponse newResourceResponse(final IResource.Attributes attributes) {\n    ResourceResponse response = new ResourceResponse();\n\n    try {\n        final CookieUtils sessionCookieUtils = SyncopeEnduserSession.get().getCookieUtils();\n        // set XSRF_TOKEN cookie\n        if (!SyncopeEnduserSession.get().isXsrfTokenGenerated() && (sessionCookieUtils.getCookie(\n                SyncopeEnduserConstants.XSRF_COOKIE) == null || StringUtils.isBlank(\n                        sessionCookieUtils.getCookie(SyncopeEnduserConstants.XSRF_COOKIE).getValue()))) {\n            LOG.info(\"Set XSRF-TOKEN cookie\");\n            SyncopeEnduserSession.get().setXsrfTokenGenerated(true);\n            sessionCookieUtils.save(SyncopeEnduserConstants.XSRF_COOKIE, SaltGenerator.generate(\n                    SyncopeEnduserSession.get().getId()));\n        }\n        response.setWriteCallback(new WriteCallback() {\n\n            @Override\n            public void writeData(final IResource.Attributes attributes) throws IOException {\n                attributes.getResponse().write(\n                        MAPPER.writeValueAsString(\n                                PlatformInfoAdapter.toPlatformInfoRequest(\n                                        SyncopeEnduserSession.get().getPlatformInfo())));\n            }\n        });\n        response.setStatusCode(Response.Status.OK.getStatusCode());\n    } catch (Exception e) {\n        LOG.error(\"Error retrieving syncope info\", e);\n        response.setError(Response.Status.INTERNAL_SERVER_ERROR.getStatusCode(), new StringBuilder()\n                .append(\"ErrorMessage{{ \")\n                .append(e.getMessage())\n                .append(\" }}\")\n                .toString());\n    }\n\n    return response;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2715,
        "instruction": "@Override\nprotected boolean needsBalance(Cluster cluster) {\n  ClusterLoadState cs = new ClusterLoadState(cluster.clusterState);\n  if (cs.getNumServers() < MIN_SERVER_BALANCE) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Not running balancer because only \" + cs.getNumServers()\n          + \" active regionserver(s)\");\n    }\n    return false;\n  }\n  if (areSomeRegionReplicasColocated(cluster)) {\n    return true;\n  }\n\n  double total = 0.0;\n  float sumMultiplier = 0.0f;\n  for (CostFunction c : costFunctions) {\n    float multiplier = c.getMultiplier();\n    if (multiplier <= 0) {\n      continue;\n    }\n    if (!c.isNeeded()) {\n      LOG.debug(c.getClass().getName() + \" indicated that its cost should not be considered\");\n      continue;\n    }\n    sumMultiplier += multiplier;\n    total += c.cost() * multiplier;\n  }\n\n  if (total <= 0 || sumMultiplier <= 0\n      || (sumMultiplier > 0 && (total / sumMultiplier) < minCostNeedBalance)) {\n    final String loadBalanceTarget =\n        isByTable ? String.format(\"table (%s)\", tableName) : \"cluster\";\n    return false;\n  }\n  return true;\n}",
        "accept_response": "@Override\nprotected boolean needsBalance(Cluster cluster) {\n  ClusterLoadState cs = new ClusterLoadState(cluster.clusterState);\n  if (cs.getNumServers() < MIN_SERVER_BALANCE) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Not running balancer because only \" + cs.getNumServers()\n          + \" active regionserver(s)\");\n    }\n    return false;\n  }\n  if (areSomeRegionReplicasColocated(cluster)) {\n    return true;\n  }\n\n  double total = 0.0;\n  float sumMultiplier = 0.0f;\n  for (CostFunction c : costFunctions) {\n    float multiplier = c.getMultiplier();\n    if (multiplier <= 0) {\n      continue;\n    }\n    if (!c.isNeeded()) {\n      LOG.debug(c.getClass().getName() + \" indicated that its cost should not be considered\");\n      continue;\n    }\n    sumMultiplier += multiplier;\n    total += c.cost() * multiplier;\n  }\n\n  if (total <= 0 || sumMultiplier <= 0\n      || (sumMultiplier > 0 && (total / sumMultiplier) < minCostNeedBalance)) {\n    final String loadBalanceTarget =\n        isByTable ? String.format(\"table (%s)\", tableName) : \"cluster\";\n    LOG.info(String.format(\"Skipping load balancing because the %s is balanced. Total cost: %s, \"\n        + \"Sum multiplier: %s, Minimum cost needed for balance: %s\", loadBalanceTarget, total,\n        sumMultiplier, minCostNeedBalance));\n    return false;\n  }\n  return true;\n}",
        "reject_response": "@Override\nprotected boolean needsBalance(Cluster cluster) {\n  ClusterLoadState cs = new ClusterLoadState(cluster.clusterState);\n  if (cs.getNumServers() < MIN_SERVER_BALANCE) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Not running balancer because only \" + cs.getNumServers()\n          + \" active regionserver(s)\");\n    }\n    return false;\n  }\n  if (areSomeRegionReplicasColocated(cluster)) {\n    return true;\n  }\n\n  double total = 0.0;\n  float sumMultiplier = 0.0f;\n  for (CostFunction c : costFunctions) {\n    float multiplier = c.getMultiplier();\n    if (multiplier <= 0) {\n      continue;\n    }\n    if (!c.isNeeded()) {\n      LOG.debug(c.getClass().getName() + \" indicated that its cost should not be considered\");\n      continue;\n    }\n    sumMultiplier += multiplier;\n    total += c.cost() * multiplier;\n  }\n\n  if (total <= 0 || sumMultiplier <= 0\n      || (sumMultiplier > 0 && (total / sumMultiplier) < minCostNeedBalance)) {\n    final String loadBalanceTarget =\n        isByTable ? String.format(\"table (%s)\", tableName) : \"cluster\";\n    LOG.info(\"Skipping load balancing because balanced cluster; \" + \"total cost is \" + total\n        + \", sum multiplier is \" + sumMultiplier + \" min cost which need balance is \"\n        + minCostNeedBalance);\n    return false;\n  }\n  return true;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2717,
        "instruction": "protected Path closeRecoveredEditsWriter(RecoveredEditsWriter editsWriter,\n    List<IOException> thrown) throws IOException {\n  try {\n    editsWriter.writer.close();\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not close recovered edits at \" + editsWriter.path;\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  final String msg = \"Closed recovered edits writer path=\" + editsWriter.path + \" (wrote \"\n    + editsWriter.editsWritten + \" edits, skipped \" + editsWriter.editsSkipped + \" edits in \" + (\n    editsWriter.nanosSpent / 1000 / 1000) + \" ms)\";\n  LOG.info(msg);\n  updateStatusWithMsg(msg);\n  if (editsWriter.editsWritten == 0) {\n    // just remove the empty recovered.edits file\n    if (walSplitter.walFS.exists(editsWriter.path)\n        && !walSplitter.walFS.delete(editsWriter.path, false)) {\n      final String errorMsg = \"Failed deleting empty \" + editsWriter.path;\n      LOG.warn(errorMsg);\n      updateStatusWithMsg(errorMsg);\n      throw new IOException(\"Failed deleting empty  \" + editsWriter.path);\n    }\n    return null;\n  }\n\n  Path dst = getCompletedRecoveredEditsFilePath(editsWriter.path,\n    regionMaximumEditLogSeqNum.get(Bytes.toString(editsWriter.encodedRegionName)));\n  try {\n    if (!dst.equals(editsWriter.path) && walSplitter.walFS.exists(dst)) {\n      deleteOneWithFewerEntries(editsWriter, dst);\n    }\n    // Skip the unit tests which create a splitter that reads and\n    // writes the data without touching disk.\n    // TestHLogSplit#testThreading is an example.\n    if (walSplitter.walFS.exists(editsWriter.path)) {\n      if (!walSplitter.walFS.rename(editsWriter.path, dst)) {\n        final String errorMsg =\n          \"Failed renaming recovered edits \" + editsWriter.path + \" to \" + dst;\n        updateStatusWithMsg(errorMsg);\n        throw new IOException(errorMsg);\n      }\n      final String renameEditMsg = \"Rename recovered edits \" + editsWriter.path + \" to \" + dst;\n      LOG.info(renameEditMsg);\n      updateStatusWithMsg(renameEditMsg);\n    }\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not rename recovered edits \" + editsWriter.path\n      + \" to \" + dst;\n    LOG.error(errorMsg, ioe);\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  return dst;\n}",
        "accept_response": "protected Path closeRecoveredEditsWriter(RecoveredEditsWriter editsWriter,\n    List<IOException> thrown) throws IOException {\n  try {\n    editsWriter.writer.close();\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not close recovered edits at \" + editsWriter.path;\n    LOG.error(errorMsg, ioe);\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  final String msg = \"Closed recovered edits writer path=\" + editsWriter.path + \" (wrote \"\n    + editsWriter.editsWritten + \" edits, skipped \" + editsWriter.editsSkipped + \" edits in \" + (\n    editsWriter.nanosSpent / 1000 / 1000) + \" ms)\";\n  LOG.info(msg);\n  updateStatusWithMsg(msg);\n  if (editsWriter.editsWritten == 0) {\n    // just remove the empty recovered.edits file\n    if (walSplitter.walFS.exists(editsWriter.path)\n        && !walSplitter.walFS.delete(editsWriter.path, false)) {\n      final String errorMsg = \"Failed deleting empty \" + editsWriter.path;\n      LOG.warn(errorMsg);\n      updateStatusWithMsg(errorMsg);\n      throw new IOException(\"Failed deleting empty  \" + editsWriter.path);\n    }\n    return null;\n  }\n\n  Path dst = getCompletedRecoveredEditsFilePath(editsWriter.path,\n    regionMaximumEditLogSeqNum.get(Bytes.toString(editsWriter.encodedRegionName)));\n  try {\n    if (!dst.equals(editsWriter.path) && walSplitter.walFS.exists(dst)) {\n      deleteOneWithFewerEntries(editsWriter, dst);\n    }\n    // Skip the unit tests which create a splitter that reads and\n    // writes the data without touching disk.\n    // TestHLogSplit#testThreading is an example.\n    if (walSplitter.walFS.exists(editsWriter.path)) {\n      if (!walSplitter.walFS.rename(editsWriter.path, dst)) {\n        final String errorMsg =\n          \"Failed renaming recovered edits \" + editsWriter.path + \" to \" + dst;\n        updateStatusWithMsg(errorMsg);\n        throw new IOException(errorMsg);\n      }\n      final String renameEditMsg = \"Rename recovered edits \" + editsWriter.path + \" to \" + dst;\n      LOG.info(renameEditMsg);\n      updateStatusWithMsg(renameEditMsg);\n    }\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not rename recovered edits \" + editsWriter.path\n      + \" to \" + dst;\n    LOG.error(errorMsg, ioe);\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  return dst;\n}",
        "reject_response": "protected Path closeRecoveredEditsWriter(RecoveredEditsWriter editsWriter,\n    List<IOException> thrown) throws IOException {\n  try {\n    editsWriter.writer.close();\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not close recovered edits at \" + editsWriter.path;\n    LOG.error(\"Could not close recovered edits at {}\", editsWriter.path, ioe);\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  final String msg = \"Closed recovered edits writer path=\" + editsWriter.path + \" (wrote \"\n    + editsWriter.editsWritten + \" edits, skipped \" + editsWriter.editsSkipped + \" edits in \" + (\n    editsWriter.nanosSpent / 1000 / 1000) + \" ms)\";\n  LOG.info(msg);\n  updateStatusWithMsg(msg);\n  if (editsWriter.editsWritten == 0) {\n    // just remove the empty recovered.edits file\n    if (walSplitter.walFS.exists(editsWriter.path)\n        && !walSplitter.walFS.delete(editsWriter.path, false)) {\n      final String errorMsg = \"Failed deleting empty \" + editsWriter.path;\n      LOG.warn(errorMsg);\n      updateStatusWithMsg(errorMsg);\n      throw new IOException(\"Failed deleting empty  \" + editsWriter.path);\n    }\n    return null;\n  }\n\n  Path dst = getCompletedRecoveredEditsFilePath(editsWriter.path,\n    regionMaximumEditLogSeqNum.get(Bytes.toString(editsWriter.encodedRegionName)));\n  try {\n    if (!dst.equals(editsWriter.path) && walSplitter.walFS.exists(dst)) {\n      deleteOneWithFewerEntries(editsWriter, dst);\n    }\n    // Skip the unit tests which create a splitter that reads and\n    // writes the data without touching disk.\n    // TestHLogSplit#testThreading is an example.\n    if (walSplitter.walFS.exists(editsWriter.path)) {\n      if (!walSplitter.walFS.rename(editsWriter.path, dst)) {\n        final String errorMsg =\n          \"Failed renaming recovered edits \" + editsWriter.path + \" to \" + dst;\n        updateStatusWithMsg(errorMsg);\n        throw new IOException(errorMsg);\n      }\n      final String renameEditMsg = \"Rename recovered edits \" + editsWriter.path + \" to \" + dst;\n      LOG.info(renameEditMsg);\n      updateStatusWithMsg(renameEditMsg);\n    }\n  } catch (IOException ioe) {\n    final String errorMsg = \"Could not rename recovered edits \" + editsWriter.path\n      + \" to \" + dst;\n    LOG.error(errorMsg, ioe);\n    updateStatusWithMsg(errorMsg);\n    thrown.add(ioe);\n    return null;\n  }\n  return dst;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3010,
        "instruction": "@Override\npublic float getProgress() {\n  try {\n    return Math.max(Math.min(1, progressProvider.get().getProgress()), 0);\n  } catch (final Exception e) {\n    // An Exception must be caught and logged here because YARN swallows the Exception and fails the job.\n    return 0;\n  }\n}",
        "accept_response": "@Override\npublic float getProgress() {\n  try {\n    return Math.max(Math.min(1, progressProvider.get().getProgress()), 0);\n  } catch (final Exception e) {\n    // An Exception must be caught and logged here because YARN swallows the Exception and fails the job.\n    LOG.log(Level.WARNING, \"Cannot get the application progress. Will return 0.\", e);\n    return 0;\n  }\n}",
        "reject_response": "@Override\npublic float getProgress() {\n  try {\n    return Math.max(Math.min(1, progressProvider.get().getProgress()), 0);\n  } catch (final Exception e) {\n    // An Exception must be caught and logged here because YARN swallows the Exception and fails the job.\n    LOG.log(Level.WARNING, \"An exception occurred in ProgressProvider.getProgress(), with message : \" +\n        e.getMessage() + \". Returning 0 as progress.\");\n    return 0f;\n    return 0;\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2768,
        "instruction": "@Override\npublic InetAddress[] resolve(final String host) throws UnknownHostException {\n    final InetAddress[] resolvedAddresses = dnsMap.get(host);\n    if(resolvedAddresses == null){\n        throw new UnknownHostException(host + \" cannot be resolved\");\n    }\n    return resolvedAddresses;\n}",
        "accept_response": "@Override\npublic InetAddress[] resolve(final String host) throws UnknownHostException {\n    final InetAddress[] resolvedAddresses = dnsMap.get(host);\n    if (log.isInfoEnabled()) {\n        log.info(\"Resolving {} to {}\", host, Arrays.deepToString(resolvedAddresses));\n    }\n    if(resolvedAddresses == null){\n        throw new UnknownHostException(host + \" cannot be resolved\");\n    }\n    return resolvedAddresses;\n}",
        "reject_response": "@Override\npublic InetAddress[] resolve(final String host) throws UnknownHostException {\n    final InetAddress[] resolvedAddresses = dnsMap.get(host);\n    if (log.isInfoEnabled()) {\n        log.info(\"Resolving \" + host + \" to \" + Arrays.deepToString(resolvedAddresses));\n    }\n    if(resolvedAddresses == null){\n        throw new UnknownHostException(host + \" cannot be resolved\");\n    }\n    return resolvedAddresses;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2640,
        "instruction": "private String getStringAttribute(Object object, String name) {\n  if (object == null) {\n    return \"\";\n  }\n\n  try {\n    if (!(object.getClass().equals(String.class))) {\n      return \"\";\n    } else {\n      return (String) object;\n    }\n  } catch (Exception e) {\n    logger.info(\"Exception occurred: \", e);\n    return \"\";\n  }\n}",
        "accept_response": "private String getStringAttribute(Object object, String name) {\n  if (object == null) {\n    return \"\";\n  }\n\n  try {\n    if (!(object.getClass().equals(String.class))) {\n      logger.info(\n          \"************************Unexpected type for attribute: {}; Expected type: {}; Received type: {}************************\",\n          name, String.class.getName(), object.getClass().getName());\n      return \"\";\n    } else {\n      return (String) object;\n    }\n  } catch (Exception e) {\n    logger.info(\"Exception occurred: \", e);\n    return \"\";\n  }\n}",
        "reject_response": "private String getStringAttribute(Object object, String name) {\n  if (object == null) {\n    return \"\";\n  }\n\n  try {\n    if (!(object.getClass().equals(String.class))) {\n      if (LOGGER.infoEnabled()) {\n        LOGGER.info(\"************************Unexpected type for attribute: \" + name\n            + \" Expected type: \" + String.class.getName() + \" Received type: \"\n            + object.getClass().getName() + \"************************\");\n      }\n      return \"\";\n    } else {\n      return (String) object;\n    }\n  } catch (Exception e) {\n    logger.info(\"Exception occurred: \", e);\n    return \"\";\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2516,
        "instruction": "void start()\n{\n    // The run loop for the manager thread\n    Runnable runnable = new WrappedRunnable()\n    {\n        public void runMayThrow() throws Exception\n        {\n            while (!shutdown)\n            {\n                try\n                {\n                    assert availableSegment == null;\n                    availableSegment = createSegment();\n                    if (shutdown)\n                    {\n                        // If shutdown() started and finished during segment creation, we are now left with a\n                        // segment that no one will consume. Discard it.\n                        discardAvailableSegment();\n                        return;\n                    }\n\n                    segmentPrepared.signalAll();\n                    Thread.yield();\n\n                    if (availableSegment == null && !atSegmentBufferLimit())\n                        // Writing threads need another segment now.\n                        continue;\n\n                    // Writing threads are not waiting for new segments, we can spend time on other tasks.\n                    // flush old Cfs if we're full\n                    maybeFlushToReclaim();\n                }\n                catch (Throwable t)\n                {\n                    JVMStabilityInspector.inspectThrowable(t);\n                    if (!CommitLog.handleCommitError(\"Failed managing commit log segments\", t))\n                        return;\n                    // sleep some arbitrary period to avoid spamming CL\n                    Uninterruptibles.sleepUninterruptibly(1, TimeUnit.SECONDS);\n\n                    // If we offered a segment, wait for it to be taken before reentering the loop.\n                    // There could be a new segment in next not offered, but only on failure to discard it while\n                    // shutting down-- nothing more can or needs to be done in that case.\n                }\n\n                WaitQueue.waitOnCondition(managerThreadWaitCondition, managerThreadWaitQueue);\n            }\n        }\n    };\n\n    shutdown = false;\n    managerThread = NamedThreadFactory.createThread(runnable, \"COMMIT-LOG-ALLOCATOR\");\n    managerThread.start();\n\n    // for simplicity, ensure the first segment is allocated before continuing\n    advanceAllocatingFrom(null);\n}",
        "accept_response": "void start()\n{\n    // The run loop for the manager thread\n    Runnable runnable = new WrappedRunnable()\n    {\n        public void runMayThrow() throws Exception\n        {\n            while (!shutdown)\n            {\n                try\n                {\n                    assert availableSegment == null;\n                    logger.trace(\"No segments in reserve; creating a fresh one\");\n                    availableSegment = createSegment();\n                    if (shutdown)\n                    {\n                        // If shutdown() started and finished during segment creation, we are now left with a\n                        // segment that no one will consume. Discard it.\n                        discardAvailableSegment();\n                        return;\n                    }\n\n                    segmentPrepared.signalAll();\n                    Thread.yield();\n\n                    if (availableSegment == null && !atSegmentBufferLimit())\n                        // Writing threads need another segment now.\n                        continue;\n\n                    // Writing threads are not waiting for new segments, we can spend time on other tasks.\n                    // flush old Cfs if we're full\n                    maybeFlushToReclaim();\n                }\n                catch (Throwable t)\n                {\n                    JVMStabilityInspector.inspectThrowable(t);\n                    if (!CommitLog.handleCommitError(\"Failed managing commit log segments\", t))\n                        return;\n                    // sleep some arbitrary period to avoid spamming CL\n                    Uninterruptibles.sleepUninterruptibly(1, TimeUnit.SECONDS);\n\n                    // If we offered a segment, wait for it to be taken before reentering the loop.\n                    // There could be a new segment in next not offered, but only on failure to discard it while\n                    // shutting down-- nothing more can or needs to be done in that case.\n                }\n\n                WaitQueue.waitOnCondition(managerThreadWaitCondition, managerThreadWaitQueue);\n            }\n        }\n    };\n\n    shutdown = false;\n    managerThread = NamedThreadFactory.createThread(runnable, \"COMMIT-LOG-ALLOCATOR\");\n    managerThread.start();\n\n    // for simplicity, ensure the first segment is allocated before continuing\n    advanceAllocatingFrom(null);\n}",
        "reject_response": "void start()\n{\n    // The run loop for the manager thread\n    Runnable runnable = new WrappedRunnable()\n    {\n        public void runMayThrow() throws Exception\n        {\n            while (!shutdown)\n            {\n                try\n                {\n                    assert availableSegment == null;\n                    logger.debug(\"No segments in reserve; creating a fresh one\");\n                    availableSegment = createSegment();\n                    if (shutdown)\n                    {\n                        // If shutdown() started and finished during segment creation, we are now left with a\n                        // segment that no one will consume. Discard it.\n                        discardAvailableSegment();\n                        return;\n                    }\n\n                    segmentPrepared.signalAll();\n                    Thread.yield();\n\n                    if (availableSegment == null && !atSegmentBufferLimit())\n                        // Writing threads need another segment now.\n                        continue;\n\n                    // Writing threads are not waiting for new segments, we can spend time on other tasks.\n                    // flush old Cfs if we're full\n                    maybeFlushToReclaim();\n                }\n                catch (Throwable t)\n                {\n                    JVMStabilityInspector.inspectThrowable(t);\n                    if (!CommitLog.handleCommitError(\"Failed managing commit log segments\", t))\n                        return;\n                    // sleep some arbitrary period to avoid spamming CL\n                    Uninterruptibles.sleepUninterruptibly(1, TimeUnit.SECONDS);\n\n                    // If we offered a segment, wait for it to be taken before reentering the loop.\n                    // There could be a new segment in next not offered, but only on failure to discard it while\n                    // shutting down-- nothing more can or needs to be done in that case.\n                }\n\n                WaitQueue.waitOnCondition(managerThreadWaitCondition, managerThreadWaitQueue);\n            }\n        }\n    };\n\n    shutdown = false;\n    managerThread = NamedThreadFactory.createThread(runnable, \"COMMIT-LOG-ALLOCATOR\");\n    managerThread.start();\n\n    // for simplicity, ensure the first segment is allocated before continuing\n    advanceAllocatingFrom(null);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 3213,
        "instruction": "private void closeSock() {\n    if (sock.isOpen() == false) {\n        return;\n    }\n\n    closeSock(sock);\n}",
        "accept_response": "private void closeSock() {\n    if (sock.isOpen() == false) {\n        return;\n    }\n\n    LOG.debug(\"Closed socket connection for client \"\n            + sock.socket().getRemoteSocketAddress()\n            + (sessionId != 0 ?\n                    \" which had sessionid 0x\" + Long.toHexString(sessionId) :\n                    \" (no session established for client)\"));\n    closeSock(sock);\n}",
        "reject_response": "private void closeSock() {\n    if (sock.isOpen() == false) {\n        return;\n    }\n\n    LOG.info(\"Closed socket connection for client \"\n    closeSock(sock);\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2981,
        "instruction": "protected boolean validateUserSession(UserSessionBase userSession,\n\t\tString currentLoginId) {\n\tif (currentLoginId\n\t\t\t.equalsIgnoreCase(userSession.getXXPortalUser().getLoginId())) {\n\t\treturn true;\n\t} else {\n\t\treturn false;\n\t}\n}",
        "accept_response": "protected boolean validateUserSession(UserSessionBase userSession,\n\t\tString currentLoginId) {\n\tif (currentLoginId\n\t\t\t.equalsIgnoreCase(userSession.getXXPortalUser().getLoginId())) {\n\t\treturn true;\n\t} else {\n\t\tlogger.warn(\n\t\t\t\t\"loginId doesn't match loginId from HTTPSession. Will create new session. loginId=\"\n\t\t\t\t\t\t+ currentLoginId + \", userSession=\" + userSession,\n\t\t\t\tnew Exception());\n\t\treturn false;\n\t}\n}",
        "reject_response": "protected boolean validateUserSession(UserSessionBase userSession,\n\t\tString currentLoginId) {\n\tif (currentLoginId\n\t\t\t.equalsIgnoreCase(userSession.getXXPortalUser().getLoginId())) {\n\t\treturn true;\n\t} else {\n\t\tlogger.info(\n\t\treturn false;\n\t}\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2877,
        "instruction": "@Deprecated\npublic static String setBaseWriterClassName(String lang, String className) {\n    if ( rewiredAlternative != null )\n    String oldClassName = currentEntry(lang);\n    try {\n        @SuppressWarnings(\"unchecked\")\n        Class<? extends RDFWriter> newClass = (Class<? extends RDFWriter>)Class.forName(className, false,\n                                                                                        Thread.currentThread().getContextClassLoader());\n        custom.put(lang, newClass);\n        return oldClassName;\n    }\n    catch (ClassNotFoundException e) {\n        throw new ConfigException(\"Reader not found on classpath\", e);\n    }\n    catch (Exception e) {\n        throw new JenaException(e);\n    }\n}",
        "accept_response": "@Deprecated\npublic static String setBaseWriterClassName(String lang, String className) {\n    if ( rewiredAlternative != null )\n        Log.error(RDFWriterFImpl.class, \"Rewired RDFWriterFImpl2 - configuration changes have no effect on writing\");\n    String oldClassName = currentEntry(lang);\n    try {\n        @SuppressWarnings(\"unchecked\")\n        Class<? extends RDFWriter> newClass = (Class<? extends RDFWriter>)Class.forName(className, false,\n                                                                                        Thread.currentThread().getContextClassLoader());\n        custom.put(lang, newClass);\n        return oldClassName;\n    }\n    catch (ClassNotFoundException e) {\n        throw new ConfigException(\"Reader not found on classpath\", e);\n    }\n    catch (Exception e) {\n        throw new JenaException(e);\n    }\n}",
        "reject_response": "@Deprecated\npublic static String setBaseWriterClassName(String lang, String className) {\n    if ( rewiredAlternative != null )\n        Log.fatal(RDFWriterFImpl.class, \"Rewired RDFWriterFImpl2 - configuration changes have no effect on writing\");\n    String oldClassName = currentEntry(lang);\n    try {\n        @SuppressWarnings(\"unchecked\")\n        Class<? extends RDFWriter> newClass = (Class<? extends RDFWriter>)Class.forName(className, false,\n                                                                                        Thread.currentThread().getContextClassLoader());\n        custom.put(lang, newClass);\n        return oldClassName;\n    }\n    catch (ClassNotFoundException e) {\n        throw new ConfigException(\"Reader not found on classpath\", e);\n    }\n    catch (Exception e) {\n        throw new JenaException(e);\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2749,
        "instruction": "@Override // ClientProtocol\npublic LocatedBlock getAdditionalDatanode(final String src,\n    final long fileId, final ExtendedBlock blk,\n    final DatanodeInfo[] existings, final String[] existingStorageIDs,\n    final DatanodeInfo[] excludes,\n    final int numAdditionalNodes, final String clientName\n    ) throws IOException {\n  checkNNStartup();\n\n  metrics.incrGetAdditionalDatanodeOps();\n\n  Set<Node> excludeSet = null;\n  if (excludes != null) {\n    excludeSet = new HashSet<Node>(excludes.length);\n    for (Node node : excludes) {\n      excludeSet.add(node);\n    }\n  }\n  LocatedBlock locatedBlock = namesystem.getAdditionalDatanode(src, fileId,\n      blk, existings, existingStorageIDs, excludeSet, numAdditionalNodes,\n      clientName);\n  return locatedBlock;\n}",
        "accept_response": "@Override // ClientProtocol\npublic LocatedBlock getAdditionalDatanode(final String src,\n    final long fileId, final ExtendedBlock blk,\n    final DatanodeInfo[] existings, final String[] existingStorageIDs,\n    final DatanodeInfo[] excludes,\n    final int numAdditionalNodes, final String clientName\n    ) throws IOException {\n  checkNNStartup();\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"getAdditionalDatanode: src={}, fileId={}, blk={}, existings={}, excludes={}\"\n        + \", numAdditionalNodes={}, clientName={}\", src, fileId, blk, Arrays.asList(existings),\n        Arrays.asList(excludes), numAdditionalNodes, clientName);\n  }\n\n  metrics.incrGetAdditionalDatanodeOps();\n\n  Set<Node> excludeSet = null;\n  if (excludes != null) {\n    excludeSet = new HashSet<Node>(excludes.length);\n    for (Node node : excludes) {\n      excludeSet.add(node);\n    }\n  }\n  LocatedBlock locatedBlock = namesystem.getAdditionalDatanode(src, fileId,\n      blk, existings, existingStorageIDs, excludeSet, numAdditionalNodes,\n      clientName);\n  return locatedBlock;\n}",
        "reject_response": "@Override // ClientProtocol\npublic LocatedBlock getAdditionalDatanode(final String src,\n    final long fileId, final ExtendedBlock blk,\n    final DatanodeInfo[] existings, final String[] existingStorageIDs,\n    final DatanodeInfo[] excludes,\n    final int numAdditionalNodes, final String clientName\n    ) throws IOException {\n  checkNNStartup();\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"getAdditionalDatanode: src=\" + src\n        + \", fileId=\" + fileId\n        + \", blk=\" + blk\n        + \", existings=\" + Arrays.asList(existings)\n        + \", excludes=\" + Arrays.asList(excludes)\n        + \", numAdditionalNodes=\" + numAdditionalNodes\n        + \", clientName=\" + clientName);\n  }\n\n  metrics.incrGetAdditionalDatanodeOps();\n\n  Set<Node> excludeSet = null;\n  if (excludes != null) {\n    excludeSet = new HashSet<Node>(excludes.length);\n    for (Node node : excludes) {\n      excludeSet.add(node);\n    }\n  }\n  LocatedBlock locatedBlock = namesystem.getAdditionalDatanode(src, fileId,\n      blk, existings, existingStorageIDs, excludeSet, numAdditionalNodes,\n      clientName);\n  return locatedBlock;\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2579,
        "instruction": "@Override\npublic void runTheCheckForProduct(final Long entityId, final String entityName, final Long statusCode, String foreignKeyColumn,\n        long productId) {\n    List<EntityDatatableChecks> tableRequiredBeforAction = entityDatatableChecksRepository.findByEntityStatusAndProduct(entityName,\n            statusCode, productId);\n\n    if (tableRequiredBeforAction == null || tableRequiredBeforAction.size() < 1) {\n        tableRequiredBeforAction = entityDatatableChecksRepository.findByEntityStatusAndNoProduct(entityName, statusCode);\n    }\n    if (tableRequiredBeforAction != null) {\n        List<String> reqDatatables = new ArrayList<>();\n        for (EntityDatatableChecks t : tableRequiredBeforAction) {\n\n            final String datatableName = t.getDatatableName();\n            final Long countEntries = readWriteNonCoreDataService.countDatatableEntries(datatableName, entityId, foreignKeyColumn);\n\n            if (countEntries.intValue() == 0) {\n                reqDatatables.add(datatableName);\n            }\n        }\n        if (reqDatatables.size() > 0) { throw new DatatableEntryRequiredException(reqDatatables.toString()); }\n    }\n\n}",
        "accept_response": "@Override\npublic void runTheCheckForProduct(final Long entityId, final String entityName, final Long statusCode, String foreignKeyColumn,\n        long productId) {\n    List<EntityDatatableChecks> tableRequiredBeforAction = entityDatatableChecksRepository.findByEntityStatusAndProduct(entityName,\n            statusCode, productId);\n\n    if (tableRequiredBeforAction == null || tableRequiredBeforAction.size() < 1) {\n        tableRequiredBeforAction = entityDatatableChecksRepository.findByEntityStatusAndNoProduct(entityName, statusCode);\n    }\n    if (tableRequiredBeforAction != null) {\n        List<String> reqDatatables = new ArrayList<>();\n        for (EntityDatatableChecks t : tableRequiredBeforAction) {\n\n            final String datatableName = t.getDatatableName();\n            final Long countEntries = readWriteNonCoreDataService.countDatatableEntries(datatableName, entityId, foreignKeyColumn);\n\n            logger.info(\"The are {} entries in the table {}\", countEntries, datatableName);\n            if (countEntries.intValue() == 0) {\n                reqDatatables.add(datatableName);\n            }\n        }\n        if (reqDatatables.size() > 0) { throw new DatatableEntryRequiredException(reqDatatables.toString()); }\n    }\n\n}",
        "reject_response": "@Override\npublic void runTheCheckForProduct(final Long entityId, final String entityName, final Long statusCode, String foreignKeyColumn,\n        long productId) {\n    List<EntityDatatableChecks> tableRequiredBeforAction = entityDatatableChecksRepository.findByEntityStatusAndProduct(entityName,\n            statusCode, productId);\n\n    if (tableRequiredBeforAction == null || tableRequiredBeforAction.size() < 1) {\n        tableRequiredBeforAction = entityDatatableChecksRepository.findByEntityStatusAndNoProduct(entityName, statusCode);\n    }\n    if (tableRequiredBeforAction != null) {\n        List<String> reqDatatables = new ArrayList<>();\n        for (EntityDatatableChecks t : tableRequiredBeforAction) {\n\n            final String datatableName = t.getDatatableName();\n            final Long countEntries = readWriteNonCoreDataService.countDatatableEntries(datatableName, entityId, foreignKeyColumn);\n\n            logger.info(\"The are \" + countEntries + \" entries in the table \" + datatableName);\n            if (countEntries.intValue() == 0) {\n                reqDatatables.add(datatableName);\n            }\n        }\n        if (reqDatatables.size() > 0) { throw new DatatableEntryRequiredException(reqDatatables.toString()); }\n    }\n\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2511,
        "instruction": "@VisibleForTesting\npublic static boolean handleCommitError(String message, Throwable t)\n{\n    JVMStabilityInspector.inspectCommitLogThrowable(t);\n    switch (DatabaseDescriptor.getCommitFailurePolicy())\n    {\n        // Needed here for unit tests to not fail on default assertion\n        case die:\n        case stop:\n            StorageService.instance.stopTransports();\n            //$FALL-THROUGH$\n        case stop_commit:\n            String errorMsg = String.format(\"%s. Commit disk failure policy is %s; terminating thread.\", message, DatabaseDescriptor.getCommitFailurePolicy());\n            return false;\n        case ignore:\n            logger.error(addAdditionalInformationIfPossible(message), t);\n            return true;\n        default:\n            throw new AssertionError(DatabaseDescriptor.getCommitFailurePolicy());\n    }\n}",
        "accept_response": "@VisibleForTesting\npublic static boolean handleCommitError(String message, Throwable t)\n{\n    JVMStabilityInspector.inspectCommitLogThrowable(t);\n    switch (DatabaseDescriptor.getCommitFailurePolicy())\n    {\n        // Needed here for unit tests to not fail on default assertion\n        case die:\n        case stop:\n            StorageService.instance.stopTransports();\n            //$FALL-THROUGH$\n        case stop_commit:\n            String errorMsg = String.format(\"%s. Commit disk failure policy is %s; terminating thread.\", message, DatabaseDescriptor.getCommitFailurePolicy());\n            logger.error(addAdditionalInformationIfPossible(errorMsg), t);\n            return false;\n        case ignore:\n            logger.error(addAdditionalInformationIfPossible(message), t);\n            return true;\n        default:\n            throw new AssertionError(DatabaseDescriptor.getCommitFailurePolicy());\n    }\n}",
        "reject_response": "@VisibleForTesting\npublic static boolean handleCommitError(String message, Throwable t)\n{\n    JVMStabilityInspector.inspectCommitLogThrowable(t);\n    switch (DatabaseDescriptor.getCommitFailurePolicy())\n    {\n        // Needed here for unit tests to not fail on default assertion\n        case die:\n        case stop:\n            StorageService.instance.stopTransports();\n            //$FALL-THROUGH$\n        case stop_commit:\n            String errorMsg = String.format(\"%s. Commit disk failure policy is %s; terminating thread.\", message, DatabaseDescriptor.getCommitFailurePolicy());\n            logger.error(String.format(\"%s. Commit disk failure policy is %s; terminating thread\", message, DatabaseDescriptor.getCommitFailurePolicy()), t);\n            return false;\n        case ignore:\n            logger.error(addAdditionalInformationIfPossible(message), t);\n            return true;\n        default:\n            throw new AssertionError(DatabaseDescriptor.getCommitFailurePolicy());\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2746,
        "instruction": "private void wipeDatanode(final DatanodeID node) {\n  final String key = node.getDatanodeUuid();\n  synchronized (this) {\n    host2DatanodeMap.remove(datanodeMap.remove(key));\n  }\n}",
        "accept_response": "private void wipeDatanode(final DatanodeID node) {\n  final String key = node.getDatanodeUuid();\n  synchronized (this) {\n    host2DatanodeMap.remove(datanodeMap.remove(key));\n  }\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"{}.wipeDatanode({}): storage {} is removed from datanodeMap.\",\n        getClass().getSimpleName(), node, key);\n  }\n}",
        "reject_response": "private void wipeDatanode(final DatanodeID node) {\n  final String key = node.getDatanodeUuid();\n  synchronized (this) {\n    host2DatanodeMap.remove(datanodeMap.remove(key));\n  }\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(getClass().getSimpleName() + \".wipeDatanode(\"\n        + node + \"): storage \" + key \n        + \" is removed from datanodeMap.\");\n  }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    },
    {
        "data_id": 2847,
        "instruction": "private void body0() throws InterruptedException, IgniteCheckedException {\n    long timeout = cctx.gridConfig().getNetworkTimeout();\n\n    long cnt = 0;\n\n    while (!isCancelled()) {\n        onIdle();\n\n        cnt++;\n\n        CachePartitionExchangeWorkerTask task = null;\n\n        try {\n            boolean preloadFinished = true;\n\n            for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                if (grp.isLocal())\n                    continue;\n\n                preloadFinished &= grp.preloader() != null && grp.preloader().syncFuture().isDone();\n\n                if (!preloadFinished)\n                    break;\n            }\n\n            // If not first preloading and no more topology events present.\n            if (!cctx.kernalContext().clientNode() && !hasPendingExchange() && preloadFinished)\n                timeout = cctx.gridConfig().getNetworkTimeout();\n\n            // After workers line up and before preloading starts we initialize all futures.\n            if (log.isTraceEnabled()) {\n                Collection<IgniteInternalFuture> unfinished = new HashSet<>();\n\n                for (GridDhtPartitionsExchangeFuture fut : exchFuts.values()) {\n                    if (!fut.isDone())\n                        unfinished.add(fut);\n                }\n\n                log.trace(\"Before waiting for exchange futures [futs\" + unfinished + \", worker=\" + this + ']');\n            }\n\n            // Take next exchange future.\n            if (isCancelled())\n                Thread.currentThread().interrupt();\n\n            updateHeartbeat();\n\n            task = futQ.poll(timeout, MILLISECONDS);\n\n            updateHeartbeat();\n\n            if (task == null)\n                continue; // Main while loop.\n\n            if (!isExchangeTask(task)) {\n                processCustomTask(task);\n\n                continue;\n            }\n\n            busy = true;\n\n            Map<Integer, GridDhtPreloaderAssignments> assignsMap = null;\n\n            boolean forcePreload = false;\n\n            GridDhtPartitionExchangeId exchId;\n\n            GridDhtPartitionsExchangeFuture exchFut = null;\n\n            AffinityTopologyVersion resVer = null;\n\n            try {\n                if (isCancelled())\n                    break;\n\n                if (task instanceof RebalanceReassignExchangeTask)\n                    exchId = ((RebalanceReassignExchangeTask) task).exchangeId();\n                else if (task instanceof ForceRebalanceExchangeTask) {\n                    forcePreload = true;\n\n                    timeout = 0; // Force refresh.\n\n                    exchId = ((ForceRebalanceExchangeTask)task).exchangeId();\n                }\n                else {\n                    assert task instanceof GridDhtPartitionsExchangeFuture : task;\n\n                    exchFut = (GridDhtPartitionsExchangeFuture)task;\n\n                    exchId = exchFut.exchangeId();\n\n                    lastInitializedFut = exchFut;\n\n                    boolean newCrd = false;\n\n                    if (!crd) {\n                        List<ClusterNode> srvNodes = exchFut.firstEventCache().serverNodes();\n\n                        crd = newCrd = !srvNodes.isEmpty() && srvNodes.get(0).isLocal();\n                    }\n\n                    exchFut.init(newCrd);\n\n                    int dumpCnt = 0;\n\n                    long waitStart = U.currentTimeMillis();\n\n                    // Call rollback logic only for client node, for server nodes\n                    // rollback logic is in GridDhtPartitionsExchangeFuture.\n                    boolean txRolledBack = !cctx.localNode().isClient();\n\n                    IgniteConfiguration cfg = cctx.gridConfig();\n\n                    final long dumpTimeout = 2 * cfg.getNetworkTimeout();\n\n                    long nextDumpTime = 0;\n\n                    while (true) {\n                        // Read txTimeoutOnPME from configuration after every iteration.\n                        long curTimeout = cfg.getTransactionConfiguration().getTxTimeoutOnPartitionMapExchange();\n\n                        try {\n                            long exchTimeout = curTimeout > 0 && !txRolledBack\n                                ? Math.min(curTimeout, dumpTimeout)\n                                : dumpTimeout;\n\n                            blockingSectionEnd();\n\n                            try {\n                                resVer = exchFut.get(exchTimeout, TimeUnit.MILLISECONDS);\n                            } finally {\n                                blockingSectionEnd();\n                            }\n\n                            onIdle();\n\n                            break;\n                        }\n                        catch (IgniteFutureTimeoutCheckedException ignored) {\n                            updateHeartbeat();\n\n                            if (nextDumpTime <= U.currentTimeMillis()) {\n                                U.warn(diagnosticLog, \"Failed to wait for partition map exchange [\" +\n                                    \"topVer=\" + exchFut.initialVersion() +\n                                    \", node=\" + cctx.localNodeId() + \"]. \" +\n                                    (curTimeout <= 0 && !txRolledBack ? \"Consider changing \" +\n                                            \"TransactionConfiguration.txTimeoutOnPartitionMapSynchronization\" +\n                                            \" to non default value to avoid this message. \" : \"\") +\n                                    \"Dumping pending objects that might be the cause: \");\n\n                                try {\n                                    dumpDebugInfo(exchFut);\n                                }\n                                catch (Exception e) {\n                                    U.error(diagnosticLog, \"Failed to dump debug information: \" + e, e);\n                                }\n\n                                nextDumpTime = U.currentTimeMillis() + nextDumpTimeout(dumpCnt++, dumpTimeout);\n                            }\n\n                            if (!txRolledBack && curTimeout > 0 && U.currentTimeMillis() - waitStart >= curTimeout) {\n                                txRolledBack = true; // Try automatic rollback only once.\n\n                                cctx.tm().rollbackOnTopologyChange(exchFut.initialVersion());\n                            }\n                        }\n                        catch (Exception e) {\n                            if (exchFut.reconnectOnError(e))\n                                throw new IgniteNeedReconnectException(cctx.localNode(), e);\n\n                            throw e;\n                        }\n                    }\n\n                    removeMergedFutures(resVer, exchFut);\n\n                    if (log.isTraceEnabled())\n\n                    if (exchFut.exchangeId().nodeId().equals(cctx.localNodeId()))\n                        lastRefresh.compareAndSet(-1, U.currentTimeMillis());\n\n                    // Just pick first worker to do this, so we don't\n                    // invoke topology callback more than once for the\n                    // same event.\n\n                    boolean changed = false;\n\n                    for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                        if (grp.isLocal())\n                            continue;\n\n                        if (grp.preloader().rebalanceRequired(rebTopVer, exchFut))\n                            rebTopVer = AffinityTopologyVersion.NONE;\n\n                        changed |= grp.topology().afterExchange(exchFut);\n                    }\n\n                    if (!cctx.kernalContext().clientNode() && changed && !hasPendingServerExchange())\n                        refreshPartitions();\n                }\n\n                // Schedule rebalance if force rebalance or force reassign occurs.\n                if (exchFut == null)\n                    rebTopVer = AffinityTopologyVersion.NONE;\n\n                if (!cctx.kernalContext().clientNode() && rebTopVer.equals(AffinityTopologyVersion.NONE)) {\n                    assignsMap = new HashMap<>();\n\n                    IgniteCacheSnapshotManager snp = cctx.snapshot();\n\n                    for (final CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                        long delay = grp.config().getRebalanceDelay();\n\n                        boolean disableRebalance = snp.partitionsAreFrozen(grp);\n\n                        GridDhtPreloaderAssignments assigns = null;\n\n                        // Don't delay for dummy reassigns to avoid infinite recursion.\n                        if ((delay == 0 || forcePreload) && !disableRebalance)\n                            assigns = grp.preloader().generateAssignments(exchId, exchFut);\n\n                        assignsMap.put(grp.groupId(), assigns);\n\n                        if (resVer == null && !grp.isLocal())\n                            resVer = grp.topology().readyTopologyVersion();\n                    }\n                }\n\n                if (resVer == null)\n                    resVer = exchId.topologyVersion();\n            }\n            finally {\n                // Must flip busy flag before assignments are given to demand workers.\n                busy = false;\n            }\n\n            if (assignsMap != null && rebTopVer.equals(AffinityTopologyVersion.NONE)) {\n                int size = assignsMap.size();\n\n                NavigableMap<Integer, List<Integer>> orderMap = new TreeMap<>();\n\n                for (Map.Entry<Integer, GridDhtPreloaderAssignments> e : assignsMap.entrySet()) {\n                    int grpId = e.getKey();\n\n                    CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                    int order = grp.config().getRebalanceOrder();\n\n                    if (orderMap.get(order) == null)\n                        orderMap.put(order, new ArrayList<Integer>(size));\n\n                    orderMap.get(order).add(grpId);\n                }\n\n                Runnable r = null;\n\n                List<String> rebList = new LinkedList<>();\n\n                boolean assignsCancelled = false;\n\n                GridCompoundFuture<Boolean, Boolean> forcedRebFut = null;\n\n                if (task instanceof ForceRebalanceExchangeTask)\n                    forcedRebFut = ((ForceRebalanceExchangeTask)task).forcedRebalanceFuture();\n\n                for (Integer order : orderMap.descendingKeySet()) {\n                    for (Integer grpId : orderMap.get(order)) {\n                        CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                        GridDhtPreloaderAssignments assigns = assignsMap.get(grpId);\n\n                        if (assigns != null)\n                            assignsCancelled |= assigns.cancelled();\n\n                        Runnable cur = grp.preloader().addAssignments(assigns,\n                            forcePreload,\n                            cnt,\n                            r,\n                            forcedRebFut);\n\n                        if (cur != null) {\n                            rebList.add(grp.cacheOrGroupName());\n\n                            r = cur;\n                        }\n                    }\n                }\n\n                if (forcedRebFut != null)\n                    forcedRebFut.markInitialized();\n\n                if (assignsCancelled || hasPendingExchange()) {\n                    U.log(log, \"Skipping rebalancing (obsolete exchange ID) \" +\n                        \"[top=\" + resVer + \", evt=\" + exchId.discoveryEventName() +\n                        \", node=\" + exchId.nodeId() + ']');\n                }\n                else if (r != null) {\n                    Collections.reverse(rebList);\n\n                    U.log(log, \"Rebalancing scheduled [order=\" + rebList +\n                        \", top=\" + resVer + \", force=\" + (exchFut == null) +\n                        \", evt=\" + exchId.discoveryEventName() +\n                        \", node=\" + exchId.nodeId() + ']');\n\n                    rebTopVer = resVer;\n\n                    // Start rebalancing cache groups chain. Each group will be rebalanced\n                    // sequentially one by one e.g.:\n                    // ignite-sys-cache -> cacheGroupR1 -> cacheGroupP2 -> cacheGroupR3\n                    r.run();\n                }\n                else\n                    U.log(log, \"Skipping rebalancing (nothing scheduled) \" +\n                        \"[top=\" + resVer + \", force=\" + (exchFut == null) +\n                        \", evt=\" + exchId.discoveryEventName() +\n                        \", node=\" + exchId.nodeId() + ']');\n            }\n            else\n                U.log(log, \"Skipping rebalancing (no affinity changes) \" +\n                    \"[top=\" + resVer +\n                    \", rebTopVer=\" + rebTopVer +\n                    \", evt=\" + exchId.discoveryEventName() +\n                    \", evtNode=\" + exchId.nodeId() +\n                    \", client=\" + cctx.kernalContext().clientNode() + ']');\n        }\n        catch (IgniteInterruptedCheckedException e) {\n            throw e;\n        }\n        catch (IgniteClientDisconnectedCheckedException | IgniteNeedReconnectException e) {\n            if (cctx.discovery().reconnectSupported()) {\n                U.warn(log, \"Local node failed to complete partition map exchange due to \" +\n                    \"exception, will try to reconnect to cluster: \" + e.getMessage(), e);\n\n                cctx.discovery().reconnect();\n\n                reconnectNeeded = true;\n            }\n            else\n                U.warn(log, \"Local node received IgniteClientDisconnectedCheckedException or \" +\n                    \" IgniteNeedReconnectException exception but doesn't support reconnect, stopping node: \" +\n                    e.getMessage(), e);\n\n            return;\n        }\n        catch (IgniteCheckedException e) {\n            U.error(log, \"Failed to wait for completion of partition map exchange \" +\n                \"(preloading will not start): \" + task, e);\n\n            throw e;\n        }\n    }\n}",
        "accept_response": "private void body0() throws InterruptedException, IgniteCheckedException {\n    long timeout = cctx.gridConfig().getNetworkTimeout();\n\n    long cnt = 0;\n\n    while (!isCancelled()) {\n        onIdle();\n\n        cnt++;\n\n        CachePartitionExchangeWorkerTask task = null;\n\n        try {\n            boolean preloadFinished = true;\n\n            for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                if (grp.isLocal())\n                    continue;\n\n                preloadFinished &= grp.preloader() != null && grp.preloader().syncFuture().isDone();\n\n                if (!preloadFinished)\n                    break;\n            }\n\n            // If not first preloading and no more topology events present.\n            if (!cctx.kernalContext().clientNode() && !hasPendingExchange() && preloadFinished)\n                timeout = cctx.gridConfig().getNetworkTimeout();\n\n            // After workers line up and before preloading starts we initialize all futures.\n            if (log.isTraceEnabled()) {\n                Collection<IgniteInternalFuture> unfinished = new HashSet<>();\n\n                for (GridDhtPartitionsExchangeFuture fut : exchFuts.values()) {\n                    if (!fut.isDone())\n                        unfinished.add(fut);\n                }\n\n                log.trace(\"Before waiting for exchange futures [futs\" + unfinished + \", worker=\" + this + ']');\n            }\n\n            // Take next exchange future.\n            if (isCancelled())\n                Thread.currentThread().interrupt();\n\n            updateHeartbeat();\n\n            task = futQ.poll(timeout, MILLISECONDS);\n\n            updateHeartbeat();\n\n            if (task == null)\n                continue; // Main while loop.\n\n            if (!isExchangeTask(task)) {\n                processCustomTask(task);\n\n                continue;\n            }\n\n            busy = true;\n\n            Map<Integer, GridDhtPreloaderAssignments> assignsMap = null;\n\n            boolean forcePreload = false;\n\n            GridDhtPartitionExchangeId exchId;\n\n            GridDhtPartitionsExchangeFuture exchFut = null;\n\n            AffinityTopologyVersion resVer = null;\n\n            try {\n                if (isCancelled())\n                    break;\n\n                if (task instanceof RebalanceReassignExchangeTask)\n                    exchId = ((RebalanceReassignExchangeTask) task).exchangeId();\n                else if (task instanceof ForceRebalanceExchangeTask) {\n                    forcePreload = true;\n\n                    timeout = 0; // Force refresh.\n\n                    exchId = ((ForceRebalanceExchangeTask)task).exchangeId();\n                }\n                else {\n                    assert task instanceof GridDhtPartitionsExchangeFuture : task;\n\n                    exchFut = (GridDhtPartitionsExchangeFuture)task;\n\n                    exchId = exchFut.exchangeId();\n\n                    lastInitializedFut = exchFut;\n\n                    boolean newCrd = false;\n\n                    if (!crd) {\n                        List<ClusterNode> srvNodes = exchFut.firstEventCache().serverNodes();\n\n                        crd = newCrd = !srvNodes.isEmpty() && srvNodes.get(0).isLocal();\n                    }\n\n                    exchFut.init(newCrd);\n\n                    int dumpCnt = 0;\n\n                    long waitStart = U.currentTimeMillis();\n\n                    // Call rollback logic only for client node, for server nodes\n                    // rollback logic is in GridDhtPartitionsExchangeFuture.\n                    boolean txRolledBack = !cctx.localNode().isClient();\n\n                    IgniteConfiguration cfg = cctx.gridConfig();\n\n                    final long dumpTimeout = 2 * cfg.getNetworkTimeout();\n\n                    long nextDumpTime = 0;\n\n                    while (true) {\n                        // Read txTimeoutOnPME from configuration after every iteration.\n                        long curTimeout = cfg.getTransactionConfiguration().getTxTimeoutOnPartitionMapExchange();\n\n                        try {\n                            long exchTimeout = curTimeout > 0 && !txRolledBack\n                                ? Math.min(curTimeout, dumpTimeout)\n                                : dumpTimeout;\n\n                            blockingSectionEnd();\n\n                            try {\n                                resVer = exchFut.get(exchTimeout, TimeUnit.MILLISECONDS);\n                            } finally {\n                                blockingSectionEnd();\n                            }\n\n                            onIdle();\n\n                            break;\n                        }\n                        catch (IgniteFutureTimeoutCheckedException ignored) {\n                            updateHeartbeat();\n\n                            if (nextDumpTime <= U.currentTimeMillis()) {\n                                U.warn(diagnosticLog, \"Failed to wait for partition map exchange [\" +\n                                    \"topVer=\" + exchFut.initialVersion() +\n                                    \", node=\" + cctx.localNodeId() + \"]. \" +\n                                    (curTimeout <= 0 && !txRolledBack ? \"Consider changing \" +\n                                            \"TransactionConfiguration.txTimeoutOnPartitionMapSynchronization\" +\n                                            \" to non default value to avoid this message. \" : \"\") +\n                                    \"Dumping pending objects that might be the cause: \");\n\n                                try {\n                                    dumpDebugInfo(exchFut);\n                                }\n                                catch (Exception e) {\n                                    U.error(diagnosticLog, \"Failed to dump debug information: \" + e, e);\n                                }\n\n                                nextDumpTime = U.currentTimeMillis() + nextDumpTimeout(dumpCnt++, dumpTimeout);\n                            }\n\n                            if (!txRolledBack && curTimeout > 0 && U.currentTimeMillis() - waitStart >= curTimeout) {\n                                txRolledBack = true; // Try automatic rollback only once.\n\n                                cctx.tm().rollbackOnTopologyChange(exchFut.initialVersion());\n                            }\n                        }\n                        catch (Exception e) {\n                            if (exchFut.reconnectOnError(e))\n                                throw new IgniteNeedReconnectException(cctx.localNode(), e);\n\n                            throw e;\n                        }\n                    }\n\n                    removeMergedFutures(resVer, exchFut);\n\n                    if (log.isTraceEnabled())\n                        log.trace(\"After waiting for exchange future [exchFut=\" + exchFut + \", worker=\" +\n                            this + ']');\n\n                    if (exchFut.exchangeId().nodeId().equals(cctx.localNodeId()))\n                        lastRefresh.compareAndSet(-1, U.currentTimeMillis());\n\n                    // Just pick first worker to do this, so we don't\n                    // invoke topology callback more than once for the\n                    // same event.\n\n                    boolean changed = false;\n\n                    for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                        if (grp.isLocal())\n                            continue;\n\n                        if (grp.preloader().rebalanceRequired(rebTopVer, exchFut))\n                            rebTopVer = AffinityTopologyVersion.NONE;\n\n                        changed |= grp.topology().afterExchange(exchFut);\n                    }\n\n                    if (!cctx.kernalContext().clientNode() && changed && !hasPendingServerExchange())\n                        refreshPartitions();\n                }\n\n                // Schedule rebalance if force rebalance or force reassign occurs.\n                if (exchFut == null)\n                    rebTopVer = AffinityTopologyVersion.NONE;\n\n                if (!cctx.kernalContext().clientNode() && rebTopVer.equals(AffinityTopologyVersion.NONE)) {\n                    assignsMap = new HashMap<>();\n\n                    IgniteCacheSnapshotManager snp = cctx.snapshot();\n\n                    for (final CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                        long delay = grp.config().getRebalanceDelay();\n\n                        boolean disableRebalance = snp.partitionsAreFrozen(grp);\n\n                        GridDhtPreloaderAssignments assigns = null;\n\n                        // Don't delay for dummy reassigns to avoid infinite recursion.\n                        if ((delay == 0 || forcePreload) && !disableRebalance)\n                            assigns = grp.preloader().generateAssignments(exchId, exchFut);\n\n                        assignsMap.put(grp.groupId(), assigns);\n\n                        if (resVer == null && !grp.isLocal())\n                            resVer = grp.topology().readyTopologyVersion();\n                    }\n                }\n\n                if (resVer == null)\n                    resVer = exchId.topologyVersion();\n            }\n            finally {\n                // Must flip busy flag before assignments are given to demand workers.\n                busy = false;\n            }\n\n            if (assignsMap != null && rebTopVer.equals(AffinityTopologyVersion.NONE)) {\n                int size = assignsMap.size();\n\n                NavigableMap<Integer, List<Integer>> orderMap = new TreeMap<>();\n\n                for (Map.Entry<Integer, GridDhtPreloaderAssignments> e : assignsMap.entrySet()) {\n                    int grpId = e.getKey();\n\n                    CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                    int order = grp.config().getRebalanceOrder();\n\n                    if (orderMap.get(order) == null)\n                        orderMap.put(order, new ArrayList<Integer>(size));\n\n                    orderMap.get(order).add(grpId);\n                }\n\n                Runnable r = null;\n\n                List<String> rebList = new LinkedList<>();\n\n                boolean assignsCancelled = false;\n\n                GridCompoundFuture<Boolean, Boolean> forcedRebFut = null;\n\n                if (task instanceof ForceRebalanceExchangeTask)\n                    forcedRebFut = ((ForceRebalanceExchangeTask)task).forcedRebalanceFuture();\n\n                for (Integer order : orderMap.descendingKeySet()) {\n                    for (Integer grpId : orderMap.get(order)) {\n                        CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                        GridDhtPreloaderAssignments assigns = assignsMap.get(grpId);\n\n                        if (assigns != null)\n                            assignsCancelled |= assigns.cancelled();\n\n                        Runnable cur = grp.preloader().addAssignments(assigns,\n                            forcePreload,\n                            cnt,\n                            r,\n                            forcedRebFut);\n\n                        if (cur != null) {\n                            rebList.add(grp.cacheOrGroupName());\n\n                            r = cur;\n                        }\n                    }\n                }\n\n                if (forcedRebFut != null)\n                    forcedRebFut.markInitialized();\n\n                if (assignsCancelled || hasPendingExchange()) {\n                    U.log(log, \"Skipping rebalancing (obsolete exchange ID) \" +\n                        \"[top=\" + resVer + \", evt=\" + exchId.discoveryEventName() +\n                        \", node=\" + exchId.nodeId() + ']');\n                }\n                else if (r != null) {\n                    Collections.reverse(rebList);\n\n                    U.log(log, \"Rebalancing scheduled [order=\" + rebList +\n                        \", top=\" + resVer + \", force=\" + (exchFut == null) +\n                        \", evt=\" + exchId.discoveryEventName() +\n                        \", node=\" + exchId.nodeId() + ']');\n\n                    rebTopVer = resVer;\n\n                    // Start rebalancing cache groups chain. Each group will be rebalanced\n                    // sequentially one by one e.g.:\n                    // ignite-sys-cache -> cacheGroupR1 -> cacheGroupP2 -> cacheGroupR3\n                    r.run();\n                }\n                else\n                    U.log(log, \"Skipping rebalancing (nothing scheduled) \" +\n                        \"[top=\" + resVer + \", force=\" + (exchFut == null) +\n                        \", evt=\" + exchId.discoveryEventName() +\n                        \", node=\" + exchId.nodeId() + ']');\n            }\n            else\n                U.log(log, \"Skipping rebalancing (no affinity changes) \" +\n                    \"[top=\" + resVer +\n                    \", rebTopVer=\" + rebTopVer +\n                    \", evt=\" + exchId.discoveryEventName() +\n                    \", evtNode=\" + exchId.nodeId() +\n                    \", client=\" + cctx.kernalContext().clientNode() + ']');\n        }\n        catch (IgniteInterruptedCheckedException e) {\n            throw e;\n        }\n        catch (IgniteClientDisconnectedCheckedException | IgniteNeedReconnectException e) {\n            if (cctx.discovery().reconnectSupported()) {\n                U.warn(log, \"Local node failed to complete partition map exchange due to \" +\n                    \"exception, will try to reconnect to cluster: \" + e.getMessage(), e);\n\n                cctx.discovery().reconnect();\n\n                reconnectNeeded = true;\n            }\n            else\n                U.warn(log, \"Local node received IgniteClientDisconnectedCheckedException or \" +\n                    \" IgniteNeedReconnectException exception but doesn't support reconnect, stopping node: \" +\n                    e.getMessage(), e);\n\n            return;\n        }\n        catch (IgniteCheckedException e) {\n            U.error(log, \"Failed to wait for completion of partition map exchange \" +\n                \"(preloading will not start): \" + task, e);\n\n            throw e;\n        }\n    }\n}",
        "reject_response": "private void body0() throws InterruptedException, IgniteCheckedException {\n    long timeout = cctx.gridConfig().getNetworkTimeout();\n\n    long cnt = 0;\n\n    while (!isCancelled()) {\n        onIdle();\n\n        cnt++;\n\n        CachePartitionExchangeWorkerTask task = null;\n\n        try {\n            boolean preloadFinished = true;\n\n            for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                if (grp.isLocal())\n                    continue;\n\n                preloadFinished &= grp.preloader() != null && grp.preloader().syncFuture().isDone();\n\n                if (!preloadFinished)\n                    break;\n            }\n\n            // If not first preloading and no more topology events present.\n            if (!cctx.kernalContext().clientNode() && !hasPendingExchange() && preloadFinished)\n                timeout = cctx.gridConfig().getNetworkTimeout();\n\n            // After workers line up and before preloading starts we initialize all futures.\n            if (log.isTraceEnabled()) {\n                Collection<IgniteInternalFuture> unfinished = new HashSet<>();\n\n                for (GridDhtPartitionsExchangeFuture fut : exchFuts.values()) {\n                    if (!fut.isDone())\n                        unfinished.add(fut);\n                }\n\n                log.trace(\"Before waiting for exchange futures [futs\" + unfinished + \", worker=\" + this + ']');\n            }\n\n            // Take next exchange future.\n            if (isCancelled())\n                Thread.currentThread().interrupt();\n\n            updateHeartbeat();\n\n            task = futQ.poll(timeout, MILLISECONDS);\n\n            updateHeartbeat();\n\n            if (task == null)\n                continue; // Main while loop.\n\n            if (!isExchangeTask(task)) {\n                processCustomTask(task);\n\n                continue;\n            }\n\n            busy = true;\n\n            Map<Integer, GridDhtPreloaderAssignments> assignsMap = null;\n\n            boolean forcePreload = false;\n\n            GridDhtPartitionExchangeId exchId;\n\n            GridDhtPartitionsExchangeFuture exchFut = null;\n\n            AffinityTopologyVersion resVer = null;\n\n            try {\n                if (isCancelled())\n                    break;\n\n                if (task instanceof RebalanceReassignExchangeTask)\n                    exchId = ((RebalanceReassignExchangeTask) task).exchangeId();\n                else if (task instanceof ForceRebalanceExchangeTask) {\n                    forcePreload = true;\n\n                    timeout = 0; // Force refresh.\n\n                    exchId = ((ForceRebalanceExchangeTask)task).exchangeId();\n                }\n                else {\n                    assert task instanceof GridDhtPartitionsExchangeFuture : task;\n\n                    exchFut = (GridDhtPartitionsExchangeFuture)task;\n\n                    exchId = exchFut.exchangeId();\n\n                    lastInitializedFut = exchFut;\n\n                    boolean newCrd = false;\n\n                    if (!crd) {\n                        List<ClusterNode> srvNodes = exchFut.firstEventCache().serverNodes();\n\n                        crd = newCrd = !srvNodes.isEmpty() && srvNodes.get(0).isLocal();\n                    }\n\n                    exchFut.init(newCrd);\n\n                    int dumpCnt = 0;\n\n                    long waitStart = U.currentTimeMillis();\n\n                    // Call rollback logic only for client node, for server nodes\n                    // rollback logic is in GridDhtPartitionsExchangeFuture.\n                    boolean txRolledBack = !cctx.localNode().isClient();\n\n                    IgniteConfiguration cfg = cctx.gridConfig();\n\n                    final long dumpTimeout = 2 * cfg.getNetworkTimeout();\n\n                    long nextDumpTime = 0;\n\n                    while (true) {\n                        // Read txTimeoutOnPME from configuration after every iteration.\n                        long curTimeout = cfg.getTransactionConfiguration().getTxTimeoutOnPartitionMapExchange();\n\n                        try {\n                            long exchTimeout = curTimeout > 0 && !txRolledBack\n                                ? Math.min(curTimeout, dumpTimeout)\n                                : dumpTimeout;\n\n                            blockingSectionEnd();\n\n                            try {\n                                resVer = exchFut.get(exchTimeout, TimeUnit.MILLISECONDS);\n                            } finally {\n                                blockingSectionEnd();\n                            }\n\n                            onIdle();\n\n                            break;\n                        }\n                        catch (IgniteFutureTimeoutCheckedException ignored) {\n                            updateHeartbeat();\n\n                            if (nextDumpTime <= U.currentTimeMillis()) {\n                                U.warn(diagnosticLog, \"Failed to wait for partition map exchange [\" +\n                                    \"topVer=\" + exchFut.initialVersion() +\n                                    \", node=\" + cctx.localNodeId() + \"]. \" +\n                                    (curTimeout <= 0 && !txRolledBack ? \"Consider changing \" +\n                                            \"TransactionConfiguration.txTimeoutOnPartitionMapSynchronization\" +\n                                            \" to non default value to avoid this message. \" : \"\") +\n                                    \"Dumping pending objects that might be the cause: \");\n\n                                try {\n                                    dumpDebugInfo(exchFut);\n                                }\n                                catch (Exception e) {\n                                    U.error(diagnosticLog, \"Failed to dump debug information: \" + e, e);\n                                }\n\n                                nextDumpTime = U.currentTimeMillis() + nextDumpTimeout(dumpCnt++, dumpTimeout);\n                            }\n\n                            if (!txRolledBack && curTimeout > 0 && U.currentTimeMillis() - waitStart >= curTimeout) {\n                                txRolledBack = true; // Try automatic rollback only once.\n\n                                cctx.tm().rollbackOnTopologyChange(exchFut.initialVersion());\n                            }\n                        }\n                        catch (Exception e) {\n                            if (exchFut.reconnectOnError(e))\n                                throw new IgniteNeedReconnectException(cctx.localNode(), e);\n\n                            throw e;\n                        }\n                    }\n\n                    removeMergedFutures(resVer, exchFut);\n\n                    if (log.isTraceEnabled())\n                    if (log.isDebugEnabled())\n                        log.debug(\"After waiting for exchange future [exchFut=\" + exchFut + \", worker=\" +\n\n                    if (exchFut.exchangeId().nodeId().equals(cctx.localNodeId()))\n                        lastRefresh.compareAndSet(-1, U.currentTimeMillis());\n\n                    // Just pick first worker to do this, so we don't\n                    // invoke topology callback more than once for the\n                    // same event.\n\n                    boolean changed = false;\n\n                    for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                        if (grp.isLocal())\n                            continue;\n\n                        if (grp.preloader().rebalanceRequired(rebTopVer, exchFut))\n                            rebTopVer = AffinityTopologyVersion.NONE;\n\n                        changed |= grp.topology().afterExchange(exchFut);\n                    }\n\n                    if (!cctx.kernalContext().clientNode() && changed && !hasPendingServerExchange())\n                        refreshPartitions();\n                }\n\n                // Schedule rebalance if force rebalance or force reassign occurs.\n                if (exchFut == null)\n                    rebTopVer = AffinityTopologyVersion.NONE;\n\n                if (!cctx.kernalContext().clientNode() && rebTopVer.equals(AffinityTopologyVersion.NONE)) {\n                    assignsMap = new HashMap<>();\n\n                    IgniteCacheSnapshotManager snp = cctx.snapshot();\n\n                    for (final CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                        long delay = grp.config().getRebalanceDelay();\n\n                        boolean disableRebalance = snp.partitionsAreFrozen(grp);\n\n                        GridDhtPreloaderAssignments assigns = null;\n\n                        // Don't delay for dummy reassigns to avoid infinite recursion.\n                        if ((delay == 0 || forcePreload) && !disableRebalance)\n                            assigns = grp.preloader().generateAssignments(exchId, exchFut);\n\n                        assignsMap.put(grp.groupId(), assigns);\n\n                        if (resVer == null && !grp.isLocal())\n                            resVer = grp.topology().readyTopologyVersion();\n                    }\n                }\n\n                if (resVer == null)\n                    resVer = exchId.topologyVersion();\n            }\n            finally {\n                // Must flip busy flag before assignments are given to demand workers.\n                busy = false;\n            }\n\n            if (assignsMap != null && rebTopVer.equals(AffinityTopologyVersion.NONE)) {\n                int size = assignsMap.size();\n\n                NavigableMap<Integer, List<Integer>> orderMap = new TreeMap<>();\n\n                for (Map.Entry<Integer, GridDhtPreloaderAssignments> e : assignsMap.entrySet()) {\n                    int grpId = e.getKey();\n\n                    CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                    int order = grp.config().getRebalanceOrder();\n\n                    if (orderMap.get(order) == null)\n                        orderMap.put(order, new ArrayList<Integer>(size));\n\n                    orderMap.get(order).add(grpId);\n                }\n\n                Runnable r = null;\n\n                List<String> rebList = new LinkedList<>();\n\n                boolean assignsCancelled = false;\n\n                GridCompoundFuture<Boolean, Boolean> forcedRebFut = null;\n\n                if (task instanceof ForceRebalanceExchangeTask)\n                    forcedRebFut = ((ForceRebalanceExchangeTask)task).forcedRebalanceFuture();\n\n                for (Integer order : orderMap.descendingKeySet()) {\n                    for (Integer grpId : orderMap.get(order)) {\n                        CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                        GridDhtPreloaderAssignments assigns = assignsMap.get(grpId);\n\n                        if (assigns != null)\n                            assignsCancelled |= assigns.cancelled();\n\n                        Runnable cur = grp.preloader().addAssignments(assigns,\n                            forcePreload,\n                            cnt,\n                            r,\n                            forcedRebFut);\n\n                        if (cur != null) {\n                            rebList.add(grp.cacheOrGroupName());\n\n                            r = cur;\n                        }\n                    }\n                }\n\n                if (forcedRebFut != null)\n                    forcedRebFut.markInitialized();\n\n                if (assignsCancelled || hasPendingExchange()) {\n                    U.log(log, \"Skipping rebalancing (obsolete exchange ID) \" +\n                        \"[top=\" + resVer + \", evt=\" + exchId.discoveryEventName() +\n                        \", node=\" + exchId.nodeId() + ']');\n                }\n                else if (r != null) {\n                    Collections.reverse(rebList);\n\n                    U.log(log, \"Rebalancing scheduled [order=\" + rebList +\n                        \", top=\" + resVer + \", force=\" + (exchFut == null) +\n                        \", evt=\" + exchId.discoveryEventName() +\n                        \", node=\" + exchId.nodeId() + ']');\n\n                    rebTopVer = resVer;\n\n                    // Start rebalancing cache groups chain. Each group will be rebalanced\n                    // sequentially one by one e.g.:\n                    // ignite-sys-cache -> cacheGroupR1 -> cacheGroupP2 -> cacheGroupR3\n                    r.run();\n                }\n                else\n                    U.log(log, \"Skipping rebalancing (nothing scheduled) \" +\n                        \"[top=\" + resVer + \", force=\" + (exchFut == null) +\n                        \", evt=\" + exchId.discoveryEventName() +\n                        \", node=\" + exchId.nodeId() + ']');\n            }\n            else\n                U.log(log, \"Skipping rebalancing (no affinity changes) \" +\n                    \"[top=\" + resVer +\n                    \", rebTopVer=\" + rebTopVer +\n                    \", evt=\" + exchId.discoveryEventName() +\n                    \", evtNode=\" + exchId.nodeId() +\n                    \", client=\" + cctx.kernalContext().clientNode() + ']');\n        }\n        catch (IgniteInterruptedCheckedException e) {\n            throw e;\n        }\n        catch (IgniteClientDisconnectedCheckedException | IgniteNeedReconnectException e) {\n            if (cctx.discovery().reconnectSupported()) {\n                U.warn(log, \"Local node failed to complete partition map exchange due to \" +\n                    \"exception, will try to reconnect to cluster: \" + e.getMessage(), e);\n\n                cctx.discovery().reconnect();\n\n                reconnectNeeded = true;\n            }\n            else\n                U.warn(log, \"Local node received IgniteClientDisconnectedCheckedException or \" +\n                    \" IgniteNeedReconnectException exception but doesn't support reconnect, stopping node: \" +\n                    e.getMessage(), e);\n\n            return;\n        }\n        catch (IgniteCheckedException e) {\n            U.error(log, \"Failed to wait for completion of partition map exchange \" +\n                \"(preloading will not start): \" + task, e);\n\n            throw e;\n        }\n    }\n}",
        "language": "java",
        "remark": "modified",
        "input": ""
    }
]